## T√©cnicas de Amostragem Avan√ßadas para Gera√ß√£o de Texto

### Introdu√ß√£o
Este cap√≠tulo aprofunda as t√©cnicas de **amostragem** como m√©todos de decodifica√ß√£o em **Large Language Models (LLMs)**, expandindo os conceitos apresentados nos cap√≠tulos anteriores [^1]. Vimos que a decodifica√ß√£o autoregressiva envolve a sele√ß√£o iterativa de palavras, com base nas probabilidades do modelo, e que a amostragem √© uma alternativa para a decodifica√ß√£o *greedy* para adicionar variedade ao texto gerado [^2]. Aqui, vamos explorar t√©cnicas mais sofisticadas de amostragem, cada uma com um mecanismo √∫nico para refinar o equil√≠brio entre a qualidade e diversidade do texto gerado. O foco ser√° em como essas t√©cnicas manipulam as distribui√ß√µes de probabilidade para alcan√ßar resultados de texto mais interessantes e contextualmente relevantes.

### Amostragem: T√©cnicas Avan√ßadas
Como discutido previamente, o processo de amostragem √© fundamental para gerar textos variados e interessantes [^2]. A escolha da pr√≥xima palavra, baseada na distribui√ß√£o de probabilidade fornecida pelo modelo, e a maneira como essa distribui√ß√£o √© manipulada, t√™m um impacto significativo na sa√≠da gerada. As t√©cnicas de amostragem mais avan√ßadas, como **top-k**, **top-p** (ou n√∫cleo), e a **amostragem por temperatura**, oferecem diferentes mecanismos para refinar o processo de escolha, evitando a previsibilidade e a gera√ß√£o de palavras improv√°veis. Exploraremos cada uma dessas t√©cnicas em detalhes.

#### Amostragem Aleat√≥ria: Um M√©todo Base
A **amostragem aleat√≥ria** √© o m√©todo mais direto, onde cada palavra √© selecionada com base na sua probabilidade na distribui√ß√£o do modelo [^2]. Embora simples, a amostragem aleat√≥ria n√£o impede que palavras de baixa probabilidade na cauda da distribui√ß√£o sejam selecionadas, o que pode gerar textos sem sentido. Como j√° discutimos, esse m√©todo pode resultar em uma sa√≠da de baixa qualidade, pois permite a ocorr√™ncia de palavras inesperadas [^1].
$$w_t \sim P(w | w_{<t})$$
onde $w_t$ √© a pr√≥xima palavra gerada e $P$ √© a distribui√ß√£o de probabilidade dada pelo modelo.

#### Amostragem Top-k: Truncando a Distribui√ß√£o
A **amostragem top-k** aborda as limita√ß√µes da amostragem aleat√≥ria, truncando a distribui√ß√£o de probabilidade para as $k$ palavras mais prov√°veis e, em seguida, amostrando uma palavra dentre essas $k$ op√ß√µes [^2]. O processo envolve os seguintes passos:
1. Selecionar as $k$ palavras com as maiores probabilidades de acordo com a distribui√ß√£o $P(w|w_{<t})$.
2. Renormalizar as probabilidades dessas $k$ palavras, formando uma nova distribui√ß√£o $P'(w|w_{<t})$.
3. Amostrar uma palavra dessa nova distribui√ß√£o.

A amostragem top-k oferece um bom equil√≠brio entre qualidade e diversidade, ao restringir a escolha para as palavras mais prov√°veis, ao mesmo tempo em que introduz alguma aleatoriedade ao selecionar uma palavra dentre as k op√ß√µes. O par√¢metro k precisa ser ajustado dependendo da aplica√ß√£o e dos resultados desejados [^2].

> üí° **Exemplo Num√©rico:** Considere um vocabul√°rio com as seguintes probabilidades: P(A)=0.5, P(B)=0.3, P(C)=0.1, P(D)=0.05 e P(E)=0.05. Para k=3, selecionamos A, B e C. As probabilidades s√£o renormalizadas para P'(A)=0.5/(0.5+0.3+0.1) = 0.555, P'(B)=0.333 e P'(C)=0.111, e a amostragem √© feita sobre essa nova distribui√ß√£o. Se uma amostragem aleat√≥ria dentro do conjunto {A, B, C} resulta em B, a palavra B √© selecionada. Se k fosse 2, escolher√≠amos apenas A e B, renormalizando as probabilidades para P'(A)=0.625 e P'(B)=0.375. Uma amostragem sobre essa distribui√ß√£o pode resultar em A ou B. Observe que a probabilidade de selecionar C foi eliminada quando k=2, e a escolha se concentra nas op√ß√µes mais prov√°veis.

**Lema 3:** A amostragem top-k, ao truncar a distribui√ß√£o, garante que palavras de probabilidade muito baixa n√£o ser√£o geradas, melhorando a qualidade do texto gerado em compara√ß√£o com a amostragem aleat√≥ria, mantendo uma certa diversidade.
*Prova:*
I. Seja $P(w)$ a distribui√ß√£o de probabilidade sobre o vocabul√°rio $V$.
II. A amostragem top-k seleciona as $k$ palavras mais prov√°veis de acordo com $P(w)$, formando um conjunto $S_k = \{w_1, w_2, \ldots, w_k\}$ tal que $P(w_1) \geq P(w_2) \geq \ldots \geq P(w_k)$.
III. As probabilidades das palavras em $S_k$ s√£o renormalizadas, resultando em uma nova distribui√ß√£o $P'(w)$ para $w \in S_k$.
IV. Palavras fora de $S_k$, ou seja, palavras que seriam de baixa probabilidade na distribui√ß√£o original, s√£o descartadas.
V. A amostragem √© feita com base em $P'(w)$, o que impede a sele√ß√£o de palavras de baixa probabilidade.
VI. A escolha aleat√≥ria dentro de $S_k$ mant√©m a diversidade.
Portanto, a amostragem top-k melhora a qualidade do texto gerado, restringindo a sele√ß√£o para um subconjunto de palavras mais prov√°veis, mantendo a diversidade. $\blacksquare$

**Lema 3.1:** A amostragem top-k com $k = 1$ √© equivalente √† decodifica√ß√£o *greedy*, pois sempre seleciona a palavra de maior probabilidade.
*Prova:*
I. Na amostragem top-k, as $k$ palavras mais prov√°veis s√£o selecionadas.
II. Quando $k=1$, apenas a palavra mais prov√°vel √© selecionada, ou seja, a palavra $w_1$ tal que $P(w_1) \geq P(w_i)$ para todo $i$.
III. A decodifica√ß√£o *greedy* sempre seleciona a palavra com a maior probabilidade $P(w|w_{<t})$ em cada passo.
IV. Portanto, a amostragem top-k com $k=1$ sempre escolher√° a mesma palavra que a decodifica√ß√£o *greedy* escolheria, tornando as duas estrat√©gias equivalentes. $\blacksquare$

#### Amostragem por N√∫cleo (Top-p): Ajuste Din√¢mico √† Distribui√ß√£o
A **amostragem por n√∫cleo** ou **top-p** √© uma varia√ß√£o da amostragem top-k, em que o n√∫mero de palavras a serem consideradas n√£o √© fixo, mas sim determinado dinamicamente [^1]. Em vez de selecionar as k palavras mais prov√°veis, esse m√©todo seleciona o menor conjunto de palavras cuja probabilidade cumulativa exceda um limiar $p$ [^2]. Isso permite que o m√©todo se adapte √† forma da distribui√ß√£o de probabilidade, selecionando um n√∫mero maior de palavras quando a distribui√ß√£o √© mais plana e um n√∫mero menor quando a distribui√ß√£o √© mais concentrada. O processo envolve:
1. Ordenar as palavras de acordo com suas probabilidades da distribui√ß√£o $P(w|w_{<t})$.
2. Calcular as probabilidades cumulativas e selecionar o menor conjunto de palavras que atinja ou ultrapasse o limiar $p$.
3. Renormalizar as probabilidades desse conjunto e amostrar.

> üí° **Exemplo Num√©rico:** Se as probabilidades forem P(A)=0.4, P(B)=0.3, P(C)=0.1, P(D)=0.09 e P(E)=0.01, e o limiar for p=0.8, top-p selecionaria A, B e C (pois 0.4+0.3+0.1 = 0.8), enquanto um top-k com k=3 tamb√©m selecionaria as mesmas palavras, mas um top-k com k=2 s√≥ consideraria A e B. Em outra situa√ß√£o, se as probabilidades fossem P(A)=0.7, P(B)=0.08, P(C)=0.07, P(D)=0.06 e P(E)=0.09, com p=0.8, apenas a palavra A seria selecionada pelo top-p, enquanto um top-k com k=3 selecionaria A, B e C. Notamos como o top-p ajusta dinamicamente o conjunto de palavras.  Neste √∫ltimo exemplo, para um p=0.9, top-p selecionaria A, B, C e E. A diferen√ßa entre top-k e top-p √© que o primeiro fixa o n√∫mero de palavras, enquanto o segundo ajusta o n√∫mero de palavras de acordo com a massa de probabilidade acumulada. Se a amostragem top-p, com p=0.8, sobre a distribui√ß√£o de {A, B, C} resultar em B, ent√£o a palavra B √© selecionada.

**Proposi√ß√£o 3:** A amostragem top-p se adapta melhor √† forma da distribui√ß√£o de probabilidade em compara√ß√£o com a amostragem top-k, pois ajusta dinamicamente o n√∫mero de palavras consideradas, equilibrando qualidade e diversidade.
*Prova:*
I. Seja $P(w_i)$ a probabilidade da i-√©sima palavra no vocabul√°rio.
II. Na amostragem top-k, as k primeiras palavras s√£o selecionadas independentemente de suas probabilidades.
III. Na amostragem top-p, o menor conjunto de palavras $S$ √© selecionado tal que $\sum_{w_i \in S} P(w_i) \geq p$.
IV. Se a distribui√ß√£o for concentrada em poucas palavras, a amostragem top-p selecionar√° um conjunto $S$ menor, aumentando a probabilidade de sele√ß√£o de palavras mais prov√°veis, o que pode melhorar a qualidade.
V. Se a distribui√ß√£o for mais uniforme, a amostragem top-p selecionar√° um conjunto $S$ maior, o que aumenta a diversidade na gera√ß√£o.
VI. A amostragem top-p, por se adaptar √† distribui√ß√£o, equilibra a qualidade e a diversidade, ao contr√°rio da amostragem top-k, que tem um n√∫mero fixo de palavras independentemente da forma da distribui√ß√£o.
Portanto, a amostragem top-p se adapta melhor √† distribui√ß√£o de probabilidade em compara√ß√£o com a amostragem top-k, equilibrando a qualidade e diversidade dinamicamente. $\blacksquare$

**Proposi√ß√£o 3.1:**  Existe uma equival√™ncia entre amostragem top-p e top-k em casos espec√≠ficos de distribui√ß√£o e limiares.
*Prova:*
I. Sejam $P(w_1) \geq P(w_2) \geq \ldots \geq P(w_n)$ as probabilidades ordenadas do vocabul√°rio.
II. Para um dado $p$ na amostragem top-p, seja $S_p = \{w_1, w_2, \ldots, w_m\}$ o conjunto de palavras tal que $\sum_{i=1}^{m} P(w_i) \geq p$ e $\sum_{i=1}^{m-1} P(w_i) < p$.
III. Se para um valor de $k$, o conjunto das top-k palavras corresponder a $S_p$ (ou seja, as $k$ palavras mais prov√°veis forem exatamente $w_1, w_2, \ldots, w_m$), ent√£o a amostragem top-k e top-p selecionar√£o o mesmo conjunto de palavras para amostragem.
IV. Em situa√ß√µes onde a distribui√ß√£o √© tal que a probabilidade cumulativa dos $k$ primeiros elementos coincide com o limiar $p$, o top-k com esse k espec√≠fico √© equivalente ao top-p com esse $p$ espec√≠fico.
Portanto, em casos espec√≠ficos onde o conjunto de palavras das top-k coincide com o conjunto das top-p, as duas amostragens s√£o equivalentes. $\blacksquare$

#### Amostragem por Temperatura: Ajustando a Nitidez da Distribui√ß√£o
A **amostragem por temperatura** n√£o trunca a distribui√ß√£o, mas a remodela ao ajustar os logits antes de aplicar a fun√ß√£o softmax [^1, 2]. Esse m√©todo introduz um par√¢metro de temperatura $\tau$ que divide os logits, resultando em uma distribui√ß√£o mais n√≠tida (para $\tau < 1$) ou mais uniforme (para $\tau > 1$) [^2]. Uma temperatura mais baixa leva a uma gera√ß√£o mais determin√≠stica, enquanto uma temperatura mais alta aumenta a aleatoriedade. A amostragem por temperatura √© formalizada da seguinte maneira:
$$y = \text{softmax}(u/\tau)$$
onde $u$ s√£o os logits do modelo e $y$ s√£o as probabilidades ajustadas pela temperatura.

**Teorema 2:** A amostragem por temperatura oferece um controle cont√≠nuo sobre a nitidez da distribui√ß√£o, permitindo ajustar a gera√ß√£o de texto entre a previsibilidade e a criatividade, usando o par√¢metro $\tau$ .
*Prova:*
I. Seja $P(w_i)$ a probabilidade da i-√©sima palavra no vocabul√°rio.
II. A amostragem por temperatura modifica a distribui√ß√£o original, dividindo os logits por $\tau$.
III. Para $\tau \rightarrow 0$, os logits s√£o amplificados e tendem a $\pm \infty$, resultando em uma distribui√ß√£o onde uma palavra se aproxima de probabilidade 1, e as outras de 0 (decodifica√ß√£o greedy).
IV. Para $\tau = 1$, a distribui√ß√£o n√£o √© modificada.
V. Para $\tau \rightarrow \infty$, os logits s√£o reduzidos e a distribui√ß√£o se torna mais uniforme, aproximando-se de uma distribui√ß√£o uniforme, aumentando a diversidade.
VI. A varia√ß√£o cont√≠nua de $\tau$ oferece controle preciso entre o extremo determin√≠stico ($\tau \rightarrow 0$) e o extremo mais uniforme ($\tau \rightarrow \infty$).
Portanto, a amostragem por temperatura permite um controle cont√≠nuo entre previsibilidade e criatividade, ajustando a nitidez da distribui√ß√£o com $\tau$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um modelo com logits u = [3, 2, 1]. Com $\tau$ = 1, as probabilidades s√£o aproximadamente P(A) ‚âà 0.66, P(B) ‚âà 0.24 e P(C) ‚âà 0.09. Com $\tau$ = 0.5, as probabilidades mudam para P'(A) ‚âà 0.88, P'(B) ‚âà 0.11 e P'(C) ‚âà 0.01. Com $\tau$ = 2, as probabilidades mudam para P''(A) ‚âà 0.48, P''(B) ‚âà 0.3, e P''(C) ‚âà 0.22. Vamos detalhar o c√°lculo para $\tau=0.5$. Primeiro, dividimos os logits por $\tau$: u/0.5 = [6, 4, 2]. Em seguida, aplicamos a fun√ß√£o softmax. Para a palavra A, temos: $e^6 / (e^6 + e^4 + e^2) \approx 403.4 / (403.4 + 54.6 + 7.39) \approx 0.88$. Para a palavra B: $e^4 / (e^6 + e^4 + e^2) \approx 54.6 / (403.4 + 54.6 + 7.39) \approx 0.11$. Para C: $e^2 / (e^6 + e^4 + e^2) \approx 7.39 / (403.4 + 54.6 + 7.39) \approx 0.01$. A temperatura ajusta a distribui√ß√£o de probabilidade para uma gera√ß√£o mais determin√≠stica (temperaturas baixas) ou mais aleat√≥ria (temperaturas altas). Note como a redu√ß√£o da temperatura ($\tau$=0.5) torna a probabilidade de A mais dominante, enquanto o aumento ($\tau$=2) torna a distribui√ß√£o mais uniforme.

**Observa√ß√£o 1:** A amostragem por temperatura, com $\tau=1$, resulta na amostragem aleat√≥ria padr√£o, demonstrando que a amostragem aleat√≥ria √© um caso particular da amostragem por temperatura.

### Conclus√£o
Neste cap√≠tulo, exploramos em detalhes v√°rias t√©cnicas de **amostragem** que podem ser usadas para refinar a gera√ß√£o de texto em **LLMs**. Cada uma dessas t√©cnicas ‚Äî amostragem aleat√≥ria, top-k, top-p e por temperatura ‚Äî tem seu pr√≥prio mecanismo para manipular as distribui√ß√µes de probabilidade, com um impacto direto na qualidade e diversidade do texto gerado [^2]. A amostragem top-k e top-p truncam a distribui√ß√£o, enquanto a amostragem por temperatura a remodela, oferecendo um controle fino sobre o processo de gera√ß√£o. A escolha entre esses m√©todos depende da aplica√ß√£o espec√≠fica e das necessidades de qualidade e diversidade. A combina√ß√£o dessas t√©cnicas, como discutido, pode oferecer maior flexibilidade para gera√ß√£o de texto adaptada a cen√°rios espec√≠ficos, melhorando a aplicabilidade dos LLMs [^1].

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^2]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
