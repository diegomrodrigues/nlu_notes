## Gera√ß√£o de Texto com Modelos de Linguagem: Decodifica√ß√£o Greedy, Amostragem e T√©cnicas Avan√ßadas

### Introdu√ß√£o
Este cap√≠tulo explora os m√©todos de **decodifica√ß√£o** e **amostragem** para a gera√ß√£o de texto em **Large Language Models (LLMs)**, consolidando os conceitos introduzidos nos cap√≠tulos anteriores e conectando-os ao processo pr√°tico de gera√ß√£o de texto [^1, 2]. Anteriormente, introduzimos a ideia de que a gera√ß√£o de texto em LLMs √© um processo autoregressivo, onde cada palavra √© selecionada com base nas palavras geradas previamente e na distribui√ß√£o de probabilidade fornecida pelo modelo. Agora, vamos explorar um m√©todo b√°sico chamado decodifica√ß√£o *greedy* e entender suas limita√ß√µes, e como m√©todos de amostragem, tais como amostragem aleat√≥ria, top-k, top-p e por temperatura resolvem essas limita√ß√µes e oferecem um controle mais preciso sobre o processo de gera√ß√£o de texto. Este cap√≠tulo tem como objetivo fornecer uma compreens√£o abrangente de como essas t√©cnicas moldam a qualidade e a diversidade do texto gerado, preparando o terreno para a explora√ß√£o de m√©todos de gera√ß√£o mais avan√ßados [^1, 2].

### Decodifica√ß√£o Greedy: Uma Abordagem Simples, Mas Limitada
A forma mais simples de gerar texto com um LLM √© atrav√©s da **decodifica√ß√£o *greedy***. Neste m√©todo, a cada passo, o modelo sempre escolhe a palavra com a maior probabilidade condicional, de acordo com a distribui√ß√£o de probabilidade do modelo para o contexto atual [^1, 2]. Em outras palavras, o modelo seleciona a palavra que ele considera mais prov√°vel em cada momento, sem levar em considera√ß√£o outras possibilidades, o que o torna um processo determin√≠stico. Embora a decodifica√ß√£o *greedy* seja computacionalmente eficiente e f√°cil de implementar, ela geralmente resulta em textos gen√©ricos, repetitivos, e pouco criativos, al√©m de ser suscet√≠vel a ciclos repetitivos, uma vez que n√£o h√° nenhuma aleatoriedade no processo de gera√ß√£o [^2].

> üí° **Exemplo Num√©rico:** Suponha que um LLM, ap√≥s processar a sequ√™ncia "O c√©u √©", atribua as seguintes probabilidades para a pr√≥xima palavra: P("azul")=0.7, P("claro")=0.2, P("nublado")=0.05, P("profundo")=0.05. Usando a decodifica√ß√£o *greedy*, o modelo sempre escolher√° a palavra "azul", pois ela tem a maior probabilidade. Caso a pr√≥xima palavra tamb√©m tenha "azul" como a palavra mais prov√°vel, ent√£o o modelo sempre ir√° gerar uma sequ√™ncia como "O c√©u √© azul azul azul ...". Este exemplo ilustra um dos problemas da decodifica√ß√£o *greedy*: a gera√ß√£o de textos repetitivos e previs√≠veis.

**Lema 5:** A decodifica√ß√£o *greedy*, embora simples, resulta em textos repetitivos e gen√©ricos devido √† sua natureza determin√≠stica, onde sempre a palavra mais prov√°vel √© selecionada.
*Prova*:
I. Seja $P(w|w_{<t})$ a distribui√ß√£o de probabilidade sobre o vocabul√°rio, dado o contexto $w_{<t}$.
II. Na decodifica√ß√£o *greedy*, a pr√≥xima palavra $w_t$ √© selecionada como: $w_t = \text{argmax}_{w} P(w|w_{<t})$.
III. O processo √© determin√≠stico e sempre seleciona a palavra com maior probabilidade para o contexto.
IV. Se, em um passo posterior, a mesma palavra for a mais prov√°vel, o modelo a selecionar√° novamente, gerando repeti√ß√µes.
V. Devido a sua natureza determin√≠stica, a decodifica√ß√£o *greedy* leva a textos pouco criativos.
Portanto, a decodifica√ß√£o *greedy* resulta em textos repetitivos e gen√©ricos devido a sua natureza determin√≠stica. $\blacksquare$

**Teorema 5.1** A decodifica√ß√£o greedy √© um caso especial da amostragem por temperatura quando a temperatura $\tau$ tende a zero.
*Prova:*
I. Na decodifica√ß√£o greedy, a palavra selecionada √© $w_t = \text{argmax}_{w} P(w|w_{<t})$.
II. Na amostragem por temperatura, a probabilidade de cada palavra √© dada por $P_\tau(w_i) = \frac{e^{u_i / \tau}}{\sum_{j} e^{u_j / \tau}}$, onde $u_i$ s√£o os logits e $\tau$ √© a temperatura.
III. Quando $\tau \rightarrow 0$, o termo $u_i / \tau$ domina, fazendo com que a distribui√ß√£o se concentre na palavra com o maior logit, ou seja, $w_t = \text{argmax}_{w} u_i$, que √© equivalente a $w_t = \text{argmax}_{w} P(w|w_{<t})$ no limite.
IV. Portanto, a decodifica√ß√£o *greedy* √© um caso limite da amostragem por temperatura quando $\tau$ tende a zero, pois a distribui√ß√£o de probabilidades se concentra na palavra mais prov√°vel. $\blacksquare$

### Amostragem: Uma Abordagem Probabil√≠stica para a Gera√ß√£o de Texto
Como vimos, a decodifica√ß√£o *greedy* tem suas limita√ß√µes. Para superar esses problemas, a **amostragem** surge como uma alternativa, ao introduzir um componente de aleatoriedade na escolha da pr√≥xima palavra, selecionando palavras de acordo com a distribui√ß√£o de probabilidade dada pelo modelo [^1, 2]. Ao inv√©s de sempre escolher a palavra mais prov√°vel, a amostragem permite que o modelo explore outras op√ß√µes com probabilidades menores, o que resulta em textos mais diversos e criativos [^2].

#### Amostragem Aleat√≥ria: Explorando o Espa√ßo de Probabilidades
A forma mais b√°sica de amostragem √© a **amostragem aleat√≥ria**, onde as palavras s√£o escolhidas diretamente de acordo com a distribui√ß√£o de probabilidade do modelo [^1]. Apesar de aumentar a diversidade em rela√ß√£o √† decodifica√ß√£o *greedy*, a amostragem aleat√≥ria n√£o evita que palavras de baixa probabilidade sejam selecionadas, o que pode levar a textos incoerentes [^2].  
A amostragem aleat√≥ria √© formalizada da seguinte maneira:
$$w_t \sim P(w|w_{<t})$$
Onde $w_t$ √© a palavra selecionada, e $P(w|w_{<t})$ √© a distribui√ß√£o de probabilidade do modelo dada as palavras anteriores.

> üí° **Exemplo Num√©rico:** Mantendo as probabilidades do exemplo anterior, P("azul")=0.7, P("claro")=0.2, P("nublado")=0.05, P("profundo")=0.05, na amostragem aleat√≥ria, a palavra "azul" √© selecionada com 70% de chance, "claro" com 20%, e as outras duas com 5% cada. √â poss√≠vel que, com amostragem aleat√≥ria, o modelo gere uma sequ√™ncia como "O c√©u √© profundo nublado azul claro...". Note que palavras que dificilmente seriam selecionadas pela decodifica√ß√£o *greedy* s√£o geradas, tornando o texto mais diverso, mas tamb√©m com risco de incoer√™ncia.

**Lema 6.1:** A amostragem aleat√≥ria com uma distribui√ß√£o uniforme sobre o vocabul√°rio pode gerar textos sem sentido e incoerentes.
*Prova:*
I. Seja $V$ o vocabul√°rio do modelo, e seja $P(w|w_{<t}) = \frac{1}{|V|}$ a distribui√ß√£o uniforme.
II.  Com uma distribui√ß√£o uniforme, todas as palavras no vocabul√°rio t√™m a mesma probabilidade de serem selecionadas, independentemente do contexto.
III. A sele√ß√£o aleat√≥ria de palavras do vocabul√°rio sem considerar a probabilidade do modelo ou o contexto anterior levar√° a uma gera√ß√£o de texto sem sentido e incoerente.
Portanto, a amostragem aleat√≥ria com distribui√ß√£o uniforme pode resultar em textos incoerentes, dada a aus√™ncia de contexto na sele√ß√£o. $\blacksquare$

#### Amostragem Top-k: Limitando o Espa√ßo de Amostragem
A **amostragem top-k** refina a amostragem aleat√≥ria ao limitar o espa√ßo de palavras a serem consideradas √†s $k$ palavras mais prov√°veis [^1, 2]. Depois que as $k$ palavras s√£o selecionadas, suas probabilidades s√£o renormalizadas e uma palavra √© amostrada a partir dessa nova distribui√ß√£o [^1]. Este m√©todo oferece um melhor balan√ßo entre qualidade e diversidade, j√° que impede a sele√ß√£o de palavras de baixa probabilidade, enquanto ainda permite certa aleatoriedade na escolha entre as op√ß√µes mais prov√°veis [^2].

> üí° **Exemplo Num√©rico:** Usando as probabilidades anteriores, P("azul")=0.7, P("claro")=0.2, P("nublado")=0.05, P("profundo")=0.05, se $k=2$, apenas "azul" e "claro" seriam considerados. As probabilidades seriam ent√£o renormalizadas para P'("azul")=0.7/(0.7+0.2) = 0.77 e P'("claro")=0.2/(0.7+0.2)=0.22. A amostragem ent√£o selecionaria entre essas duas op√ß√µes. Note como a sele√ß√£o da pr√≥xima palavra se restringe √†s mais prov√°veis.

> üí° **Exemplo Num√©rico:**  Consideremos um vocabul√°rio simplificado com 5 palavras e suas probabilidades dadas pelo modelo: P("cachorro") = 0.4, P("gato") = 0.3, P("p√°ssaro") = 0.15, P("peixe") = 0.1, P("rato") = 0.05. Se utilizarmos top-k com k=3, iremos selecionar as 3 palavras mais prov√°veis: "cachorro", "gato" e "p√°ssaro". As probabilidades renormalizadas ser√£o: P'("cachorro") = 0.4 / (0.4 + 0.3 + 0.15) ‚âà 0.53, P'("gato") = 0.3 / (0.4 + 0.3 + 0.15) ‚âà 0.4, P'("p√°ssaro") = 0.15 / (0.4 + 0.3 + 0.15) ‚âà 0.2.  A pr√≥xima palavra ser√° escolhida aleatoriamente com base nessas probabilidades renormalizadas. Isso garante que palavras menos prov√°veis como "peixe" e "rato" n√£o sejam geradas neste passo, melhorando a qualidade do texto sem eliminar a aleatoriedade.

**Lema 6:** A amostragem top-k oferece um melhor balan√ßo entre qualidade e diversidade do que a amostragem aleat√≥ria, evitando palavras de baixa probabilidade, mas mantendo um n√≠vel de aleatoriedade entre as op√ß√µes mais prov√°veis.
*Prova*:
I. Na amostragem top-k, as $k$ palavras mais prov√°veis s√£o selecionadas de acordo com a distribui√ß√£o de probabilidade $P(w|w_{<t})$.
II. As probabilidades dessas $k$ palavras s√£o renormalizadas, formando uma nova distribui√ß√£o $P'(w|w_{<t})$.
III. A pr√≥xima palavra $w_t$ √© amostrada a partir dessa nova distribui√ß√£o, $w_t \sim P'(w|w_{<t})$.
IV. Palavras que n√£o est√£o entre as $k$ mais prov√°veis n√£o ser√£o selecionadas, o que evita a gera√ß√£o de textos incoerentes gerados pela amostragem aleat√≥ria.
V. A amostragem dentro do conjunto de $k$ palavras mais prov√°veis mant√©m a diversidade do texto gerado.
Portanto, a amostragem top-k oferece melhor qualidade que a amostragem aleat√≥ria, ao truncar a distribui√ß√£o, mantendo um n√≠vel de diversidade. $\blacksquare$

#### Amostragem por N√∫cleo (Top-p): Adaptando o Espa√ßo de Amostragem
A **amostragem por n√∫cleo** ou **top-p** aborda uma das limita√ß√µes da amostragem top-k, que √© a fixa√ß√£o do n√∫mero de palavras consideradas (k) [^1, 2].  Em vez de selecionar as $k$ palavras mais prov√°veis, este m√©todo seleciona o menor conjunto de palavras cuja probabilidade cumulativa atinge um limiar $p$ [^2].  A amostragem top-p ajusta dinamicamente o n√∫mero de palavras consideradas, adaptando-se √† forma da distribui√ß√£o e fornecendo um mecanismo mais robusto para controlar o balan√ßo entre diversidade e qualidade.
O processo envolve:
1.  Ordenar as palavras por probabilidade.
2. Calcular a probabilidade cumulativa das palavras.
3. Selecionar o menor conjunto de palavras onde a probabilidade cumulativa excede um limiar $p$.
4. Renormalizar as probabilidades do conjunto selecionado.
5. Amostrar uma palavra desse novo conjunto.

> üí° **Exemplo Num√©rico:** Usando as probabilidades do exemplo anterior, P("azul")=0.7, P("claro")=0.2, P("nublado")=0.05, P("profundo")=0.05, se $p=0.9$, o top-p consideraria as palavras "azul" e "claro" (0.7 + 0.2 = 0.9). Se $p=0.95$, o top-p selecionaria as palavras "azul", "claro" e "nublado" (0.7 + 0.2 + 0.05 = 0.95). Note que o n√∫mero de palavras muda dinamicamente dependendo de $p$ e da distribui√ß√£o.

> üí° **Exemplo Num√©rico:**  Vamos usar o mesmo exemplo de antes: P("cachorro") = 0.4, P("gato") = 0.3, P("p√°ssaro") = 0.15, P("peixe") = 0.1, P("rato") = 0.05.  Se usarmos top-p com p=0.8, as probabilidades cumulativas s√£o: "cachorro": 0.4, "cachorro" + "gato": 0.7, "cachorro" + "gato" + "p√°ssaro": 0.85.  Como queremos um valor cumulativo maior ou igual a 0.8, selecionamos as palavras "cachorro", "gato" e "p√°ssaro". As probabilidades s√£o ent√£o renormalizadas: P'("cachorro") ‚âà 0.47, P'("gato") ‚âà 0.35, e P'("p√°ssaro") ‚âà 0.18. Agora, uma palavra √© amostrada desta distribui√ß√£o. Se usarmos p=0.9, "peixe" tamb√©m seria inclu√≠do, alterando a distribui√ß√£o renormalizada e a palavra amostrada.

**Proposi√ß√£o 7:** A amostragem top-p se adapta melhor a distribui√ß√µes de probabilidade diferentes do que a amostragem top-k.
*Prova*:
I. Seja $P(w_i)$ a probabilidade da i-√©sima palavra no vocabul√°rio.
II. Na amostragem top-k, um n√∫mero fixo $k$ de palavras s√£o selecionadas independentemente da forma da distribui√ß√£o.
III. Na amostragem top-p, um conjunto $S$ de palavras √© selecionado tal que a probabilidade cumulativa de $S$ seja maior ou igual a um dado $p$.
IV. Se a distribui√ß√£o for concentrada, poucas palavras ter√£o uma probabilidade muito alta, e o conjunto $S$ da amostragem top-p ter√° poucas palavras, enquanto o top-k selecionar√° mais palavras, muitas com baixa probabilidade, o que pode diminuir a qualidade.
V. Se a distribui√ß√£o for mais uniforme, v√°rias palavras ter√£o probabilidades similares, e a amostragem top-p selecionar√° mais palavras do que a amostragem top-k, aumentando a diversidade.
VI. A amostragem top-p, portanto, adapta o n√∫mero de palavras dinamicamente, levando a um melhor balanceamento entre qualidade e diversidade do que a amostragem top-k.
Portanto, a amostragem top-p se adapta melhor a diferentes distribui√ß√µes de probabilidade do que a amostragem top-k. $\blacksquare$

**Proposi√ß√£o 7.1:** Em casos extremos, a amostragem top-p pode se aproximar tanto da amostragem aleat√≥ria quanto da decodifica√ß√£o greedy.
*Prova*:
I. Se $p$ √© muito pr√≥ximo de 0, apenas a palavra mais prov√°vel ser√° selecionada, e a amostragem top-p se comporta de forma similar √† decodifica√ß√£o greedy.
II. Se $p$ √© muito pr√≥ximo de 1, o conjunto de palavras selecionado se aproxima do conjunto completo de palavras, e a amostragem top-p se comporta de forma similar √† amostragem aleat√≥ria.
III. Portanto, ao ajustar o valor de $p$, a amostragem top-p pode se aproximar dos outros m√©todos. $\blacksquare$

#### Amostragem por Temperatura: Modelando a Distribui√ß√£o
A **amostragem por temperatura**  n√£o trunca a distribui√ß√£o de probabilidades, mas a remodela usando um par√¢metro de temperatura ($\tau$) antes de passar a distribui√ß√£o pela fun√ß√£o *softmax* [^1, 2]. Dividir os *logits* pela temperatura antes de aplicar o *softmax* permite controlar a nitidez da distribui√ß√£o, com uma temperatura mais baixa ($\tau < 1$) concentrando as probabilidades nas palavras mais prov√°veis e uma temperatura mais alta ($\tau > 1$) suavizando a distribui√ß√£o, o que aumenta a diversidade e a aleatoriedade na amostragem [^2]. Este m√©todo oferece um controle refinado do equil√≠brio entre previsibilidade e criatividade.

> üí° **Exemplo Num√©rico:** Usando os logits anteriores, u = [3, 1, 0, -1], com $\tau = 1$, obtemos a distribui√ß√£o original do modelo. Se usarmos $\tau=0.5$, os logits s√£o transformados para u/0.5 = [6, 2, 0, -2] e a distribui√ß√£o se torna mais concentrada. Se usarmos $\tau=2$, os logits se tornam u/2 = [1.5, 0.5, 0, -0.5] e a distribui√ß√£o se torna mais suave. Note que ao contr√°rio dos outros m√©todos, a temperatura n√£o descarta palavras, mas remodela a distribui√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha que os logits de um modelo para a pr√≥xima palavra sejam: u = [2.0, 1.0, 0.5, -0.5]. A probabilidade para cada palavra pode ser calculada usando a fun√ß√£o softmax. Primeiro, vamos calcular a distribui√ß√£o original com $\tau$ = 1:
>
> $P(w_i) = \frac{e^{u_i}}{\sum_{j} e^{u_j}}$
>
> $P(w_1) = \frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.5} + e^{-0.5}}  \approx \frac{7.39}{7.39+2.72+1.65+0.61} \approx 0.61$
> $P(w_2) = \frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.5} + e^{-0.5}}  \approx \frac{2.72}{7.39+2.72+1.65+0.61} \approx 0.22$
> $P(w_3) = \frac{e^{0.5}}{e^{2.0} + e^{1.0} + e^{0.5} + e^{-0.5}}  \approx \frac{1.65}{7.39+2.72+1.65+0.61} \approx 0.13$
> $P(w_4) = \frac{e^{-0.5}}{e^{2.0} + e^{1.0} + e^{0.5} + e^{-0.5}}  \approx \frac{0.61}{7.39+2.72+1.65+0.61} \approx 0.05$
>
> Agora, vamos usar $\tau = 0.5$:
> $P_{\tau}(w_i) = \frac{e^{u_i / \tau}}{\sum_{j} e^{u_j / \tau}}$
>
> $P_{0.5}(w_1) = \frac{e^{2.0/0.5}}{e^{2.0/0.5} + e^{1.0/0.5} + e^{0.5/0.5} + e^{-0.5/0.5}} \approx \frac{e^{4}}{e^{4} + e^{2} + e^{1} + e^{-1}} \approx 0.84$
> $P_{0.5}(w_2) \approx 0.14$
> $P_{0.5}(w_3) \approx 0.02$
> $P_{0.5}(w_4) \approx 0.002$
>
> Com $\tau=0.5$, a distribui√ß√£o se torna mais concentrada em $w_1$. Agora, vamos usar $\tau = 2$:
>
> $P_{2}(w_1) = \frac{e^{2.0/2}}{e^{2.0/2} + e^{1.0/2} + e^{0.5/2} + e^{-0.5/2}} \approx \frac{e^{1}}{e^{1} + e^{0.5} + e^{0.25} + e^{-0.25}} \approx 0.45$
> $P_{2}(w_2) \approx 0.31$
> $P_{2}(w_3) \approx 0.17$
> $P_{2}(w_4) \approx 0.07$
>
> Com $\tau=2$, a distribui√ß√£o se torna mais suave.  Note como a temperatura afeta a forma da distribui√ß√£o.

**Teorema 4:** O par√¢metro de temperatura $\tau$ ajusta continuamente o equil√≠brio entre previsibilidade e diversidade na gera√ß√£o de texto.
*Prova*:
I. Seja $P_\tau(w_i)$ a probabilidade de uma palavra $w_i$ ap√≥s aplicar a temperatura $\tau$, dada por $P_\tau(w_i) = \frac{e^{u_i / \tau}}{\sum_{j} e^{u_j / \tau}}$.
II. Quando $\tau \rightarrow 0$, as probabilidades se concentram na palavra de maior logit, tornando a amostragem quase igual √† decodifica√ß√£o *greedy*, aumentando a previsibilidade e diminuindo a diversidade.
III. Quando $\tau \rightarrow \infty$, as probabilidades se tornam uniformes, o que aumenta a diversidade e aleatoriedade, mas pode comprometer a qualidade do texto gerado.
IV. Valores de $\tau$ entre 0 e $\infty$ oferecem um espectro cont√≠nuo para a explora√ß√£o do espa√ßo de gera√ß√£o.
Portanto, o par√¢metro de temperatura $\tau$ permite ajustar o balan√ßo entre previsibilidade e diversidade na gera√ß√£o de texto. $\blacksquare$

**Corol√°rio 4.1:**  A amostragem por temperatura com $\tau = 1$ mant√©m a distribui√ß√£o de probabilidades original do modelo.
*Prova:*
I.  Seja $P_\tau(w_i)$ a probabilidade de uma palavra $w_i$ ap√≥s aplicar a temperatura $\tau$, dada por $P_\tau(w_i) = \frac{e^{u_i / \tau}}{\sum_{j} e^{u_j / \tau}}$.
II. Quando $\tau = 1$, temos  $P_1(w_i) = \frac{e^{u_i}}{\sum_{j} e^{u_j}}$, que √© a distribui√ß√£o original do modelo.
Portanto, a amostragem por temperatura com $\tau = 1$ equivale √† amostragem direta da distribui√ß√£o original do modelo. $\blacksquare$

### Amostragem por Temperatura: Uma Analogia com Sistemas F√≠sicos
Uma analogia √∫til para entender a amostragem por temperatura √© com sistemas f√≠sicos e termodin√¢mica. Uma alta temperatura aumenta a energia das part√≠culas, permitindo que elas explorem mais estados poss√≠veis, e um sistema se torna menos previs√≠vel. Uma baixa temperatura diminui a energia das part√≠culas e as confina a estados de menor energia, levando a um comportamento mais determin√≠stico do sistema. Da mesma forma, uma alta temperatura na amostragem suaviza a distribui√ß√£o, permitindo ao modelo explorar mais palavras, e uma baixa temperatura concentra a distribui√ß√£o nas palavras de alta probabilidade, tornando a gera√ß√£o mais previs√≠vel.

### Conclus√£o
Este cap√≠tulo explorou detalhadamente as nuances da decodifica√ß√£o *greedy* e das t√©cnicas de **amostragem** em **LLMs**. Ao entender o funcionamento de cada um desses m√©todos (amostragem aleat√≥ria, top-k, top-p e por temperatura), podemos controlar o balan√ßo entre qualidade e diversidade, gerando textos mais adequados aos objetivos de cada aplica√ß√£o.  Enquanto a decodifica√ß√£o *greedy* √© simples, mas limitada, a amostragem oferece diversas maneiras de injetar aleatoriedade e explorar o espa√ßo de poss√≠veis resultados.  A combina√ß√£o das t√©cnicas estudadas pode refinar ainda mais o processo de gera√ß√£o, permitindo obter o resultado desejado [^2]. A escolha da t√©cnica mais apropriada deve ser feita com base nos requisitos espec√≠ficos da aplica√ß√£o, considerando o balan√ßo entre diversidade e qualidade. O estudo apresentado aqui pode ser usado como base para a cria√ß√£o de m√©todos de amostragem mais sofisticados e adaptados a aplica√ß√µes espec√≠ficas.

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^2]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
