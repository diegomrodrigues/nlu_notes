## Amostragem por Temperatura em Modelos de Linguagem: Um Mecanismo de Controle para a Gera√ß√£o de Texto

### Introdu√ß√£o
Dando seguimento √† nossa an√°lise de t√©cnicas de **amostragem** em **Large Language Models (LLMs)**, este cap√≠tulo ir√° aprofundar o m√©todo de **amostragem por temperatura**, explorando como esse mecanismo atua na remodela√ß√£o da distribui√ß√£o de probabilidade sobre o vocabul√°rio, impactando diretamente na qualidade e diversidade do texto gerado [^1, 2]. Nos cap√≠tulos anteriores, discutimos outros m√©todos como a amostragem aleat√≥ria, top-k e top-p, todos visando um equil√≠brio entre qualidade e diversidade. A amostragem por temperatura, no entanto, oferece uma abordagem distinta, ao modificar a nitidez da distribui√ß√£o de probabilidades em vez de trunc√°-la ou selecionar um subconjunto, permitindo um controle mais refinado sobre a gera√ß√£o de texto [^2].

### Amostragem por Temperatura: Um Mecanismo de Modula√ß√£o da Distribui√ß√£o
A **amostragem por temperatura** introduz um par√¢metro, denominado *temperatura* ($\tau$), que remodela a distribui√ß√£o de probabilidades ao ajustar os *logits* antes de serem transformados em probabilidades via *softmax* [^1, 2]. Em termos pr√°ticos, este m√©todo altera o n√≠vel de confian√ßa do modelo na sele√ß√£o de cada palavra. Uma temperatura mais baixa concentra a probabilidade em um conjunto menor de palavras de alta probabilidade, tornando a gera√ß√£o mais determin√≠stica e focada nas palavras que o modelo considera mais adequadas para o contexto. Por outro lado, uma temperatura mais alta suaviza a distribui√ß√£o, aumentando a probabilidade de selecionar palavras menos prov√°veis e, consequentemente, tornando a gera√ß√£o mais aleat√≥ria e diversa [^1].

A ess√™ncia da amostragem por temperatura reside na seguinte equa√ß√£o:
$$y = \text{softmax}(u/\tau)$$ [^2]
onde $u$ √© o vetor de *logits* gerados pelo modelo, $\tau$ √© o par√¢metro de temperatura e $y$ √© o vetor de probabilidades transformadas pela aplica√ß√£o do *softmax*.

#### Funcionamento da Temperatura

O mecanismo por tr√°s da amostragem por temperatura √© sutil, mas fundamental. Ao dividir os *logits* pelo par√¢metro $\tau$ antes de aplicar a fun√ß√£o *softmax*, a escala dos *logits* √© alterada. Quando $\tau < 1$, os *logits* s√£o amplificados, e o *softmax* ent√£o produz probabilidades mais concentradas em torno das palavras com os *logits* mais altos. Quando $\tau > 1$, os *logits* s√£o reduzidos, e o *softmax* gera probabilidades mais uniformemente distribu√≠das sobre o vocabul√°rio. Essa manipula√ß√£o da escala dos *logits* permite que a amostragem por temperatura atue como um mecanismo de controle para a nitidez da distribui√ß√£o, e consequentemente para a previsibilidade ou aleatoriedade do texto gerado.

> üí° **Exemplo Num√©rico:** Considere um modelo com *logits* u = [3, 1, 0, -1]. Se usarmos $\tau=1$, a distribui√ß√£o de probabilidade resultante (ap√≥s aplicar o softmax) √© a distribui√ß√£o original do modelo. Se usarmos $\tau=0.5$, ent√£o u/$\tau$ = [6, 2, 0, -2], e a distribui√ß√£o de probabilidade resultante se torna mais concentrada em torno da palavra com o maior logit, tornando a gera√ß√£o mais previs√≠vel. Por outro lado, se usarmos $\tau=2$, ent√£o u/$\tau$ = [1.5, 0.5, 0, -0.5], e a distribui√ß√£o de probabilidade se torna mais uniforme, aumentando a aleatoriedade na amostragem.
>
> Vamos calcular os valores de probabilidade para estes casos, usando a fun√ß√£o softmax:
>
>  Para $\tau=1$:
>  $y = \text{softmax}([3, 1, 0, -1]) = [\frac{e^3}{e^3+e^1+e^0+e^{-1}}, \frac{e^1}{e^3+e^1+e^0+e^{-1}}, \frac{e^0}{e^3+e^1+e^0+e^{-1}}, \frac{e^{-1}}{e^3+e^1+e^0+e^{-1}}] \approx [0.665, 0.244, 0.089, 0.033]$
>
>  Para $\tau=0.5$:
>  $y = \text{softmax}([6, 2, 0, -2]) = [\frac{e^6}{e^6+e^2+e^0+e^{-2}}, \frac{e^2}{e^6+e^2+e^0+e^{-2}}, \frac{e^0}{e^6+e^2+e^0+e^{-2}}, \frac{e^{-2}}{e^6+e^2+e^0+e^{-2}}] \approx [0.929, 0.064, 0.006, 0.0001]$
>
> Para $\tau=2$:
> $y = \text{softmax}([1.5, 0.5, 0, -0.5]) = [\frac{e^{1.5}}{e^{1.5}+e^{0.5}+e^0+e^{-0.5}}, \frac{e^{0.5}}{e^{1.5}+e^{0.5}+e^0+e^{-0.5}}, \frac{e^0}{e^{1.5}+e^{0.5}+e^0+e^{-0.5}}, \frac{e^{-0.5}}{e^{1.5}+e^{0.5}+e^0+e^{-0.5}}] \approx [0.473, 0.286, 0.173, 0.067]$
>
> Como podemos observar, com $\tau=0.5$, a primeira palavra tem uma probabilidade muito maior do que as outras. Com $\tau=2$, a distribui√ß√£o de probabilidades √© mais uniforme.

**Lema 4:** A amostragem por temperatura com $\tau=1$ equivale √† amostragem aleat√≥ria padr√£o, recuperando a distribui√ß√£o de probabilidade original do modelo.
*Prova:*
I. Seja $u$ o vetor de *logits* gerados pelo modelo e $y$ o vetor de probabilidades sobre o vocabul√°rio.
II. Na amostragem por temperatura, as probabilidades s√£o obtidas por $y = \text{softmax}(u/\tau)$.
III. Quando $\tau = 1$, temos $y = \text{softmax}(u/1) = \text{softmax}(u)$, que √© a distribui√ß√£o original do modelo sem nenhuma modifica√ß√£o.
IV. A amostragem feita sobre $y = \text{softmax}(u)$ corresponde √† amostragem aleat√≥ria sobre a distribui√ß√£o do modelo.
Portanto, a amostragem por temperatura com $\tau=1$ recupera a distribui√ß√£o de probabilidade original, e se comporta como a amostragem aleat√≥ria padr√£o. $\blacksquare$

**Teorema 3:** O par√¢metro de temperatura $\tau$ age como um controle cont√≠nuo para ajustar o equil√≠brio entre previsibilidade e diversidade na gera√ß√£o de texto.
*Prova*:
I. A distribui√ß√£o de probabilidade modificada pela temperatura √© dada por $P_\tau(w_i) = \frac{e^{u_i / \tau}}{\sum_{j} e^{u_j / \tau}}$.
II. Para $\tau < 1$, os valores $u_i / \tau$ tendem a valores maiores, resultando em uma distribui√ß√£o mais n√≠tida, onde as palavras de maior logit se tornam ainda mais prov√°veis, levando a uma gera√ß√£o mais focada e previs√≠vel.
III. Para $\tau > 1$, os valores $u_i / \tau$ tendem a valores menores, e a distribui√ß√£o se torna mais uniforme, com as probabilidades das palavras se tornando mais parecidas, resultando em uma gera√ß√£o mais diversificada e aleat√≥ria.
IV. A varia√ß√£o cont√≠nua de $\tau$ entre 0 e $\infty$  permite ajustar continuamente o n√≠vel de nitidez da distribui√ß√£o e consequentemente o equil√≠brio entre previsibilidade e diversidade na gera√ß√£o de texto.
Portanto, a amostragem por temperatura oferece um mecanismo cont√≠nuo para controlar o balan√ßo entre previsibilidade e diversidade na gera√ß√£o de texto. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos os seguintes logits para um modelo: `u = [2.0, 1.0, 0.0, -1.0]`. Podemos calcular as probabilidades para diferentes valores de temperatura:
> - Para $\tau = 0.5$: $u/\tau$ = [4.0, 2.0, 0.0, -2.0]. Aplicando o softmax, obtemos $P_{0.5}$ ‚âà [0.83, 0.11, 0.04, 0.00].
> - Para $\tau = 1.0$: $u/\tau$ = [2.0, 1.0, 0.0, -1.0]. Aplicando o softmax, obtemos $P_{1.0}$ ‚âà [0.67, 0.24, 0.09, 0.03].
> - Para $\tau = 2.0$: $u/\tau$ = [1.0, 0.5, 0.0, -0.5]. Aplicando o softmax, obtemos $P_{2.0}$ ‚âà [0.48, 0.30, 0.18, 0.09].
> Observe como a redu√ß√£o da temperatura resulta em uma distribui√ß√£o mais concentrada em torno da palavra mais prov√°vel, enquanto o aumento da temperatura resulta em uma distribui√ß√£o mais uniforme.
>
>   Vamos detalhar os c√°lculos para $\tau=0.5$:
>
>   $u/\tau = [2.0/0.5, 1.0/0.5, 0.0/0.5, -1.0/0.5] = [4.0, 2.0, 0.0, -2.0]$
>
>   $P_{0.5}(w_i) = \frac{e^{u_i/\tau}}{\sum_j e^{u_j/\tau}}$
>
>   $P_{0.5}(w_1) = \frac{e^4}{e^4+e^2+e^0+e^{-2}} = \frac{54.598}{54.598+7.389+1+0.135} \approx 0.83$
>
>   $P_{0.5}(w_2) = \frac{e^2}{e^4+e^2+e^0+e^{-2}} = \frac{7.389}{54.598+7.389+1+0.135} \approx 0.11$
>
>   $P_{0.5}(w_3) = \frac{e^0}{e^4+e^2+e^0+e^{-2}} = \frac{1}{54.598+7.389+1+0.135} \approx 0.04$
>
>   $P_{0.5}(w_4) = \frac{e^{-2}}{e^4+e^2+e^0+e^{-2}} = \frac{0.135}{54.598+7.389+1+0.135} \approx 0.00$
>
>  Os c√°lculos para $\tau=1$ e $\tau=2$ seguem o mesmo princ√≠pio.

#### Amostragem por Temperatura em conjunto com outros m√©todos

A amostragem por temperatura pode ser combinada com outros m√©todos de amostragem como top-k e top-p para oferecer ainda mais controle sobre o processo de gera√ß√£o. Aplicar primeiro a temperatura, e em seguida truncar a distribui√ß√£o com top-k ou top-p, permite obter maior flexibilidade na gera√ß√£o de texto, usando a temperatura para ajustar a distribui√ß√£o, e top-k/p para selecionar as palavras a serem amostradas a partir dessa distribui√ß√£o modificada.

> üí° **Exemplo Num√©rico:** Imagine que ap√≥s aplicar a temperatura $\tau=0.7$ em um conjunto de *logits*, obtemos a seguinte distribui√ß√£o de probabilidades sobre um vocabul√°rio de 10 palavras: P = [0.4, 0.3, 0.15, 0.05, 0.03, 0.02, 0.01, 0.01, 0.01, 0.01]. Se aplicarmos um top-k com k=3, as probabilidades seriam normalizadas sobre as 3 primeiras palavras, resultando em uma nova distribui√ß√£o P_topk = [0.57, 0.43, 0.0]. A amostragem agora seria restrita apenas as 3 primeiras palavras, com a probabilidade definida pela distribui√ß√£o normalizada.

**Lema 4.1:** A amostragem por temperatura, ao ser combinada com outras t√©cnicas de amostragem, oferece um controle ainda mais granular sobre o processo de gera√ß√£o.
*Prova:*
I. A amostragem por temperatura modifica a distribui√ß√£o de probabilidades $P(w)$ para uma distribui√ß√£o $P_\tau(w)$.
II. T√©cnicas como top-k e top-p selecionam um subconjunto de palavras $S$ para a amostragem.
III. Ao aplicar a temperatura primeiro, a distribui√ß√£o se torna $P_\tau(w)$, e ao aplicar top-k ou top-p em seguida, o conjunto de palavras $S$ √© selecionado com base em $P_\tau(w)$.
IV. A combina√ß√£o das duas t√©cnicas permite um controle refinado: a temperatura controla a nitidez da distribui√ß√£o (previsibilidade x diversidade), e top-k ou top-p controlam a cardinalidade do espa√ßo amostrado, o que leva a textos mais personalizados.
Portanto, a combina√ß√£o da amostragem por temperatura com outras t√©cnicas oferece um controle ainda mais refinado sobre a gera√ß√£o de texto. $\blacksquare$

**Proposi√ß√£o 5:** A amostragem por temperatura com $\tau \to 0$ aproxima-se da sele√ß√£o argmax, onde a palavra com maior logit √© sempre escolhida.
*Prova:*
I.  A distribui√ß√£o de probabilidade ap√≥s a aplica√ß√£o da temperatura √© dada por $P_\tau(w_i) = \frac{e^{u_i / \tau}}{\sum_{j} e^{u_j / \tau}}$.
II. Quando $\tau$ se aproxima de zero, os valores $u_i/\tau$ tendem a $\pm \infty$, dependendo do sinal de $u_i$.
III.  Se $u_k$ √© o maior logit, ent√£o $e^{u_k/\tau}$ domina os outros termos na soma do denominador quando $\tau \to 0$.
IV. Assim,  $P_\tau(w_k) \to \frac{e^{u_k / \tau}}{e^{u_k / \tau}} = 1$,  e $P_\tau(w_i) \to 0$ para todo $i \neq k$, quando $\tau \to 0$.
V. Portanto, quando $\tau \to 0$, a distribui√ß√£o de probabilidade se concentra totalmente na palavra com maior logit, que corresponde √† sele√ß√£o argmax. $\blacksquare$

**Proposi√ß√£o 6:** A amostragem por temperatura com $\tau \to \infty$  aproxima-se de uma distribui√ß√£o uniforme sobre o vocabul√°rio.
*Prova:*
I. A distribui√ß√£o de probabilidade modificada pela temperatura √© $P_\tau(w_i) = \frac{e^{u_i / \tau}}{\sum_{j} e^{u_j / \tau}}$.
II. Quando $\tau$ tende a $\infty$, $u_i/\tau$ tende a 0 para todo $i$.
III. Assim, $e^{u_i/\tau}$ tende a $e^0 = 1$ para todo $i$.
IV. Portanto, $P_\tau(w_i) = \frac{1}{\sum_j 1} = \frac{1}{V}$, onde V √© o tamanho do vocabul√°rio, que √© uma distribui√ß√£o uniforme.
V. Consequentemente, quando $\tau \to \infty$, a distribui√ß√£o de probabilidade tende a ser uniforme sobre todo o vocabul√°rio. $\blacksquare$

### Amostragem por Temperatura: Ajustando a 'Confian√ßa' do Modelo

√â √∫til pensar na amostragem por temperatura como um mecanismo para ajustar o n√≠vel de "confian√ßa" do modelo na sele√ß√£o de cada palavra. Com uma temperatura baixa, o modelo fica mais "confiante" nas suas previs√µes, resultando na sele√ß√£o de palavras de alta probabilidade e, consequentemente, em um texto mais previs√≠vel e menos criativo. Uma temperatura alta torna o modelo menos "confiante", dando uma chance maior a palavras de baixa probabilidade, resultando em maior diversidade e, potencialmente, criatividade, mas com o risco de incoer√™ncias ou resultados sem sentido.

> üí° **Exemplo Num√©rico:** Considere um modelo que est√° gerando texto e tem os logits para a pr√≥xima palavra como u = [4, 2, 1, -1]. Se usarmos $\tau=0.2$, a distribui√ß√£o ser√° muito concentrada na primeira palavra. A gera√ß√£o de texto com essa temperatura resultar√° em texto repetitivo. Se usarmos $\tau=5$, as probabilidades ser√£o quase uniformes entre as palavras, gerando texto aleat√≥rio. Um valor de temperatura intermedi√°rio, como $\tau=1.0$, gera texto com um equil√≠brio entre previsibilidade e diversidade.

**Lema 6.1:** O ajuste da temperatura pode ser adaptado dinamicamente durante a gera√ß√£o de texto para equilibrar a necessidade de coer√™ncia e diversidade.
*Prova:*
I. Durante a gera√ß√£o de texto, diferentes partes da frase podem se beneficiar de diferentes n√≠veis de diversidade.
II. Ao usar uma temperatura baixa no in√≠cio da frase ou em partes que requerem mais precis√£o e coer√™ncia, o modelo se concentra em palavras de alta probabilidade.
III. Ao aumentar a temperatura em partes da frase onde a diversidade √© desejada, o modelo pode explorar palavras menos comuns, adicionando variedade ao texto gerado.
IV. A mudan√ßa din√¢mica da temperatura permite adaptar a gera√ß√£o de acordo com a necessidade de cada parte da frase, criando um equil√≠brio entre coer√™ncia e diversidade.
Portanto, a adapta√ß√£o din√¢mica da temperatura durante a gera√ß√£o oferece um mecanismo flex√≠vel e sofisticado para melhorar o texto gerado. $\blacksquare$

### Conclus√£o
Neste cap√≠tulo, analisamos a **amostragem por temperatura** em detalhes, mostrando como esse mecanismo afeta a distribui√ß√£o de probabilidades geradas pelos LLMs. A amostragem por temperatura, ao contr√°rio da amostragem top-k e top-p, n√£o trunca a distribui√ß√£o, mas a remodela, oferecendo um controle cont√≠nuo sobre o balan√ßo entre qualidade e diversidade do texto gerado. Ao ajustar o par√¢metro de temperatura, √© poss√≠vel obter uma ampla gama de comportamentos de gera√ß√£o, desde textos altamente previs√≠veis at√© textos mais criativos e aleat√≥rios. A flexibilidade da amostragem por temperatura a torna uma ferramenta valiosa para aplica√ß√µes que demandam um controle fino sobre o estilo e a natureza do texto gerado, permitindo que se ajuste a gera√ß√£o ao objetivo desejado [^1]. A combina√ß√£o da amostragem por temperatura com outros m√©todos, como o top-k e o top-p,  permite criar estrat√©gias de amostragem ainda mais refinadas, que maximizam tanto a qualidade quanto a diversidade do texto.

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^2]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
