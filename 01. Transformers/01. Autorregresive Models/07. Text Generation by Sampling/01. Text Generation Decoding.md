## Gera√ß√£o de Texto via Amostragem em Modelos de Linguagem

### Introdu√ß√£o
Em continuidade ao nosso estudo de **Large Language Models (LLMs)** e **Transformers**, este cap√≠tulo se aprofunda em um aspecto crucial desses modelos: a gera√ß√£o de texto. Como vimos anteriormente, os LLMs n√£o apenas compreendem a linguagem, mas tamb√©m s√£o capazes de gerar texto que imita a linguagem humana com not√°vel fidelidade. O processo de gera√ß√£o de texto √© fundamental para uma variedade de aplica√ß√µes, desde a cria√ß√£o de resumos at√© o desenvolvimento de chatbots. Este cap√≠tulo explora os detalhes da gera√ß√£o de texto atrav√©s de amostragem, um m√©todo chave para obter resultados diversos e de alta qualidade [^1].

### Conceitos Fundamentais
No cora√ß√£o da gera√ß√£o de texto em modelos de linguagem reside a tarefa de **decodifica√ß√£o**, ou seja, a escolha da pr√≥xima palavra a ser gerada com base no contexto e nas probabilidades atribu√≠das pelo modelo [^2]. Este processo √© essencialmente uma busca pela palavra mais apropriada em um determinado ponto da sequ√™ncia textual. A decodifica√ß√£o geralmente envolve a considera√ß√£o de uma distribui√ß√£o de probabilidade sobre todas as palavras poss√≠veis no vocabul√°rio do modelo, e a escolha da palavra com a maior probabilidade ou atrav√©s de m√©todos de amostragem mais complexos [^1].

O processo de gera√ß√£o √©, em geral, *autoregressivo*, ou seja, a decis√£o sobre qual palavra gerar a seguir depende das palavras geradas anteriormente. Esse mecanismo, tamb√©m chamado de *gera√ß√£o causal*, √© a base da maioria dos modelos de gera√ß√£o de texto. Em termos pr√°ticos, isso significa que, durante a decodifica√ß√£o, o modelo escolhe palavras da esquerda para a direita (ou da direita para a esquerda, dependendo do idioma) [^2], utilizando as palavras previamente geradas como contexto para prever as palavras subsequentes. Esse processo iterativo permite que o modelo gere texto coerente e contextualmente relevante [^1].

**Caixa de destaque:**

>  *A gera√ß√£o autoregressiva ou causal √© um processo iterativo onde a pr√≥xima palavra √© selecionada com base nas probabilidades do modelo e nas palavras previamente escolhidas.*

√â crucial notar que, embora a gera√ß√£o autoregressiva seja uma abordagem eficaz, ela n√£o est√° isenta de limita√ß√µes. Por exemplo, ela pode ser propensa a produzir texto repetitivo ou genericamente previs√≠vel se a amostragem n√£o for cuidadosamente controlada. Para mitigar esses problemas, t√©cnicas mais avan√ßadas de amostragem foram desenvolvidas [^2].

**Observa√ß√£o 1:** A natureza autoregressiva da gera√ß√£o de texto implica uma depend√™ncia temporal intr√≠nseca. O modelo n√£o pode gerar uma palavra sem levar em conta o hist√≥rico de palavras geradas anteriormente. Isso tamb√©m significa que erros nas primeiras etapas da gera√ß√£o podem se propagar e afetar negativamente as etapas seguintes.

### M√©todos de Amostragem
A **amostragem** √© um m√©todo fundamental para a decodifica√ß√£o em modelos de linguagem [^2]. Ao inv√©s de simplesmente selecionar a palavra mais prov√°vel a cada passo (como na decodifica√ß√£o *greedy*), a amostragem introduz um elemento de aleatoriedade, permitindo que o modelo explore outras op√ß√µes menos prov√°veis, o que resulta em maior diversidade no texto gerado [^2].

#### Amostragem Aleat√≥ria
A forma mais simples de amostragem √© a **amostragem aleat√≥ria**. Conforme discutido anteriormente, ela envolve a sele√ß√£o de palavras com base em suas probabilidades atribu√≠das pelo modelo, sem qualquer truncamento ou modifica√ß√£o da distribui√ß√£o de probabilidade. No entanto, a amostragem aleat√≥ria pura pode n√£o ser ideal, pois, mesmo que as probabilidades da cauda da distribui√ß√£o sejam baixas, pode haver muitas palavras de baixa probabilidade juntas que, no agregado, acabam sendo amostradas com mais frequ√™ncia, levando a resultados estranhos e pouco coerentes. Por isso, m√©todos de amostragem mais sofisticados s√£o preferidos [^2].

> üí° **Exemplo Num√©rico:** Imagine que temos um vocabul√°rio de 5 palavras {A, B, C, D, E} e que, ap√≥s o processamento da entrada, o modelo atribui as seguintes probabilidades: P(A) = 0.6, P(B) = 0.2, P(C) = 0.1, P(D) = 0.05 e P(E) = 0.05. Em uma amostragem aleat√≥ria, a palavra 'A' seria selecionada com 60% de chance, e as demais com suas respectivas probabilidades. Em dez amostras, seria comum que 'A' fosse selecionada 6 vezes, 'B' 2 vezes, e 'C', 'D', e 'E' uma vez cada, ou nenhuma vez. Notamos que mesmo as palavras 'D' e 'E', que individualmente t√™m baixa probabilidade, t√™m 10% de chances agregadas de serem selecionadas, o que pode levar a sequ√™ncias incoerentes.

**Lema 1:** A amostragem aleat√≥ria pura, embora simples, pode levar √† gera√ß√£o de texto de baixa qualidade devido √† alta probabilidade de selecionar palavras de baixa probabilidade agregada na cauda da distribui√ß√£o.
*Prova*:
I. Considere uma distribui√ß√£o de probabilidade $P(w)$ sobre um vocabul√°rio $V$, onde $w$ representa uma palavra.
II. A amostragem aleat√≥ria pura seleciona uma palavra $w$ com probabilidade $P(w)$.
III. Seja $C$ o conjunto de palavras de baixa probabilidade na cauda da distribui√ß√£o, ou seja, $C = \{w \in V \, | \, P(w) < \epsilon\}$ para um $\epsilon$ pequeno.
IV. Embora cada $P(w)$ para $w \in C$ seja pequena, o n√∫mero de palavras em $C$ pode ser grande.
V. A probabilidade de selecionar pelo menos uma palavra em $C$ ao longo de m√∫ltiplas etapas de amostragem √© n√£o desprez√≠vel, pois a probabilidade do evento complementar (nunca selecionar uma palavra de $C$) diminui a cada etapa.
VI. A sele√ß√£o de palavras de baixa probabilidade com mais frequ√™ncia leva a sequ√™ncias de texto incoerentes e de baixa qualidade.
Portanto, a amostragem aleat√≥ria pura pode levar √† gera√ß√£o de texto de baixa qualidade. ‚ñ†

#### Amostragem Top-k
A **amostragem top-k** √© uma generaliza√ß√£o da decodifica√ß√£o *greedy* que aborda o problema das baixas probabilidades [^2]. Em vez de escolher apenas a palavra mais prov√°vel, este m√©todo primeiro trunca a distribui√ß√£o para as k palavras mais prov√°veis, em seguida renormaliza a probabilidade dessas k palavras para formar uma nova distribui√ß√£o, e ent√£o seleciona uma palavra dessa nova distribui√ß√£o. O par√¢metro k controla a diversidade do texto gerado, com valores menores favorecendo palavras mais prov√°veis e valores maiores introduzindo mais aleatoriedade [^2].

> üí° **Exemplo Num√©rico:** Usando as mesmas probabilidades do exemplo anterior (P(A) = 0.6, P(B) = 0.2, P(C) = 0.1, P(D) = 0.05 e P(E) = 0.05), se k=2, apenas as palavras 'A' e 'B' seriam consideradas. As probabilidades seriam renormalizadas: P'(A) = 0.6 / (0.6 + 0.2) = 0.75 e P'(B) = 0.2 / (0.6 + 0.2) = 0.25. A amostragem agora escolheria entre 'A' (75% de chance) e 'B' (25% de chance). Se k=3, as probabilidades seriam renormalizadas sobre as palavras A, B e C, resultando em uma distribui√ß√£o diferente e, portanto, uma amostragem diferente.

#### Amostragem por N√∫cleo ou Top-p
Outra t√©cnica √© a **amostragem por n√∫cleo** ou **top-p** [^2]. Este m√©todo tamb√©m visa truncar a distribui√ß√£o de probabilidade, mas, em vez de usar um n√∫mero fixo de palavras, o top-p seleciona o menor conjunto de palavras cujas probabilidades somadas s√£o maiores do que um par√¢metro p. Isso permite que o tamanho do conjunto de palavras a serem consideradas se adapte ao formato da distribui√ß√£o [^2]. Isso √© importante porque em algumas situa√ß√µes a probabilidade pode estar muito concentrada em poucas palavras, enquanto em outras √© mais distribu√≠da [^2].

> üí° **Exemplo Num√©rico:**  Mantendo as probabilidades originais, se p=0.8, o top-p selecionaria as palavras 'A' e 'B', pois P(A) + P(B) = 0.8. Se p=0.9, o top-p selecionaria 'A', 'B' e 'C', pois P(A) + P(B) + P(C) = 0.9. Se as probabilidades fossem diferentes, por exemplo, P(A)=0.7, P(B)=0.1, P(C)=0.08, P(D)=0.07, P(E)=0.05, com p=0.8, apenas a palavra 'A' seria selecionada. Observe que ao contr√°rio do top-k, o n√∫mero de palavras selecionadas varia. Este m√©todo adapta-se √† distribui√ß√£o das probabilidades.

**Proposi√ß√£o 1:** A amostragem top-p se adapta melhor √† distribui√ß√£o de probabilidade do que a amostragem top-k, resultando em texto mais diversificado quando a distribui√ß√£o √© mais uniforme e mais focado quando a distribui√ß√£o √© mais concentrada.
*Prova*:
I. Seja $P(w_i)$ a probabilidade da i-√©sima palavra no vocabul√°rio, com as palavras ordenadas de forma que $P(w_1) \geq P(w_2) \geq \ldots$.
II. Na amostragem top-k, as primeiras k palavras s√£o selecionadas independentemente da forma da distribui√ß√£o.
III. Na amostragem top-p, o menor conjunto de palavras $S$ √© selecionado tal que $\sum_{w_i \in S} P(w_i) \geq p$.
IV. Se a distribui√ß√£o for concentrada, ou seja, uma pequena quantidade de palavras tiver probabilidade alta, o conjunto $S$ em top-p conter√° menos palavras do que k (assumindo que k √© constante), resultando em maior foco.
V. Se a distribui√ß√£o for mais uniforme, ou seja, v√°rias palavras tiverem probabilidades similares, o conjunto $S$ em top-p conter√° mais palavras do que k, resultando em maior diversidade.
VI. Portanto, a amostragem top-p se adapta melhor √† forma da distribui√ß√£o, selecionando um n√∫mero vari√°vel de palavras que levam √† diversidade em distribui√ß√µes uniformes e maior foco em distribui√ß√µes concentradas, ao contr√°rio da amostragem top-k.
Portanto, a amostragem top-p se adapta melhor √† distribui√ß√£o de probabilidade do que a amostragem top-k. ‚ñ†

#### Amostragem por Temperatura
A **amostragem por temperatura** √© um m√©todo que n√£o trunca a distribui√ß√£o, mas a remodela ao dividir os *logits* por um par√¢metro $\tau$ antes de aplicar o *softmax* [^2]. Uma temperatura mais baixa ($\tau < 1$) aumenta a probabilidade das palavras mais prov√°veis, tornando a gera√ß√£o mais determin√≠stica e concentrada nas palavras de maior probabilidade. Uma temperatura mais alta ($\tau > 1$) achata a distribui√ß√£o, tornando a gera√ß√£o mais aleat√≥ria e diversificada. Este par√¢metro permite um ajuste fino do equil√≠brio entre qualidade e diversidade [^2].

$$y = \text{softmax}(u/\tau)$$ [^2]

> üí° **Exemplo Num√©rico:** Suponha que os logits de um modelo para as palavras {A, B, C} sejam u = [2, 1, 0]. Aplicando softmax sem temperatura ($\tau$=1), obtemos as probabilidades originais P(A) ‚âà 0.67, P(B) ‚âà 0.24, P(C) ‚âà 0.09. Se usarmos $\tau$=0.5, os logits modificados ser√£o u/$\tau$ = [4, 2, 0], e as probabilidades ser√£o P'(A) ‚âà 0.88, P'(B) ‚âà 0.12, P'(C) ‚âà 0.01. Observe como a temperatura mais baixa torna a palavra 'A' ainda mais prov√°vel, tornando a gera√ß√£o mais focada. Por outro lado, se usarmos $\tau$=2, os logits ser√£o u/$\tau$ = [1, 0.5, 0] resultando em P''(A) ‚âà 0.48, P''(B) ‚âà 0.30, P''(C) ‚âà 0.22, tornando as probabilidades mais parecidas e a amostragem mais aleat√≥ria.

**Lema 1.1:** A amostragem por temperatura com $\tau$=1 recupera a distribui√ß√£o original de probabilidade, enquanto $\tau$ -> 0 aproxima a decodifica√ß√£o greedy.
*Prova*:
I. Seja $u$ o vetor de logits gerados pelo modelo.
II. A distribui√ß√£o de probabilidade $P(w_i)$ para uma palavra $w_i$ no vocabul√°rio √© dada pelo softmax dos logits: $P(w_i) = \frac{e^{u_i}}{\sum_{j} e^{u_j}}$.
III. Na amostragem por temperatura, os logits s√£o divididos por um par√¢metro de temperatura $\tau$ antes de aplicar o softmax: $P_{\tau}(w_i) = \frac{e^{u_i / \tau}}{\sum_{j} e^{u_j / \tau}}$.
IV. Quando $\tau = 1$, temos $P_{\tau=1}(w_i) = \frac{e^{u_i}}{\sum_{j} e^{u_j}} = P(w_i)$, recuperando a distribui√ß√£o original.
V. Quando $\tau \rightarrow 0$, os valores $u_i / \tau$ tendem a valores muito grandes (positivos ou negativos).
VI. O softmax transforma esses valores grandes em uma distribui√ß√£o pr√≥xima de 0 para todas as palavras, exceto para a palavra com maior logit, cuja probabilidade se aproxima de 1, levando √† decodifica√ß√£o greedy.
Portanto, a amostragem por temperatura com $\tau$=1 recupera a distribui√ß√£o original, e $\tau$ -> 0 aproxima a decodifica√ß√£o greedy. ‚ñ†

**Teorema 1:** A amostragem por temperatura oferece um mecanismo cont√≠nuo para controlar o equil√≠brio entre qualidade e diversidade na gera√ß√£o de texto, atrav√©s do ajuste do par√¢metro $\tau$.
*Prova*:
I. A distribui√ß√£o de probabilidade ap√≥s a aplica√ß√£o da temperatura √© dada por $P_{\tau}(w_i) = \frac{e^{u_i / \tau}}{\sum_{j} e^{u_j / \tau}}$.
II. Para $\tau < 1$, a divis√£o dos logits por $\tau$ amplifica as diferen√ßas entre os logits, resultando em uma distribui√ß√£o de probabilidade mais n√≠tida e concentrada.
III. Uma distribui√ß√£o mais n√≠tida favorece as palavras de maior probabilidade, levando a textos mais previs√≠veis e de maior qualidade (em termos de adequa√ß√£o ao contexto).
IV. Para $\tau > 1$, a divis√£o dos logits por $\tau$ reduz as diferen√ßas entre os logits, resultando em uma distribui√ß√£o de probabilidade mais suave e uniforme.
V. Uma distribui√ß√£o mais suave favorece palavras de menor probabilidade, o que aumenta a diversidade, mas pode comprometer a coer√™ncia.
VI. Ao variar continuamente o valor de $\tau$ entre valores pequenos e grandes, podemos controlar a nitidez da distribui√ß√£o e, portanto, o balan√ßo entre qualidade e diversidade na gera√ß√£o do texto.
Portanto, a amostragem por temperatura oferece um mecanismo cont√≠nuo para controlar o equil√≠brio entre qualidade e diversidade na gera√ß√£o de texto. ‚ñ†

### Conclus√£o
A gera√ß√£o de texto atrav√©s de amostragem √© uma √°rea crucial no campo de LLMs e processamento de linguagem natural. A escolha do m√©todo de amostragem correto, e a configura√ß√£o dos hiperpar√¢metros associados, t√™m um grande impacto na qualidade e diversidade do texto gerado. Cada um dos m√©todos descritos oferece uma maneira de ajustar o trade-off entre qualidade e diversidade na gera√ß√£o. A compreens√£o dessas t√©cnicas √© essencial para qualquer pessoa que busque usar LLMs para aplica√ß√µes que envolvam a gera√ß√£o de texto. Os m√©todos descritos aqui tamb√©m podem ser usados como um ponto de partida para se desenvolver m√©todos de amostragem ainda mais refinados [^2].

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^2]:  Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
