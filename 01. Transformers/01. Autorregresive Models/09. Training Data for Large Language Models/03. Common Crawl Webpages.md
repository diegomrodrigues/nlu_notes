## Common Crawl como Base para Large Language Models

### Introdu√ß√£o
Este cap√≠tulo se aprofunda no **Common Crawl**, um vasto reposit√≥rio de *snapshots* de p√°ginas web, e sua import√¢ncia como base para muitos Large Language Models (LLMs). Construindo sobre os conceitos de corpora de treinamento abordados nos cap√≠tulos anteriores [^26], vamos explorar em detalhes o que √© o Common Crawl, como ele √© constru√≠do, seus desafios e por que ele se tornou uma funda√ß√£o para o treinamento de muitos LLMs. Como vimos anteriormente, a escolha da fonte de dados e o seu tratamento s√£o essenciais para a qualidade de um LLM [^26].

### Conceitos Fundamentais

O Common Crawl √© uma iniciativa sem fins lucrativos que periodicamente realiza *crawling* em grandes por√ß√µes da web e disponibiliza os dados publicamente [^26]. Ele √© uma cole√ß√£o de *snapshots* de p√°ginas web, que √©, em sua ess√™ncia, um *dump* de dados da internet, servindo como base para muitos projetos de pesquisa e desenvolvimento, incluindo o treinamento de LLMs.

#### O que √© o Common Crawl?

O Common Crawl consiste em *snapshots* regulares da internet, onde um *crawler* autom√°tico visita um grande n√∫mero de p√°ginas web, seguindo links, e armazena o conte√∫do de cada p√°gina em um formato estruturado. Os *snapshots* s√£o disponibilizados para download e incluem o HTML da p√°gina, bem como metadados sobre o *crawling*.
> üí° **Exemplo Num√©rico:** Um *snapshot* t√≠pico do Common Crawl pode incluir cerca de 50 bilh√µes de p√°ginas web, com um tamanho de armazenamento de v√°rias centenas de terabytes. Se cada p√°gina tiver em m√©dia 20KB de HTML, um *snapshot* de 50 bilh√µes de p√°ginas ocupar√° aproximadamente 1.000.000 TB. Cada *snapshot* √© coletado a cada poucos meses, sendo um recurso essencial para o desenvolvimento de modelos de linguagem.

**Proposi√ß√£o 1:** A frequ√™ncia com que novos *snapshots* s√£o coletados e disponibilizados √© crucial para garantir que os LLMs sejam treinados em dados atualizados e representativos da web.
**Prova da Proposi√ß√£o 1:**
I. A internet est√° em constante mudan√ßa, com novas p√°ginas e conte√∫dos sendo criados e alterados a cada instante.
II. *Snapshots* mais antigos podem n√£o refletir com precis√£o a estrutura e o conte√∫do da web no momento atual, resultando em modelos com conhecimento desatualizado ou tendencioso.
III. *Snapshots* frequentes garantem que os LLMs sejam treinados em dados mais recentes, refletindo o estado atual da informa√ß√£o e da linguagem na internet.
IV. Dados atualizados ajudam os modelos a entender as nuances mais recentes da linguagem e a ter conhecimento sobre temas emergentes, resultando em modelos mais eficazes e atuais.
Portanto, a frequ√™ncia de coleta dos *snapshots* √© crucial para que os LLMs sejam treinados em dados atualizados e representativos. ‚ñ†
**Proposi√ß√£o 1.1:** *A frequ√™ncia ideal de coleta de snapshots deve equilibrar a necessidade de dados atualizados com os custos computacionais e de armazenamento associados ao processamento de grandes volumes de dados.*
  **Prova da Proposi√ß√£o 1.1:**
  I. Coletas mais frequentes garantem dados mais recentes, mas tamb√©m exigem mais recursos computacionais para processamento e mais espa√ßo de armazenamento para os snapshots.
  II. Coletas menos frequentes reduzem os custos, mas podem levar a modelos treinados com dados desatualizados, afetando seu desempenho em tarefas que exigem conhecimento atual.
  III. A escolha da frequ√™ncia ideal depende do tipo de LLM, da tarefa a que se destina e dos recursos dispon√≠veis.
  IV. √â necess√°rio um equil√≠brio para otimizar a rela√ß√£o entre a qualidade dos dados e a viabilidade pr√°tica da coleta e processamento de snapshots.
  Portanto, a frequ√™ncia ideal de coleta de snapshots deve considerar a necessidade de dados atualizados e os custos computacionais associados. ‚ñ†

#### Como o Common Crawl √© Constru√≠do?
O processo de constru√ß√£o do Common Crawl envolve diversas etapas, desde o *crawling* at√© a disponibiliza√ß√£o dos dados:
*   **Crawling:** O processo come√ßa com um *crawler* web que segue links a partir de uma lista inicial de URLs (a *seed list*). O *crawler* visita as p√°ginas web, armazena o conte√∫do HTML e os links encontrados em cada p√°gina.
> üí° **Exemplo Num√©rico:** Um *crawler* do Common Crawl pode visitar e armazenar cerca de 1 milh√£o de p√°ginas web por hora, seguindo links de diversas origens, incluindo a *seed list* inicial e links encontrados em p√°ginas j√° coletadas. Um cen√°rio comum √© come√ßar com uma *seed list* contendo 10.000 URLs e, a partir da√≠, explorar em m√©dia 20 links por p√°gina, expandindo rapidamente a base de dados. Ap√≥s uma hora, cerca de 1 milh√£o de p√°ginas foram visitadas, e a cada nova p√°gina visitada, mais links s√£o adicionados √† fila, mantendo o processo ativo.
    **Lema 1.1:** *A escolha da *seed list* e a estrat√©gia de *crawling* influenciam significativamente a diversidade e a representatividade do Common Crawl, exigindo cuidado para evitar vieses e maximizar a cobertura da web.*
    **Prova do Lema 1.1:**
    I. A *seed list* define os pontos de partida do *crawler*, e se esta lista for enviesada, o resultado do *crawl* tamb√©m ser√°.
    II. Uma *seed list* muito focada em certas regi√µes geogr√°ficas, tipos de websites ou assuntos, pode resultar em um *corpus* que n√£o representa a diversidade da internet.
    III. A estrat√©gia de *crawling* define como o *crawler* navega entre os links. Estrat√©gias que priorizam certos tipos de links ou regi√µes da web podem resultar em um *corpus* com cobertura incompleta.
    IV. A escolha da *seed list* e da estrat√©gia de *crawling* devem ser feitas de forma criteriosa para maximizar a cobertura da web e minimizar a introdu√ß√£o de vieses, garantindo a representatividade do Common Crawl.
    Portanto, a escolha da *seed list* e a estrat√©gia de *crawling* influenciam a diversidade e a representatividade do Common Crawl. ‚ñ†
    **Lema 1.2:** *O uso de uma *seed list* diversificada e uma estrat√©gia de *crawling* que equilibra profundidade e amplitude na explora√ß√£o da web s√£o fundamentais para mitigar vieses e maximizar a cobertura do Common Crawl.*
    **Prova do Lema 1.2:**
        I. Uma *seed list* diversificada, que inclua URLs de diferentes regi√µes, tipos de sites e temas, garante que o *crawler* explore uma variedade maior de conte√∫do na web.
        II. Uma estrat√©gia de *crawling* que alterna entre explorar links em profundidade (seguindo links em sequ√™ncia) e em amplitude (explorando v√°rios links simultaneamente) aumenta a diversidade e a cobertura do *crawl*.
        III. Um equil√≠brio entre profundidade e amplitude evita que o *crawler* fique preso em regi√µes espec√≠ficas da web ou ignore outras.
        IV. Ao diversificar a *seed list* e equilibrar a estrat√©gia de *crawling*, √© poss√≠vel mitigar vieses e maximizar a representatividade do Common Crawl, resultando em um corpus mais √∫til para treinamento de LLMs.
    Portanto, diversificar a *seed list* e equilibrar a estrat√©gia de *crawling* s√£o essenciais para mitigar vieses e maximizar a cobertura do Common Crawl. ‚ñ†

*   **Armazenamento:** O conte√∫do HTML de cada p√°gina e seus metadados s√£o armazenados em um formato espec√≠fico, geralmente em arquivos WARC (Web ARChive), que facilita o processamento e an√°lise posterior.
> üí° **Exemplo Num√©rico:** Os arquivos WARC podem ser armazenados em um sistema de arquivos distribu√≠dos, como o Hadoop Distributed File System (HDFS), com um tamanho total de centenas de terabytes. Um √∫nico *snapshot* do Common Crawl pode ocupar at√© 500 TB. Se cada arquivo WARC tiver em m√©dia 1GB, um *snapshot* de 500TB resultaria em 500,000 arquivos WARC. Esses arquivos s√£o frequentemente compactados para otimizar o armazenamento.
*   **Disponibiliza√ß√£o:** Os *snapshots* s√£o disponibilizados publicamente para download, geralmente atrav√©s de servi√ßos de armazenamento em nuvem, como o Amazon S3 ou o Google Cloud Storage, com instru√ß√µes detalhadas para acesso e utiliza√ß√£o.
*   **Dados Complementares:** Al√©m do HTML e metadados, o Common Crawl tamb√©m fornece informa√ß√µes adicionais como arquivos JSON contendo informa√ß√µes estruturadas extra√≠das das p√°ginas e outros dados relevantes.

#### Desafios do Uso do Common Crawl
Embora o Common Crawl seja uma fonte valiosa para o treinamento de LLMs, ele tamb√©m apresenta desafios que devem ser abordados:
*   **Ru√≠do:** O Common Crawl cont√©m uma grande quantidade de conte√∫do de baixa qualidade, como *spam*, p√°ginas de baixa relev√¢ncia, conte√∫do duplicado, c√≥digo *HTML* excessivo, e outros tipos de ru√≠do [^26].
    > üí° **Exemplo Num√©rico:** Em um *snapshot* t√≠pico do Common Crawl, cerca de 20% do conte√∫do pode ser considerado ru√≠do. Esse n√∫mero representa uma parte significativa dos dados que deve ser removida atrav√©s de filtragens. A filtragem precisa detectar e remover esse ru√≠do, mantendo apenas o texto relevante e √∫til. Por exemplo, se um *snapshot* cont√©m 50 bilh√µes de p√°ginas, cerca de 10 bilh√µes de p√°ginas podem ser classificadas como ru√≠do. Em termos de tamanho, se cada p√°gina tem em m√©dia 20KB, 200TB de dados seriam ru√≠do que deve ser filtrado.
  **Teorema 1.2:** A presen√ßa de ru√≠do no Common Crawl pode diminuir a efic√°cia do treinamento, levando a modelos com dificuldades em aprender padr√µes lingu√≠sticos importantes e com maior propens√£o a generalizar de forma errada.
    **Prova do Teorema 1.2:**
    I. Ru√≠dos como *spam*, *HTML* e texto irrelevante s√£o exemplos de dados que n√£o contribuem para a aprendizagem da linguagem natural.
    II. Modelos treinados com dados ruidosos podem aprender padr√µes errados e reproduzir conte√∫dos irrelevantes ou incoerentes.
    III. A presen√ßa de ru√≠do nos dados de treino pode levar o modelo a focar em padr√µes n√£o relevantes, em detrimento de padr√µes lingu√≠sticos significativos.
    IV. Ao diminuir a qualidade dos dados, o ru√≠do tamb√©m reduz a capacidade do modelo de aprender padr√µes gerais, diminuindo a qualidade geral do modelo.
    Portanto, a presen√ßa de ru√≠do diminui a efic√°cia do treinamento, afetando a capacidade do modelo de aprender padr√µes importantes e generalizar corretamente. ‚ñ†
    **Teorema 1.3:** *A aplica√ß√£o de t√©cnicas de filtragem robustas para remover ru√≠do √© um passo crucial para garantir que os LLMs sejam treinados com dados de alta qualidade e aprendam padr√µes lingu√≠sticos precisos.*
     **Prova do Teorema 1.3:**
        I. T√©cnicas de filtragem identificam e removem dados de baixa qualidade, como *spam*, c√≥digo *HTML* excessivo e outros ru√≠dos, antes do treinamento do LLM.
        II. Ao remover ru√≠dos, o modelo se concentra em dados de texto mais relevantes e significativos, o que facilita a aprendizagem de padr√µes lingu√≠sticos importantes.
        III. Dados de alta qualidade levam a modelos com maior precis√£o na compreens√£o e gera√ß√£o de linguagem natural, reduzindo a propens√£o a generalizar de forma errada ou reproduzir conte√∫do irrelevante.
        IV. T√©cnicas de filtragem eficazes melhoram a qualidade do corpus de treinamento e, consequentemente, a qualidade do LLM.
    Portanto, a aplica√ß√£o de t√©cnicas de filtragem robustas √© essencial para o treinamento de LLMs com dados de alta qualidade. ‚ñ†
*   **Vieses:** O *crawling* da web pode, inadvertidamente, capturar vieses presentes na internet, como vieses geogr√°ficos, demogr√°ficos, de g√™nero e outros [^28]. Esses vieses podem se refletir nos modelos treinados com o Common Crawl.
> üí° **Exemplo Num√©rico:** Se a *seed list* do *crawler* incluir poucos sites de um determinado pa√≠s, as p√°ginas desse pa√≠s ser√£o sub-representadas no *corpus*, criando um vi√©s geogr√°fico. Se, por exemplo, a *seed list* incluir 80% de sites dos EUA, 10% de sites da Europa e apenas 1% de sites da √Åfrica, o *corpus* ter√° uma forte tend√™ncia a refletir a realidade dos EUA, com informa√ß√µes muito mais detalhadas e abundantes sobre essa regi√£o. Se um dado modelo for treinado com esses dados, ele poder√° ter um desempenho menor em contextos relacionados a este pa√≠s, gerando respostas menos precisas para outros locais.
    **Corol√°rio 1.1:** O *crawling* web pode, mesmo que de forma n√£o intencional, capturar vieses presentes na internet, que podem resultar em modelos enviesados.
      **Prova do Corol√°rio 1.1:**
    I. O processo de *crawling* parte de uma *seed list*, que por si s√≥ pode direcionar o *crawler* para determinadas regi√µes, estilos de escrita ou pontos de vista.
    II. A estrutura da web n√£o √© uniforme, havendo uma concentra√ß√£o de informa√ß√£o em certos sites e regi√µes, enquanto outras s√£o sub-representadas.
    III. Modelos treinados com corpora enviesados aprendem e reproduzem os vieses presentes nesses dados, podendo gerar resultados tendenciosos ou injustos.
    IV. √â crucial realizar uma an√°lise cuidadosa dos dados e aplicar t√©cnicas de balanceamento para mitigar vieses presentes no Common Crawl.
    Portanto, o *crawling* web pode capturar e propagar vieses que devem ser tratados no processo de treinamento de LLMs. ‚ñ†
    **Corol√°rio 1.2:** *A identifica√ß√£o e mitiga√ß√£o de vieses no Common Crawl s√£o etapas essenciais para o desenvolvimento de LLMs justos e imparciais, capazes de atender a diversas comunidades e contextos.*
        **Prova do Corol√°rio 1.2:**
        I. Vieses presentes no Common Crawl podem levar a modelos que reproduzem estere√≥tipos e tratam dados de diferentes grupos de forma desigual.
        II. T√©cnicas de an√°lise de dados e balanceamento podem ser utilizadas para identificar e mitigar esses vieses no corpus de treinamento.
        III. Modelos treinados com dados balanceados e livres de vieses tendem a ser mais justos e imparciais, garantindo que suas respostas e a√ß√µes sejam equitativas para todos os usu√°rios.
        IV. O esfor√ßo para mitigar vieses no Common Crawl contribui para a cria√ß√£o de LLMs √©ticos e socialmente respons√°veis.
    Portanto, a identifica√ß√£o e mitiga√ß√£o de vieses s√£o cruciais para o desenvolvimento de LLMs justos e imparciais. ‚ñ†
*  **Textos Duplicados:** O Common Crawl cont√©m muitas p√°ginas com conte√∫do id√™ntico ou similar, o que pode levar a uma super-representa√ß√£o de certas frases ou par√°grafos no *corpus* de treino.
   > üí° **Exemplo Num√©rico:** Em um *snapshot* do Common Crawl, cerca de 15% do conte√∫do pode ser duplicado, devido a *mirroring* de sites ou replica√ß√£o de texto entre diferentes p√°ginas. Se um modelo for treinado sem a remo√ß√£o de dados duplicados, esse modelo pode se concentrar em padr√µes repetitivos, afetando a capacidade de generaliza√ß√£o do modelo. Por exemplo, se um *snapshot* cont√©m 50 bilh√µes de p√°ginas, 7.5 bilh√µes delas podem ser duplicadas. Em termos de texto, se um trecho de 100 palavras √© repetido em 1 milh√£o de p√°ginas, o modelo ir√° super enfatizar esse trecho.
*  **Dados Desatualizados:** Embora o Common Crawl realize *snapshots* periodicamente, ainda existe um atraso entre a coleta dos dados e sua disponibiliza√ß√£o, fazendo com que alguns dados estejam desatualizados. Isso pode ser problem√°tico quando um modelo √© usado para tarefas que exigem informa√ß√µes em tempo real.
   > üí° **Exemplo Num√©rico:** Pode levar algumas semanas ou meses para um *snapshot* do Common Crawl ser coletado e disponibilizado. Durante esse tempo, muitas p√°ginas na internet podem ter mudado. Isso pode ser um problema para tarefas que necessitam de informa√ß√µes recentes como not√≠cias e fatos da atualidade. Por exemplo, uma not√≠cia sobre um evento que ocorreu em janeiro, pode n√£o estar presente em um *snapshot* coletado em novembro do ano anterior.
*  **Conte√∫do em Diversas L√≠nguas:** O Common Crawl captura p√°ginas da web em v√°rios idiomas, o que pode ser tanto uma vantagem quanto um desafio. Para modelos monol√≠ngues, √© necess√°rio filtrar o conte√∫do para se concentrar no idioma desejado [^26]. Para modelos multil√≠ngues, os dados precisam ser bem balanceados.
   > üí° **Exemplo Num√©rico:** Um *snapshot* do Common Crawl pode conter conte√∫do em mais de 100 idiomas. Para treinar um LLM especificamente em ingl√™s, √© necess√°rio filtrar os dados para garantir que ele seja treinado apenas com texto em ingl√™s. Para treinar um modelo multil√≠ngue, √© importante ter dados suficientes em cada idioma que o modelo ir√° suportar. Se um *snapshot* cont√©m 100 TB de dados, e um modelo multil√≠ngue deve suportar 5 idiomas, √© ideal que cada idioma tenha aproximadamente 20 TB de dados para um treinamento balanceado.

#### Utiliza√ß√£o do Common Crawl no Treinamento de LLMs
Apesar dos desafios, o Common Crawl √© uma fonte essencial para o treinamento de LLMs, devido √† sua grande escala e diversidade de conte√∫do. No entanto, para utiliz√°-lo eficazmente, √© necess√°rio aplicar t√©cnicas de processamento e filtragem para reduzir o ru√≠do, mitigar os vieses e garantir a qualidade dos dados. Estas incluem:
*   **Filtragem e Limpeza:** Remo√ß√£o de *spam*, *HTML*, *JavaScript*, textos duplicados, e outros elementos indesejados [^26]. O *corpus* C4 √© um exemplo de um conjunto de dados processado a partir do Common Crawl.
*   **Deduplica√ß√£o:** Remo√ß√£o de conte√∫do duplicado para evitar uma influ√™ncia excessiva de certos textos no treinamento [^26].
*   **Balanceamento:** Em modelos multil√≠ngues, o *corpus* √© balanceado para garantir que cada idioma seja adequadamente representado.
*   **Segmenta√ß√£o:** Extra√ß√£o de trechos de texto relevantes, como par√°grafos ou senten√ßas, que s√£o mais adequados para o treinamento de modelos de linguagem.
* **Extra√ß√£o de Metadados:** A extra√ß√£o de informa√ß√µes relevantes das p√°ginas, como t√≠tulos, *headers*, e links, pode enriquecer os dados de treinamento e melhorar o desempenho dos modelos.
> üí° **Exemplo Num√©rico:** Em um processo de limpeza, dados do Common Crawl podem passar por diversas etapas: em um primeiro momento, cerca de 20% dos dados podem ser descartados devido a seu baixo valor ou conte√∫do como *spam*; ap√≥s uma etapa de deduplica√ß√£o, pode haver uma redu√ß√£o de 15% do volume restante, eliminando-se repeti√ß√µes de texto; depois, outras etapas como filtragem por idioma e remo√ß√£o de conte√∫do ofensivo podem reduzir ainda mais o volume, mas garantem a qualidade e seguran√ßa do dataset final. Por exemplo, em um dataset inicial de 100TB, ap√≥s a remo√ß√£o de *spam* e dados de baixa qualidade, restar√£o 80TB. Ap√≥s a deduplica√ß√£o, restar√£o 68TB. Ap√≥s filtragem por idioma, e remo√ß√£o de conte√∫do ofensivo, podem restar 50TB. O objetivo √© reduzir o volume, mas aumentar a qualidade dos dados restantes.
**Proposi√ß√£o 2:** *O uso eficiente do Common Crawl no treinamento de LLMs requer um conjunto de t√©cnicas de processamento e filtragem, que, se bem implementadas, resultam em um corpus de alta qualidade, capaz de gerar modelos mais eficazes e menos enviesados.*
  **Prova da Proposi√ß√£o 2:**
    I. O Common Crawl cont√©m uma grande quantidade de dados √∫teis, mas esses dados est√£o misturados com ru√≠dos e outros problemas.
    II. T√©cnicas de processamento e filtragem removem dados indesejados e reduzem o ru√≠do, concentrando a aten√ß√£o do LLM em dados relevantes para a linguagem natural.
    III. A remo√ß√£o de conte√∫do duplicado, *spam*, c√≥digo e textos ofensivos melhora a qualidade e a seguran√ßa do treinamento.
    IV. Ao balancear dados multil√≠ngues e extrair informa√ß√µes importantes, √© poss√≠vel obter um corpus mais adequado para o treinamento, favorecendo a cria√ß√£o de modelos mais eficientes.
    Portanto, o uso eficiente do Common Crawl exige t√©cnicas de processamento que, se bem implementadas, resultam em modelos mais eficazes e menos enviesados. ‚ñ†
    **Proposi√ß√£o 2.1:** *A escolha das t√©cnicas de processamento e filtragem deve ser adaptada √†s caracter√≠sticas espec√≠ficas do LLM a ser treinado e √† natureza das tarefas que ele realizar√°.*
    **Prova da Proposi√ß√£o 2.1:**
      I. LLMs treinados para tarefas diferentes podem ter requisitos distintos em termos de qualidade e tipo de dados de treinamento.
      II. T√©cnicas de filtragem mais agressivas podem ser adequadas para LLMs que precisam de dados muito limpos e precisos, enquanto t√©cnicas mais leves podem ser usadas para modelos que precisam de um espectro mais amplo de dados.
      III. A escolha das t√©cnicas de processamento deve levar em conta a necessidade de equilibrar a qualidade dos dados com o volume, evitando a perda excessiva de informa√ß√µes importantes.
      IV. A adapta√ß√£o das t√©cnicas de processamento e filtragem aos requisitos do LLM garante que ele seja treinado com um corpus ideal para suas tarefas espec√≠ficas.
    Portanto, a escolha das t√©cnicas de processamento e filtragem deve ser adaptada √†s caracter√≠sticas espec√≠ficas do LLM e √†s tarefas que ele realizar√°. ‚ñ†

### Conclus√£o

O Common Crawl √© um recurso valioso para o treinamento de LLMs, oferecendo uma fonte rica e diversificada de dados da web [^26]. No entanto, seu uso eficaz requer uma compreens√£o cuidadosa de seus desafios e a aplica√ß√£o de t√©cnicas de processamento e filtragem apropriadas. A disponibilidade do Common Crawl como fonte de dados p√∫blica tem sido crucial para o avan√ßo da pesquisa e desenvolvimento em LLMs, tornando poss√≠vel a cria√ß√£o de modelos de linguagem cada vez mais poderosos e eficientes. Nos cap√≠tulos seguintes, abordaremos como os modelos treinados com esses dados s√£o usados em diversas aplica√ß√µes e como podemos avaliar seus resultados.

### Refer√™ncias
[^26]: Jurafsky, Daniel e James H. Martin. *Speech and Language Processing*. Draft de 3 de Fevereiro de 2024. Cap√≠tulo 10.
[^28]: Jurafsky, Daniel e James H. Martin. *Speech and Language Processing*. Draft de 3 de Fevereiro de 2024. Cap√≠tulo 10.
<!-- END -->
