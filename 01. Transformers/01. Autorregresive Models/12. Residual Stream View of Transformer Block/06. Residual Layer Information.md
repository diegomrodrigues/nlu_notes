## As Contribui√ß√µes das Camadas Feedforward e Normaliza√ß√£o no Fluxo Residual do Transformer

### Introdu√ß√£o
Este cap√≠tulo aprofunda a nossa an√°lise do **fluxo residual** (*residual stream*) no bloco *transformer*, com foco nas contribui√ß√µes das camadas *feedforward* (*FFN*) e normaliza√ß√£o (*LayerNorm*) [^13]. Expandindo sobre os cap√≠tulos anteriores que estabeleceram os fundamentos do fluxo residual e o papel da autoaten√ß√£o na integra√ß√£o contextual [^1, ^6, ^7, ^12, ^13], exploraremos como estas camadas, em conjunto com a autoaten√ß√£o e conex√µes residuais, moldam a representa√ß√£o de cada *token* ao longo do *residual stream*. Especificamente, analisaremos como a rede *FFN* adiciona diferentes vis√µes √† representa√ß√£o do *token*, enquanto as camadas *LayerNorm* garantem a estabilidade do treinamento e normaliza√ß√£o dos vetores que representam os *tokens*. Entendemos que enquanto a autoaten√ß√£o integra informa√ß√£o contextual, a rede *FFN* e a *LayerNorm* adicionam diferentes perspectivas ao fluxo da representa√ß√£o e estabilidade ao processo [^13].

### A Rede *Feedforward* e Novas Perspectivas no *Residual Stream*
Como vimos anteriormente, o *residual stream* √© um fluxo cont√≠nuo de vetores *d*-dimensionais que representam cada *token* ao longo das camadas do *transformer* [^13, Proposi√ß√£o 1]. A autoaten√ß√£o desempenha um papel fundamental na integra√ß√£o das informa√ß√µes contextuais. Contudo, outras camadas do bloco *transformer* tamb√©m contribuem para a transforma√ß√£o e enriquecimento do *embedding* do *token*. Em particular, a camada *feedforward* (*FFN*) adiciona uma vis√£o n√£o-linear da representa√ß√£o do *token*, que √© fundamental para a capacidade do *transformer* de modelar depend√™ncias complexas na linguagem [^13].

A camada *FFN* √© uma rede neural de duas camadas, aplicada de forma independente para cada *token* da sequ√™ncia. Ap√≥s a autoaten√ß√£o e a primeira conex√£o residual, a representa√ß√£o do *token* ($t^3$) passa pela camada *FFN*, que aplica uma transforma√ß√£o linear seguida de uma n√£o-linearidade, resultando em uma nova representa√ß√£o $t^4$ do *token*:
$$
t^4 = FFN(t^3)
$$
[10.35]
Essa opera√ß√£o adiciona uma perspectiva diferente da representa√ß√£o do *token*, enriquecendo o *residual stream*. Ao contr√°rio da autoaten√ß√£o que integra informa√ß√µes do contexto, a rede *FFN* opera sobre o *embedding* de cada *token* de forma independente, adicionando uma transforma√ß√£o n√£o-linear que √© essencial para a capacidade de representa√ß√£o do modelo.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um *token* com o *embedding* $t^3$ de dimens√£o $d=4$ que chega √† camada *FFN* ap√≥s passar pela autoaten√ß√£o e pela primeira normaliza√ß√£o:
> $$t^3 = \begin{bmatrix} 0.8 \\ -0.2 \\ 0.5 \\ 0.2 \end{bmatrix}$$
> A camada *FFN* transforma esse vetor em um novo vetor $t^4$ com a mesma dimensionalidade, mas com uma perspectiva n√£o linear. Para fins ilustrativos, vamos supor que a *FFN* aplica uma transforma√ß√£o linear seguida por uma fun√ß√£o de ativa√ß√£o ReLU.  Considere que a primeira camada linear da FFN transforma $t^3$ em um vetor $z$ de dimens√£o $d_{ff} = 8$ e a segunda camada linear transforma esse $z$ de volta para um vetor da mesma dimens√£o $d=4$. As matrizes de peso e bias s√£o aprendidas durante o treinamento do modelo.
>
>  Suponha que ap√≥s a transforma√ß√£o linear da primeira camada e a aplica√ß√£o do ReLU, o vetor intermedi√°rio $z$ seja:
>  $$z = \begin{bmatrix} 0.9 \\ 0.0 \\ 1.2 \\ 0.7 \\ 0.1 \\ 0.6 \\ 0.0 \\ 0.4 \end{bmatrix}$$
>  E que a segunda camada linear da FFN resulta em:
> $$t^4 = FFN(t^3) = \begin{bmatrix} 0.1 \\ -0.4 \\ 0.2 \\ -0.1 \end{bmatrix}$$
>
> Observe que tanto $t^3$ quanto $t^4$ mant√©m a dimens√£o $d=4$, conforme o conceito do fluxo residual. A opera√ß√£o da FFN introduz n√£o-linearidades na representa√ß√£o do *token*, permitindo ao modelo aprender rela√ß√µes complexas nos dados.
> ```mermaid
> graph LR
>     A[t3] --> B(Linear Layer 1)
>     B --> C(ReLU)
>     C --> D[z]
>     D --> E(Linear Layer 2)
>     E --> F[t4]
> ```
>

**Proposi√ß√£o 1.1** (Invari√¢ncia da Dimensionalidade da *FFN*): A camada *FFN* preserva a dimensionalidade do *embedding* do *token*, ou seja, se a entrada $t^3$ tem dimens√£o *d*, a sa√≠da $t^4 = FFN(t^3)$ tamb√©m ter√° dimens√£o *d*.
*Prova:*
I. A camada *FFN* √© composta por duas transforma√ß√µes lineares com ativa√ß√£o n√£o-linear entre elas. Cada transforma√ß√£o linear √© definida por uma matriz de pesos $W$ que mapeia um vetor de dimens√£o *d* para um vetor de dimens√£o *d_ff*, e posteriormente de *d_ff* para *d*.
II. Se a dimens√£o de entrada √© *d*, a primeira transforma√ß√£o linear projeta para um espa√ßo de dimens√£o *d_ff*, e a segunda transforma√ß√£o linear projeta de volta para um espa√ßo de dimens√£o *d*.
III. Portanto, a camada *FFN* mapeia um vetor de dimens√£o *d* para outro vetor de dimens√£o *d*.
‚ñ†

**As Camadas de Normaliza√ß√£o (*LayerNorm*) e a Estabiliza√ß√£o do *Residual Stream***
As camadas de normaliza√ß√£o, e particularmente a *LayerNorm*, s√£o cruciais para a estabiliza√ß√£o do fluxo residual e para o bom funcionamento do *transformer*. Existem duas formas de aplicar o LayerNorm: *pre-norm* e *post-norm* [^13]. Na arquitetura *post-norm*, como vimos anteriormente, o *LayerNorm* √© aplicado ap√≥s as opera√ß√µes de autoaten√ß√£o e *FFN*. J√° na arquitetura *pre-norm*, o *LayerNorm* √© aplicado antes dessas opera√ß√µes [^13]. Em ambos os casos, a *LayerNorm* garante que as ativa√ß√µes em cada camada tenham uma m√©dia pr√≥xima de zero e desvio padr√£o pr√≥ximo de um, estabilizando o treinamento do modelo e facilitando a propaga√ß√£o do gradiente.

Na arquitetura *post-norm*, o primeiro *LayerNorm* √© aplicado ao *embedding* $t^2$ (resultado da autoaten√ß√£o e da primeira conex√£o residual):
$$
t^3 = LayerNorm(t^2)
$$
[10.34]
E o segundo *LayerNorm* √© aplicado ao *embedding* $t^5$ (resultado da segunda adi√ß√£o residual):
$$
h_i = LayerNorm(t^5)
$$
[10.37]
Enquanto que na arquitetura *pre-norm* a normaliza√ß√£o √© aplicada antes das opera√ß√µes de *MultiHeadAttention* e *FFN*, como j√° demonstrado anteriormente [Lema 1.1]. Em ambas as arquiteturas, a normaliza√ß√£o de camada preserva a dimensionalidade, atuando como um mecanismo que estabiliza o fluxo residual e garante que as representa√ß√µes n√£o se tornem muito grandes ou muito pequenas.

> üí° **Exemplo Num√©rico:**
> Para exemplificar o efeito do *LayerNorm*, considere um vetor $t^2 = \begin{bmatrix} 1.2 \\ 0.4 \\ 0.1 \\ 0.9 \end{bmatrix}$. O *LayerNorm* centraliza e normaliza o vetor. Vamos calcular a m√©dia ($\mu$) e o desvio padr√£o ($\sigma$) do vetor $t^2$:
>
> $\mu = \frac{1.2 + 0.4 + 0.1 + 0.9}{4} = 0.65$
>
> $\sigma = \sqrt{\frac{(1.2-0.65)^2 + (0.4-0.65)^2 + (0.1-0.65)^2 + (0.9-0.65)^2}{4}} \approx 0.44$
>
> O *LayerNorm* normaliza o vetor subtraindo a m√©dia e dividindo pelo desvio padr√£o, resultando em um vetor $t^3_{norm}$:
>
> $t^3_{norm} = \begin{bmatrix} \frac{1.2 - 0.65}{0.44} \\ \frac{0.4 - 0.65}{0.44} \\ \frac{0.1 - 0.65}{0.44} \\ \frac{0.9 - 0.65}{0.44} \end{bmatrix} \approx \begin{bmatrix} 1.25 \\ -0.57 \\ -1.25 \\ 0.57 \end{bmatrix}$
>
> Note que, em um cen√°rio real, o *LayerNorm* tamb√©m inclui par√¢metros aprend√≠veis de ganho ($\gamma$) e bias ($\beta$). A sa√≠da final da LayerNorm √© ent√£o dada por $t^3 = \gamma \times t^3_{norm} + \beta$. Para simplificar o exemplo, vamos assumir que $\gamma$ e $\beta$ s√£o pr√≥ximos de 1 e 0, respectivamente:
>  $$t^3 = LayerNorm(t^2) \approx \begin{bmatrix} 0.8 \\ -0.2 \\ 0.5 \\ 0.2 \end{bmatrix}$$
>
> Note que tanto $t^2$ quanto $t^3$ mant√©m a mesma dimensionalidade $d=4$. A *LayerNorm* garante que os valores estejam na escala adequada para que o treinamento seja mais eficaz, prevenindo o problema da explos√£o ou desaparecimento do gradiente.

**Proposi√ß√£o 2.1** (Invari√¢ncia da Dimensionalidade da *LayerNorm*): A camada *LayerNorm* preserva a dimensionalidade do *embedding* do *token*, ou seja, se a entrada $t^2$ tem dimens√£o *d*, a sa√≠da $t^3 = LayerNorm(t^2)$ tamb√©m ter√° dimens√£o *d*.
*Prova:*
I. A camada *LayerNorm* realiza a normaliza√ß√£o de cada vetor individualmente, sem alterar sua dimens√£o.
II. A normaliza√ß√£o √© feita subtraindo a m√©dia e dividindo pelo desvio padr√£o, o que n√£o altera o n√∫mero de componentes do vetor.
III. Portanto, a camada *LayerNorm* mapeia um vetor de dimens√£o *d* para outro vetor de dimens√£o *d*.
‚ñ†

**O Papel das Conex√µes Residuais**
As conex√µes residuais desempenham um papel crucial na manuten√ß√£o do fluxo de informa√ß√£o ao longo do *residual stream* [^13]. Elas permitem que informa√ß√µes das camadas anteriores sejam passadas diretamente para as camadas seguintes, evitando o desaparecimento do gradiente e facilitando o treinamento de modelos mais profundos. Como j√° vimos, as conex√µes residuais s√£o aplicadas ap√≥s a autoaten√ß√£o e a rede *FFN*:
$$
t^2 = t^1 + x_i
$$
[10.33]
$$
t^5 = t^4 + t^3
$$
[10.36]
Estas conex√µes residuais garantem que as representa√ß√µes originais dos *tokens* sejam preservadas e combinadas com as novas informa√ß√µes contextuais e n√£o lineares.

**A Integra√ß√£o das Diferentes Camadas no Fluxo Residual**
O fluxo residual, portanto, √© o resultado da integra√ß√£o das diferentes camadas do bloco *transformer*: a autoaten√ß√£o, que integra informa√ß√µes contextuais; a rede *FFN*, que adiciona uma vis√£o n√£o-linear da representa√ß√£o; e a *LayerNorm*, que estabiliza o fluxo e garante que os valores dos vetores sejam normalizados. As conex√µes residuais, por sua vez, garantem a propaga√ß√£o das informa√ß√µes ao longo do modelo. Em conjunto, estas camadas transformam o *embedding* inicial de cada *token*, enriquecendo-o com informa√ß√µes contextuais e com diferentes perspectivas do seu significado e uso.

> üí° **Exemplo Num√©rico:**
> Retomando nosso exemplo, consideremos que o *embedding* inicial de um *token* seja $x_i = \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix}$. Ap√≥s a autoaten√ß√£o, obtemos $t^1 = \begin{bmatrix} 0.5 \\ 0.0 \\ 0.2 \\ 0.1 \end{bmatrix}$, e a adi√ß√£o residual resulta em $t^2 = t^1 + x_i = \begin{bmatrix} 0.7 \\ 0.1 \\ -0.1 \\ 0.5 \end{bmatrix}$. Em seguida, a LayerNorm √© aplicada:
> $$t^3 = LayerNorm(t^2) \approx \begin{bmatrix} 0.8 \\ -0.2 \\ 0.5 \\ 0.2 \end{bmatrix}$$
> e a rede FFN aplica uma transforma√ß√£o n√£o-linear:
> $$t^4 = FFN(t^3) = \begin{bmatrix} 0.1 \\ -0.4 \\ 0.2 \\ -0.1 \end{bmatrix}$$
> A segunda adi√ß√£o residual combina $t^3$ e $t^4$:
> $$t^5 = t^4 + t^3 = \begin{bmatrix} 0.9 \\ -0.6 \\ 0.7 \\ 0.1 \end{bmatrix}$$
> Finalmente, a √∫ltima normaliza√ß√£o de camada produz a sa√≠da do bloco *transformer*:
> $$h_i = LayerNorm(t^5) \approx \begin{bmatrix} 0.4 \\ -0.8 \\ 1.2 \\ -0.2 \end{bmatrix}$$
>
> O diagrama abaixo ilustra o fluxo das diferentes camadas:
> ```mermaid
> graph LR
>     A[xi] --> B(MultiHeadAttention);
>     C(Contextos) --> B
>     B --> D[t1]
>     A --> E[t2=x+t1]
>     D --> E
>     E --> F(LayerNorm)
>     F --> G[t3]
>    G --> H(FFN)
>    H --> I[t4]
>    G --> J[t5=t3+t4]
>    I --> J
>   J --> K(LayerNorm)
>   K --> L[hi]
> ```

**Teorema 1** (A Contribui√ß√£o da *FFN* e *LayerNorm* para o *Residual Stream*): A rede *FFN* e a *LayerNorm* desempenham pap√©is complementares na transforma√ß√£o do *residual stream*. Enquanto a *FFN* adiciona uma perspectiva n√£o-linear √† representa√ß√£o do *token*, o *LayerNorm* garante a estabilidade do fluxo e a normaliza√ß√£o dos vetores.

*Prova:*
I. A rede *FFN*, definida como $t^4 = FFN(t^3)$, aplica uma transforma√ß√£o n√£o linear ao vetor $t^3$, que √© resultado da autoaten√ß√£o e da primeira normaliza√ß√£o, adicionando uma perspectiva diferente √† representa√ß√£o do *token*.
II. A *LayerNorm*, definida como $t^3 = LayerNorm(t^2)$ e $h_i = LayerNorm(t^5)$, normaliza os vetores $t^2$ e $t^5$, garantindo que suas componentes tenham m√©dia pr√≥xima a zero e desvio padr√£o pr√≥ximo a um, o que estabiliza o fluxo e facilita a propaga√ß√£o do gradiente.
III. A combina√ß√£o da a√ß√£o da *FFN*, que adiciona n√£o-linearidade, e do *LayerNorm*, que garante a estabilidade, possibilita que o *transformer* aprenda representa√ß√µes mais robustas e complexas.
IV. Portanto, a *FFN* e o *LayerNorm* s√£o componentes essenciais do *residual stream* que, em conjunto com a autoaten√ß√£o e as conex√µes residuais, contribuem para o bom desempenho do *transformer*.
‚ñ†

**Teorema 2** (O Papel das Conex√µes Residuais na Preserva√ß√£o da Informa√ß√£o): As conex√µes residuais garantem a propaga√ß√£o da informa√ß√£o ao longo do *residual stream*, preservando a informa√ß√£o original dos *embeddings* e facilitando o treinamento do modelo.

*Prova:*
I. As conex√µes residuais, definidas por $t^2 = t^1 + x_i$ e $t^5 = t^4 + t^3$, adicionam a sa√≠da de cada camada ao *embedding* da camada anterior, permitindo que as informa√ß√µes de cada camada sejam propagadas para as camadas subsequentes.
II. A combina√ß√£o da informa√ß√£o de diferentes camadas atrav√©s das conex√µes residuais possibilita que o modelo aprenda representa√ß√µes mais complexas e que a informa√ß√£o original dos *embeddings* seja preservada ao longo da transforma√ß√£o.
III. Portanto, as conex√µes residuais s√£o um componente crucial do fluxo residual, que contribuem para o bom desempenho do *transformer* e a capacidade de aprender representa√ß√µes complexas.
‚ñ†
**Corol√°rio 2.1** (Conex√µes Residuais e a Estabilidade do Gradiente): As conex√µes residuais facilitam a propaga√ß√£o do gradiente durante o treinamento, evitando o desaparecimento do gradiente e melhorando a converg√™ncia do modelo.

*Prova:*
I. Conforme demonstrado no Teorema 2, as conex√µes residuais permitem que a informa√ß√£o flua diretamente de camadas anteriores para camadas posteriores, "saltando" as transforma√ß√µes que poderiam atenuar ou distorcer a informa√ß√£o original.
II. Essa passagem direta de informa√ß√£o tamb√©m impacta a propaga√ß√£o do gradiente durante o treinamento, facilitando a propaga√ß√£o do gradiente atrav√©s das camadas e evitando que ele desapare√ßa ao longo do *backpropagation*.
III. A preserva√ß√£o do gradiente √© essencial para o treinamento de modelos mais profundos, como os *transformers*, permitindo que os pesos do modelo sejam atualizados de forma eficiente e garantindo a converg√™ncia para um bom m√≠nimo local.
IV. Portanto, as conex√µes residuais s√£o cruciais para o treinamento eficaz dos *transformers* devido a sua capacidade de preservar a informa√ß√£o e facilitar a propaga√ß√£o do gradiente ao longo do modelo.
‚ñ†
**Corol√°rio 2.2** (A Preserva√ß√£o da Dimensionalidade): A propriedade de invari√¢ncia da dimensionalidade dos *embeddings* ao longo do *residual stream* (Proposi√ß√£o 1.1), garante que todas as opera√ß√µes, mesmo a adi√ß√£o residual, mantenham a dimensionalidade *d*, o que √© essencial para a arquitetura do *transformer*.
*Prova:*
I. A adi√ß√£o residual, definida como $t^2 = t^1 + x_i$ e $t^5 = t^4 + t^3$, adiciona dois vetores com dimensionalidade *d*, resultando em vetores tamb√©m de dimensionalidade *d*.
II. Como demonstrado na Proposi√ß√£o 1.1 e Proposi√ß√£o 2.1, tanto na arquitetura *post-norm* quanto na *pre-norm*, todas as opera√ß√µes aplicadas ao *residual stream* preservam a dimensionalidade *d*.
III. Portanto, as conex√µes residuais n√£o apenas garantem a propaga√ß√£o da informa√ß√£o, mas tamb√©m preservam a dimensionalidade do *embedding*, o que √© essencial para o bom funcionamento do modelo.
‚ñ†
**Observa√ß√£o 1** (Rela√ß√£o entre *LayerNorm*, *FFN* e a Modula√ß√£o do *Residual Stream*): A *LayerNorm*, ao normalizar o *embedding* ap√≥s a autoaten√ß√£o e a rede *FFN*, contribui para um fluxo mais est√°vel e controlado, enquanto a rede *FFN*, ao aplicar uma transforma√ß√£o n√£o linear ao *embedding* de cada *token*, permite que o modelo aprenda rela√ß√µes mais complexas entre os *tokens*. O *LayerNorm* atua como um regulador que impede que os valores dos vetores do *residual stream* fujam de uma escala adequada, e a *FFN* como um componente de enriquecimento da representa√ß√£o.

**Lema 3.1** (A N√£o-Linearidade da FFN e a Capacidade de Modelagem): A introdu√ß√£o de n√£o-linearidade pela camada FFN, atrav√©s da fun√ß√£o de ativa√ß√£o (e.g. ReLU, GeLU), √© fundamental para a capacidade do *transformer* de modelar rela√ß√µes complexas entre os *tokens*.
*Prova:*
I.  A camada FFN consiste em duas transforma√ß√µes lineares separadas por uma fun√ß√£o de ativa√ß√£o n√£o linear. Sem essa fun√ß√£o de ativa√ß√£o, a FFN se reduziria a uma √∫nica transforma√ß√£o linear, limitada na modelagem de rela√ß√µes complexas.
II. A n√£o-linearidade permite que o modelo aprenda uma variedade maior de padr√µes e depend√™ncias nos dados, como intera√ß√µes n√£o-lineares entre as palavras.
III.  Em modelos baseados em redes neurais, fun√ß√µes n√£o lineares s√£o essenciais para a aproxima√ß√£o de fun√ß√µes complexas. Portanto, a fun√ß√£o de ativa√ß√£o na FFN garante que o modelo possa aprender representa√ß√µes mais ricas e expressivas.
‚ñ†

### Conclus√£o
Neste cap√≠tulo, analisamos como as camadas *feedforward* e de normaliza√ß√£o, juntamente com as conex√µes residuais, moldam a representa√ß√£o de cada *token* ao longo do *residual stream*. Enquanto a autoaten√ß√£o integra informa√ß√µes contextuais, a rede *FFN* adiciona novas perspectivas √† representa√ß√£o do *token*, e as camadas *LayerNorm* garantem a estabilidade do fluxo e a normaliza√ß√£o dos vetores. As conex√µes residuais, por sua vez, asseguram a propaga√ß√£o da informa√ß√£o original atrav√©s do modelo, facilitando o treinamento. Em conjunto, estas camadas transformam o *embedding* inicial de cada *token*, enriquecendo-o com informa√ß√µes contextuais e n√£o lineares, e garantindo que a informa√ß√£o flua de forma est√°vel e controlada. Compreender as contribui√ß√µes dessas diferentes camadas para o fluxo residual √© essencial para analisar o comportamento dos *transformers* e para o desenvolvimento de modelos mais avan√ßados.

### Refer√™ncias
[^1]: *‚ÄúIn this chapter we formalize this idea of pretraining‚Äîlearning knowledge about language and the world from vast amounts of text‚Äîand call the resulting pretrained language models large language models.‚Äù*
[^6]: *‚ÄúTo capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value.‚Äù*
[^7]: *‚ÄúThis description of the self-attention process has been from the perspective of computing a single output at a single time step i.‚Äù*
[^12]: *‚ÄúThe previous sections viewed the transformer block as applied to the entire N-token input X of shape [N √ó d], producing an output also of shape [N √ó d].‚Äù*
[^13]: *‚Äúthese layers as a stream of d-dimensional representations, called the residual stream and visualized in Fig. 10.7.‚Äù*
<!-- END -->
