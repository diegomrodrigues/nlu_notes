## As Contribuições das Camadas Feedforward e Normalização no Fluxo Residual do Transformer

### Introdução
Este capítulo aprofunda a nossa análise do **fluxo residual** (*residual stream*) no bloco *transformer*, com foco nas contribuições das camadas *feedforward* (*FFN*) e normalização (*LayerNorm*) [^13]. Expandindo sobre os capítulos anteriores que estabeleceram os fundamentos do fluxo residual e o papel da autoatenção na integração contextual [^1, ^6, ^7, ^12, ^13], exploraremos como estas camadas, em conjunto com a autoatenção e conexões residuais, moldam a representação de cada *token* ao longo do *residual stream*. Especificamente, analisaremos como a rede *FFN* adiciona diferentes visões à representação do *token*, enquanto as camadas *LayerNorm* garantem a estabilidade do treinamento e normalização dos vetores que representam os *tokens*. Entendemos que enquanto a autoatenção integra informação contextual, a rede *FFN* e a *LayerNorm* adicionam diferentes perspectivas ao fluxo da representação e estabilidade ao processo [^13].

### A Rede *Feedforward* e Novas Perspectivas no *Residual Stream*
Como vimos anteriormente, o *residual stream* é um fluxo contínuo de vetores *d*-dimensionais que representam cada *token* ao longo das camadas do *transformer* [^13, Proposição 1]. A autoatenção desempenha um papel fundamental na integração das informações contextuais. Contudo, outras camadas do bloco *transformer* também contribuem para a transformação e enriquecimento do *embedding* do *token*. Em particular, a camada *feedforward* (*FFN*) adiciona uma visão não-linear da representação do *token*, que é fundamental para a capacidade do *transformer* de modelar dependências complexas na linguagem [^13].

A camada *FFN* é uma rede neural de duas camadas, aplicada de forma independente para cada *token* da sequência. Após a autoatenção e a primeira conexão residual, a representação do *token* ($t^3$) passa pela camada *FFN*, que aplica uma transformação linear seguida de uma não-linearidade, resultando em uma nova representação $t^4$ do *token*:
$$
t^4 = FFN(t^3)
$$
[10.35]
Essa operação adiciona uma perspectiva diferente da representação do *token*, enriquecendo o *residual stream*. Ao contrário da autoatenção que integra informações do contexto, a rede *FFN* opera sobre o *embedding* de cada *token* de forma independente, adicionando uma transformação não-linear que é essencial para a capacidade de representação do modelo.

> 💡 **Exemplo Numérico:**
> Vamos considerar um *token* com o *embedding* $t^3$ de dimensão $d=4$ que chega à camada *FFN* após passar pela autoatenção e pela primeira normalização:
> $$t^3 = \begin{bmatrix} 0.8 \\ -0.2 \\ 0.5 \\ 0.2 \end{bmatrix}$$
> A camada *FFN* transforma esse vetor em um novo vetor $t^4$ com a mesma dimensionalidade, mas com uma perspectiva não linear. Para fins ilustrativos, vamos supor que a *FFN* aplica uma transformação linear seguida por uma função de ativação ReLU.  Considere que a primeira camada linear da FFN transforma $t^3$ em um vetor $z$ de dimensão $d_{ff} = 8$ e a segunda camada linear transforma esse $z$ de volta para um vetor da mesma dimensão $d=4$. As matrizes de peso e bias são aprendidas durante o treinamento do modelo.
>
>  Suponha que após a transformação linear da primeira camada e a aplicação do ReLU, o vetor intermediário $z$ seja:
>  $$z = \begin{bmatrix} 0.9 \\ 0.0 \\ 1.2 \\ 0.7 \\ 0.1 \\ 0.6 \\ 0.0 \\ 0.4 \end{bmatrix}$$
>  E que a segunda camada linear da FFN resulta em:
> $$t^4 = FFN(t^3) = \begin{bmatrix} 0.1 \\ -0.4 \\ 0.2 \\ -0.1 \end{bmatrix}$$
>
> Observe que tanto $t^3$ quanto $t^4$ mantém a dimensão $d=4$, conforme o conceito do fluxo residual. A operação da FFN introduz não-linearidades na representação do *token*, permitindo ao modelo aprender relações complexas nos dados.
> ```mermaid
> graph LR
>     A[t3] --> B(Linear Layer 1)
>     B --> C(ReLU)
>     C --> D[z]
>     D --> E(Linear Layer 2)
>     E --> F[t4]
> ```
>

**Proposição 1.1** (Invariância da Dimensionalidade da *FFN*): A camada *FFN* preserva a dimensionalidade do *embedding* do *token*, ou seja, se a entrada $t^3$ tem dimensão *d*, a saída $t^4 = FFN(t^3)$ também terá dimensão *d*.
*Prova:*
I. A camada *FFN* é composta por duas transformações lineares com ativação não-linear entre elas. Cada transformação linear é definida por uma matriz de pesos $W$ que mapeia um vetor de dimensão *d* para um vetor de dimensão *d_ff*, e posteriormente de *d_ff* para *d*.
II. Se a dimensão de entrada é *d*, a primeira transformação linear projeta para um espaço de dimensão *d_ff*, e a segunda transformação linear projeta de volta para um espaço de dimensão *d*.
III. Portanto, a camada *FFN* mapeia um vetor de dimensão *d* para outro vetor de dimensão *d*.
■

**As Camadas de Normalização (*LayerNorm*) e a Estabilização do *Residual Stream***
As camadas de normalização, e particularmente a *LayerNorm*, são cruciais para a estabilização do fluxo residual e para o bom funcionamento do *transformer*. Existem duas formas de aplicar o LayerNorm: *pre-norm* e *post-norm* [^13]. Na arquitetura *post-norm*, como vimos anteriormente, o *LayerNorm* é aplicado após as operações de autoatenção e *FFN*. Já na arquitetura *pre-norm*, o *LayerNorm* é aplicado antes dessas operações [^13]. Em ambos os casos, a *LayerNorm* garante que as ativações em cada camada tenham uma média próxima de zero e desvio padrão próximo de um, estabilizando o treinamento do modelo e facilitando a propagação do gradiente.

Na arquitetura *post-norm*, o primeiro *LayerNorm* é aplicado ao *embedding* $t^2$ (resultado da autoatenção e da primeira conexão residual):
$$
t^3 = LayerNorm(t^2)
$$
[10.34]
E o segundo *LayerNorm* é aplicado ao *embedding* $t^5$ (resultado da segunda adição residual):
$$
h_i = LayerNorm(t^5)
$$
[10.37]
Enquanto que na arquitetura *pre-norm* a normalização é aplicada antes das operações de *MultiHeadAttention* e *FFN*, como já demonstrado anteriormente [Lema 1.1]. Em ambas as arquiteturas, a normalização de camada preserva a dimensionalidade, atuando como um mecanismo que estabiliza o fluxo residual e garante que as representações não se tornem muito grandes ou muito pequenas.

> 💡 **Exemplo Numérico:**
> Para exemplificar o efeito do *LayerNorm*, considere um vetor $t^2 = \begin{bmatrix} 1.2 \\ 0.4 \\ 0.1 \\ 0.9 \end{bmatrix}$. O *LayerNorm* centraliza e normaliza o vetor. Vamos calcular a média ($\mu$) e o desvio padrão ($\sigma$) do vetor $t^2$:
>
> $\mu = \frac{1.2 + 0.4 + 0.1 + 0.9}{4} = 0.65$
>
> $\sigma = \sqrt{\frac{(1.2-0.65)^2 + (0.4-0.65)^2 + (0.1-0.65)^2 + (0.9-0.65)^2}{4}} \approx 0.44$
>
> O *LayerNorm* normaliza o vetor subtraindo a média e dividindo pelo desvio padrão, resultando em um vetor $t^3_{norm}$:
>
> $t^3_{norm} = \begin{bmatrix} \frac{1.2 - 0.65}{0.44} \\ \frac{0.4 - 0.65}{0.44} \\ \frac{0.1 - 0.65}{0.44} \\ \frac{0.9 - 0.65}{0.44} \end{bmatrix} \approx \begin{bmatrix} 1.25 \\ -0.57 \\ -1.25 \\ 0.57 \end{bmatrix}$
>
> Note que, em um cenário real, o *LayerNorm* também inclui parâmetros aprendíveis de ganho ($\gamma$) e bias ($\beta$). A saída final da LayerNorm é então dada por $t^3 = \gamma \times t^3_{norm} + \beta$. Para simplificar o exemplo, vamos assumir que $\gamma$ e $\beta$ são próximos de 1 e 0, respectivamente:
>  $$t^3 = LayerNorm(t^2) \approx \begin{bmatrix} 0.8 \\ -0.2 \\ 0.5 \\ 0.2 \end{bmatrix}$$
>
> Note que tanto $t^2$ quanto $t^3$ mantém a mesma dimensionalidade $d=4$. A *LayerNorm* garante que os valores estejam na escala adequada para que o treinamento seja mais eficaz, prevenindo o problema da explosão ou desaparecimento do gradiente.

**Proposição 2.1** (Invariância da Dimensionalidade da *LayerNorm*): A camada *LayerNorm* preserva a dimensionalidade do *embedding* do *token*, ou seja, se a entrada $t^2$ tem dimensão *d*, a saída $t^3 = LayerNorm(t^2)$ também terá dimensão *d*.
*Prova:*
I. A camada *LayerNorm* realiza a normalização de cada vetor individualmente, sem alterar sua dimensão.
II. A normalização é feita subtraindo a média e dividindo pelo desvio padrão, o que não altera o número de componentes do vetor.
III. Portanto, a camada *LayerNorm* mapeia um vetor de dimensão *d* para outro vetor de dimensão *d*.
■

**O Papel das Conexões Residuais**
As conexões residuais desempenham um papel crucial na manutenção do fluxo de informação ao longo do *residual stream* [^13]. Elas permitem que informações das camadas anteriores sejam passadas diretamente para as camadas seguintes, evitando o desaparecimento do gradiente e facilitando o treinamento de modelos mais profundos. Como já vimos, as conexões residuais são aplicadas após a autoatenção e a rede *FFN*:
$$
t^2 = t^1 + x_i
$$
[10.33]
$$
t^5 = t^4 + t^3
$$
[10.36]
Estas conexões residuais garantem que as representações originais dos *tokens* sejam preservadas e combinadas com as novas informações contextuais e não lineares.

**A Integração das Diferentes Camadas no Fluxo Residual**
O fluxo residual, portanto, é o resultado da integração das diferentes camadas do bloco *transformer*: a autoatenção, que integra informações contextuais; a rede *FFN*, que adiciona uma visão não-linear da representação; e a *LayerNorm*, que estabiliza o fluxo e garante que os valores dos vetores sejam normalizados. As conexões residuais, por sua vez, garantem a propagação das informações ao longo do modelo. Em conjunto, estas camadas transformam o *embedding* inicial de cada *token*, enriquecendo-o com informações contextuais e com diferentes perspectivas do seu significado e uso.

> 💡 **Exemplo Numérico:**
> Retomando nosso exemplo, consideremos que o *embedding* inicial de um *token* seja $x_i = \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix}$. Após a autoatenção, obtemos $t^1 = \begin{bmatrix} 0.5 \\ 0.0 \\ 0.2 \\ 0.1 \end{bmatrix}$, e a adição residual resulta em $t^2 = t^1 + x_i = \begin{bmatrix} 0.7 \\ 0.1 \\ -0.1 \\ 0.5 \end{bmatrix}$. Em seguida, a LayerNorm é aplicada:
> $$t^3 = LayerNorm(t^2) \approx \begin{bmatrix} 0.8 \\ -0.2 \\ 0.5 \\ 0.2 \end{bmatrix}$$
> e a rede FFN aplica uma transformação não-linear:
> $$t^4 = FFN(t^3) = \begin{bmatrix} 0.1 \\ -0.4 \\ 0.2 \\ -0.1 \end{bmatrix}$$
> A segunda adição residual combina $t^3$ e $t^4$:
> $$t^5 = t^4 + t^3 = \begin{bmatrix} 0.9 \\ -0.6 \\ 0.7 \\ 0.1 \end{bmatrix}$$
> Finalmente, a última normalização de camada produz a saída do bloco *transformer*:
> $$h_i = LayerNorm(t^5) \approx \begin{bmatrix} 0.4 \\ -0.8 \\ 1.2 \\ -0.2 \end{bmatrix}$$
>
> O diagrama abaixo ilustra o fluxo das diferentes camadas:
> ```mermaid
> graph LR
>     A[xi] --> B(MultiHeadAttention);
>     C(Contextos) --> B
>     B --> D[t1]
>     A --> E[t2=x+t1]
>     D --> E
>     E --> F(LayerNorm)
>     F --> G[t3]
>    G --> H(FFN)
>    H --> I[t4]
>    G --> J[t5=t3+t4]
>    I --> J
>   J --> K(LayerNorm)
>   K --> L[hi]
> ```

**Teorema 1** (A Contribuição da *FFN* e *LayerNorm* para o *Residual Stream*): A rede *FFN* e a *LayerNorm* desempenham papéis complementares na transformação do *residual stream*. Enquanto a *FFN* adiciona uma perspectiva não-linear à representação do *token*, o *LayerNorm* garante a estabilidade do fluxo e a normalização dos vetores.

*Prova:*
I. A rede *FFN*, definida como $t^4 = FFN(t^3)$, aplica uma transformação não linear ao vetor $t^3$, que é resultado da autoatenção e da primeira normalização, adicionando uma perspectiva diferente à representação do *token*.
II. A *LayerNorm*, definida como $t^3 = LayerNorm(t^2)$ e $h_i = LayerNorm(t^5)$, normaliza os vetores $t^2$ e $t^5$, garantindo que suas componentes tenham média próxima a zero e desvio padrão próximo a um, o que estabiliza o fluxo e facilita a propagação do gradiente.
III. A combinação da ação da *FFN*, que adiciona não-linearidade, e do *LayerNorm*, que garante a estabilidade, possibilita que o *transformer* aprenda representações mais robustas e complexas.
IV. Portanto, a *FFN* e o *LayerNorm* são componentes essenciais do *residual stream* que, em conjunto com a autoatenção e as conexões residuais, contribuem para o bom desempenho do *transformer*.
■

**Teorema 2** (O Papel das Conexões Residuais na Preservação da Informação): As conexões residuais garantem a propagação da informação ao longo do *residual stream*, preservando a informação original dos *embeddings* e facilitando o treinamento do modelo.

*Prova:*
I. As conexões residuais, definidas por $t^2 = t^1 + x_i$ e $t^5 = t^4 + t^3$, adicionam a saída de cada camada ao *embedding* da camada anterior, permitindo que as informações de cada camada sejam propagadas para as camadas subsequentes.
II. A combinação da informação de diferentes camadas através das conexões residuais possibilita que o modelo aprenda representações mais complexas e que a informação original dos *embeddings* seja preservada ao longo da transformação.
III. Portanto, as conexões residuais são um componente crucial do fluxo residual, que contribuem para o bom desempenho do *transformer* e a capacidade de aprender representações complexas.
■
**Corolário 2.1** (Conexões Residuais e a Estabilidade do Gradiente): As conexões residuais facilitam a propagação do gradiente durante o treinamento, evitando o desaparecimento do gradiente e melhorando a convergência do modelo.

*Prova:*
I. Conforme demonstrado no Teorema 2, as conexões residuais permitem que a informação flua diretamente de camadas anteriores para camadas posteriores, "saltando" as transformações que poderiam atenuar ou distorcer a informação original.
II. Essa passagem direta de informação também impacta a propagação do gradiente durante o treinamento, facilitando a propagação do gradiente através das camadas e evitando que ele desapareça ao longo do *backpropagation*.
III. A preservação do gradiente é essencial para o treinamento de modelos mais profundos, como os *transformers*, permitindo que os pesos do modelo sejam atualizados de forma eficiente e garantindo a convergência para um bom mínimo local.
IV. Portanto, as conexões residuais são cruciais para o treinamento eficaz dos *transformers* devido a sua capacidade de preservar a informação e facilitar a propagação do gradiente ao longo do modelo.
■
**Corolário 2.2** (A Preservação da Dimensionalidade): A propriedade de invariância da dimensionalidade dos *embeddings* ao longo do *residual stream* (Proposição 1.1), garante que todas as operações, mesmo a adição residual, mantenham a dimensionalidade *d*, o que é essencial para a arquitetura do *transformer*.
*Prova:*
I. A adição residual, definida como $t^2 = t^1 + x_i$ e $t^5 = t^4 + t^3$, adiciona dois vetores com dimensionalidade *d*, resultando em vetores também de dimensionalidade *d*.
II. Como demonstrado na Proposição 1.1 e Proposição 2.1, tanto na arquitetura *post-norm* quanto na *pre-norm*, todas as operações aplicadas ao *residual stream* preservam a dimensionalidade *d*.
III. Portanto, as conexões residuais não apenas garantem a propagação da informação, mas também preservam a dimensionalidade do *embedding*, o que é essencial para o bom funcionamento do modelo.
■
**Observação 1** (Relação entre *LayerNorm*, *FFN* e a Modulação do *Residual Stream*): A *LayerNorm*, ao normalizar o *embedding* após a autoatenção e a rede *FFN*, contribui para um fluxo mais estável e controlado, enquanto a rede *FFN*, ao aplicar uma transformação não linear ao *embedding* de cada *token*, permite que o modelo aprenda relações mais complexas entre os *tokens*. O *LayerNorm* atua como um regulador que impede que os valores dos vetores do *residual stream* fujam de uma escala adequada, e a *FFN* como um componente de enriquecimento da representação.

**Lema 3.1** (A Não-Linearidade da FFN e a Capacidade de Modelagem): A introdução de não-linearidade pela camada FFN, através da função de ativação (e.g. ReLU, GeLU), é fundamental para a capacidade do *transformer* de modelar relações complexas entre os *tokens*.
*Prova:*
I.  A camada FFN consiste em duas transformações lineares separadas por uma função de ativação não linear. Sem essa função de ativação, a FFN se reduziria a uma única transformação linear, limitada na modelagem de relações complexas.
II. A não-linearidade permite que o modelo aprenda uma variedade maior de padrões e dependências nos dados, como interações não-lineares entre as palavras.
III.  Em modelos baseados em redes neurais, funções não lineares são essenciais para a aproximação de funções complexas. Portanto, a função de ativação na FFN garante que o modelo possa aprender representações mais ricas e expressivas.
■

### Conclusão
Neste capítulo, analisamos como as camadas *feedforward* e de normalização, juntamente com as conexões residuais, moldam a representação de cada *token* ao longo do *residual stream*. Enquanto a autoatenção integra informações contextuais, a rede *FFN* adiciona novas perspectivas à representação do *token*, e as camadas *LayerNorm* garantem a estabilidade do fluxo e a normalização dos vetores. As conexões residuais, por sua vez, asseguram a propagação da informação original através do modelo, facilitando o treinamento. Em conjunto, estas camadas transformam o *embedding* inicial de cada *token*, enriquecendo-o com informações contextuais e não lineares, e garantindo que a informação flua de forma estável e controlada. Compreender as contribuições dessas diferentes camadas para o fluxo residual é essencial para analisar o comportamento dos *transformers* e para o desenvolvimento de modelos mais avançados.

### Referências
[^1]: *“In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models.”*
[^6]: *“To capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value.”*
[^7]: *“This description of the self-attention process has been from the perspective of computing a single output at a single time step i.”*
[^12]: *“The previous sections viewed the transformer block as applied to the entire N-token input X of shape [N × d], producing an output also of shape [N × d].”*
[^13]: *“these layers as a stream of d-dimensional representations, called the residual stream and visualized in Fig. 10.7.”*
<!-- END -->
