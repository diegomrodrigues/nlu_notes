## O Fluxo Residual: Uma Sequ√™ncia de Representa√ß√µes *$d$*-Dimensionais no Bloco Transformer

### Introdu√ß√£o
Este cap√≠tulo tem como objetivo explorar o conceito do **fluxo residual** (*residual stream*) sob uma nova perspectiva, entendendo-o como uma sequ√™ncia de representa√ß√µes *$d$*-dimensionais, onde cada *token* possui uma representa√ß√£o inicial que √© enriquecida pelas diferentes camadas do modelo *transformer*. Construindo sobre a nossa discuss√£o anterior sobre autoaten√ß√£o, blocos *transformer* e a perspectiva do fluxo residual [^1, ^6, ^7, ^12, ^13], aqui analisaremos como cada componente do modelo contribui para essa representa√ß√£o. Ao inv√©s de focar na transforma√ß√£o da matriz de *embeddings*, nosso objetivo √© entender o processo de transforma√ß√£o para cada *token* individual, compreendendo o *residual stream* como um fluxo cont√≠nuo de vetores *$d$*-dimensionais onde cada camada adiciona uma "nova vis√£o" do *token* [^13].

### Conceitos Fundamentais
Como j√° vimos anteriormente, no contexto do *transformer*, cada *token* de entrada √© inicialmente representado por um *embedding* vetorial com dimensionalidade *$d$* [^15]. Esta representa√ß√£o inicial, denotada como $x_i$, serve como ponto de partida para o *residual stream* de um determinado *token* *$i$*. Ao longo do processamento do bloco *transformer*, o *embedding* $x_i$ passa por uma s√©rie de transforma√ß√µes, onde cada camada adiciona ou modifica a sua representa√ß√£o, mantendo a dimensionalidade *$d$* do vetor [^13, Proposi√ß√£o 1]. O fluxo residual, portanto, pode ser entendido como o percurso deste vetor *$d$*-dimensional atrav√©s das diversas camadas do modelo. As conex√µes residuais, como discutimos anteriormente [^13], desempenham um papel fundamental neste processo, permitindo que informa√ß√µes de camadas anteriores sejam propagadas para camadas posteriores, evitando que a informa√ß√£o original se perca ao longo da transforma√ß√£o. Cada componente do bloco *transformer*, como a camada de autoaten√ß√£o (*MultiHeadAttention*), a camada *feedforward* (*FFN*), e a normaliza√ß√£o de camada (*LayerNorm*), aplicam suas pr√≥prias transforma√ß√µes ao vetor, acrescentando diferentes "vis√µes" √† representa√ß√£o do *token*.

> üí° **Exemplo Num√©rico:**
> Consideremos um *token* representado por um *embedding* vetorial inicial $x_i$ de dimens√£o $d = 4$:
> $$x_i = \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}$$
> Este vetor representa a representa√ß√£o inicial do *token* e √© a base para o *residual stream*. Ao longo do processamento no bloco *transformer*, o vetor $x_i$ ser√° transformado e enriquecido pelas diferentes camadas, mantendo a sua dimensionalidade *$d=4$*.

**O Papel da Autoaten√ß√£o**

Dentro do contexto do fluxo residual, a autoaten√ß√£o √© a √∫nica camada que interage diretamente com outros *tokens* na sequ√™ncia [^13]. A opera√ß√£o *MultiHeadAttention*, recebe o *embedding* $x_i$ e incorpora informa√ß√µes contextuais dos demais *tokens* da sequ√™ncia, produzindo uma nova representa√ß√£o $t^1$ [^13]. Esta nova representa√ß√£o reflete como o *token* atual se relaciona com os outros *tokens* do contexto. Como vimos anteriormente [^6, ^7], a autoaten√ß√£o opera sobre *queries*, *keys* e *values* calculados para cada *token* da sequ√™ncia. Esta opera√ß√£o pode ser descrita formalmente pela equa√ß√£o:
$$
t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])
$$
[10.32]

A adi√ß√£o residual, que ocorre logo ap√≥s a autoaten√ß√£o, soma o *embedding* original $x_i$ √† sa√≠da da autoaten√ß√£o $t^1$:
$$
t^2 = t^1 + x_i
$$
[10.33]
Essa opera√ß√£o garante que a informa√ß√£o original do *embedding* n√£o seja perdida durante a transforma√ß√£o, facilitando a propaga√ß√£o do gradiente durante o treinamento.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma sequ√™ncia de 3 tokens e estamos focando no segundo token. Seu embedding inicial √© $x_2 = \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}$ ($d=4$). Ap√≥s o c√°lculo de *queries*, *keys* e *values* para todos os tokens e a aplica√ß√£o da opera√ß√£o *MultiHeadAttention*, obtemos a representa√ß√£o $t^1$ para o segundo token. Vamos assumir que
>  $$t^1 = \begin{bmatrix} 0.3 \\ -0.1 \\ 0.2 \\ 0.5 \end{bmatrix}$$
> A adi√ß√£o residual soma $t^1$ com o embedding original $x_2$, resultando em
> $$t^2 = t^1 + x_2 = \begin{bmatrix} 0.3 \\ -0.1 \\ 0.2 \\ 0.5 \end{bmatrix} +  \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix} = \begin{bmatrix} 1.3 \\ 0.4 \\ 0.0 \\ 1.3 \end{bmatrix}$$
>
> Note que tanto $x_2$, $t^1$ e $t^2$ mant√™m a mesma dimensionalidade *$d=4$*.

**A Normaliza√ß√£o e a Rede *Feedforward***

Ap√≥s a opera√ß√£o de autoaten√ß√£o e a adi√ß√£o residual, o vetor passa por uma camada de normaliza√ß√£o (*LayerNorm*), que resulta em uma representa√ß√£o normalizada $t^3$:
$$
t^3 = LayerNorm(t^2)
$$
[10.34]

A normaliza√ß√£o de camada estabiliza o treinamento do modelo, garantindo que as ativa√ß√µes n√£o sejam muito grandes ou muito pequenas. Em seguida, o vetor √© processado pela rede *feedforward* (*FFN*), que adiciona n√£o-linearidade √† representa√ß√£o:
$$
t^4 = FFN(t^3)
$$
[10.35]
A rede *FFN* aplica uma transforma√ß√£o linear seguida de uma n√£o-linearidade, enriquecendo a representa√ß√£o do *token*.

> üí° **Exemplo Num√©rico:**
> Continuando o exemplo anterior, vamos aplicar LayerNorm em $t^2 = \begin{bmatrix} 1.3 \\ 0.4 \\ 0.0 \\ 1.3 \end{bmatrix}$. Suponha que ap√≥s a normaliza√ß√£o, obtemos:
> $$t^3 = LayerNorm(t^2) =  \begin{bmatrix} 0.8 \\ -0.1 \\ -0.5 \\ 0.8 \end{bmatrix}$$
>
> Em seguida, o vetor $t^3$ √© passado pela rede *FFN*. Vamos supor que a *FFN* produz a seguinte sa√≠da:
>$$t^4 = FFN(t^3) = \begin{bmatrix} 0.2 \\ -0.3 \\ 0.1 \\ 0.4 \end{bmatrix}$$
>
> Observe que todas as transforma√ß√µes preservam a dimens√£o *$d=4$* do vetor.

**A Vis√£o Final e a Sa√≠da do Bloco**
A sa√≠da da rede *FFN* √© novamente adicionada ao vetor anterior, atrav√©s de uma conex√£o residual:
$$
t^5 = t^4 + t^3
$$
[10.36]
Essa segunda conex√£o residual preserva a informa√ß√£o e facilita o treinamento. Finalmente, o vetor √© normalizado novamente com *LayerNorm*, produzindo a sa√≠da do bloco *transformer*:
$$
h_i = LayerNorm(t^5)
$$
[10.37]
Essa √∫ltima camada de normaliza√ß√£o garante que a sa√≠da tenha uma distribui√ß√£o bem comportada, resultando em uma representa√ß√£o final do *token* que pode ser utilizada nas pr√≥ximas camadas do modelo.

Todo o processo, desde a representa√ß√£o inicial $x_i$ at√© a sa√≠da $h_i$, ocorre de forma que a dimensionalidade do vetor se mant√©m constante e igual a *$d$*. Portanto, o *residual stream* representa uma sequ√™ncia de vetores com a mesma dimensionalidade, onde cada camada adiciona uma nova perspectiva a esse vetor, enriquecendo a representa√ß√£o do *token*.

> üí° **Exemplo Num√©rico:**
> Continuando com o exemplo num√©rico anterior, suponhamos que a sa√≠da da camada *MultiHeadAttention* seja:
> $$t^1 = \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix}$$
> A adi√ß√£o residual resulta em:
> $$t^2 = t^1 + x_i =  \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix} + \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 0.6 \\ -0.5 \\ 1.2 \end{bmatrix}$$
>
> Vamos ilustrar o efeito da normaliza√ß√£o de camada com uma simplifica√ß√£o:
> O *LayerNorm* centraliza e normaliza o vetor $t^2$, resultando em:
> $$t^3 = LayerNorm(t^2) \approx \begin{bmatrix} 0.77 \\ -0.03 \\ -1.50 \\ 0.77 \end{bmatrix}$$
>
> Assumindo uma sa√≠da simplificada para a rede *FFN*:
>$$t^4 = FFN(t^3) = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.3 \\ -0.1 \end{bmatrix}$$
>
> A segunda adi√ß√£o residual resulta em:
>$$t^5 = t^4 + t^3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.3 \\ -0.1 \end{bmatrix} + \begin{bmatrix} 0.77 \\ -0.03 \\ -1.50 \\ 0.77 \end{bmatrix} = \begin{bmatrix} 1.27 \\ -0.23 \\ -1.20 \\ 0.67 \end{bmatrix}$$
>
> Finalmente, a √∫ltima *LayerNorm* normaliza a sa√≠da do bloco *transformer* para:
>$$h_i = LayerNorm(t^5) \approx \begin{bmatrix} 1.35 \\ -0.69 \\ -1.65 \\ 0.98 \end{bmatrix}$$
>
> Note que, em cada etapa, a dimens√£o do vetor se mant√©m constante em *$d=4$*, como estabelecido pelo conceito do *residual stream*. Cada camada adiciona uma nova perspectiva ao vetor, enriquecendo a representa√ß√£o do *token* ao longo do processamento do bloco *transformer*.

**O Movimento da Informa√ß√£o**

Elhage et al. (2021) demonstraram que os *attention heads* transferem informa√ß√£o do *residual stream* de um *token* vizinho para o *stream* do *token* atual. Isto √©, o espa√ßo de *embedding* em cada posi√ß√£o cont√©m informa√ß√µes sobre o *token* atual e sobre os seus vizinhos [^13]. Essa intera√ß√£o contextual, realizada pela autoaten√ß√£o, permite que o modelo aprenda depend√™ncias complexas entre os *tokens*.

A visualiza√ß√£o do *residual stream* permite compreender o fluxo de informa√ß√£o atrav√©s do *transformer*, como ilustrado na Fig. 10.7 do texto [^12]. Nessa figura, podemos observar como o vetor *$d$*-dimensional que representa um *token* √© transformado atrav√©s das diversas camadas do bloco *transformer*, com as conex√µes residuais garantindo que a informa√ß√£o original se propague atrav√©s do modelo e cada camada adicione uma nova vis√£o ao vetor que representa o *token*.

**As Arquiteturas *Pre-norm* e *Post-norm***

As arquiteturas *pre-norm* e *post-norm* representam abordagens distintas para a aplica√ß√£o da normaliza√ß√£o de camada (*LayerNorm*) dentro do bloco *transformer*. Na arquitetura *post-norm*, a *LayerNorm* √© aplicada ap√≥s as opera√ß√µes de *MultiHeadAttention* e *FFN*, como descrito acima [^13]. J√° na arquitetura *pre-norm*, a *LayerNorm* √© aplicada antes dessas opera√ß√µes [^13]. Embora ambas as abordagens compartilhem o conceito do *residual stream*, a arquitetura *pre-norm* tem demonstrado, em geral, um melhor desempenho em diversas tarefas [^13]. A aplica√ß√£o da *LayerNorm* antes das opera√ß√µes de *MultiHeadAttention* e *FFN* estabiliza o treinamento e facilita a propaga√ß√£o do gradiente ao longo do modelo.

**Lema 1** (Equival√™ncia da Arquitetura *Post-norm*): Na arquitetura *post-norm*, o processamento de um *token* atrav√©s de um bloco *transformer* pode ser descrito por uma fun√ß√£o *$f$* que mapeia um vetor *$d$*-dimensional $x_i$ (o *embedding* de um *token* *$i$*) para outro vetor *$d$*-dimensional $h_i$, preservando a dimensionalidade do fluxo residual. Assim, para uma entrada $x_i$, a sa√≠da $h_i$ √© dada por $h_i$ = $f$($x_i$, $x_1$,..., $x_n$), onde $x_1$,..., $x_n$ representam os *embeddings* dos outros *tokens*.

*Prova:*
I. A opera√ß√£o *MultiHeadAttention*, dada por $t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])$, mapeia vetores *$d$*-dimensionais para um vetor de sa√≠da $t^1$ com a mesma dimensionalidade *$d$*.
II. A adi√ß√£o residual, $t^2 = t^1 + x_i$, resulta na soma de dois vetores *$d$*-dimensionais, produzindo um vetor $t^2$ tamb√©m *$d$*-dimensional.
III. A normaliza√ß√£o da camada, $t^3 = LayerNorm(t^2)$, preserva a dimensionalidade, transformando o vetor *$d$*-dimensional $t^2$ em outro vetor *$d$*-dimensional $t^3$.
IV. A rede *feedforward*, $t^4 = FFN(t^3)$, mapeia o vetor *$d$*-dimensional $t^3$ para outro vetor *$d$*-dimensional $t^4$.
V.  A segunda adi√ß√£o residual, $t^5 = t^4 + t^3$, resulta na soma de dois vetores *$d$*-dimensionais, gerando um vetor $t^5$ tamb√©m *$d$*-dimensional.
VI. Finalmente, a normaliza√ß√£o da camada final, $h_i = LayerNorm(t^5)$, transforma o vetor *$d$*-dimensional $t^5$ em um vetor de sa√≠da $h_i$ que tamb√©m √© *$d$*-dimensional.
VII. Portanto, cada etapa mant√©m a dimensionalidade *$d$*. A fun√ß√£o composta *$f$* que mapeia $x_i$ para $h_i$ atrav√©s dessas opera√ß√µes preserva a dimensionalidade, podendo ser escrita formalmente como $f(x_i, x_1, \ldots, x_N) = LayerNorm(FFN(LayerNorm(MultiHeadAttention(x_i, [x_1, \ldots, x_N]) + x_i)) + LayerNorm(MultiHeadAttention(x_i, [x_1, \ldots, x_N]) + x_i))$.
‚ñ†

**Proposi√ß√£o 1** (Propriedade de Invari√¢ncia da Dimensionalidade): Em ambas as arquiteturas, *pre-norm* e *post-norm*, o fluxo residual preserva a dimensionalidade dos *embeddings*. Em cada transforma√ß√£o aplicada a um *embedding d*-dimensional, o resultado √© outro *embedding d*-dimensional.

*Prova:*
I. A arquitetura *post-norm*, conforme demonstrado no Lema 1, preserva a dimensionalidade atrav√©s de todas as suas opera√ß√µes: *MultiHeadAttention*, adi√ß√µes residuais, *LayerNorm* e *FFN*.
II.  Na arquitetura *pre-norm*, a *LayerNorm* √© aplicada antes das opera√ß√µes de *MultiHeadAttention* e *FFN*, mas ela tamb√©m preserva a dimensionalidade.
III. Como a *LayerNorm* n√£o altera a dimensionalidade dos vetores, as outras opera√ß√µes (MultiHeadAttention, adi√ß√µes residuais e FFN), quando aplicadas a vetores *$d$*-dimensionais, retornam vetores tamb√©m *$d$*-dimensionais.
IV. A ordem das opera√ß√µes, seja *pre-norm* ou *post-norm*, n√£o afeta o fato de que cada opera√ß√£o mant√©m a dimensionalidade *$d$*.
V. Portanto, em ambas as arquiteturas, o fluxo residual manipula vetores *$d$*-dimensionais em cada etapa, preservando a dimensionalidade dos *embeddings*.
‚ñ†

**Lema 1.1** (Representa√ß√£o da Arquitetura *Pre-norm*): Na arquitetura *pre-norm*, a sequ√™ncia de opera√ß√µes pode ser descrita por uma fun√ß√£o *$g$* que mapeia um vetor *$d$*-dimensional $x_i$ para outro vetor *$d$*-dimensional $h_i$, mantendo a dimensionalidade do fluxo residual. Assim, para uma entrada $x_i$, a sa√≠da $h_i$ √© expressa como $h_i = g(x_i, x_1, ..., x_N)$.

*Prova:*
I.  Na arquitetura *pre-norm*, a opera√ß√£o de normaliza√ß√£o da camada √© aplicada *antes* da *MultiHeadAttention*, resultando em $t^1 = LayerNorm(x_i)$. Como a *LayerNorm* preserva a dimensionalidade, $t^1$ tamb√©m √© *$d$*-dimensional.
II.  A opera√ß√£o *MultiHeadAttention* recebe $t^1$ (um vetor *$d$*-dimensional) e os outros *embeddings*, resultando em $t^2 = MultiHeadAttention(t^1, [x_1, \ldots, x_N])$. Portanto, $t^2$ √© *$d$*-dimensional.
III. A adi√ß√£o residual √© aplicada somando $t^2$ com $t^1$, resultando em $t^3 = t^2 + t^1$. Logo, $t^3$ tamb√©m √© *$d$*-dimensional.
IV. A normaliza√ß√£o da camada √© aplicada novamente, $t^4 = LayerNorm(t^3)$, preservando a dimens√£o, ou seja, $t^4$ √© *$d$*-dimensional.
V. A rede *feedforward* mapeia $t^4$ (um vetor *$d$*-dimensional) para outro vetor *$d$*-dimensional: $t^5 = FFN(t^4)$.
VI. A adi√ß√£o residual √© aplicada somando $t^5$ com $t^4$, resultando em $t^6 = t^5 + t^4$. Logo, $t^6$ √© *$d$*-dimensional.
VII. Finalmente, a normaliza√ß√£o da camada √© aplicada a $t^6$, resultando em $h_i = LayerNorm(t^6)$. Portanto, $h_i$ √© tamb√©m *$d$*-dimensional.
VIII.  Cada passo preserva a dimensionalidade *$d$*. A fun√ß√£o *$g$* composta por essas opera√ß√µes tamb√©m preserva a dimensionalidade, podendo ser escrita formalmente como $g(x_i, x_1, \ldots, x_N) = LayerNorm(FFN(LayerNorm(MultiHeadAttention(LayerNorm(x_i), [x_1, \ldots, x_N]) + LayerNorm(x_i))) + MultiHeadAttention(LayerNorm(x_i), [x_1, \ldots, x_N]) + LayerNorm(x_i))$.
‚ñ†
**Proposi√ß√£o 1.1** (Invari√¢ncia da Dimensionalidade do Fluxo Residual): A caracter√≠stica fundamental do fluxo residual, em ambas arquiteturas (*pre-norm* e *post-norm*), √© a invari√¢ncia da dimensionalidade dos *embeddings* dos *tokens*. Ou seja, o processamento do bloco *transformer* garante que a dimens√£o do vetor de representa√ß√£o de cada *token* seja sempre mantida em *$d$*.

*Prova:*
I. Conforme demonstrado nos Lemas 1 e 1.1, a aplica√ß√£o da fun√ß√£o *$f$* (post-norm) ou da fun√ß√£o *$g$* (pre-norm) sobre um vetor de entrada *$d$*-dimensional $x_i$ resulta em um vetor de sa√≠da $h_i$ que tamb√©m possui dimens√£o *$d$*.
II. O processamento do bloco *transformer*, descrito pelas fun√ß√µes *$f$* e *$g$*, √© iterativo, ou seja, a sa√≠da de um bloco serve como entrada para o pr√≥ximo.
III. Como as fun√ß√µes *$f$* e *$g$* preservam a dimensionalidade *$d$*, e a entrada de cada bloco seguinte tem a mesma dimensionalidade da sa√≠da do bloco anterior, o fluxo residual garante que a dimensionalidade *$d$* seja mantida ao longo de todo o processamento.
IV. Portanto, a dimensionalidade dos *embeddings* dos *tokens* permanece invariante ao longo do fluxo residual, mantendo-se sempre igual a *$d$* em todas as camadas e blocos do *transformer*.
‚ñ†

**Observa√ß√£o 1** (Camadas Lineares e Invari√¢ncia da Dimensionalidade):  A preserva√ß√£o da dimensionalidade no fluxo residual √© diretamente dependente do uso consistente de transforma√ß√µes lineares (nas camadas de aten√ß√£o e feedforward) e adi√ß√µes residuais. A *LayerNorm* e as conex√µes residuais tamb√©m mant√™m essa propriedade.  Se outras transforma√ß√µes n√£o lineares que alterassem a dimensionalidade fossem introduzidas, a invari√¢ncia da dimensionalidade do fluxo residual seria perdida.

> üí° **Exemplo Num√©rico:**
> Para ilustrar a import√¢ncia das transforma√ß√µes lineares, vamos considerar uma modifica√ß√£o hipot√©tica onde a camada FFN, ao inv√©s de usar transforma√ß√µes lineares, aplica uma fun√ß√£o que altera a dimensionalidade:
> $$t^4 = FFN_{modified}(t^3) = \begin{bmatrix} t^3[1] + t^3[2] \\ t^3[3] - t^3[0]  \end{bmatrix}$$
> Onde $t^3[i]$ representa o i-√©simo elemento do vetor $t^3$.
> Se $t^3 = \begin{bmatrix} 0.8 \\ -0.1 \\ -0.5 \\ 0.8 \end{bmatrix}$, ent√£o:
>$$t^4 = FFN_{modified}(t^3) = \begin{bmatrix} -0.1 - 0.5 \\ 0.8 - 0.8 \end{bmatrix} = \begin{bmatrix} -0.6 \\ 0.0 \end{bmatrix}$$
>
> Aqui, a dimensionalidade do vetor passou de *$d=4$* para *$d=2$*, quebrando a invari√¢ncia da dimensionalidade do *residual stream*. Este exemplo demonstra como a escolha cuidadosa das opera√ß√µes (transforma√ß√µes lineares e adi√ß√µes residuais) √© essencial para garantir a propriedade do *residual stream*.

**Teorema 1** (Generaliza√ß√£o do Fluxo Residual): O conceito do fluxo residual pode ser generalizado para outros modelos de *Deep Learning* que utilizam conex√µes residuais e mecanismos de aten√ß√£o, como as redes convolucionais com aten√ß√£o e modelos h√≠bridos.

*Prova:*
I. A ideia central do fluxo residual reside no uso de conex√µes residuais (ou *skip connections*) para preservar informa√ß√µes da entrada original, enquanto camadas adicionais aplicam transforma√ß√µes.
II. Modelos como redes convolucionais com aten√ß√£o e modelos h√≠bridos tamb√©m empregam conex√µes residuais, seja entre blocos convolucionais ou camadas de aten√ß√£o.
III. A no√ß√£o de que representa√ß√µes intermedi√°rias s√£o propagadas atrav√©s do modelo, com conex√µes residuais atuando como "atalhos" que preservam a informa√ß√£o original e opera√ß√µes como a aten√ß√£o e convolu√ß√µes enriquecendo a representa√ß√£o, √© universal a esses modelos.
IV. Podemos, ent√£o, generalizar o conceito de *residual stream*, observando que ele n√£o √© exclusivo dos *transformers*, mas sim um conceito que se aplica a qualquer arquitetura que utiliza conex√µes residuais.
V. Em cada caso, o *residual stream* representa o fluxo de representa√ß√µes da entrada atrav√©s do modelo, com a dimensionalidade das representa√ß√µes (que pode n√£o ser *$d$* em todos os casos) mantida ou transformada de forma consistente em cada bloco/camada.
VI. Seja a representa√ß√£o da entrada um *embedding* de *token* (como em *transformers*) ou um mapa de caracter√≠sticas de uma imagem (como em redes convolucionais), o conceito de fluxo residual √© aplic√°vel, desde que existam conex√µes residuais que "atalhem" o processamento e que as opera√ß√µes preservem ou transformem a representa√ß√£o de maneira consistente.
VII. Portanto, o conceito do *residual stream* pode ser aplicado a modelos que utilizam conex√µes residuais, generalizando o conceito para al√©m da arquitetura do *transformer*.
‚ñ†

**Teorema 1.1** (Fluxo Residual em Redes Convolucionais com Aten√ß√£o): Em redes convolucionais com aten√ß√£o que utilizam conex√µes residuais, o conceito de *residual stream* √© an√°logo ao do *transformer*, com a diferen√ßa de que as representa√ß√µes n√£o s√£o necessariamente *embeddings* de *tokens*, mas mapas de caracter√≠sticas (*feature maps*). A dimensionalidade desses mapas pode ser alterada por camadas convolucionais, mas as conex√µes residuais garantem que a informa√ß√£o original seja preservada.

*Prova:*
I. Redes convolucionais com aten√ß√£o utilizam camadas convolucionais para extrair caracter√≠sticas (*features*) de uma entrada e camadas de aten√ß√£o para ponderar a import√¢ncia dessas caracter√≠sticas.
II.  Conex√µes residuais s√£o usadas para conectar a sa√≠da de um bloco de camadas convolucionais (ou camadas de aten√ß√£o) diretamente √† entrada de um bloco seguinte, permitindo que a informa√ß√£o original da entrada seja preservada.
III. O *residual stream* nesse contexto representa o fluxo dos mapas de caracter√≠sticas atrav√©s dos blocos de convolu√ß√µes, camadas de aten√ß√£o e conex√µes residuais.
IV.  A dimensionalidade do mapa de caracter√≠sticas pode ser alterada ao longo do *residual stream* pelas camadas convolucionais (por exemplo, atrav√©s do uso de *strides*), no entanto, os *skip connections*  permitem que a informa√ß√£o original seja preservada mesmo ap√≥s essas mudan√ßas de dimensionalidade.
V.  Portanto, o conceito do *residual stream* se aplica a essas arquiteturas, indicando que a informa√ß√£o flui atrav√©s do modelo, com conex√µes residuais evitando que a informa√ß√£o original se perca ao longo da transforma√ß√£o.
‚ñ†
**Corol√°rio 1.1** (Composi√ß√£o de Blocos Transformer): A partir da propriedade de invari√¢ncia da dimensionalidade do fluxo residual (Proposi√ß√£o 1.1), a sa√≠da da aplica√ß√£o de um bloco *transformer*, seja *$f$* ou *$g$*, pode ser usada como entrada para outro bloco *transformer*. A composi√ß√£o iterativa de v√°rios blocos n√£o modifica a dimens√£o *$d$* do *embedding*.

*Prova:*
I. A Proposi√ß√£o 1.1 estabelece que a sa√≠da de um bloco *transformer* (tanto na arquitetura *pre-norm* quanto *post-norm*), representada por $h_i$, tem a mesma dimens√£o *$d$* da entrada $x_i$.
II. Ao aplicar um segundo bloco *transformer* √† sa√≠da $h_i$ do primeiro bloco, a sa√≠da deste segundo bloco, denotada como $h_i'$, tamb√©m ter√° dimens√£o *$d$*.
III. Este processo pode ser repetido iterativamente, com a sa√≠da de cada bloco servindo como entrada para o seguinte, sem que a dimensionalidade *$d$* seja alterada.
IV. A composi√ß√£o iterativa de blocos *transformer*, portanto, preserva a dimensionalidade *$d$* do *embedding* ao longo de todo o fluxo residual, permitindo a cria√ß√£o de modelos mais profundos e complexos.
‚ñ†

**Corol√°rio 1.2** (Implica√ß√µes da Invari√¢ncia da Dimensionalidade): A invari√¢ncia da dimensionalidade do fluxo residual, al√©m de permitir a composi√ß√£o de blocos, tamb√©m simplifica a implementa√ß√£o de modelos *transformers*. A necessidade de projetar as sa√≠das das diferentes camadas para uma mesma dimens√£o n√£o existe, o que reduz a complexidade do projeto arquitet√¥nico.

*Prova:*
I.  A Proposi√ß√£o 1.1 e o Corol√°rio 1.1 demonstram que a dimensionalidade *$d$* do vetor *embedding* √© preservada atrav√©s de todas as opera√ß√µes em um bloco *transformer* e pela composi√ß√£o iterativa de blocos.
II.  Essa caracter√≠stica elimina a necessidade de projetar as sa√≠das das diferentes camadas para uma mesma dimens√£o, simplificando o processo de projeto e implementa√ß√£o do modelo.
III. Ao garantir que todas as representa√ß√µes ao longo do *residual stream* tenham a mesma dimens√£o, a arquitetura se torna mais modular e f√°cil de compor, possibilitando a constru√ß√£o de modelos com um n√∫mero elevado de camadas sem a complexidade de um gerenciamento adicional da dimensionalidade.
IV. Portanto, a invari√¢ncia da dimensionalidade n√£o s√≥ permite a cria√ß√£o de modelos mais profundos, mas tamb√©m simplifica a arquitetura e sua implementa√ß√£o, tornando os *transformers* mais vers√°teis e eficientes.
‚ñ†

### Conclus√£o
Neste cap√≠tulo, exploramos o conceito do fluxo residual a partir de uma nova perspectiva, visualizando-o como uma sequ√™ncia de representa√ß√µes *$d$*-dimensionais. Cada componente do bloco *transformer* adiciona uma nova vis√£o ao *embedding* do *token*, contribuindo para a modelagem das complexidades da linguagem. A autoaten√ß√£o, como vimos, desempenha um papel fundamental neste processo, pois √© a √∫nica camada que interage diretamente com outros *tokens*, adicionando informa√ß√£o contextual ao vetor. As conex√µes residuais garantem que as informa√ß√µes originais n√£o sejam perdidas, permitindo a constru√ß√£o de redes mais profundas e complexas. Em ess√™ncia, o *residual stream* oferece uma vis√£o detalhada do funcionamento interno do *transformer*, onde cada *token* √© transformado de maneira individual, mantendo a dimensionalidade do *embedding* ao longo de todo o processo. A invari√¢ncia da dimensionalidade no fluxo residual √© crucial para a arquitetura do *transformer*, pois garante que a composi√ß√£o de blocos possa ocorrer sem altera√ß√µes na forma dos *embeddings*, permitindo a constru√ß√£o de modelos com um n√∫mero elevado de camadas.

### Refer√™ncias
[^1]: *‚ÄúIn this chapter we formalize this idea of pretraining‚Äîlearning knowledge about language and the world from vast amounts of text‚Äîand call the resulting pretrained language models large language models.‚Äù*
[^6]: *‚ÄúTo capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value.‚Äù*
[^7]: *‚ÄúThis description of the self-attention process has been from the perspective of computing a single output at a single time step i.‚Äù*
[^12]: *‚ÄúThe previous sections viewed the transformer block as applied to the entire N-token input X of shape [N √ó d], producing an output also of shape [N √ó d].‚Äù*
[^13]: *‚Äúthese layers as a stream of d-dimensional representations, called the residual stream and visualized in Fig. 10.7.‚Äù*
[^15]: "Given an input token string like Thanks for all the we first convert the tokens into vocabulary indices (these were created when we first tokenized the input using BPE or SentencePiece). "
<!-- END -->
