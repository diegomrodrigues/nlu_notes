## O Fluxo Residual no Bloco Transformer: Uma An√°lise Detalhada

### Introdu√ß√£o
Neste cap√≠tulo, exploraremos o conceito do **fluxo residual** (*residual stream*) dentro do contexto dos blocos *transformer*. Construindo sobre a compreens√£o das opera√ß√µes de autoaten√ß√£o e blocos *transformer* que vimos anteriormente [^1, ^6, ^7], aqui focaremos em como cada *token* individual, representado por um vetor *d*-dimensional, √© processado atrav√©s das v√°rias camadas do *transformer*. Essa perspectiva, conhecida como a *residual stream view*, oferece uma compreens√£o mais intuitiva do funcionamento interno do modelo. O *residual stream* √© o fluxo de representa√ß√µes *d*-dimensionais atrav√©s das camadas do *transformer*, com as conex√µes residuais copiando informa√ß√µes de camadas anteriores para camadas posteriores [^13]. Assim, vamos analisar o funcionamento do *transformer* sob a √≥tica do processamento de *tokens* individuais.

### Conceitos Fundamentais

A vis√£o tradicional de um bloco *transformer* considera uma entrada *X* de dimens√£o *[N x d]*, onde *N* √© o n√∫mero de *tokens* e *d* a dimensionalidade dos *embeddings*. O bloco *transformer* transforma esta entrada em uma sa√≠da *H* tamb√©m de dimens√£o *[N x d]*. No entanto, a perspectiva do fluxo residual foca no que acontece a cada *token* individual, ou seja, a cada linha dessa matriz [^12].

Cada *token* de entrada $x_i$ √© um vetor *d*-dimensional. O *embedding* inicial √© a base do *residual stream*, e este √© transmitido atrav√©s das conex√µes residuais. As sa√≠das das camadas de *feedforward* e de aten√ß√£o s√£o somadas ao *stream*. Em cada bloco e camada, estamos passando um *embedding* de forma *[1 x d]* [^13]. Isso significa que, em cada etapa do processamento, a dimensionalidade do vetor que representa o *token* √© mantida constante em *d*.

As conex√µes residuais s√£o cruciais para este fluxo. Elas copiam informa√ß√µes de *embeddings* anteriores para os posteriores, garantindo que as informa√ß√µes iniciais n√£o se percam durante o processamento.  Os componentes do bloco *transformer* adicionam novas perspectivas a esta representa√ß√£o [^13]. Os *feedforward networks* acrescentam uma vis√£o diferente do *embedding* anterior [^13].

> üí° **Exemplo Num√©rico:**
> Vamos supor que temos um *token* representado por um vetor $x_i$ de dimens√£o $d = 4$:
> $$x_i = \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}$$
> Este vetor $x_i$ representa o *embedding* inicial do *token* e ser√° o ponto de partida do *residual stream*. Ao longo do processamento no bloco *transformer*, novas informa√ß√µes ser√£o adicionadas a este vetor, sem alterar sua dimensionalidade.

**Autoaten√ß√£o no Fluxo Residual**

Um ponto crucial da *residual stream view* √© que a autoaten√ß√£o √© o √∫nico componente que realmente interage com informa√ß√µes de outros *tokens* no contexto [^13]. Essa intera√ß√£o ocorre atrav√©s da concatena√ß√£o dos *embeddings* de *query*, *key* e *value*, onde, como j√° visto anteriormente, cada *token* desempenha essas tr√™s fun√ß√µes [^6, ^7].

*   A camada de *MultiHeadAttention* recebe a entrada $x_i$, e considera as informa√ß√µes de todos os outros *tokens* $x_1...x_n$ no contexto, de acordo com a equa√ß√£o:
$$
t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])
$$
[10.32]
Essa opera√ß√£o, como j√° vimos [^7], consiste em calcular *queries*, *keys* e *values* para cada *token*, e aplicar um mecanismo de aten√ß√£o para combinar informa√ß√µes de acordo com o contexto. O resultado $t^1$ √© uma representa√ß√£o *d*-dimensional do *token* atual, influenciada pelos *tokens* vizinhos.

*   O resultado $t^1$ √© ent√£o adicionado ao *embedding* original atrav√©s de uma conex√£o residual:
$$
t^2 = t^1 + x_i
$$
[10.33]
A conex√£o residual garante que a informa√ß√£o original $x_i$ n√£o seja perdida durante a transforma√ß√£o, somando-se diretamente √† sa√≠da da camada de autoaten√ß√£o. Isso facilita o fluxo de gradiente durante o treinamento e evita o desaparecimento de informa√ß√µes.

*   Em seguida, o resultado √© normalizado com Layer Norm:
$$
t^3 = LayerNorm(t^2)
$$
[10.34]
A normaliza√ß√£o da camada garante que as ativa√ß√µes em cada camada tenham uma m√©dia pr√≥xima de zero e um desvio padr√£o pr√≥ximo de um. Isso ajuda a estabilizar o treinamento, evitando que as ativa√ß√µes sejam muito grandes ou muito pequenas.

*   E o resultado √© passado pela rede *feedforward*:
$$
t^4 = FFN(t^3)
$$
[10.35]
A rede *feedforward* √© uma rede neural com duas camadas, que adiciona n√£o-linearidade ao *embedding*. Essa camada √© aplicada de forma independente a cada *token*.

*   A sa√≠da da rede *feedforward* tamb√©m √© adicionada atrav√©s de uma conex√£o residual:
$$
t^5 = t^4 + t^3
$$
[10.36]
Assim como na primeira conex√£o residual, esta segunda tamb√©m preserva a informa√ß√£o e facilita o fluxo de gradientes.

*   E por fim normalizada novamente, produzindo a sa√≠da do bloco *transformer*:
$$
h_i = LayerNorm(t^5)
$$
[10.37]
A normaliza√ß√£o da camada final √© aplicada para garantir que a sa√≠da tenha uma distribui√ß√£o bem comportada.

Esta sequ√™ncia de opera√ß√µes representa o processamento de um *token* individual ao longo do *residual stream*. A autoaten√ß√£o permite que a informa√ß√£o flua de outros *tokens* para o *stream* do *token* atual, enquanto as outras camadas adicionam diferentes perspectivas √† representa√ß√£o do mesmo [^13]. Este processo garante que a informa√ß√£o flua ao longo do modelo, mantendo a dimensionalidade do *embedding* em cada etapa.

> üí° **Exemplo Num√©rico:**
> Continuando o exemplo anterior, vamos supor que o resultado da camada de *MultiHeadAttention* seja:
> $$t^1 = \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix}$$
> A adi√ß√£o residual resulta em:
> $$t^2 = t^1 + x_i =  \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix} + \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 0.6 \\ -0.5 \\ 1.2 \end{bmatrix}$$
>
> Para ilustrar o efeito do LayerNorm, vamos calcular a m√©dia e o desvio padr√£o de $t^2$:
> $$\text{M√©dia}(t^2) = \frac{1.2 + 0.6 - 0.5 + 1.2}{4} = 0.625$$
> $$\text{Desvio Padr√£o}(t^2) \approx 0.75$$
>
> LayerNorm centraliza e normaliza cada elemento usando:
> $$t^3_j = \frac{t^2_j - \text{M√©dia}(t^2)}{\text{Desvio Padr√£o}(t^2)}$$
>
> Aplicando a f√≥rmula, temos uma aproxima√ß√£o de:
> $$t^3 = LayerNorm(t^2) \approx \begin{bmatrix} 0.77 \\ -0.03 \\ -1.50 \\ 0.77 \end{bmatrix}$$
> Nota: LayerNorm tamb√©m aplica um ganho e bias aprendidos aos valores normalizados. Esta √© uma simplifica√ß√£o para fins did√°ticos.
>
>
> A rede *feedforward* aplica uma transforma√ß√£o linear seguida por uma n√£o linearidade, produzindo:
>$$t^4 = FFN(t^3) = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.3 \\ -0.1 \end{bmatrix}$$
>
> A segunda adi√ß√£o residual resulta em:
>$$t^5 = t^4 + t^3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.3 \\ -0.1 \end{bmatrix} + \begin{bmatrix} 0.77 \\ -0.03 \\ -1.50 \\ 0.77 \end{bmatrix} = \begin{bmatrix} 1.27 \\ -0.23 \\ -1.20 \\ 0.67 \end{bmatrix}$$
>
> Finalmente, a √∫ltima LayerNorm normaliza a sa√≠da do bloco *transformer* para:
>$$h_i = LayerNorm(t^5) \approx \begin{bmatrix} 1.35 \\ -0.69 \\ -1.65 \\ 0.98 \end{bmatrix}$$
>
> Observe que cada etapa mant√©m a dimens√£o do vetor em $d=4$, conforme descrito pela teoria do *residual stream*.
>
> √â importante notar que os valores ap√≥s o LayerNorm s√£o normalizados, com m√©dia pr√≥xima de 0 e vari√¢ncia pr√≥xima de 1, isso estabiliza o treinamento do modelo. As conex√µes residuais tamb√©m auxiliam no treinamento, permitindo que o fluxo do gradiente n√£o se perca no processo.

**Visualiza√ß√£o do Movimento da Informa√ß√£o**
Elhage et al. (2021) mostram que os *attention heads* movem informa√ß√£o do *residual stream* de um *token* vizinho para o *stream* do *token* atual. O espa√ßo de *embedding* de alta dimens√£o em cada posi√ß√£o cont√©m, assim, informa√ß√µes sobre o *token* atual e sobre os seus vizinhos [^13]. A autoaten√ß√£o, portanto, √© o principal mecanismo para a integra√ß√£o de informa√ß√µes contextuais no *residual stream*.

A visualiza√ß√£o do *residual stream*  nos ajuda a entender o fluxo de informa√ß√£o atrav√©s do *transformer*, como ilustrado na Fig. 10.7 do texto [^12]. Esta figura mostra como um *token* espec√≠fico, representado por um vetor, √© processado atrav√©s das v√°rias camadas do bloco *transformer*, com as conex√µes residuais garantindo que a informa√ß√£o original flua atrav√©s do modelo.

**Arquitetura Pre-norm vs. Post-norm**
√â importante notar que existem duas formas comuns para a ordem das opera√ß√µes no bloco *transformer*: *pre-norm* e *post-norm*. Na arquitetura *post-norm*, a normaliza√ß√£o da camada (*LayerNorm*) √© aplicada ap√≥s as opera√ß√µes de *MultiHeadAttention* e *FFN* (como descrito at√© agora) [^13]. Na arquitetura *pre-norm*, a *LayerNorm* √© aplicada antes dessas opera√ß√µes [^13].  A arquitetura *pre-norm* normalmente apresenta um melhor desempenho em muitos casos [^13]. A arquitetura *pre-norm* ajuda no treino do modelo, permitindo a propaga√ß√£o dos gradientes por todas as camadas. A diferen√ßa entre as arquiteturas *pre* e *post-norm* √© essencialmente a ordem de aplica√ß√£o da normaliza√ß√£o.

**Lema 1** (Equival√™ncia da Arquitetura Post-Norm): A sequ√™ncia de opera√ß√µes na arquitetura *post-norm* pode ser expressa como uma fun√ß√£o *f* que mapeia um vetor *d*-dimensional $x_i$ (o *embedding* de um token *i*) para outro vetor *d*-dimensional $h_i$, preservando a dimensionalidade do fluxo residual. Ou seja, para uma entrada $x_i$, a sa√≠da $h_i$ do bloco transformer √© dada por  $h_i$ = $f$($x_i$, $x_1$,..., $x_n$), onde $x_1$,..., $x_n$ s√£o os *embeddings* dos outros tokens.

*Prova:*
I.  A opera√ß√£o de *MultiHeadAttention*, dada por $t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])$, mapeia os *embeddings* de entrada, que s√£o vetores *d*-dimensionais, para um vetor de sa√≠da $t^1$ com a mesma dimensionalidade *d*.
II.  A adi√ß√£o residual, definida por $t^2 = t^1 + x_i$, soma dois vetores *d*-dimensionais, resultando em um vetor $t^2$ tamb√©m *d*-dimensional.
III. A normaliza√ß√£o da camada, $t^3 = LayerNorm(t^2)$, preserva a dimensionalidade, transformando o vetor *d*-dimensional $t^2$ em outro vetor *d*-dimensional $t^3$.
IV. A rede *feedforward*, $t^4 = FFN(t^3)$, mapeia o vetor *d*-dimensional $t^3$ para outro vetor *d*-dimensional $t^4$.
V. A segunda adi√ß√£o residual, $t^5 = t^4 + t^3$, soma dois vetores *d*-dimensionais, gerando o vetor $t^5$, tamb√©m *d*-dimensional.
VI. Finalmente, a normaliza√ß√£o da camada final, $h_i = LayerNorm(t^5)$, transforma o vetor *d*-dimensional $t^5$ em um vetor de sa√≠da $h_i$ que tamb√©m √© *d*-dimensional.
VII. Portanto, cada etapa preserva a dimensionalidade *d*. Assim, a fun√ß√£o composta *f* que mapeia $x_i$ para $h_i$ atrav√©s de todas essas opera√ß√µes tamb√©m preserva a dimensionalidade. Formalmente, podemos definir *f* como $f(x_i, x_1, \ldots, x_N) = LayerNorm(FFN(LayerNorm(MultiHeadAttention(x_i, [x_1, \ldots, x_N]) + x_i)) + LayerNorm(MultiHeadAttention(x_i, [x_1, \ldots, x_N]) + x_i))$.
‚ñ†

**Proposi√ß√£o 1** (Propriedade de Preserva√ß√£o da Dimensionalidade): Em ambas as arquiteturas, *pre-norm* e *post-norm*, o fluxo residual preserva a dimensionalidade dos *embeddings*. Ou seja, cada transforma√ß√£o aplicada a um *embedding d*-dimensional, resulta em outro *embedding d*-dimensional.

*Prova:*
I.  A arquitetura *post-norm*, como demonstrado no Lema 1, preserva a dimensionalidade atrav√©s de todas as suas opera√ß√µes: *MultiHeadAttention*, adi√ß√µes residuais, *LayerNorm* e *FFN*.
II.  Na arquitetura *pre-norm*, a *LayerNorm* √© aplicada antes das opera√ß√µes de *MultiHeadAttention* e *FFN*, mas ela tamb√©m preserva a dimensionalidade.
III. Como a *LayerNorm* n√£o altera a dimensionalidade dos vetores, as outras opera√ß√µes (MultiHeadAttention, adi√ß√µes residuais e FFN), quando aplicadas a vetores *d*-dimensionais, retornam vetores tamb√©m *d*-dimensionais, como demonstrado no Lema 1.
IV. A ordem das opera√ß√µes, seja *pre-norm* ou *post-norm*, n√£o afeta o fato de que cada opera√ß√£o preserva a dimensionalidade *d*.
V.  Portanto, em ambas as arquiteturas, o fluxo residual manipula vetores *d*-dimensionais em cada etapa, preservando a dimensionalidade dos *embeddings*.
‚ñ†

**Lema 1.1** (Representa√ß√£o da Arquitetura Pre-norm): Similarmente ao Lema 1, a sequ√™ncia de opera√ß√µes na arquitetura *pre-norm* tamb√©m pode ser expressa como uma fun√ß√£o *g* que mapeia um vetor *d*-dimensional $x_i$ para outro vetor *d*-dimensional $h_i$, mantendo a dimensionalidade do fluxo residual. Nesta arquitetura, a fun√ß√£o *g* aplicada √† entrada $x_i$ produz a sa√≠da $h_i$,  onde $h_i = g(x_i, x_1, ..., x_N)$.

*Prova:*
I. Na arquitetura *pre-norm*, a opera√ß√£o de normaliza√ß√£o da camada √© aplicada *antes* da *MultiHeadAttention*, resultando em $t^1 = LayerNorm(x_i)$. Como *LayerNorm* preserva a dimensionalidade, $t^1$ tamb√©m √© *d*-dimensional.
II. A opera√ß√£o *MultiHeadAttention* recebe $t^1$ (um vetor *d*-dimensional) e os outros *embeddings*, resultando em $t^2 = MultiHeadAttention(t^1, [x_1, \ldots, x_N])$. Assim, $t^2$ √© *d*-dimensional.
III. A adi√ß√£o residual √© aplicada somando $t^2$ com $t^1$, resultando em $t^3 = t^2 + t^1$. Portanto, $t^3$ tamb√©m √© *d*-dimensional.
IV. A normaliza√ß√£o da camada √© aplicada novamente, $t^4 = LayerNorm(t^3)$, preservando a dimens√£o, ou seja, $t^4$ √© *d*-dimensional.
V. A rede *feedforward* mapeia $t^4$ (um vetor *d*-dimensional) para outro vetor *d*-dimensional: $t^5 = FFN(t^4)$.
VI. A adi√ß√£o residual √© aplicada somando $t^5$ com $t^4$, resultando em $t^6 = t^5 + t^4$. Logo, $t^6$ √© *d*-dimensional.
VII. Finalmente, a normaliza√ß√£o da camada √© aplicada √† $t^6$, resultando em $h_i = LayerNorm(t^6)$. Portanto, $h_i$ √© tamb√©m *d*-dimensional.
VIII. Cada passo preserva a dimensionalidade *d*. A fun√ß√£o *g* composta por essas opera√ß√µes tamb√©m preserva a dimensionalidade. Formalmente, podemos definir *g* como $g(x_i, x_1, \ldots, x_N) = LayerNorm(FFN(LayerNorm(MultiHeadAttention(LayerNorm(x_i), [x_1, \ldots, x_N]) + LayerNorm(x_i))) + MultiHeadAttention(LayerNorm(x_i), [x_1, \ldots, x_N]) + LayerNorm(x_i))$.
‚ñ†

**Teorema 1** (Generaliza√ß√£o do Fluxo Residual): O fluxo residual pode ser generalizado para outros tipos de modelos de *Deep Learning* que utilizam conex√µes residuais e opera√ß√µes de aten√ß√£o, tais como redes convolucionais com *attention* e modelos h√≠bridos.

*Prova:*
I. O conceito central do fluxo residual √© o uso de conex√µes residuais (ou *skip connections*) para preservar informa√ß√µes da entrada original enquanto camadas adicionais aplicam transforma√ß√µes.
II. Modelos como redes convolucionais com *attention* e modelos h√≠bridos tamb√©m utilizam essas conex√µes residuais, seja entre blocos convolucionais ou *attention layers*.
III.  A ideia de que representa√ß√µes intermedi√°rias s√£o propagadas atrav√©s do modelo, onde as conex√µes residuais atuam como um "atalho" que preserva a informa√ß√£o original, e opera√ß√µes como a aten√ß√£o ou convolu√ß√µes enriquecem a representa√ß√£o, √© universal para esses modelos.
IV. Podemos generalizar que o *residual stream* n√£o √© espec√≠fico para *transformers*, mas sim um conceito que pode ser aplicado a qualquer arquitetura que utilize conex√µes residuais.
V. Em cada caso, o "fluxo residual" representa o fluxo de representa√ß√µes da entrada atrav√©s do modelo, com a dimensionalidade das representa√ß√µes (que pode n√£o ser *d* em todos os casos) sendo preservada ou transformada de maneira consistente em cada bloco/camada.
VI. Seja a representa√ß√£o da entrada um *embedding* de *token* (como nos *transformers*) ou um mapa de caracter√≠sticas de uma imagem (como em redes convolucionais), o conceito do fluxo residual √© aplic√°vel desde que existam conex√µes residuais que "atalham" o processamento e que as opera√ß√µes aplicadas preservam ou transformam a representa√ß√£o de maneira consistente.
VII. Portanto, o conceito do *residual stream* pode ser aplicado a modelos que utilizam conex√µes residuais, generalizando o conceito para al√©m da arquitetura do *transformer*.
‚ñ†
**Teorema 1.1** (Fluxo Residual e Invari√¢ncia de Dimensionalidade): O fluxo residual, como definido pelo Lema 1 e Lema 1.1, implica que a fun√ß√£o de transforma√ß√£o do bloco, seja ela *f* (post-norm) ou *g* (pre-norm), preserva a dimensionalidade do embedding de entrada. Isso garante que o fluxo de informa√ß√£o atrav√©s das camadas mantenha a mesma dimens√£o, facilitando a modelagem de depend√™ncias complexas.

*Prova:*
I. No Lema 1, demonstramos que a fun√ß√£o *f* na arquitetura *post-norm* mapeia um vetor de entrada $x_i$ de dimens√£o *d* para um vetor de sa√≠da $h_i$ tamb√©m de dimens√£o *d*, preservando a dimensionalidade.
II. Similarmente, no Lema 1.1, demonstramos que a fun√ß√£o *g* na arquitetura *pre-norm* tamb√©m mapeia um vetor de entrada $x_i$ de dimens√£o *d* para um vetor de sa√≠da $h_i$ de dimens√£o *d*, igualmente preservando a dimensionalidade.
III. Em ambos os casos, a preserva√ß√£o da dimensionalidade ocorre porque cada opera√ß√£o individual dentro das fun√ß√µes *f* e *g* (MultiHeadAttention, adi√ß√µes residuais, LayerNorm, FFN) mant√©m a dimensionalidade *d*.
IV. Portanto, o fluxo residual, seja na arquitetura *pre-norm* ou *post-norm*, garante que a dimensionalidade do embedding seja preservada em cada etapa da transforma√ß√£o.
V. Essa invari√¢ncia de dimensionalidade √© uma propriedade fundamental do fluxo residual, permitindo a aplica√ß√£o iterativa dos blocos *transformer* e o processamento sequencial de *embeddings* atrav√©s do modelo.
‚ñ†

### Conclus√£o
O conceito de *residual stream* oferece uma vis√£o detalhada do funcionamento interno de um *transformer*. Em vez de ver os blocos *transformer* como transforma√ß√µes sobre matrizes de *tokens*, visualizamos o processamento de cada *token* individualmente atrav√©s de um fluxo de *embeddings d-dimensional*. As conex√µes residuais garantem que as informa√ß√µes de *embeddings* anteriores sejam passadas para camadas posteriores, enquanto a autoaten√ß√£o permite a incorpora√ß√£o de informa√ß√µes contextuais. A arquitetura do *transformer* e a sua capacidade de modelar depend√™ncias de longo alcance s√£o, em grande parte, uma consequ√™ncia desta abordagem. Compreender o fluxo residual √© crucial para analisar o comportamento dos *transformers*, bem como para desenvolver modelos com arquiteturas alternativas.

### Refer√™ncias
[^1]: *‚ÄúIn this chapter we formalize this idea of pretraining‚Äîlearning knowledge about language and the world from vast amounts of text‚Äîand call the resulting pretrained language models large language models.‚Äù*
[^6]: *‚ÄúTo capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value.‚Äù*
[^7]: *‚ÄúThis description of the self-attention process has been from the perspective of computing a single output at a single time step i.‚Äù*
[^12]: *‚ÄúThe previous sections viewed the transformer block as applied to the entire N-token input X of shape [N √ó d], producing an output also of shape [N √ó d].‚Äù*
[^13]: *‚Äúthese layers as a stream of d-dimensional representations, called the residual stream and visualized in Fig. 10.7.‚Äù*
<!-- END -->
