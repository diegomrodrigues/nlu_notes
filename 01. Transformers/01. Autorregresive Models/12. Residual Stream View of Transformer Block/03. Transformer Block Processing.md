## O Fluxo Residual no Bloco Transformer: Uma An√°lise Detalhada

### Introdu√ß√£o
Neste cap√≠tulo, exploraremos o conceito do **fluxo residual** (*residual stream*) dentro do contexto dos blocos *transformer*. Em continuidade com a discuss√£o anterior sobre o conceito de autoaten√ß√£o e as diversas camadas do *transformer* [^1, ^6, ^7], aprofundaremos nossa an√°lise sobre o processamento de cada *token* individual dentro do bloco *transformer*. Ao inv√©s de tratar o bloco *transformer* como uma opera√ß√£o sobre uma matriz de *tokens*, iremos focar em como cada *token* individual, representado por um vetor *d*-dimensional, √© processado sequencialmente atrav√©s das camadas do *transformer*, utilizando conex√µes residuais para preservar a informa√ß√£o original do *embedding* [^13]. Essa perspectiva, conhecida como a *residual stream view*, oferece uma compreens√£o mais intuitiva do funcionamento interno do modelo. O *residual stream*, portanto, representa o fluxo de representa√ß√µes *d*-dimensionais atrav√©s do *transformer*, onde as conex√µes residuais garantem que informa√ß√µes de camadas anteriores sejam copiadas para camadas posteriores [^13].

### Conceitos Fundamentais
A vis√£o tradicional de um bloco *transformer* descreve o processamento de uma matriz de *tokens* de entrada *X*, com dimens√µes *[N x d]*, onde *N* representa o n√∫mero de *tokens* e *d* a dimensionalidade dos *embeddings*. A sa√≠da *H* do bloco *transformer* tamb√©m possui as mesmas dimens√µes *[N x d]*. Contudo, a perspectiva do fluxo residual nos convida a examinar o processamento de cada *token* individualmente [^12].

Cada *token* de entrada $x_i$ √© representado por um vetor *d*-dimensional. O *embedding* inicial deste vetor constitui a base do *residual stream*. Este *embedding* √© ent√£o transmitido atrav√©s das conex√µes residuais, que permitem que as informa√ß√µes da entrada original sejam preservadas ao longo do processamento. As sa√≠das das camadas de *feedforward* (*FFN*) e da camada de aten√ß√£o (*MultiHeadAttention*) s√£o adicionadas a este *stream* atrav√©s de conex√µes residuais, com o prop√≥sito de adicionar diferentes perspectivas √† representa√ß√£o do *token*.  Em cada bloco e camada do *transformer*, o *embedding* que est√° sendo processado mant√©m a forma *[1 x d]* [^13].

As conex√µes residuais s√£o componentes cruciais para este fluxo. Elas garantem a passagem de informa√ß√µes de *embeddings* anteriores para os posteriores, evitando a perda de informa√ß√µes ao longo do processamento.  Al√©m disso, os componentes do bloco *transformer* adicionam diferentes vis√µes da representa√ß√£o do *token*, cada uma contribuindo para o enriquecimento do *embedding* [^13]. O *feedforward network* adiciona uma perspectiva n√£o-linear do *embedding* anterior [^13].

> üí° **Exemplo Num√©rico:**
> Para ilustrar este conceito, vamos considerar um *token* representado por um vetor $x_i$ com dimens√£o $d = 4$:
> $$x_i = \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}$$
> Este vetor $x_i$ serve como o *embedding* inicial do *token* e representa o ponto de partida do *residual stream*. Ao longo do processamento do bloco *transformer*, novas informa√ß√µes contextuais s√£o adicionadas a este vetor, atrav√©s das camadas de aten√ß√£o e *feedforward*, sem modificar a sua dimensionalidade.

**Autoaten√ß√£o no Contexto do Fluxo Residual**

Um aspecto chave da *residual stream view* √© que a autoaten√ß√£o √© o √∫nico componente que explicitamente interage com informa√ß√µes de outros *tokens* no contexto [^13]. Essa intera√ß√£o ocorre por meio da concatena√ß√£o dos *embeddings* de *query*, *key* e *value*, onde, como vimos previamente, cada *token* desempenha estas tr√™s fun√ß√µes no mecanismo de autoaten√ß√£o [^6, ^7].

*   A camada de *MultiHeadAttention* recebe o *embedding* de entrada $x_i$, e integra informa√ß√µes de todos os outros *tokens* $x_1...x_N$ no contexto, de acordo com a equa√ß√£o:
$$
t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])
$$
[10.32]
A opera√ß√£o de autoaten√ß√£o, como j√° demonstrado [^7], calcula *queries*, *keys* e *values* para cada *token*, utilizando um mecanismo de aten√ß√£o para combinar informa√ß√µes contextuais. O resultado $t^1$ √© uma representa√ß√£o *d*-dimensional do *token* atual que reflete a influ√™ncia dos *tokens* vizinhos.

*   O resultado $t^1$ √© ent√£o adicionado ao *embedding* original atrav√©s de uma conex√£o residual:
$$
t^2 = t^1 + x_i
$$
[10.33]
Esta conex√£o residual assegura que a informa√ß√£o original $x_i$ n√£o se perca durante a transforma√ß√£o, somando-a diretamente √† sa√≠da da camada de autoaten√ß√£o. Isso facilita o fluxo do gradiente durante o treinamento e evita o desaparecimento de informa√ß√µes.

*  Em seguida, o resultado passa por uma normaliza√ß√£o de camada (*LayerNorm*):
$$
t^3 = LayerNorm(t^2)
$$
[10.34]
A normaliza√ß√£o da camada garante que as ativa√ß√µes em cada camada tenham uma m√©dia pr√≥xima de zero e um desvio padr√£o pr√≥ximo de um, o que auxilia na estabiliza√ß√£o do treinamento, evitando que as ativa√ß√µes sejam muito grandes ou muito pequenas.

*   O resultado √© ent√£o processado pela rede *feedforward* (*FFN*):
$$
t^4 = FFN(t^3)
$$
[10.35]
A rede *feedforward* consiste de uma rede neural de duas camadas que adiciona n√£o-linearidade √† representa√ß√£o. Essa camada √© aplicada de maneira independente a cada *token*, o que permite o processamento paralelo dos *tokens*.

*   A sa√≠da da rede *feedforward* tamb√©m √© adicionada atrav√©s de uma conex√£o residual:
$$
t^5 = t^4 + t^3
$$
[10.36]
Similar √† primeira conex√£o residual, esta segunda preserva a informa√ß√£o e facilita o fluxo de gradientes ao longo da rede.

*   Finalmente, o resultado √© normalizado novamente usando *LayerNorm*, gerando a sa√≠da do bloco *transformer*:
$$
h_i = LayerNorm(t^5)
$$
[10.37]
A normaliza√ß√£o final garante que a sa√≠da tenha uma distribui√ß√£o bem comportada, o que contribui para a estabilidade do modelo.

Esta sequ√™ncia de opera√ß√µes descreve o processamento de um √∫nico *token* ao longo do *residual stream*. A autoaten√ß√£o permite que a informa√ß√£o flua de outros *tokens* para o *stream* do *token* atual, enquanto as demais camadas adicionam diferentes perspectivas e transforma√ß√µes √† representa√ß√£o do mesmo [^13]. Todo este processo ocorre de forma a preservar a dimensionalidade do *embedding*, que se mant√©m em *d* em todas as etapas.

> üí° **Exemplo Num√©rico:**
> Continuando com o exemplo anterior, vamos assumir que a sa√≠da da camada de *MultiHeadAttention* seja:
> $$t^1 = \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix}$$
> A adi√ß√£o residual resulta em:
> $$t^2 = t^1 + x_i =  \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix} + \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 0.6 \\ -0.5 \\ 1.2 \end{bmatrix}$$
>
> Para ilustrar o efeito do *LayerNorm*, vamos calcular a m√©dia e o desvio padr√£o de $t^2$:
> $$\text{M√©dia}(t^2) = \frac{1.2 + 0.6 - 0.5 + 1.2}{4} = 0.625$$
> $$\text{Desvio Padr√£o}(t^2) \approx 0.75$$
>
> O *LayerNorm* centraliza e normaliza cada elemento utilizando a seguinte f√≥rmula (que tamb√©m inclui um ganho e um bias aprendidos, omitidos aqui para simplificar):
> $$t^3_j = \frac{t^2_j - \text{M√©dia}(t^2)}{\text{Desvio Padr√£o}(t^2)}$$
>
> Aplicando esta f√≥rmula, temos uma aproxima√ß√£o de:
> $$t^3 = LayerNorm(t^2) \approx \begin{bmatrix} 0.77 \\ -0.03 \\ -1.50 \\ 0.77 \end{bmatrix}$$
>
> A rede *feedforward* aplica uma transforma√ß√£o linear seguida por uma n√£o linearidade, resultando em:
>$$t^4 = FFN(t^3) = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.3 \\ -0.1 \end{bmatrix}$$
>
> A segunda adi√ß√£o residual resulta em:
>$$t^5 = t^4 + t^3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.3 \\ -0.1 \end{bmatrix} + \begin{bmatrix} 0.77 \\ -0.03 \\ -1.50 \\ 0.77 \end{bmatrix} = \begin{bmatrix} 1.27 \\ -0.23 \\ -1.20 \\ 0.67 \end{bmatrix}$$
>
> Finalmente, a √∫ltima *LayerNorm* normaliza a sa√≠da do bloco *transformer* para:
>$$h_i = LayerNorm(t^5) \approx \begin{bmatrix} 1.35 \\ -0.69 \\ -1.65 \\ 0.98 \end{bmatrix}$$
>
> Observe que cada etapa mant√©m a dimens√£o do vetor em $d=4$, conforme descrito pela teoria do *residual stream*. A aplica√ß√£o do LayerNorm tamb√©m normaliza a distribui√ß√£o dos valores ao longo de cada dimens√£o, o que auxilia na estabiliza√ß√£o do treinamento.

**Visualiza√ß√£o do Movimento da Informa√ß√£o**
Elhage et al. (2021) demonstraram que os *attention heads* transferem informa√ß√£o do *residual stream* de um *token* vizinho para o *stream* do *token* atual. Assim, o espa√ßo de *embedding* de alta dimens√£o em cada posi√ß√£o cont√©m informa√ß√£o sobre o *token* atual e seus vizinhos [^13]. A autoaten√ß√£o atua como o principal mecanismo de integra√ß√£o de informa√ß√µes contextuais no *residual stream*.

A visualiza√ß√£o do *residual stream* nos ajuda a entender como a informa√ß√£o flui atrav√©s do *transformer*, como ilustrado na Fig. 10.7 do texto [^12]. A figura representa o processamento de um *token* espec√≠fico por meio das v√°rias camadas do bloco *transformer*, com conex√µes residuais garantindo a manuten√ß√£o do fluxo original da informa√ß√£o e as camadas de aten√ß√£o e feedforward a adicionando informa√ß√£o contextual.

**Arquitetura Pre-norm vs. Post-norm**
Existem duas formas principais de organiza√ß√£o das opera√ß√µes no bloco *transformer*: as arquiteturas *pre-norm* e *post-norm*. Na arquitetura *post-norm*, a normaliza√ß√£o da camada (*LayerNorm*) √© aplicada ap√≥s as opera√ß√µes de *MultiHeadAttention* e *FFN*, como descrito acima [^13]. Por outro lado, na arquitetura *pre-norm*, a *LayerNorm* √© aplicada antes dessas opera√ß√µes [^13]. Em geral, a arquitetura *pre-norm* demonstra um desempenho superior em muitos cen√°rios [^13]. A arquitetura *pre-norm* facilita a propaga√ß√£o dos gradientes durante o treinamento, o que ajuda no aprendizado do modelo. A diferen√ßa entre as arquiteturas *pre* e *post-norm* reside, primariamente, na ordem da aplica√ß√£o da normaliza√ß√£o.

**Lema 1** (Equival√™ncia da Arquitetura Post-Norm): A sequ√™ncia de opera√ß√µes na arquitetura *post-norm* pode ser expressa por uma fun√ß√£o *f* que mapeia um vetor *d*-dimensional $x_i$ (o *embedding* de um *token* *i*) para outro vetor *d*-dimensional $h_i$, preservando a dimensionalidade do fluxo residual. Formalmente, dada uma entrada $x_i$, a sa√≠da $h_i$ do bloco *transformer* √© expressa como  $h_i$ = $f$($x_i$, $x_1$,..., $x_n$), onde $x_1$,..., $x_n$ representam os *embeddings* dos demais *tokens*.

*Prova:*
I.  A opera√ß√£o de *MultiHeadAttention*, dada por $t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])$, mapeia os *embeddings* de entrada, vetores *d*-dimensionais, para um vetor de sa√≠da $t^1$ com a mesma dimensionalidade *d*.
II. A adi√ß√£o residual, definida por $t^2 = t^1 + x_i$, resulta na soma de dois vetores *d*-dimensionais, obtendo-se um vetor $t^2$ tamb√©m *d*-dimensional.
III. A normaliza√ß√£o da camada, $t^3 = LayerNorm(t^2)$, preserva a dimensionalidade, transformando o vetor *d*-dimensional $t^2$ em outro vetor *d*-dimensional $t^3$.
IV. A rede *feedforward*, $t^4 = FFN(t^3)$, mapeia o vetor *d*-dimensional $t^3$ para outro vetor *d*-dimensional $t^4$.
V.  A segunda adi√ß√£o residual, $t^5 = t^4 + t^3$, resulta na soma de dois vetores *d*-dimensionais, gerando um vetor $t^5$, tamb√©m *d*-dimensional.
VI. Finalmente, a normaliza√ß√£o da camada final, $h_i = LayerNorm(t^5)$, transforma o vetor *d*-dimensional $t^5$ em um vetor de sa√≠da $h_i$ que tamb√©m √© *d*-dimensional.
VII. Portanto, cada etapa preserva a dimensionalidade *d*. Assim, a fun√ß√£o composta *f*, que mapeia $x_i$ para $h_i$ atrav√©s de todas as opera√ß√µes, tamb√©m preserva a dimensionalidade. Formalmente, podemos definir *f* como $f(x_i, x_1, \ldots, x_N) = LayerNorm(FFN(LayerNorm(MultiHeadAttention(x_i, [x_1, \ldots, x_N]) + x_i)) + LayerNorm(MultiHeadAttention(x_i, [x_1, \ldots, x_N]) + x_i))$.
‚ñ†

**Proposi√ß√£o 1** (Propriedade de Preserva√ß√£o da Dimensionalidade): Em ambas as arquiteturas, *pre-norm* e *post-norm*, o fluxo residual preserva a dimensionalidade dos *embeddings*. Ou seja, cada transforma√ß√£o aplicada a um *embedding d*-dimensional, resulta em outro *embedding d*-dimensional.

*Prova:*
I. A arquitetura *post-norm*, conforme demonstrado no Lema 1, preserva a dimensionalidade atrav√©s de todas as suas opera√ß√µes: *MultiHeadAttention*, adi√ß√µes residuais, *LayerNorm* e *FFN*.
II. Na arquitetura *pre-norm*, a *LayerNorm* √© aplicada antes das opera√ß√µes de *MultiHeadAttention* e *FFN*, contudo, ela tamb√©m preserva a dimensionalidade.
III. Como o *LayerNorm* n√£o altera a dimensionalidade dos vetores, as demais opera√ß√µes (MultiHeadAttention, adi√ß√µes residuais e *FFN*), quando aplicadas a vetores *d*-dimensionais, tamb√©m retornam vetores *d*-dimensionais, como demonstrado no Lema 1.
IV. A ordem das opera√ß√µes, seja *pre-norm* ou *post-norm*, n√£o afeta o fato de que cada opera√ß√£o mant√©m a dimensionalidade *d*.
V. Portanto, em ambas as arquiteturas, o fluxo residual manipula vetores *d*-dimensionais em cada etapa, preservando a dimensionalidade dos *embeddings*.
‚ñ†

**Lema 1.1** (Representa√ß√£o da Arquitetura Pre-norm): De maneira similar ao Lema 1, a sequ√™ncia de opera√ß√µes na arquitetura *pre-norm* tamb√©m pode ser expressa como uma fun√ß√£o *g* que mapeia um vetor *d*-dimensional $x_i$ para outro vetor *d*-dimensional $h_i$, mantendo a dimensionalidade do fluxo residual. Nesta arquitetura, a fun√ß√£o *g* aplicada √† entrada $x_i$ produz a sa√≠da $h_i$,  onde $h_i = g(x_i, x_1, ..., x_N)$.

*Prova:*
I. Na arquitetura *pre-norm*, a opera√ß√£o de normaliza√ß√£o da camada √© aplicada *antes* da *MultiHeadAttention*, resultando em $t^1 = LayerNorm(x_i)$. Como o *LayerNorm* preserva a dimensionalidade, $t^1$ tamb√©m √© *d*-dimensional.
II. A opera√ß√£o *MultiHeadAttention* recebe $t^1$ (um vetor *d*-dimensional) e os outros *embeddings*, resultando em $t^2 = MultiHeadAttention(t^1, [x_1, \ldots, x_N])$. Assim, $t^2$ √© *d*-dimensional.
III. A adi√ß√£o residual √© aplicada somando $t^2$ com $t^1$, resultando em $t^3 = t^2 + t^1$. Portanto, $t^3$ tamb√©m √© *d*-dimensional.
IV. A normaliza√ß√£o da camada √© aplicada novamente, $t^4 = LayerNorm(t^3)$, preservando a dimens√£o, ou seja, $t^4$ √© *d*-dimensional.
V. A rede *feedforward* mapeia $t^4$ (um vetor *d*-dimensional) para outro vetor *d*-dimensional: $t^5 = FFN(t^4)$.
VI. A adi√ß√£o residual √© aplicada somando $t^5$ com $t^4$, resultando em $t^6 = t^5 + t^4$. Logo, $t^6$ √© *d*-dimensional.
VII. Finalmente, a normaliza√ß√£o da camada √© aplicada √† $t^6$, resultando em $h_i = LayerNorm(t^6)$. Portanto, $h_i$ tamb√©m √© *d*-dimensional.
VIII. Cada passo preserva a dimensionalidade *d*. A fun√ß√£o *g* composta por essas opera√ß√µes tamb√©m preserva a dimensionalidade. Formalmente, podemos definir *g* como $g(x_i, x_1, \ldots, x_N) = LayerNorm(FFN(LayerNorm(MultiHeadAttention(LayerNorm(x_i), [x_1, \ldots, x_N]) + LayerNorm(x_i))) + MultiHeadAttention(LayerNorm(x_i), [x_1, \ldots, x_N]) + LayerNorm(x_i))$.
‚ñ†
**Proposi√ß√£o 1.1** (Invari√¢ncia da Dimensionalidade do Fluxo Residual): O fluxo residual, tanto na arquitetura pre-norm quanto na post-norm, garante que a dimens√£o do vetor de representa√ß√£o de cada token, ao longo do processamento do bloco transformer, seja invariante, permanecendo sempre igual a d.

*Prova:*
I. Conforme demonstrado nos Lemas 1 e 1.1, a aplica√ß√£o da fun√ß√£o *f* (post-norm) ou da fun√ß√£o *g* (pre-norm) sobre um vetor de entrada *d*-dimensional $x_i$ resulta em um vetor de sa√≠da *d*-dimensional $h_i$.
II. O processamento do bloco transformer, descrito pelas fun√ß√µes *f* e *g*, √© iterativo, ou seja, a sa√≠da de um bloco torna-se a entrada do pr√≥ximo bloco.
III. Dado que as fun√ß√µes *f* e *g* preservam a dimensionalidade *d*, e que a entrada de cada bloco seguinte tem a mesma dimensionalidade da sa√≠da do bloco anterior, o *residual stream* garante que a dimensionalidade *d* seja mantida ao longo de todo o processamento.
IV. Portanto, a dimensionalidade dos *embeddings* dos *tokens* √© invariante ao longo do fluxo residual, mantendo-se sempre igual a *d* em todas as camadas e blocos do *transformer*.
‚ñ†

**Teorema 1** (Generaliza√ß√£o do Fluxo Residual): O conceito do fluxo residual pode ser generalizado para outros tipos de modelos de *Deep Learning* que utilizam conex√µes residuais e opera√ß√µes de aten√ß√£o, incluindo redes convolucionais com aten√ß√£o e modelos h√≠bridos.

*Prova:*
I. O conceito chave do fluxo residual reside na utiliza√ß√£o de conex√µes residuais (ou *skip connections*) para manter as informa√ß√µes da entrada original, enquanto camadas adicionais aplicam transforma√ß√µes.
II. Modelos como redes convolucionais com aten√ß√£o e modelos h√≠bridos tamb√©m empregam conex√µes residuais, seja entre blocos convolucionais ou camadas de aten√ß√£o.
III. A ideia de que representa√ß√µes intermedi√°rias s√£o propagadas atrav√©s do modelo, onde as conex√µes residuais atuam como um "atalho" que preserva a informa√ß√£o original, e opera√ß√µes como aten√ß√£o ou convolu√ß√µes enriquecem a representa√ß√£o, √© aplic√°vel a esses modelos.
IV. Podemos generalizar que o *residual stream* n√£o √© espec√≠fico para *transformers*, sendo um conceito que se aplica a qualquer arquitetura que utilize conex√µes residuais.
V. Em cada caso, o *residual stream* representa o fluxo de representa√ß√µes da entrada atrav√©s do modelo, onde a dimensionalidade das representa√ß√µes (que pode n√£o ser *d* em todos os casos) √© mantida ou transformada de maneira consistente em cada bloco/camada.
VI. Quer a representa√ß√£o da entrada seja um *embedding* de *token* (como em *transformers*) ou um mapa de caracter√≠sticas de uma imagem (como em redes convolucionais), o conceito do fluxo residual √© aplic√°vel, desde que existam conex√µes residuais que "atalhem" o processamento e que as opera√ß√µes preservem ou transformem a representa√ß√£o de maneira consistente.
VII. Portanto, o conceito do *residual stream* √© uma ferramenta de an√°lise geral, que pode ser aplicada a diversos modelos com conex√µes residuais, generalizando o conceito para al√©m da arquitetura do *transformer*.
‚ñ†
**Teorema 1.1** (Fluxo Residual e Invari√¢ncia de Dimensionalidade): O fluxo residual, conforme definido pelos Lemas 1 e 1.1, implica que a fun√ß√£o de transforma√ß√£o do bloco, seja ela *f* (post-norm) ou *g* (pre-norm), preserva a dimensionalidade do *embedding* de entrada. Isso garante que o fluxo de informa√ß√£o atrav√©s das camadas mantenha a mesma dimens√£o, facilitando a modelagem de depend√™ncias complexas.

*Prova:*
I. No Lema 1, demonstramos que a fun√ß√£o *f* na arquitetura *post-norm* mapeia um vetor de entrada $x_i$ de dimens√£o *d* para um vetor de sa√≠da $h_i$ tamb√©m de dimens√£o *d*, preservando a dimensionalidade.
II. Similarmente, no Lema 1.1, demonstramos que a fun√ß√£o *g* na arquitetura *pre-norm* tamb√©m mapeia um vetor de entrada $x_i$ de dimens√£o *d* para um vetor de sa√≠da $h_i$ de dimens√£o *d*, igualmente preservando a dimensionalidade.
III. Em ambos os casos, a preserva√ß√£o da dimensionalidade ocorre porque cada opera√ß√£o individual dentro das fun√ß√µes *f* e *g* (*MultiHeadAttention*, adi√ß√µes residuais, *LayerNorm*, *FFN*) mant√©m a dimensionalidade *d*.
IV. Portanto, o fluxo residual, seja na arquitetura *pre-norm* ou *post-norm*, garante que a dimensionalidade do *embedding* seja preservada em cada etapa da transforma√ß√£o.
V. Essa invari√¢ncia de dimensionalidade √© uma propriedade fundamental do fluxo residual, permitindo a aplica√ß√£o iterativa dos blocos *transformer* e o processamento sequencial de *embeddings* atrav√©s do modelo.
‚ñ†
**Corol√°rio 1.1** (Composi√ß√£o de Blocos Transformer): Pela propriedade de invari√¢ncia da dimensionalidade do fluxo residual (Teorema 1.1), o resultado da aplica√ß√£o de um bloco transformer, seja *f* ou *g*, pode ser usado como entrada para outro bloco transformer. A composi√ß√£o iterativa de v√°rios blocos n√£o altera a dimens√£o *d* do embedding.

*Prova:*
I. O Teorema 1.1 estabelece que a sa√≠da de um bloco transformer (seja na arquitetura pre-norm ou post-norm), representada por $h_i$, tem a mesma dimens√£o *d* da entrada $x_i$.
II. Se aplicarmos um segundo bloco transformer √† sa√≠da $h_i$ do primeiro bloco, a sa√≠da deste segundo bloco, que podemos denotar como $h_i'$, tamb√©m ter√° dimens√£o *d*.
III. Este processo pode ser repetido v√°rias vezes, com a sa√≠da de cada bloco servindo como entrada para o pr√≥ximo, sem que a dimensionalidade *d* seja alterada.
IV. A composi√ß√£o iterativa de blocos transformer, portanto, preserva a dimensionalidade *d* do *embedding* ao longo de todo o fluxo residual, permitindo a cria√ß√£o de modelos mais profundos e complexos.
‚ñ†

### Conclus√£o
A perspectiva do *residual stream* oferece uma compreens√£o detalhada do funcionamento interno de um *transformer*, em que cada *token* √© processado individualmente como um vetor *d*-dimensional. As conex√µes residuais s√£o fundamentais para garantir a passagem de informa√ß√µes de *embeddings* anteriores para as camadas posteriores, enquanto a autoaten√ß√£o permite a incorpora√ß√£o de informa√ß√µes contextuais no *stream* de cada *token*. A autoaten√ß√£o, portanto, representa o √∫nico componente do *transformer* que interage com outros *tokens*, adicionando informa√ß√£o contextual ao *embedding* de cada *token*. Esta abordagem, combinada com a invari√¢ncia de dimensionalidade garantida pelo fluxo residual, contribui para a modelagem de depend√™ncias complexas no texto. A compreens√£o do fluxo residual √© essencial para analisar o comportamento dos *transformers*, bem como para o desenvolvimento de novas arquiteturas de redes neurais.

### Refer√™ncias
[^1]: *‚ÄúIn this chapter we formalize this idea of pretraining‚Äîlearning knowledge about language and the world from vast amounts of text‚Äîand call the resulting pretrained language models large language models.‚Äù*
[^6]: *‚ÄúTo capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value.‚Äù*
[^7]: *‚ÄúThis description of the self-attention process has been from the perspective of computing a single output at a single time step i.‚Äù*
[^12]: *‚ÄúThe previous sections viewed the transformer block as applied to the entire N-token input X of shape [N √ó d], producing an output also of shape [N √ó d].‚Äù*
[^13]: *‚Äúthese layers as a stream of d-dimensional representations, called the residual stream and visualized in Fig. 10.7.‚Äù*
<!-- END -->
