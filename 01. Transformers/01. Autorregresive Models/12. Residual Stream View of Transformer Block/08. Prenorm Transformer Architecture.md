## A Arquitetura *Prenorm* do Transformer: Normaliza√ß√£o Pr√©-Camada e seu Impacto no Fluxo Residual

### Introdu√ß√£o
Este cap√≠tulo explora em detalhes a arquitetura *prenorm* do *transformer*, uma varia√ß√£o da arquitetura tradicional que aplica a normaliza√ß√£o da camada (*LayerNorm*) antes das camadas de autoaten√ß√£o e *feedforward*, ao inv√©s de depois [^13]. Em continuidade com os cap√≠tulos anteriores que estabeleceram os fundamentos do *residual stream*, o papel da autoaten√ß√£o e a contribui√ß√£o da *FFN* e *LayerNorm* [^1, ^6, ^7, ^12, ^13], aqui analisaremos como a mudan√ßa na ordem da aplica√ß√£o da *LayerNorm* afeta o fluxo residual e o desempenho do modelo. O objetivo √© compreender as nuances da arquitetura *prenorm*, como ela difere da arquitetura *postnorm* e por que ela frequentemente apresenta um desempenho superior em muitas aplica√ß√µes. A arquitetura *prenorm* altera o fluxo de informa√ß√£o e o seu impacto na performance ser√° o foco principal.

### Arquitetura *Prenorm*: Normaliza√ß√£o Pr√©-Camada
Como discutido anteriormente, o *residual stream* representa um fluxo cont√≠nuo de vetores *d*-dimensionais que descrevem o processamento de um *token* atrav√©s das camadas do *transformer* [^13, Proposi√ß√£o 1]. A principal diferen√ßa entre as arquiteturas *prenorm* e *postnorm* reside na ordem em que a normaliza√ß√£o de camada (*LayerNorm*) √© aplicada em rela√ß√£o √†s camadas de autoaten√ß√£o (*multi-head attention*) e *feedforward* (*FFN*). Enquanto a arquitetura *postnorm* aplica o *LayerNorm* ap√≥s cada uma dessas camadas, a arquitetura *prenorm* aplica o *LayerNorm* antes dessas opera√ß√µes [^13].

Na arquitetura *prenorm*, o *LayerNorm* √© aplicado diretamente ao *embedding* do *token* $x_i$ antes de passar pela camada de autoaten√ß√£o:
$$
t^1 = LayerNorm(x_i)
$$
[10.38]
Em seguida, a sa√≠da normalizada $t^1$ √© processada pela camada *MultiHeadAttention*, juntamente com os outros *tokens* do contexto:
$$
t^2 = MultiHeadAttention(t^1, [x_1, \ldots, x_N])
$$
[10.39]
A sa√≠da da autoaten√ß√£o √© ent√£o combinada com a entrada normalizada $t^1$ atrav√©s de uma conex√£o residual:
$$
t^3 = t^2 + t^1
$$
[10.40]
A seguir, a sa√≠da √© normalizada novamente atrav√©s do *LayerNorm*:
$$
t^4 = LayerNorm(t^3)
$$
[10.41]
A representa√ß√£o normalizada √© processada pela rede *FFN*:
$$
t^5 = FFN(t^4)
$$
[10.42]
E a sa√≠da da FFN √© adicionada, atrav√©s de uma conex√£o residual:
$$
h_i = t^5 + t^4
$$
[10.43]
Finalmente, uma √∫ltima camada de normaliza√ß√£o √© aplicada √† sa√≠da do bloco, o que representa a principal diferen√ßa com rela√ß√£o ao *post-norm*:
$$
h_i = LayerNorm(h_i)
$$
No final, toda a cadeia de blocos tem a sua sa√≠da tamb√©m normalizada.

> üí° **Exemplo Num√©rico:**
> Vamos considerar um *token* representado pelo *embedding* $x_i$ de dimens√£o $d=4$:
> $$x_i = \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}$$
>
> Na arquitetura *prenorm*, o primeiro passo √© aplicar o *LayerNorm* em $x_i$. Suponha que ap√≥s calcular a m√©dia $\mu$ e o desvio padr√£o $\sigma$ (para um exemplo simplificado, vamos assumir que $\mu = 0.525$ e $\sigma = 0.5$), o *LayerNorm* resulta em:
> $$t^1 = LayerNorm(x_i) = \frac{x_i - \mu}{\sigma} = \begin{bmatrix} \frac{1.0-0.525}{0.5} \\ \frac{0.5-0.525}{0.5} \\ \frac{-0.2-0.525}{0.5} \\ \frac{0.8-0.525}{0.5} \end{bmatrix} \approx \begin{bmatrix} 0.95 \\ -0.05 \\ -1.45 \\ 0.55 \end{bmatrix}$$
>
> (Para simplicidade, vamos omitir os par√¢metros de ganho e bias do *LayerNorm* nos exemplos num√©ricos).
> Em seguida, a camada *MultiHeadAttention* √© aplicada, integrando a informa√ß√£o contextual com o vetor normalizado $t^1$, resultando em:
> $$t^2 = MultiHeadAttention(t^1, [x_1, \ldots, x_N]) = \begin{bmatrix} 0.2 \\ -0.1 \\ 0.3 \\ 0.1 \end{bmatrix}$$
>
> A adi√ß√£o residual resulta em:
> $$t^3 = t^2 + t^1 = \begin{bmatrix} 0.2 \\ -0.1 \\ 0.3 \\ 0.1 \end{bmatrix} + \begin{bmatrix} 0.95 \\ -0.05 \\ -1.45 \\ 0.55 \end{bmatrix} =  \begin{bmatrix} 1.15 \\ -0.15 \\ -1.15 \\ 0.65 \end{bmatrix}$$
>
> O segundo *LayerNorm* √© aplicado em $t^3$. Supondo, novamente, para simplificar que $\mu = -0.025$ e $\sigma=0.92$, temos:
> $$t^4 = LayerNorm(t^3) = \frac{t^3 - \mu}{\sigma} \approx \begin{bmatrix} \frac{1.15 + 0.025}{0.92} \\ \frac{-0.15 + 0.025}{0.92} \\ \frac{-1.15 + 0.025}{0.92} \\ \frac{0.65 + 0.025}{0.92} \end{bmatrix} \approx  \begin{bmatrix} 1.28 \\ -0.136 \\ -1.22 \\ 0.73 \end{bmatrix}$$
>
>  A sa√≠da $t^4$ √© ent√£o passada pela camada *FFN*:
>
> $$t^5 = FFN(t^4) = \begin{bmatrix} 0.1 \\ -0.4 \\ 0.2 \\ -0.1 \end{bmatrix}$$
>
> A adi√ß√£o residual resulta em
> $$h_i = t^5 + t^4 = \begin{bmatrix} 0.1 \\ -0.4 \\ 0.2 \\ -0.1 \end{bmatrix} + \begin{bmatrix} 1.28 \\ -0.136 \\ -1.22 \\ 0.73 \end{bmatrix} = \begin{bmatrix} 1.38 \\ -0.536 \\ -1.02 \\ 0.63 \end{bmatrix}$$
>
> Finalmente, a √∫ltima camada de normaliza√ß√£o, supondo $\mu = 0.1185$ e $\sigma = 1.0$ para simplificar:
> $$h_i = LayerNorm(h_i) = \frac{h_i - \mu}{\sigma} = \begin{bmatrix} \frac{1.38 - 0.1185}{1.0} \\ \frac{-0.536 - 0.1185}{1.0} \\ \frac{-1.02 - 0.1185}{1.0} \\ \frac{0.63 - 0.1185}{1.0} \end{bmatrix} =  \begin{bmatrix} 1.26 \\ -0.65 \\ -1.14 \\ 0.51 \end{bmatrix}$$
>
> Observe que todas as etapas mant√™m a mesma dimens√£o $d=4$. O *LayerNorm* em cada passo recalcula a m√©dia e o desvio padr√£o, normalizando os dados em cada etapa do processo.
> ```mermaid
> graph LR
>    A[xi] --> B(LayerNorm);
>    B --> C[t1]
>     C --> D(MultiHeadAttention);
>     E(Contextos) --> D;
>     D --> F[t2];
>     C --> G[t3=t2+t1];
>     F --> G;
>     G --> H(LayerNorm);
>     H --> I[t4];
>    I --> J(FFN);
>    J --> K[t5]
>    I --> L[hi=t4+t5];
>    K --> L;
>    L --> M(LayerNorm)
>    M --> N[hi_final]
> ```
> O diagrama acima demonstra o fluxo na arquitetura *prenorm*, com o *LayerNorm* sendo aplicado antes da *MultiHeadAttention*, novamente ap√≥s a conex√£o residual e ap√≥s a *FFN*, e uma √∫ltima camada de *LayerNorm* ap√≥s a sa√≠da do bloco.

**O Impacto da Normaliza√ß√£o Pr√©-Camada no Fluxo Residual**
A mudan√ßa na ordem da aplica√ß√£o do *LayerNorm* impacta o fluxo de informa√ß√£o no *residual stream*. Na arquitetura *postnorm*, o *LayerNorm* atua como um normalizador das sa√≠das das camadas de autoaten√ß√£o e *FFN*. Na arquitetura *prenorm*, o *LayerNorm* atua como um regulador das entradas dessas camadas, e da sa√≠da do bloco, al√©m de um estabilizador do gradiente nas primeiras camadas, uma vez que a entrada $x_i$ j√° √© normalizada.
A aplica√ß√£o da *LayerNorm* antes da autoaten√ß√£o e da *FFN* possibilita que estas camadas operem sobre dados normalizados, o que frequentemente resulta em um aprendizado mais est√°vel e eficiente, facilitando a converg√™ncia do modelo e melhorando a generaliza√ß√£o. Al√©m disso, como o *embedding* inicial do *token* j√° √© normalizado, √© poss√≠vel que o modelo tenha mais facilidade em processar essa informa√ß√£o inicial.

**An√°lise Comparativa: *Prenorm* vs. *Postnorm***
Embora ambas as arquiteturas, *prenorm* e *postnorm*, compartilhem o mesmo conceito de *residual stream*, a aplica√ß√£o da *LayerNorm* em posi√ß√µes diferentes resulta em diferentes din√¢micas de aprendizado. A arquitetura *prenorm*, ao normalizar a entrada das camadas de autoaten√ß√£o e *FFN*, permite que estas camadas trabalhem com dados dentro de uma escala mais controlada, evitando problemas como a explos√£o ou o desaparecimento do gradiente. Al√©m disso, a *LayerNorm* na sa√≠da do √∫ltimo bloco, garante a estabilidade da sa√≠da final para a pr√≥xima camada.

**Lema 1** (Equival√™ncia do *Residual Stream* na Arquitetura *Prenorm*): O fluxo residual na arquitetura *prenorm* tamb√©m preserva a dimensionalidade dos *embeddings*, com as transforma√ß√µes ocorrendo sobre vetores *d*-dimensionais em cada etapa.
*Prova:*
I.  A opera√ß√£o *LayerNorm* na entrada do bloco, definida por $t^1 = LayerNorm(x_i)$, mapeia o *embedding* $x_i$ (de dimens√£o *d*) para um vetor $t^1$ tamb√©m de dimens√£o *d*, como estabelecido pela Proposi√ß√£o 2.1.
II.  A opera√ß√£o *MultiHeadAttention*, definida por $t^2 = MultiHeadAttention(t^1, [x_1, \ldots, x_N])$, mapeia o vetor $t^1$ (de dimens√£o *d*) para um vetor $t^2$ tamb√©m de dimens√£o *d*.
III. A adi√ß√£o residual, definida por $t^3 = t^2 + t^1$, combina dois vetores de dimens√£o *d*, resultando em um vetor $t^3$ tamb√©m de dimens√£o *d*.
IV. A opera√ß√£o *LayerNorm* em $t^3$, definida por $t^4 = LayerNorm(t^3)$, mapeia o vetor $t^3$ (de dimens√£o *d*) para um vetor $t^4$ tamb√©m de dimens√£o *d*, como estabelecido pela Proposi√ß√£o 2.1.
V. A opera√ß√£o *FFN*, definida por $t^5 = FFN(t^4)$, mapeia o vetor $t^4$ (de dimens√£o *d*) para um vetor $t^5$ tamb√©m de dimens√£o *d*.
VI.  A adi√ß√£o residual definida por $h_i = t^5 + t^4$, combina dois vetores de dimens√£o *d*, resultando em um vetor $h_i$ tamb√©m de dimens√£o *d*.
VII. A opera√ß√£o LayerNorm final, definida por $h_i = LayerNorm(h_i)$, mapeia o vetor $h_i$ (de dimens√£o *d*) para um vetor $h_i$ tamb√©m de dimens√£o *d*, como estabelecido pela Proposi√ß√£o 2.1.
VIII. Portanto, a arquitetura *prenorm* preserva a dimensionalidade do *embedding* ao longo do *residual stream*.
‚ñ†
**Teorema 1** (A Vantagem da Arquitetura *Prenorm* no Treinamento): A arquitetura *prenorm* pode melhorar a estabilidade e a converg√™ncia durante o treinamento em compara√ß√£o com a arquitetura *postnorm*, devido √† normaliza√ß√£o da entrada das camadas de autoaten√ß√£o e *FFN*, e a normaliza√ß√£o da sa√≠da do √∫ltimo bloco, o que facilita a propaga√ß√£o do gradiente e evita a satura√ß√£o da ativa√ß√£o.

*Prova:*
I. Na arquitetura *postnorm*, a normaliza√ß√£o de camada √© aplicada *ap√≥s* a autoaten√ß√£o e *FFN*, o que significa que as camadas operam sobre dados que podem ter escalas muito variadas. Isso pode levar a problemas de estabilidade e dificultar o treinamento, principalmente em modelos mais profundos.
II. Na arquitetura *prenorm*, a normaliza√ß√£o de camada √© aplicada *antes* da autoaten√ß√£o e *FFN*, o que significa que essas camadas recebem dados em uma escala controlada. Isso estabiliza o treinamento e facilita a converg√™ncia.
III. A normaliza√ß√£o da entrada da *MultiHeadAttention* permite que os pesos sejam aprendidos mais rapidamente, pois n√£o precisam se ajustar √†s varia√ß√µes de escala que poderiam surgir sem a normaliza√ß√£o, e os gradientes s√£o propagados de maneira mais eficaz.
IV. A normaliza√ß√£o da entrada da *FFN* tamb√©m estabiliza o treinamento da rede, evitando que as ativa√ß√µes se tornem muito grandes e garantindo que o modelo opere dentro de uma faixa de valores adequada, e da mesma forma, a normaliza√ß√£o da sa√≠da do bloco garante que o *embedding* seja transmitido para o pr√≥ximo bloco de maneira controlada.
V.  Al√©m disso, a normaliza√ß√£o da sa√≠da da √∫ltima camada garante que a representa√ß√£o do *token* seja sempre normalizada, o que pode ajudar a estabilizar o treinamento das camadas subsequentes e a generaliza√ß√£o do modelo.
VI. Portanto, a arquitetura *prenorm*, ao aplicar o *LayerNorm* antes das camadas de autoaten√ß√£o e *FFN* e na sa√≠da do √∫ltimo bloco, melhora a estabilidade do treinamento, facilita a converg√™ncia e melhora o desempenho do modelo.
‚ñ†
**Teorema 2** (O Fluxo de Informa√ß√£o na Arquitetura *Prenorm*): A arquitetura *prenorm* modifica o fluxo de informa√ß√£o no *residual stream* ao normalizar a entrada das camadas de autoaten√ß√£o e *FFN*, o que pode resultar em representa√ß√µes dos *tokens* que s√£o mais est√°veis e menos sujeitas a varia√ß√µes de escala.

*Prova:*
I. Na arquitetura *postnorm*, o *residual stream* pode apresentar varia√ß√µes de escala, pois as camadas de autoaten√ß√£o e *FFN* aplicam transforma√ß√µes que podem aumentar ou diminuir a magnitude dos vetores.
II.  Na arquitetura *prenorm*, a aplica√ß√£o da normaliza√ß√£o antes da autoaten√ß√£o e *FFN* garante que as transforma√ß√µes sejam aplicadas a vetores normalizados.
III. Isso significa que a informa√ß√£o que √© passada ao longo do *residual stream* √© mais est√°vel, e as representa√ß√µes dos *tokens* s√£o menos sujeitas a varia√ß√µes de escala que poderiam dificultar o aprendizado do modelo.
IV. Portanto, a arquitetura *prenorm* modifica o fluxo de informa√ß√£o no *residual stream* ao normalizar as entradas das camadas, o que pode resultar em representa√ß√µes mais est√°veis e menos sujeitas a varia√ß√µes de escala.
‚ñ†

**Lema 1.1** (A Opera√ß√£o da *MultiHeadAttention* em *Prenorm*): A opera√ß√£o da autoaten√ß√£o em *prenorm* pode ser descrita por:
$$
t^2 = MultiHeadAttention(LayerNorm(x_i), [x_1, \ldots, x_N])
$$
onde o *LayerNorm* √© aplicado √† entrada $x_i$ antes da *MultiHeadAttention*.

*Prova:*
I. Na arquitetura *prenorm*, o *LayerNorm* √© aplicado √† entrada $x_i$ antes de passar pela camada de autoaten√ß√£o.
II. A camada *MultiHeadAttention* recebe como entrada o vetor normalizado $LayerNorm(x_i)$ e os outros *embeddings*, processando e combinando a informa√ß√£o, resultando em $t^2$.
III. Portanto, a opera√ß√£o da autoaten√ß√£o em *prenorm* pode ser descrita por $t^2 = MultiHeadAttention(LayerNorm(x_i), [x_1, \ldots, x_N])$.
‚ñ†

**Lema 1.2** (A Opera√ß√£o da *FFN* em *Prenorm*): A opera√ß√£o da *FFN* em *prenorm* pode ser descrita por:
$$
t^5 = FFN(LayerNorm(t^3))
$$
onde o *LayerNorm* √© aplicado antes da camada *FFN*.
*Prova:*
I. Na arquitetura *prenorm*, o *LayerNorm* √© aplicado ao vetor $t^3$ antes de passar pela camada *FFN*, resultando em $t^4 = LayerNorm(t^3)$
II. A camada *FFN* recebe como entrada o vetor normalizado $t^4$ e aplica uma transforma√ß√£o linear e n√£o linear, resultando em $t^5$.
III. Portanto, a opera√ß√£o da *FFN* em *prenorm* pode ser descrita por $t^5 = FFN(LayerNorm(t^3))$
‚ñ†

**Corol√°rio 1** (Invari√¢ncia da Dimensionalidade da *Prenorm*):  O *residual stream* da arquitetura *prenorm* mant√©m a mesma dimensionalidade das representa√ß√µes ao longo das camadas, garantindo que os vetores tenham sempre uma dimens√£o *d*.
*Prova:*
I. A camada LayerNorm, definida por $t^1 = LayerNorm(x_i)$, preserva a dimensionalidade, mapeando um vetor de dimens√£o *d* em outro vetor de mesma dimens√£o *d*.
II. A camada MultiHeadAttention, definida por $t^2 = MultiHeadAttention(t^1, [x_1, \ldots, x_N])$, tamb√©m preserva a dimensionalidade, mapeando o vetor normalizado de dimens√£o *d* em outro vetor de dimens√£o *d*.
III.  A adi√ß√£o residual, definida por $t^3 = t^2 + t^1$, combina dois vetores de mesma dimens√£o *d*, resultando em um vetor de dimens√£o *d*.
IV. O segundo LayerNorm, definido por $t^4 = LayerNorm(t^3)$, preserva a dimensionalidade, mapeando um vetor de dimens√£o *d* para outro vetor de dimens√£o *d*.
V. A FFN, definida por $t^5 = FFN(t^4)$, tamb√©m preserva a dimensionalidade, mapeando um vetor de dimens√£o *d* para outro vetor de dimens√£o *d*.
VI.  A adi√ß√£o residual, definida por $h_i = t^5 + t^4$, combina dois vetores de mesma dimens√£o *d*, resultando em um vetor de dimens√£o *d*.
VII. A camada LayerNorm final, definida por $h_i = LayerNorm(h_i)$, preserva a dimensionalidade, mapeando um vetor de dimens√£o *d* para outro vetor de dimens√£o *d*.
VIII. Portanto, o *residual stream* da arquitetura *prenorm* garante a preserva√ß√£o da dimensionalidade em cada etapa do processamento, mantendo a dimensionalidade dos vetores em *d*.
‚ñ†
**Corol√°rio 1.1** (Estabilidade do Fluxo do Gradiente): A aplica√ß√£o do *LayerNorm* antes da *MultiHeadAttention* e da *FFN*, e a aplica√ß√£o do *LayerNorm* no final, estabiliza o fluxo do gradiente durante o treinamento, evitando que os gradientes explodam ou desapare√ßam, o que melhora a converg√™ncia e o desempenho do modelo.
*Prova:*
I. Como demonstrado no Teorema 1, ao normalizar a entrada da *MultiHeadAttention* e da *FFN*, a arquitetura *prenorm* garante que essas camadas operem sobre dados com escalas controladas, o que reduz a vari√¢ncia do gradiente.
II. A normaliza√ß√£o da sa√≠da do √∫ltimo bloco, e da camada de embedding de sa√≠da, garante que a representa√ß√£o do *token* tenha uma escala consistente ao longo de todas as camadas, o que facilita a propaga√ß√£o do gradiente e a estabilidade do modelo.
III. A estabiliza√ß√£o do gradiente durante o treinamento √© fundamental para o bom desempenho do modelo, permitindo que ele convirja de forma mais r√°pida e eficiente.
IV. Portanto, a aplica√ß√£o da *LayerNorm* antes das camadas de autoaten√ß√£o e *FFN*, e na sa√≠da do bloco, estabiliza o fluxo do gradiente, facilitando o treinamento e o desempenho do modelo.
‚ñ†

**Observa√ß√£o 1** (A Camada Final do *Prenorm*): Na arquitetura *prenorm*, a aplica√ß√£o do *LayerNorm* ao final do √∫ltimo bloco *transformer*, e na camada de *unembedding*, garante que a sa√≠da do modelo seja sempre normalizada. Essa normaliza√ß√£o final contribui para estabilizar o treinamento das camadas subsequentes e a generaliza√ß√£o do modelo, uma vez que a sa√≠da √© transformada para as *logits*, de onde a probabilidade de cada palavra no vocabul√°rio ser√° calculada.

**Teorema 3** (Rela√ß√£o com a Normaliza√ß√£o *Root Mean Square*): A normaliza√ß√£o *LayerNorm* utilizada na arquitetura *prenorm* pode ser vista como uma generaliza√ß√£o da normaliza√ß√£o *Root Mean Square* (RMSNorm), que tamb√©m normaliza a entrada das camadas, mas omite a subtra√ß√£o da m√©dia, reduzindo a complexidade computacional.
*Prova:*
I. A *LayerNorm*, como definido anteriormente, calcula a m√©dia e o desvio padr√£o dos valores de entrada para normalizar os vetores, o que envolve opera√ß√µes de subtra√ß√£o e divis√£o.
II. A *RMSNorm* simplifica este processo, utilizando apenas o desvio padr√£o para normalizar os vetores, evitando a subtra√ß√£o da m√©dia, o que resulta em um c√°lculo mais eficiente.
III. Ambas as t√©cnicas t√™m como objetivo estabilizar o treinamento e facilitar a converg√™ncia do modelo, garantindo que as camadas operem sobre dados com escalas controladas.
IV. Portanto, a *LayerNorm* pode ser vista como uma vers√£o mais completa da *RMSNorm*, com o *RMSNorm* representando uma vers√£o mais eficiente em termos computacionais, embora ambas as t√©cnicas sejam utilizadas para normalizar a entrada das camadas.
‚ñ†

**Lema 3.1** (A *RMSNorm* em *Prenorm*): Uma vers√£o simplificada da arquitetura *prenorm* pode utilizar a *RMSNorm* no lugar da *LayerNorm*, preservando a maioria dos benef√≠cios da normaliza√ß√£o pr√©-camada.

*Prova:*
I. Substituir a *LayerNorm* por *RMSNorm* nas equa√ß√µes [10.38], [10.41] e na camada final, simplificaria o processo de normaliza√ß√£o, reduzindo a complexidade computacional.
II. A *RMSNorm*, assim como a *LayerNorm*, normaliza a entrada das camadas de *MultiHeadAttention* e *FFN*, bem como a sa√≠da do bloco, o que garante um fluxo de informa√ß√£o mais est√°vel, mantendo o modelo dentro de uma faixa de valores adequada.
III. Apesar de omitir a subtra√ß√£o da m√©dia, a *RMSNorm* mant√©m a estabilidade do treinamento e facilita a converg√™ncia, especialmente em cen√°rios onde a efici√™ncia computacional √© cr√≠tica.
IV. Portanto, a *RMSNorm* pode ser utilizada em *prenorm* como uma alternativa mais eficiente, embora com uma poss√≠vel pequena redu√ß√£o de performance.
‚ñ†

**Observa√ß√£o 2** (Normaliza√ß√£o Adaptativa): Em modelos mais recentes, t√©cnicas de normaliza√ß√£o adaptativa podem ser utilizadas em conjunto com a arquitetura *prenorm*, ajustando os par√¢metros da normaliza√ß√£o (ganho e bias) de acordo com as caracter√≠sticas espec√≠ficas de cada camada ou *token*, o que pode resultar em um aprendizado ainda mais eficiente.

### Conclus√£o
Neste cap√≠tulo, exploramos em detalhes a arquitetura *prenorm* do *transformer*, com foco no impacto da normaliza√ß√£o pr√©-camada no *residual stream*. Ao contr√°rio da arquitetura *postnorm*, que normaliza ap√≥s as camadas de autoaten√ß√£o e *FFN*, a arquitetura *prenorm* aplica o *LayerNorm* antes dessas opera√ß√µes, al√©m de normalizar a sa√≠da do √∫ltimo bloco e da camada de *unembedding*, o que resulta em um fluxo de informa√ß√£o mais est√°vel e em um treinamento mais eficiente. A arquitetura *prenorm* representa uma evolu√ß√£o na arquitetura do *transformer*, e oferece uma alternativa robusta que frequentemente apresenta um desempenho superior, melhorando a estabilidade do treinamento e facilitando a converg√™ncia. A compreens√£o das diferen√ßas entre as arquiteturas *prenorm* e *postnorm* √© essencial para o desenvolvimento de modelos *transformers* mais avan√ßados e eficientes. A arquitetura prenorm introduz uma nova forma de organiza√ß√£o das camadas, e √© amplamente utilizada na literatura de modelos *transformers*.

### Refer√™ncias
[^1]: *‚ÄúIn this chapter we formalize this idea of pretraining‚Äîlearning knowledge about language and the world from vast amounts of text‚Äîand call the resulting pretrained language models large language models.‚Äù*
[^6]: *‚ÄúTo capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value.‚Äù*
[^7]: *‚ÄúThis description of the self-attention process has been from the perspective of computing a single output at a single time step i.‚Äù*
[^12]: *‚ÄúThe previous sections viewed the transformer block as applied to the entire N-token input X of shape [N √ó d], producing an output also of shape [N √ó d].‚Äù*
[^13]: *‚Äúthese layers as a stream of d-dimensional representations, called the residual stream and visualized in Fig. 10.7.‚Äù*
<!-- END -->
