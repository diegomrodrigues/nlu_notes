## O Fluxo Residual no Bloco Transformer: Uma An√°lise Detalhada

### Introdu√ß√£o
Neste cap√≠tulo, exploraremos o conceito do **fluxo residual** (*residual stream*) dentro do contexto dos blocos *transformer*. Em vez de tratar os blocos *transformer* como opera√ß√µes sobre matrizes de *tokens* de entrada, examinaremos como cada *token* individual, representado por um vetor d-dimensional, √© processado atrav√©s das v√°rias camadas do *transformer*. Essa perspectiva, conhecida como a *residual stream view*, oferece uma compreens√£o mais intuitiva do funcionamento interno do modelo. O *residual stream* √© o fluxo de representa√ß√µes *d-dimensional* atrav√©s das camadas do *transformer*, com as conex√µes residuais copiando informa√ß√µes de camadas anteriores para camadas posteriores [^13]. Este conceito se conecta com a ideia de autoaten√ß√£o, apresentada anteriormente, e como ela integra informa√ß√µes do contexto para computar a representa√ß√£o de um *token* [^1].

### Conceitos Fundamentais

A vis√£o tradicional de um bloco *transformer* considera uma entrada *X* de dimens√£o *[N x d]*, onde *N* √© o n√∫mero de *tokens* e *d* a dimensionalidade dos *embeddings*. O bloco *transformer* transforma esta entrada em uma sa√≠da *H* tamb√©m de dimens√£o *[N x d]*. No entanto, a perspectiva do fluxo residual foca no que acontece a cada *token* individual, ou seja, a cada linha dessa matriz [^12].

Cada *token* de entrada $x_i$ √© um vetor *d*-dimensional. O *embedding* inicial √© a base do *residual stream*, e este √© transmitido atrav√©s das conex√µes residuais. As sa√≠das das camadas de *feedforward* e de aten√ß√£o s√£o somadas ao *stream*. Em cada bloco e camada, estamos passando um *embedding* de forma *[1 x d]* [^13].

As conex√µes residuais s√£o cruciais para este fluxo. Elas copiam informa√ß√µes de *embeddings* anteriores para os posteriores, garantindo que as informa√ß√µes iniciais n√£o se percam durante o processamento.  Os componentes do bloco *transformer* adicionam novas perspectivas a esta representa√ß√£o [^13]. Os *feedforward networks* acrescentam uma vis√£o diferente do *embedding* anterior [^13].

> üí° **Exemplo Num√©rico:**
> Vamos supor que temos um *token* representado por um vetor $x_i$ de dimens√£o $d = 4$:
> $$x_i = \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}$$
> Este vetor $x_i$ representa o *embedding* inicial do *token* e ser√° o ponto de partida do *residual stream*. Ao longo do processamento no bloco *transformer*, novas informa√ß√µes ser√£o adicionadas a este vetor, sem alterar sua dimensionalidade.

**Autoaten√ß√£o no Fluxo Residual**

Um ponto crucial da *residual stream view* √© que a autoaten√ß√£o √© o √∫nico componente que realmente interage com informa√ß√µes de outros *tokens* no contexto [^13]. Essa intera√ß√£o ocorre atrav√©s da concatena√ß√£o dos *embeddings* de *query*, *key* e *value*, onde, como j√° visto anteriormente, cada *token* desempenha essas tr√™s fun√ß√µes [^6, 7].

*   A camada de *MultiHeadAttention* recebe a entrada $x_i$, e considera as informa√ß√µes de todos os outros *tokens* $x_1...x_n$ no contexto, de acordo com a equa√ß√£o:
$$
t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])
$$
[10.32]

*   O resultado $t^1$ √© ent√£o adicionado ao *embedding* original atrav√©s de uma conex√£o residual:
$$
t^2 = t^1 + x_i
$$
[10.33]

*   Em seguida, o resultado √© normalizado com Layer Norm:
$$
t^3 = LayerNorm(t^2)
$$
[10.34]

*   E o resultado √© passado pela rede *feedforward*:
$$
t^4 = FFN(t^3)
$$
[10.35]

*   A sa√≠da da rede *feedforward* tamb√©m √© adicionada atrav√©s de uma conex√£o residual:
$$
t^5 = t^4 + t^3
$$
[10.36]
*   E por fim normalizada novamente, produzindo a sa√≠da do bloco *transformer*:
$$
h_i = LayerNorm(t^5)
$$
[10.37]

Esta sequ√™ncia de opera√ß√µes representa o processamento de um *token* individual ao longo do *residual stream*. A autoaten√ß√£o permite que a informa√ß√£o flua de outros *tokens* para o *stream* do *token* atual, enquanto as outras camadas adicionam diferentes perspectivas √† representa√ß√£o do mesmo [^13].

> üí° **Exemplo Num√©rico:**
> Continuando o exemplo anterior, vamos supor que o resultado da camada de *MultiHeadAttention* seja:
> $$t^1 = \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix}$$
> A adi√ß√£o residual resulta em:
> $$t^2 = t^1 + x_i =  \begin{bmatrix} 0.2 \\ 0.1 \\ -0.3 \\ 0.4 \end{bmatrix} + \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix} = \begin{bmatrix} 1.2 \\ 0.6 \\ -0.5 \\ 1.2 \end{bmatrix}$$
>
> A normaliza√ß√£o LayerNorm ajusta os valores dos vetores para que tenham m√©dia zero e vari√¢ncia um, por exemplo:
> $$t^3 = LayerNorm(t^2) = \begin{bmatrix} -0.3 \\ -0.4 \\ -1.5 \\ 1.2 \end{bmatrix}$$
>
> A rede *feedforward* aplica uma transforma√ß√£o linear seguida por uma n√£o linearidade, produzindo:
>$$t^4 = FFN(t^3) = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.3 \\ -0.1 \end{bmatrix}$$
>
> A segunda adi√ß√£o residual resulta em:
>$$t^5 = t^4 + t^3 = \begin{bmatrix} 0.5 \\ -0.2 \\ 0.3 \\ -0.1 \end{bmatrix} + \begin{bmatrix} -0.3 \\ -0.4 \\ -1.5 \\ 1.2 \end{bmatrix} = \begin{bmatrix} 0.2 \\ -0.6 \\ -1.2 \\ 1.1 \end{bmatrix}$$
>
> Finalmente, a √∫ltima LayerNorm normaliza a sa√≠da do bloco *transformer* para:
>$$h_i = LayerNorm(t^5) = \begin{bmatrix} -0.1 \\ -0.7 \\ -1.3 \\ 1.2 \end{bmatrix}$$
>
> Observe que cada etapa mant√©m a dimens√£o do vetor em $d=4$, conforme descrito pela teoria do *residual stream*.

**Visualiza√ß√£o do Movimento da Informa√ß√£o**
Elhage et al. (2021) mostram que os *attention heads* movem informa√ß√£o do *residual stream* de um *token* vizinho para o *stream* do *token* atual. O espa√ßo de *embedding* de alta dimens√£o em cada posi√ß√£o cont√©m, assim, informa√ß√µes sobre o *token* atual e sobre os seus vizinhos [^13].

A visualiza√ß√£o do *residual stream*  nos ajuda a entender o fluxo de informa√ß√£o atrav√©s do *transformer*, como ilustrado na Fig. 10.7 do texto [^12].

**Arquitetura Pre-norm vs. Post-norm**
√â importante notar que existem duas formas comuns para a ordem das opera√ß√µes no bloco *transformer*: *pre-norm* e *post-norm*. Na arquitetura *post-norm*, a normaliza√ß√£o da camada (*LayerNorm*) √© aplicada ap√≥s as opera√ß√µes de *MultiHeadAttention* e *FFN* (como descrito at√© agora) [^13]. Na arquitetura *pre-norm*, a *LayerNorm* √© aplicada antes dessas opera√ß√µes [^13].  A arquitetura *pre-norm* normalmente apresenta um melhor desempenho em muitos casos [^13].

**Lema 1** (Equival√™ncia da Arquitetura Post-Norm): A sequ√™ncia de opera√ß√µes na arquitetura *post-norm* pode ser expressa como uma fun√ß√£o *f* que mapeia um vetor *d*-dimensional $x_i$ (o *embedding* de um token *i*) para outro vetor *d*-dimensional $h_i$, preservando a dimensionalidade do fluxo residual. Ou seja, para uma entrada $x_i$, a sa√≠da $h_i$ do bloco transformer √© dada por  $h_i$ = $f$($x_i$, $x_1$,..., $x_n$), onde $x_1$,..., $x_n$ s√£o os *embeddings* dos outros tokens.

*Prova:*
I.  A opera√ß√£o de *MultiHeadAttention*, dada por $t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])$, mapeia os *embeddings* de entrada, que s√£o vetores *d*-dimensionais, para um vetor de sa√≠da $t^1$ com a mesma dimensionalidade *d*.
II.  A adi√ß√£o residual, definida por $t^2 = t^1 + x_i$, soma dois vetores *d*-dimensionais, resultando em um vetor $t^2$ tamb√©m *d*-dimensional.
III. A normaliza√ß√£o da camada, $t^3 = LayerNorm(t^2)$, preserva a dimensionalidade, transformando o vetor *d*-dimensional $t^2$ em outro vetor *d*-dimensional $t^3$.
IV. A rede *feedforward*, $t^4 = FFN(t^3)$, mapeia o vetor *d*-dimensional $t^3$ para outro vetor *d*-dimensional $t^4$.
V. A segunda adi√ß√£o residual, $t^5 = t^4 + t^3$, soma dois vetores *d*-dimensionais, gerando o vetor $t^5$, tamb√©m *d*-dimensional.
VI. Finalmente, a normaliza√ß√£o da camada final, $h_i = LayerNorm(t^5)$, transforma o vetor *d*-dimensional $t^5$ em um vetor de sa√≠da $h_i$ que tamb√©m √© *d*-dimensional.
VII. Portanto, cada etapa preserva a dimensionalidade *d*. Assim, a fun√ß√£o composta *f* que mapeia $x_i$ para $h_i$ atrav√©s de todas essas opera√ß√µes tamb√©m preserva a dimensionalidade. Formalmente, podemos definir *f* como $f(x_i, x_1, \ldots, x_N) = LayerNorm(FFN(LayerNorm(MultiHeadAttention(x_i, [x_1, \ldots, x_N]) + x_i)) + LayerNorm(MultiHeadAttention(x_i, [x_1, \ldots, x_N]) + x_i))$.
‚ñ†

**Proposi√ß√£o 1** (Propriedade de Preserva√ß√£o da Dimensionalidade): Em ambas as arquiteturas, *pre-norm* e *post-norm*, o fluxo residual preserva a dimensionalidade dos *embeddings*. Ou seja, cada transforma√ß√£o aplicada a um *embedding d*-dimensional, resulta em outro *embedding d*-dimensional.

*Prova:*
I.  A arquitetura *post-norm*, como demonstrado no Lema 1, preserva a dimensionalidade atrav√©s de todas as suas opera√ß√µes: *MultiHeadAttention*, adi√ß√µes residuais, *LayerNorm* e *FFN*.
II.  Na arquitetura *pre-norm*, a *LayerNorm* √© aplicada antes das opera√ß√µes de *MultiHeadAttention* e *FFN*, mas ela tamb√©m preserva a dimensionalidade.
III. Como a *LayerNorm* n√£o altera a dimensionalidade dos vetores, as outras opera√ß√µes (MultiHeadAttention, adi√ß√µes residuais e FFN), quando aplicadas a vetores *d*-dimensionais, retornam vetores tamb√©m *d*-dimensionais, como demonstrado no Lema 1.
IV. A ordem das opera√ß√µes, seja *pre-norm* ou *post-norm*, n√£o afeta o fato de que cada opera√ß√£o preserva a dimensionalidade *d*.
V.  Portanto, em ambas as arquiteturas, o fluxo residual manipula vetores *d*-dimensionais em cada etapa, preservando a dimensionalidade dos *embeddings*.
‚ñ†

**Teorema 1** (Generaliza√ß√£o do Fluxo Residual): O fluxo residual pode ser generalizado para outros tipos de modelos de *Deep Learning* que utilizam conex√µes residuais e opera√ß√µes de aten√ß√£o, tais como redes convolucionais com *attention* e modelos h√≠bridos.

*Prova:*
I. O conceito central do fluxo residual √© o uso de conex√µes residuais (ou *skip connections*) para preservar informa√ß√µes da entrada original enquanto camadas adicionais aplicam transforma√ß√µes.
II. Modelos como redes convolucionais com *attention* e modelos h√≠bridos tamb√©m utilizam essas conex√µes residuais, seja entre blocos convolucionais ou *attention layers*.
III.  A ideia de que representa√ß√µes intermedi√°rias s√£o propagadas atrav√©s do modelo, onde as conex√µes residuais atuam como um "atalho" que preserva a informa√ß√£o original, e opera√ß√µes como a aten√ß√£o ou convolu√ß√µes enriquecem a representa√ß√£o, √© universal para esses modelos.
IV. Podemos generalizar que o *residual stream* n√£o √© espec√≠fico para *transformers*, mas sim um conceito que pode ser aplicado a qualquer arquitetura que utilize conex√µes residuais.
V. Em cada caso, o "fluxo residual" representa o fluxo de representa√ß√µes da entrada atrav√©s do modelo, com a dimensionalidade das representa√ß√µes (que pode n√£o ser *d* em todos os casos) sendo preservada ou transformada de maneira consistente em cada bloco/camada.
VI. Seja a representa√ß√£o da entrada um *embedding* de *token* (como nos *transformers*) ou um mapa de caracter√≠sticas de uma imagem (como em redes convolucionais), o conceito do fluxo residual √© aplic√°vel desde que existam conex√µes residuais que "atalham" o processamento e que as opera√ß√µes aplicadas preservam ou transformam a representa√ß√£o de maneira consistente.
VII. Portanto, o conceito do *residual stream* pode ser aplicado a modelos que utilizam conex√µes residuais, generalizando o conceito para al√©m da arquitetura do *transformer*.
‚ñ†

### Conclus√£o
O conceito de *residual stream* oferece uma vis√£o detalhada do funcionamento interno de um *transformer*. Em vez de ver os blocos *transformer* como transforma√ß√µes sobre matrizes de *tokens*, visualizamos o processamento de cada *token* individualmente atrav√©s de um fluxo de *embeddings d-dimensional*. As conex√µes residuais garantem que as informa√ß√µes de *embeddings* anteriores sejam passadas para camadas posteriores, enquanto a autoaten√ß√£o permite a incorpora√ß√£o de informa√ß√µes contextuais. A arquitetura do *transformer* e a sua capacidade de modelar depend√™ncias de longo alcance s√£o, em grande parte, uma consequ√™ncia desta abordagem.

### Refer√™ncias
[^1]: "In this chapter we formalize this idea of pretraining‚Äîlearning knowledge about language and the world from vast amounts of text‚Äîand call the resulting pretrained language models large language models."
[^6]: "To capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value."
[^7]: "This description of the self-attention process has been from the perspective of computing a single output at a single time step i."
[^12]: "The previous sections viewed the transformer block as applied to the entire N-token input X of shape [N √ó d], producing an output also of shape [N √ó d]."
[^13]: "these layers as a stream of d-dimensional representations, called the residual stream and visualized in Fig. 10.7."
<!-- END -->
