## A Diversidade de Perspectivas no Fluxo Residual: Camadas Feedforward, Aten√ß√£o e Normaliza√ß√£o

### Introdu√ß√£o
Este cap√≠tulo visa consolidar nossa compreens√£o sobre o **fluxo residual** (*residual stream*) nos blocos *transformer*, detalhando as contribui√ß√µes espec√≠ficas de cada componente: as camadas de autoaten√ß√£o (*multi-head attention*), as redes *feedforward* (*FFN*) e as camadas de normaliza√ß√£o (*LayerNorm*) [^13]. Construindo sobre os cap√≠tulos anteriores que introduziram o *residual stream*, o papel da autoaten√ß√£o como integradora contextual e o funcionamento da *FFN* e *LayerNorm* [^1, ^6, ^7, ^12, ^13], aqui examinaremos como essas camadas atuam em conjunto para moldar a representa√ß√£o de cada *token* ao longo do *residual stream*. Nosso objetivo √© refor√ßar que cada camada contribui com uma "nova vis√£o" da representa√ß√£o do *token*, e que as conex√µes residuais garantem que essas diferentes vis√µes sejam combinadas e propagadas eficientemente ao longo do modelo. Exploraremos a ideia de que as camadas de aten√ß√£o movem informa√ß√£o de outros "fluxos residuais" para o fluxo do token atual e que as representa√ß√µes dos *tokens* cont√™m informa√ß√£o sobre o *token* atual e seus vizinhos [^13].

### Camadas *Feedforward* e a Diversifica√ß√£o da Representa√ß√£o
Como vimos anteriormente, o *residual stream* consiste em um fluxo cont√≠nuo de representa√ß√µes *d*-dimensionais que representam cada *token* ao longo das camadas do bloco *transformer* [^13, Proposi√ß√£o 1]. Cada componente do bloco *transformer* aplica uma transforma√ß√£o que adiciona uma nova perspectiva √† representa√ß√£o do *token*. Em particular, a rede *FFN* oferece uma transforma√ß√£o n√£o linear que enriquece a representa√ß√£o do *token* de maneira independente do contexto [^13, Proposi√ß√£o 1.1]. Enquanto a autoaten√ß√£o integra informa√ß√µes contextuais, a camada *FFN* aplica uma transforma√ß√£o n√£o-linear √† representa√ß√£o do *token* ap√≥s a normaliza√ß√£o, garantindo que o modelo possa aprender rela√ß√µes complexas nos dados.

A opera√ß√£o da *FFN*, aplicada de forma independente a cada *token* ap√≥s a normaliza√ß√£o, transforma a representa√ß√£o $t^3$ em uma nova representa√ß√£o $t^4$ utilizando transforma√ß√µes lineares e n√£o lineares:
$$
t^4 = FFN(t^3)
$$
[10.35]
Essa opera√ß√£o adiciona uma perspectiva n√£o linear √† representa√ß√£o do *token*, o que √© crucial para a capacidade do modelo de aprender depend√™ncias complexas na linguagem.

> üí° **Exemplo Num√©rico:**
> Consideremos um *token* que, ap√≥s passar pela camada de autoaten√ß√£o e normaliza√ß√£o, possui um *embedding* $t^3$ de dimens√£o $d=4$:
> $$t^3 = \begin{bmatrix} 0.8 \\ -0.2 \\ 0.5 \\ 0.2 \end{bmatrix}$$
> A rede *FFN* transforma esse vetor em uma nova representa√ß√£o $t^4$ tamb√©m com dimens√£o $d=4$. Para fins ilustrativos, consideremos que a *FFN* realiza uma transforma√ß√£o linear, uma fun√ß√£o de ativa√ß√£o ReLU e outra transforma√ß√£o linear. Assumindo que a primeira camada linear tem pesos $W_1$ e bias $b_1$ e a segunda camada linear tem pesos $W_2$ e bias $b_2$:
>
> $W_1 = \begin{bmatrix} 0.1 & 0.2 & -0.1 & 0.3 \\ -0.2 & 0.1 & 0.4 & -0.1 \\ 0.3 & -0.1 & 0.2 & 0.1 \\ 0.1 & 0.4 & -0.2 & 0.2 \end{bmatrix}$, $b_1 = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.1 \\ 0.0 \end{bmatrix}$
>
>
> $W_2 = \begin{bmatrix} 0.2 & -0.1 & 0.3 & -0.2 \\ 0.1 & 0.3 & -0.1 & 0.2 \\ -0.2 & 0.1 & 0.2 & 0.3 \\ 0.3 & -0.2 & 0.1 & -0.1 \end{bmatrix}$, $b_2 = \begin{bmatrix} 0.0 \\ 0.1 \\ 0.2 \\ 0.0 \end{bmatrix}$
>
> $\text{Step 1: } z = W_1t^3 + b_1$
> $$ z = \begin{bmatrix} 0.1 & 0.2 & -0.1 & 0.3 \\ -0.2 & 0.1 & 0.4 & -0.1 \\ 0.3 & -0.1 & 0.2 & 0.1 \\ 0.1 & 0.4 & -0.2 & 0.2 \end{bmatrix} \begin{bmatrix} 0.8 \\ -0.2 \\ 0.5 \\ 0.2 \end{bmatrix} + \begin{bmatrix} 0.1 \\ 0.2 \\ 0.1 \\ 0.0 \end{bmatrix} = \begin{bmatrix} 0.11 \\ 0.19 \\ 0.21 \\ 0.05 \end{bmatrix}$$
>
> $\text{Step 2: } a = ReLU(z)$
>
> $$a = \begin{bmatrix} 0.11 \\ 0.19 \\ 0.21 \\ 0.05 \end{bmatrix}$$
>
> $\text{Step 3: } t^4 = W_2a + b_2$
>
> $$t^4 = \begin{bmatrix} 0.2 & -0.1 & 0.3 & -0.2 \\ 0.1 & 0.3 & -0.1 & 0.2 \\ -0.2 & 0.1 & 0.2 & 0.3 \\ 0.3 & -0.2 & 0.1 & -0.1 \end{bmatrix} \begin{bmatrix} 0.11 \\ 0.19 \\ 0.21 \\ 0.05 \end{bmatrix} + \begin{bmatrix} 0.0 \\ 0.1 \\ 0.2 \\ 0.0 \end{bmatrix} = \begin{bmatrix} 0.05 \\ 0.18 \\ 0.24 \\ 0.04 \end{bmatrix}$$
>
> Portanto,
>
> $$t^4 = \begin{bmatrix} 0.05 \\ 0.18 \\ 0.24 \\ 0.04 \end{bmatrix}$$
>
> Observe que o vetor $t^4$ possui a mesma dimens√£o $d=4$ que $t^3$, por√©m com valores diferentes e que foram transformados pela rede *FFN*. Esta transforma√ß√£o n√£o linear e independente do contexto, adiciona uma nova perspectiva √† representa√ß√£o do token.
> ```mermaid
> graph LR
>     A[t3] --> B(Linear Layer 1)
>     B --> C(ReLU)
>     C --> D(Linear Layer 2)
>     D --> E[t4]
> ```
> O diagrama acima ilustra o processamento da camada FFN: A entrada *t3* passa por uma primeira camada linear, seguida pela fun√ß√£o n√£o-linear ReLU e, em seguida, por uma segunda camada linear, resultando em *t4*.

**Autoaten√ß√£o: O Mecanismo de Integra√ß√£o Contextual**

Ao contr√°rio da *FFN*, que opera sobre o *embedding* de cada *token* de forma independente, a autoaten√ß√£o (*MultiHeadAttention*) √© o √∫nico componente do bloco *transformer* que interage diretamente com as representa√ß√µes de outros *tokens* na sequ√™ncia [^13]. A camada *MultiHeadAttention* computa *queries*, *keys* e *values* para cada *token* e utiliza um mecanismo de aten√ß√£o para combinar as informa√ß√µes, resultando em uma representa√ß√£o $t^1$ que leva em conta o contexto:
$$
t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])
$$
[10.32]

Essa opera√ß√£o √© fundamental para a capacidade do *transformer* de modelar rela√ß√µes complexas e depend√™ncias de longo alcance, j√° que permite que cada *token* tenha sua representa√ß√£o influenciada pelas representa√ß√µes dos outros *tokens* na sequ√™ncia.

> üí° **Exemplo Num√©rico:**
> Para exemplificar a intera√ß√£o contextual da autoaten√ß√£o, consideremos uma sequ√™ncia de 3 *tokens*, e que estamos processando o segundo *token*. Suponha que o *embedding* inicial do segundo *token* seja $x_2 = \begin{bmatrix} 1.0 \\ 0.5 \\ -0.2 \\ 0.8 \end{bmatrix}$. Suponha que ap√≥s o c√°lculo das *queries*, *keys* e *values* e aplica√ß√£o da fun√ß√£o *softmax* e da pondera√ß√£o dos *values*, a opera√ß√£o de autoaten√ß√£o resulta no seguinte vetor para o segundo *token*:
> $$t^1 = \begin{bmatrix} 0.2 \\ -0.1 \\ 0.3 \\ 0.1 \end{bmatrix}$$
>
> O vetor $t^1$ √© uma combina√ß√£o linear dos *values* ponderados pelos pesos de aten√ß√£o. Observe como $t^1$ difere do *embedding* original $x_2$, pois ela integra a informa√ß√£o contextual de outros *tokens* da sequ√™ncia, que √© ent√£o passada para a pr√≥xima camada atrav√©s de uma conex√£o residual. Suponha que a autoaten√ß√£o tenha calculado para o primeiro e terceiro *tokens* $t_1 = \begin{bmatrix} 0.1 \\ 0.2 \\ -0.1 \\ 0.2 \end{bmatrix}$ e $t_3 = \begin{bmatrix} -0.2 \\ 0.1 \\ 0.2 \\ -0.1 \end{bmatrix}$ respectivamente. O diagrama abaixo ilustra o mecanismo de aten√ß√£o:
> ```mermaid
> graph LR
>     A[x1] -->|Query| B(Attention)
>     C[x2] -->|Query| B
>     D[x3] -->|Query| B
>     A -->|Key, Value| B
>     C -->|Key, Value| B
>     D -->|Key, Value| B
>      B --> E[t1, t2, t3]
> ```
> Cada *token* (x1, x2, x3) tem sua *query* comparada com as *keys* de todos os *tokens*, para ent√£o os *values* serem ponderados e combinados, resultando em um novo *embedding* para cada *token* (t1, t2, t3), que cont√©m informa√ß√µes contextuais.

**A Normaliza√ß√£o da Camada e a Estabilidade do Fluxo Residual**
As camadas de normaliza√ß√£o, e especialmente a *LayerNorm*, desempenham um papel crucial na estabiliza√ß√£o do fluxo residual e no treinamento do modelo. A *LayerNorm* √© aplicada antes ou ap√≥s as opera√ß√µes de autoaten√ß√£o e da *FFN*, dependendo da arquitetura (pre-norm ou post-norm), garantindo que as ativa√ß√µes em cada camada tenham uma m√©dia pr√≥xima de zero e desvio padr√£o pr√≥ximo de um.  Essa normaliza√ß√£o facilita a propaga√ß√£o do gradiente durante o *backpropagation* e evita problemas como a explos√£o ou o desaparecimento do gradiente.
Na arquitetura *post-norm*, como vimos, o *LayerNorm* √© aplicado ap√≥s as opera√ß√µes da *MultiHeadAttention* e *FFN*, enquanto que na arquitetura *pre-norm*, a normaliza√ß√£o √© aplicada antes dessas opera√ß√µes.
Em ambas as arquiteturas, a opera√ß√£o *LayerNorm* preserva a dimensionalidade dos vetores, como demonstrado na Proposi√ß√£o 2.1, atuando como um regulador do fluxo residual.

> üí° **Exemplo Num√©rico:**
> Consideremos um vetor $t^2 =  \begin{bmatrix} 1.2 \\ 0.4 \\ 0.1 \\ 0.9 \end{bmatrix}$ que passou pela autoaten√ß√£o e pela adi√ß√£o residual. Para simplificar o exemplo, vamos calcular a *LayerNorm* assumindo que ela centraliza e normaliza o vetor sem aplicar os par√¢metros de ganho e bias aprend√≠veis.
>
> $\text{Step 1: Calcular a m√©dia } \mu \text{ do vetor } t^2$:
> $$ \mu = \frac{1.2 + 0.4 + 0.1 + 0.9}{4} = \frac{2.6}{4} = 0.65 $$
>
> $\text{Step 2: Calcular o desvio padr√£o } \sigma \text{ do vetor } t^2$:
> $$ \sigma = \sqrt{\frac{(1.2-0.65)^2 + (0.4-0.65)^2 + (0.1-0.65)^2 + (0.9-0.65)^2}{4}} \approx 0.44$$
>
> $\text{Step 3: Normalizar o vetor } t^2 \text{ subtraindo a m√©dia e dividindo pelo desvio padr√£o:}$
>
> $$ t^3 = \begin{bmatrix} \frac{1.2 - 0.65}{0.44} \\ \frac{0.4 - 0.65}{0.44} \\ \frac{0.1 - 0.65}{0.44} \\ \frac{0.9 - 0.65}{0.44} \end{bmatrix} = \begin{bmatrix} 1.25 \\ -0.57 \\ -1.25 \\ 0.57 \end{bmatrix}$$
>
> Assim, o vetor $t^3$ normalizado √©:
>
> $$ t^3 = \begin{bmatrix} 1.25 \\ -0.57 \\ -1.25 \\ 0.57 \end{bmatrix} $$
> Observe que $t^3$ possui a mesma dimens√£o $d=4$, por√©m seus valores foram modificados pela opera√ß√£o *LayerNorm*. Na pr√°tica, a LayerNorm tamb√©m possui um ganho e bias aprend√≠veis que s√£o aplicados ap√≥s a normaliza√ß√£o para aumentar a flexibilidade do modelo.

**As Conex√µes Residuais e a Preserva√ß√£o da Informa√ß√£o**
As conex√µes residuais, como j√° vimos, s√£o fundamentais para garantir a preserva√ß√£o da informa√ß√£o ao longo do *residual stream* [^13, Teorema 2]. As conex√µes residuais s√£o aplicadas ap√≥s a autoaten√ß√£o, antes e ap√≥s a *FFN* na arquitetura *pre-norm*, e apenas depois na arquitetura *post-norm*.  Estas conex√µes permitem que as informa√ß√µes das camadas anteriores sejam passadas diretamente para as camadas subsequentes, facilitando o treinamento de modelos mais profundos e evitando o desaparecimento do gradiente [Corol√°rio 2.1].

**A Composi√ß√£o das Camadas no *Residual Stream***
Ao longo do *residual stream*, as camadas de autoaten√ß√£o, *FFN* e *LayerNorm*, em conjunto com as conex√µes residuais, trabalham de forma coordenada para transformar e enriquecer a representa√ß√£o de cada *token*. A autoaten√ß√£o integra informa√ß√µes contextuais, a rede *FFN* adiciona uma perspectiva n√£o linear e independente do contexto, e a *LayerNorm* garante a estabilidade e normaliza√ß√£o da representa√ß√£o. As conex√µes residuais garantem que a informa√ß√£o original flua eficientemente, e os modelos *transformers* se tornem capazes de aprender depend√™ncias complexas na linguagem, como discutido anteriormente [Teorema 2].  Desta forma, cada camada contribui para a forma√ß√£o de uma representa√ß√£o rica e diversificada do *token* ao longo do fluxo residual.

> üí° **Exemplo Num√©rico:**
> Para consolidar o entendimento, consideremos um *token* que possui um *embedding* inicial $x_i = \begin{bmatrix} 0.5 \\ 0.2 \\ -0.1 \\ 0.3 \end{bmatrix}$. Ap√≥s a passagem por todas as camadas do bloco *transformer* (arquitetura *post-norm*), o vetor *embedding* √© transformado da seguinte forma:
>
> 1.  **Autoaten√ß√£o:** Integra informa√ß√£o contextual, resultando em $t^1 = \begin{bmatrix} 0.2 \\ -0.1 \\ 0.3 \\ 0.1 \end{bmatrix}$.
> 2.  **Adi√ß√£o Residual 1:** Soma $t^1$ ao *embedding* original $x_i$, resultando em $t^2 = x_i + t^1 = \begin{bmatrix} 0.5 \\ 0.2 \\ -0.1 \\ 0.3 \end{bmatrix} + \begin{bmatrix} 0.2 \\ -0.1 \\ 0.3 \\ 0.1 \end{bmatrix} = \begin{bmatrix} 0.7 \\ 0.1 \\ 0.2 \\ 0.4 \end{bmatrix}$.
> 3.  **LayerNorm 1:** Normaliza $t^2$, resultando em $t^3 \approx \begin{bmatrix} 1.25 \\ -0.7 \\ -0.3 \\ 0.1 \end{bmatrix}$ (considerando que o c√°lculo da LayerNorm j√° foi demonstrado).
> 4.  **FFN:** Aplica uma transforma√ß√£o n√£o linear, resultando em $t^4 = \begin{bmatrix} 0.1 \\ -0.3 \\ 0.2 \\ -0.1 \end{bmatrix}$ (considerando que o c√°lculo da FFN j√° foi demonstrado).
> 5.  **Adi√ß√£o Residual 2:** Soma $t^4$ ao resultado da LayerNorm $t^3$, resultando em $t^5 = t^3 + t^4 =  \begin{bmatrix} 1.25 \\ -0.7 \\ -0.3 \\ 0.1 \end{bmatrix} + \begin{bmatrix} 0.1 \\ -0.3 \\ 0.2 \\ -0.1 \end{bmatrix} = \begin{bmatrix} 1.35 \\ -1.0 \\ -0.1 \\ 0.0 \end{bmatrix}$
> 6.  **LayerNorm 2:** Normaliza $t^5$, resultando no vetor de sa√≠da do bloco *transformer* $h_i \approx \begin{bmatrix} 1.4 \\ -1.1 \\ -0.2 \\ 0.1 \end{bmatrix}$ (considerando que o c√°lculo da LayerNorm j√° foi demonstrado).
>
> Ao longo deste processo, o *embedding* do *token* √© enriquecido com informa√ß√µes contextuais e passa por transforma√ß√µes n√£o lineares, e o *LayerNorm* garante que a representa√ß√£o se mantenha est√°vel e dentro de uma escala adequada. As conex√µes residuais facilitam a propaga√ß√£o da informa√ß√£o atrav√©s do bloco *transformer*.

**Teorema 3.1** (A Autoaten√ß√£o como um Mecanismo de Movimento de Informa√ß√£o): As *attention heads* na camada de autoaten√ß√£o atuam como mecanismos de movimento de informa√ß√£o, movendo informa√ß√£o do *residual stream* de *tokens* vizinhos para o *residual stream* do *token* atual, permitindo que a representa√ß√£o de cada *token* contenha informa√ß√µes sobre ele pr√≥prio e sobre o seu contexto.

*Prova:*
I.  As *attention heads* na camada *MultiHeadAttention* calculam *queries*, *keys* e *values* para cada *token* na sequ√™ncia, onde as *queries* s√£o usadas para comparar os *tokens*, e os *values* representam a informa√ß√£o a ser agregada ao *embedding* do *token* atual.
II. Ao comparar as *queries* com as *keys* de outros *tokens*, a camada de autoaten√ß√£o pondera os *values* dos outros *tokens* de acordo com a sua relev√¢ncia para o *token* atual.
III.  O resultado desta pondera√ß√£o √© ent√£o adicionado ao *embedding* do *token* atual, ou seja, a informa√ß√£o √© movida do *residual stream* dos *tokens* vizinhos para o *residual stream* do *token* atual.
IV. Portanto, as *attention heads* atuam como um mecanismo de movimento de informa√ß√£o, permitindo que o *embedding* de cada *token* contenha informa√ß√£o sobre o *token* atual e sobre os seus vizinhos, resultando em uma representa√ß√£o contextualizada.
‚ñ†

**Teorema 3.2** (Representa√ß√µes Contextuais no Fluxo Residual): A combina√ß√£o da autoaten√ß√£o, *FFN* e *LayerNorm*, junto com as conex√µes residuais, leva a representa√ß√µes de *tokens* que cont√™m informa√ß√µes sobre o *token* atual e seu contexto, com cada componente adicionando uma nova perspectiva a essa representa√ß√£o.

*Prova:*
I.  A autoaten√ß√£o integra informa√ß√µes contextuais de outros *tokens* ao *residual stream* do *token* atual (conforme Teorema 3.1), permitindo que a representa√ß√£o do *token* seja influenciada pelo seu contexto.
II. A rede *FFN* aplica uma transforma√ß√£o n√£o linear ao *embedding* do *token*, adicionando uma nova perspectiva √† representa√ß√£o que √© independente do contexto.
III. A *LayerNorm* garante que a representa√ß√£o se mantenha est√°vel e dentro de uma escala adequada, permitindo que as camadas aprendam de forma eficaz.
IV. As conex√µes residuais garantem que a informa√ß√£o original do *embedding* seja preservada ao longo das transforma√ß√µes.
V. A combina√ß√£o dessas camadas leva a representa√ß√µes que cont√™m informa√ß√µes sobre o *token* atual, o contexto em que ele se insere e diversas perspectivas da informa√ß√£o, o que √© essencial para o bom desempenho do *transformer*.
‚ñ†

**Corol√°rio 3.1** (Fluxo de Informa√ß√£o e Aprendizado): O *residual stream* pode ser visto como o caminho pelo qual o *transformer* aprende representa√ß√µes contextuais, onde a informa√ß√£o flui de forma controlada e as conex√µes residuais garantem que a informa√ß√£o original seja preservada.
*Prova:*
I.  O *residual stream* representa o fluxo de representa√ß√µes de *tokens* atrav√©s das camadas do *transformer*, onde cada camada adiciona uma nova perspectiva ao *embedding* do *token*.
II. As conex√µes residuais garantem que as informa√ß√µes de cada camada sejam propagadas para as camadas subsequentes, permitindo que o modelo aprenda depend√™ncias complexas entre os *tokens*.
III. O mecanismo de aten√ß√£o e as camadas *FFN* permitem que a representa√ß√£o de cada *token* seja moldada pelo seu contexto e pelas rela√ß√µes n√£o lineares entre as palavras, contribuindo para o aprendizado do modelo.
IV. Portanto, o *residual stream* √© o caminho pelo qual o *transformer* aprende representa√ß√µes contextuais, e as conex√µes residuais garantem que essa informa√ß√£o flua ao longo do modelo, e que o gradiente seja propagado de forma eficaz durante o processo de treinamento.
‚ñ†

**Proposi√ß√£o 1** (Preserva√ß√£o da Dimensionalidade na FFN): A camada *FFN* mapeia vetores de dimens√£o $d$ para vetores de dimens√£o $d$, mantendo a dimensionalidade do *residual stream*.
*Prova:*
I. A *FFN* consiste em duas camadas lineares com uma ativa√ß√£o n√£o linear entre elas.
II. A primeira camada linear mapeia um vetor de dimens√£o $d$ para um vetor de dimens√£o $d_{ff}$, onde $d_{ff}$ √© a dimens√£o da camada intermedi√°ria da *FFN*.
III. A segunda camada linear mapeia o vetor de dimens√£o $d_{ff}$ de volta para um vetor de dimens√£o $d$.
IV. Portanto, a *FFN* mapeia vetores de dimens√£o $d$ para vetores de dimens√£o $d$, preservando a dimensionalidade do *residual stream*.
‚ñ†

**Lema 1.1** (Transforma√ß√£o Linear na FFN): A primeira camada linear na *FFN* pode ser representada por uma matriz de pesos $W_1 \in \mathbb{R}^{d_{ff} \times d}$ e a segunda camada linear por uma matriz de pesos $W_2 \in \mathbb{R}^{d \times d_{ff}}$. A opera√ß√£o da FFN pode ser descrita por:
$$
t^4 = W_2 \cdot ReLU(W_1 \cdot t^3 + b_1) + b_2
$$
onde $b_1 \in \mathbb{R}^{d_{ff}}$ e $b_2 \in \mathbb{R}^{d}$ s√£o os vetores de bias.

*Prova:*
I. A opera√ß√£o da *FFN* consiste em uma transforma√ß√£o linear da entrada $t^3$, seguida de uma fun√ß√£o de ativa√ß√£o n√£o linear (ReLU), e outra transforma√ß√£o linear, com a adi√ß√£o de bias em cada passo.
II. A primeira transforma√ß√£o linear √© dada por $W_1 \cdot t^3 + b_1$, onde $W_1$ √© a matriz de pesos da primeira camada linear e $b_1$ √© o bias correspondente.
III. A aplica√ß√£o da fun√ß√£o ReLU resulta em $ReLU(W_1 \cdot t^3 + b_1)$.
IV. A segunda transforma√ß√£o linear √© dada por $W_2 \cdot ReLU(W_1 \cdot t^3 + b_1) + b_2$, onde $W_2$ √© a matriz de pesos da segunda camada linear e $b_2$ √© o bias correspondente.
V. Portanto, a opera√ß√£o da FFN pode ser expressa como $t^4 = W_2 \cdot ReLU(W_1 \cdot t^3 + b_1) + b_2$
‚ñ†

**Teorema 3.3** (Invari√¢ncia da Dimens√£o pelo LayerNorm): A camada *LayerNorm* preserva a dimensionalidade do vetor de entrada, mapeando um vetor de dimens√£o $d$ para outro vetor de dimens√£o $d$.
*Prova:*
I. A *LayerNorm* opera normalizando as ativa√ß√µes dentro de cada vetor de entrada.
II. A opera√ß√£o de normaliza√ß√£o envolve a centraliza√ß√£o (subtra√ß√£o da m√©dia) e a divis√£o pelo desvio padr√£o, ambas opera√ß√µes preservando a dimensionalidade do vetor.
III. Adicionalmente, a *LayerNorm* aplica uma escala e bias aprend√≠veis, ambos com a mesma dimensionalidade do vetor de entrada, logo, a dimensionalidade tamb√©m √© preservada nesse passo.
IV. Portanto, a *LayerNorm* mapeia um vetor de dimens√£o $d$ para outro vetor de dimens√£o $d$, preservando a dimensionalidade do *residual stream*.
‚ñ†

**Observa√ß√£o 1** (Perspectivas da Informa√ß√£o): Cada camada do bloco *transformer* contribui com uma perspectiva diferente para a representa√ß√£o do token. A autoaten√ß√£o fornece uma vis√£o contextual, a FFN adiciona uma vis√£o n√£o linear e independente, e a LayerNorm estabiliza a representa√ß√£o. A combina√ß√£o dessas perspectivas resulta em representa√ß√µes robustas e ricas em informa√ß√£o.

**Lema 3.1** (Conex√µes Residuais como Soma): As conex√µes residuais podem ser expressas como uma opera√ß√£o de soma da entrada com a sa√≠da da camada, ou seja, $t^{i+1} = t^i + Layer(t^i)$, onde $Layer(t^i)$ representa a sa√≠da da camada (autoaten√ß√£o ou FFN) aplicada na entrada $t^i$.
*Prova:*
I. Por defini√ß√£o, as conex√µes residuais adicionam a entrada da camada √† sua sa√≠da.
II. Seja $t^i$ a entrada da camada e $Layer(t^i)$ a sa√≠da da camada, ent√£o a conex√£o residual resulta em $t^{i+1} = t^i + Layer(t^i)$.
III. Portanto, as conex√µes residuais atuam como uma opera√ß√£o de soma, adicionando a sa√≠da da camada √† sua entrada.
‚ñ†

### Conclus√£o
Este cap√≠tulo consolidou nossa compreens√£o sobre o papel de cada camada no *residual stream* do bloco *transformer*. Enquanto a autoaten√ß√£o integra informa√ß√£o contextual, a rede *FFN* adiciona uma perspectiva n√£o-linear e independente do contexto e a *LayerNorm* estabiliza o fluxo e garante que as ativa√ß√µes se mantenham dentro de uma escala adequada. As conex√µes residuais garantem que a informa√ß√£o flua de forma eficiente ao longo do modelo, e permitem a constru√ß√£o de arquiteturas mais profundas e complexas. A combina√ß√£o dessas camadas no *residual stream* permite ao *transformer* aprender representa√ß√µes robustas e complexas da linguagem. A vis√£o do *residual stream* como um fluxo din√¢mico de informa√ß√µes √© essencial para a compreens√£o do funcionamento e da capacidade dos *transformers*.

### Refer√™ncias
[^1]: *‚ÄúIn this chapter we formalize this idea of pretraining‚Äîlearning knowledge about language and the world from vast amounts of text‚Äîand call the resulting pretrained language models large language models.‚Äù*
[^6]: *‚ÄúTo capture these three different roles, transformers introduce weight matrices WQ, WK, and WV. These weights will be used to project each input vector xi into a representation of its role as a key, query, or value.‚Äù*
[^7]: *‚ÄúThis description of the self-attention process has been from the perspective of computing a single output at a single time step i.‚Äù*
[^12]: *‚ÄúThe previous sections viewed the transformer block as applied to the entire N-token input X of shape [N √ó d], producing an output also of shape [N √ó d].‚Äù*
[^13]: *‚Äúthese layers as a stream of d-dimensional representations, called the residual stream and visualized in Fig. 10.7.‚Äù*
<!-- END -->
