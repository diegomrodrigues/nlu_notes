## Potenciais Danos e Mitiga√ß√µes em Large Language Models

### Introdu√ß√£o

Neste cap√≠tulo, exploramos os Large Language Models (LLMs) baseados em transformers, abordando sua arquitetura, treinamento e aplica√ß√µes. Expandindo sobre os conceitos introduzidos e, em continuidade aos t√≥picos anteriores, √© crucial discutir os potenciais danos associados a essas poderosas ferramentas, bem como as estrat√©gias para mitigar tais riscos. Como vimos anteriormente, os LLMs s√£o capazes de gerar texto coerente e persuasivo, mas essa mesma capacidade pode ser explorada para fins maliciosos ou resultar em consequ√™ncias n√£o intencionais [^28, ^29]. Abordaremos aqui as principais formas de danos que os LLMs podem causar, incluindo alucina√ß√µes, gera√ß√£o de linguagem t√≥xica, perpetua√ß√£o de vieses, dissemina√ß√£o de desinforma√ß√£o e viola√ß√µes de privacidade e direitos autorais.

### Conceitos Fundamentais

√â importante reconhecer que, embora os LLMs tenham avan√ßado significativamente o campo do Processamento de Linguagem Natural (PLN), eles n√£o s√£o isentos de falhas. A capacidade de gerar texto convincente n√£o garante a veracidade ou a √©tica do conte√∫do produzido.

**Alucina√ß√µes:**
  - *Defini√ß√£o:* LLMs podem gerar informa√ß√µes falsas ou sem sentido, um fen√¥meno conhecido como alucina√ß√£o [^28]. Essa caracter√≠stica se deve ao fato de que os modelos s√£o treinados para produzir texto coerente, mas n√£o necessariamente factual.
  - *Implica√ß√µes:* Alucina√ß√µes podem comprometer a confiabilidade dos LLMs em aplica√ß√µes cr√≠ticas, como resposta a perguntas, resumos de textos e sistemas de di√°logo.
  - *Exemplo:* Um LLM pode gerar um resumo de um artigo cient√≠fico inventando dados ou conclus√µes que n√£o est√£o presentes no original.

> üí° **Exemplo Num√©rico:** Suponha que um LLM seja solicitado a resumir um artigo cient√≠fico sobre a efic√°cia de um novo medicamento. O artigo original afirma que o medicamento reduziu os sintomas em 75% dos pacientes. No entanto, o LLM, devido a uma alucina√ß√£o, gera um resumo afirmando que o medicamento reduziu os sintomas em 95% dos pacientes e causou uma melhora significativa na qualidade de vida dos pacientes, adicionando uma informa√ß√£o inexistente no texto original. Essa alucina√ß√£o pode levar a interpreta√ß√µes err√¥neas sobre a efic√°cia do medicamento e gerar falsas expectativas.

**Lema 1:** A probabilidade de um LLM gerar alucina√ß√µes aumenta com a dist√¢ncia sem√¢ntica entre o prompt de entrada e os dados de treinamento.
    *Prova:*
    I. LLMs s√£o treinados para mapear entradas para sa√≠das com base em padr√µes aprendidos nos dados de treinamento.
    II. Quando um prompt de entrada est√° semanticamente pr√≥ximo aos dados de treinamento, o modelo tem um alto n√≠vel de confian√ßa em sua capacidade de gerar uma sa√≠da correspondente.
    III. No entanto, quando o prompt se afasta da distribui√ß√£o dos dados de treinamento, o modelo pode n√£o ter padr√µes claros para seguir, levando √† gera√ß√£o de informa√ß√µes incorretas ou inventadas.
    IV. Portanto, a probabilidade de um LLM gerar alucina√ß√µes aumenta com a dist√¢ncia sem√¢ntica entre o prompt de entrada e os dados de treinamento.  $\blacksquare$

**Lema 1.1:**  A probabilidade de alucina√ß√£o tamb√©m aumenta com a complexidade da tarefa solicitada ao LLM.
    *Prova:*
    I. Tarefas complexas, como a gera√ß√£o de texto longo ou a s√≠ntese de informa√ß√µes de diversas fontes, exigem que o LLM processe e combine informa√ß√µes de maneira mais abstrata e inferencial.
    II. Quanto maior a complexidade da tarefa, maior a chance do LLM se desviar da distribui√ß√£o dos dados de treinamento ao tentar gerar a sa√≠da.
    III. Isso ocorre porque tarefas mais complexas podem exigir que o modelo extrapole ou interpole padr√µes que n√£o foram explicitamente observados durante o treinamento.
    IV. A extrapola√ß√£o e interpola√ß√£o aumentam a probabilidade de erros e alucina√ß√µes, pois o modelo pode gerar conte√∫do que n√£o corresponde √† realidade.
    V. Portanto, a probabilidade de alucina√ß√£o tamb√©m aumenta com a complexidade da tarefa solicitada ao LLM.  $\blacksquare$

**Linguagem T√≥xica:**
  - *Defini√ß√£o:* LLMs podem gerar discursos de √≥dio, abusivos ou discriminat√≥rios, mesmo quando o prompt de entrada √© completamente in√≥cuo [^28].
  - *Causas:* A toxicidade pode ser resultado de vieses presentes nos dados de treinamento ou da incapacidade do modelo de discernir nuances contextuais.
  - *Preocupa√ß√µes:* A dissemina√ß√£o de linguagem t√≥xica pode ter s√©rias consequ√™ncias sociais, incluindo o aumento da polariza√ß√£o, o ass√©dio online e a discrimina√ß√£o.
  - *Exemplo:* Um LLM pode responder a uma pergunta aparentemente neutra com um discurso que cont√©m insultos raciais ou sexistas.

> üí° **Exemplo Num√©rico:** Um usu√°rio interage com um LLM atrav√©s de um chatbot com a seguinte pergunta: "Qual sua opini√£o sobre pol√≠tica?". O modelo, influenciado por dados de treinamento com conte√∫do t√≥xico, poderia responder: "Pol√≠tica √© um lixo, feita por pessoas corruptas e incompetentes, especialmente aqueles [insira um insulto racial aqui]". Mesmo que a pergunta original n√£o tenha sido t√≥xica, a resposta do modelo cont√©m linguagem abusiva e generaliza√ß√µes negativas.

**Vieses:**
  - *Defini√ß√£o:* Os LLMs podem perpetuar e amplificar vieses presentes nos dados de treinamento, como estere√≥tipos de g√™nero, ra√ßa ou orienta√ß√£o sexual [^28].
  - *Mecanismo:* O modelo aprende as associa√ß√µes e padr√µes presentes nos dados, reproduzindo as desigualdades e preconceitos existentes.
  - *Consequ√™ncias:* Vieses em LLMs podem levar a decis√µes injustas ou discriminat√≥rias em diversas aplica√ß√µes, como sistemas de recomenda√ß√£o, recrutamento e avalia√ß√£o de cr√©dito.
  - *Exemplo:* Um LLM pode associar profiss√µes de lideran√ßa a homens e profiss√µes de cuidado a mulheres.

> üí° **Exemplo Num√©rico:** Um LLM usado em um sistema de recrutamento analisa curr√≠culos. Se os dados de treinamento contiverem mais exemplos de homens em cargos de lideran√ßa, o LLM pode aprender a atribuir maior import√¢ncia a candidatos homens para esses cargos, mesmo que as mulheres tenham qualifica√ß√µes iguais ou superiores. Por exemplo, ao avaliar curr√≠culos para uma vaga de CEO, o modelo pode pontuar curr√≠culos de candidatos homens 10% acima de candidatas mulheres, mesmo com qualifica√ß√µes similares.

 **Observa√ß√£o 1:** Vieses em LLMs podem ser complexos e multifacetados, afetando diferentes grupos e contextos de maneira desigual. A mitiga√ß√£o de vieses exige n√£o apenas a remo√ß√£o de exemplos expl√≠citos de conte√∫do enviesado nos dados, mas tamb√©m a considera√ß√£o das rela√ß√µes impl√≠citas e associa√ß√µes que o modelo aprende durante o treinamento.

**Desinforma√ß√£o:**
  - *Defini√ß√£o:* LLMs podem gerar textos convincentes e aparentemente factuais que cont√™m informa√ß√µes falsas ou enganosas, potencialmente contribuindo para a dissemina√ß√£o de desinforma√ß√£o [^28].
  - *Impacto:* A desinforma√ß√£o pode manipular a opini√£o p√∫blica, prejudicar a tomada de decis√£o e at√© mesmo incitar a viol√™ncia.
  - *Exemplo:* Um LLM pode criar um artigo de not√≠cias falso, com detalhes e cita√ß√µes inventadas, que se espalha rapidamente pelas redes sociais.

> üí° **Exemplo Num√©rico:** Um LLM √© instru√≠do a escrever uma not√≠cia sobre uma nova vacina. O modelo, sem acesso a dados verificados ou atualizados, gera um artigo afirmando que a vacina causa efeitos colaterais graves em 50% dos pacientes, com cita√ß√µes falsas de m√©dicos renomados. Este artigo falso, rapidamente compartilhado nas redes sociais, pode gerar p√¢nico e desconfian√ßa na vacina, mesmo que a mesma seja segura e eficaz.

**Teorema 1:** A veracidade das informa√ß√µes geradas por um LLM √© inversamente proporcional √† sua confian√ßa na gera√ß√£o.
    *Prova:*
    I. LLMs s√£o treinados para prever a pr√≥xima palavra em uma sequ√™ncia de texto, aprendendo uma distribui√ß√£o de probabilidade sobre o vocabul√°rio.
    II. A confian√ßa do modelo em sua gera√ß√£o est√° relacionada √† probabilidade associada √† palavra que ele seleciona para gerar.
    III. Quando o modelo tem alta confian√ßa, significa que a palavra selecionada tem uma alta probabilidade de ocorrer dado o contexto anterior dentro dos dados de treinamento.
    IV. No entanto, essa confian√ßa √© baseada na distribui√ß√£o estat√≠stica aprendida e n√£o na compreens√£o da verdade ou da veracidade das informa√ß√µes.
    V. Portanto, um modelo pode ter alta confian√ßa em gerar um texto que se parece com o que ele viu nos dados de treinamento, mesmo que este texto contenha informa√ß√µes factualmente incorretas ou n√£o verdadeiras.
    VI. Consequentemente, a veracidade das informa√ß√µes geradas por um LLM √© inversamente proporcional √† sua confian√ßa na gera√ß√£o.  $\blacksquare$

**Teorema 1.1:** A probabilidade de um LLM gerar desinforma√ß√£o √© diretamente proporcional √† sua capacidade de gerar texto persuasivo.
    *Prova:*
     I. LLMs s√£o projetados para gerar texto que seja coerente, fluente e contextualmente apropriado.
     II. Essa capacidade de gerar texto persuasivo aumenta a probabilidade de que informa√ß√µes falsas ou enganosas sejam aceitas como verdadeiras pelo p√∫blico.
     III. Um texto persuasivo, mesmo que factualmente incorreto, pode ser mais facilmente disseminado e acreditado do que um texto que seja menos convincente.
     IV. Portanto, a capacidade de um LLM de gerar texto persuasivo aumenta o risco de que desinforma√ß√£o seja espalhada de forma eficaz.
     V. Consequentemente, a probabilidade de um LLM gerar desinforma√ß√£o √© diretamente proporcional √† sua capacidade de gerar texto persuasivo. $\blacksquare$

**Viola√ß√£o de Privacidade e Direitos Autorais:**
   - *Defini√ß√£o:* LLMs podem vazar informa√ß√µes confidenciais presentes nos dados de treinamento, incluindo dados pessoais ou informa√ß√µes protegidas por direitos autorais [^28].
  - *Preocupa√ß√µes:* O uso de dados privados ou protegidos sem consentimento pode resultar em problemas √©ticos e legais.
  - *Exemplo:* Um LLM treinado com registros de sa√∫de de pacientes pode divulgar informa√ß√µes pessoais se for solicitado a gerar dados similares.
  - *Copyright:* A utiliza√ß√£o de textos protegidos por direitos autorais no treinamento de LLMs pode violar leis de propriedade intelectual.

> üí° **Exemplo Num√©rico:** Um LLM √© treinado usando um grande conjunto de dados de artigos de not√≠cias, incluindo um artigo com informa√ß√µes pessoais de um jornalista, como seu n√∫mero de telefone e endere√ßo de e-mail. Um usu√°rio solicita ao modelo que crie um artigo sobre o mesmo jornalista. O modelo pode, inadvertidamente, reproduzir o n√∫mero de telefone e endere√ßo de e-mail do jornalista, expondo dados privados. Al√©m disso, se o LLM foi treinado com livros protegidos por direitos autorais, ele pode gerar trechos desses livros de forma similar √† maneira como foram originalmente escritos, violando as leis de propriedade intelectual.

**Mitiga√ß√µes e Solu√ß√µes:**

A mitiga√ß√£o desses potenciais danos √© crucial para o desenvolvimento e implanta√ß√£o respons√°veis de LLMs. Algumas estrat√©gias para abordar esses problemas incluem:
   - **An√°lise de Dados de Treinamento:** √â fundamental analisar cuidadosamente os dados usados para treinar LLMs, identificando e corrigindo vieses e toxicidades [^28]. √â importante que as bases de dados utilizadas para treino sejam o mais representativas poss√≠vel da diversidade humana e evitar ao m√°ximo bases de dados que foram criadas com prop√≥sitos espec√≠ficos ou tendenciosos.
   - **Datasheets e Model Cards:** O desenvolvimento de datasheets e model cards, que fornecem informa√ß√µes detalhadas sobre os dados de treinamento, arquitetura do modelo e limita√ß√µes, s√£o essenciais para aumentar a transpar√™ncia e responsabilidade [^28].
   - **Filtragem de Texto Gerado:** Implementar mecanismos para filtrar texto gerado por LLMs, detectando e removendo conte√∫dos t√≥xicos, tendenciosos ou incorretos [^28].
   - **Abordagens de Treinamento:** Explorar novas abordagens de treinamento que reduzem a probabilidade de alucina√ß√µes, como o uso de t√©cnicas de aprendizado com refor√ßo.
   - **Regulamenta√ß√£o:** Desenvolver regulamenta√ß√µes e pol√≠ticas que abordem as quest√µes √©ticas e de seguran√ßa relacionadas ao uso de LLMs [^28].
   - **Pesquisa Cont√≠nua:** A pesquisa cont√≠nua √© essencial para entender melhor os riscos associados a LLMs e desenvolver solu√ß√µes mais eficazes.
   - **Ado√ß√£o de T√©cnicas de Sampling:** O uso de m√©todos de *sampling* como top-k e *nucleus sampling* permite um maior controle da qualidade e diversidade do texto gerado, e podem atenuar a repeti√ß√£o e previsibilidade associada com *greedy decoding* [^23, ^24]. A implementa√ß√£o de temperatura tamb√©m contribui para moldar a distribui√ß√£o da probabilidade das palavras geradas.

> üí° **Exemplo Num√©rico:** Um LLM √© configurado com uma temperatura de 0.2, o que torna suas respostas mais determin√≠sticas e menos criativas, diminuindo a probabilidade de alucina√ß√µes e respostas t√≥xicas. Ao aumentar a temperatura para 1.0, o modelo se torna mais diverso e criativo, por√©m mais propenso a gerar informa√ß√µes falsas e/ou conte√∫do t√≥xico. O uso do top-k sampling, com k=5, limita a escolha da pr√≥xima palavra apenas entre as 5 palavras com maior probabilidade, reduzindo respostas incoerentes.

   - **Implementa√ß√£o de Mecanismos de Feedback Humano:** Uma forma de mitigar o conte√∫do gerado por LLMs √© a incorpora√ß√£o de *feedback* humano. Em sistemas interativos que requerem uma gera√ß√£o de texto com qualidade, a inser√ß√£o de um loop onde humanos fornecem avalia√ß√µes e corre√ß√µes ao texto gerado permite que o modelo aprenda com os erros e melhore suas gera√ß√µes em itera√ß√µes futuras.

  **Proposi√ß√£o 1:** O feedback humano pode ser utilizado para reduzir vieses em LLMs, atrav√©s de um processo iterativo de avalia√ß√£o e refinamento do modelo.
  *Prova:*
  I.  LLMs aprendem a partir dos dados de treinamento, incluindo vieses existentes nesses dados.
  II.  Ao obter feedback humano sobre as respostas geradas, √© poss√≠vel identificar casos em que o LLM produz respostas tendenciosas.
  III. Este feedback pode ser usado para ajustar os pesos do modelo, direcionando-o a produzir respostas mais neutras e justas.
  IV. O processo iterativo de gerar respostas, obter feedback, e refinar o modelo permite que ele aprenda a minimizar vieses ao longo do tempo.
  V. Portanto, o feedback humano pode ser utilizado para reduzir vieses em LLMs, atrav√©s de um processo iterativo de avalia√ß√£o e refinamento do modelo. $\blacksquare$

  **Proposi√ß√£o 1.1:** A efic√°cia do feedback humano em mitigar vieses em LLMs √© influenciada pela diversidade dos *feedbacks* recebidos.
    *Prova:*
     I. O feedback humano pode corrigir vieses nos LLMs, mas se esse feedback for homog√™neo ou representar um conjunto limitado de perspectivas, o efeito ser√° limitado.
     II. Se o feedback for predominantemente de um grupo espec√≠fico, ele pode perpetuar ou at√© amplificar outros vieses, ao inv√©s de corrig√≠-los de maneira abrangente.
     III. A diversidade nos feedbacks permite a exposi√ß√£o do LLM a diferentes pontos de vista, ajudando-o a desenvolver uma vis√£o mais completa e menos enviesada do mundo.
     IV. Portanto, a efic√°cia do feedback humano em mitigar vieses em LLMs √© influenciada pela diversidade dos *feedbacks* recebidos. $\blacksquare$

> üí° **Exemplo Num√©rico:** Um LLM √© utilizado para gerar legendas para fotos. Se o feedback humano para corrigir os vieses do modelo vier exclusivamente de um grupo que representa uma parcela pequena da sociedade (ex: homens de meia idade brancos), o modelo pode internalizar novos vieses que afetam outros grupos demogr√°ficos (ex: mulheres ou outras etnias). Se, ao contr√°rio, a avalia√ß√£o do modelo for feita por diversos grupos demogr√°ficos, o modelo ter√° um entendimento mais abrangente e inclusivo, produzindo textos com menos vieses.

### Conclus√£o

Neste cap√≠tulo, exploramos os diversos aspectos relacionados aos potenciais danos e mitiga√ß√µes associados a LLMs. A crescente capacidade desses modelos de gerar texto convincente e coerente traz consigo a responsabilidade de abordar os riscos e desenvolver estrat√©gias que minimizem os potenciais danos associados.

Ao reconhecer a natureza multifacetada dos problemas, que incluem alucina√ß√µes, linguagem t√≥xica, vieses, desinforma√ß√£o e quest√µes de privacidade e direitos autorais, podemos iniciar uma jornada em dire√ß√£o ao desenvolvimento e utiliza√ß√£o respons√°vel e √©tica dessas poderosas tecnologias.

A an√°lise dos dados de treinamento, o desenvolvimento de *datasheets* e *model cards*, a implementa√ß√£o de mecanismos de filtragem, a explora√ß√£o de novas abordagens de treinamento, a promo√ß√£o da regulamenta√ß√£o, e a pesquisa cont√≠nua s√£o elementos fundamentais para garantir que os benef√≠cios dos LLMs sejam aproveitados de forma segura e justa para todos.

### Refer√™ncias
[^28]: Gehman, S., S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. 2020. RealToxicity Prompts: Evaluating neu-ral toxic degeneration in language models. Findings of EMNLP.
[^29]: Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. 2017. Atten-tion is all you need. NeurIPS.
[^23]: Holtzman, A., J. Buys, L. Du, M. Forbes, and Y. Choi. 2020. The curious case of neural text degeneration. ICLR.
[^24]: Kaplan, J., S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. 2020. Scaling laws for neural language mod-els. ArXiv preprint.
<!-- END -->
