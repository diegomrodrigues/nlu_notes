## Paralleliza√ß√£o da Autoaten√ß√£o com Multiplica√ß√£o de Matrizes
### Introdu√ß√£o
Em continuidade ao t√≥pico anterior, que introduziu o conceito de autoaten√ß√£o causal e sua formula√ß√£o matem√°tica [^5], esta se√ß√£o explora como a computa√ß√£o da autoaten√ß√£o pode ser eficientemente paralela utilizando multiplica√ß√£o de matrizes. Como vimos anteriormente, a autoaten√ß√£o envolve comparar cada elemento de uma sequ√™ncia com todos os elementos anteriores [^5]. Esta se√ß√£o detalha como essa opera√ß√£o pode ser transformada em opera√ß√µes matriciais, possibilitando processar a sequ√™ncia completa de tokens de forma paralela, o que √© crucial para a efici√™ncia de modelos de linguagem grandes.

### Conceitos Fundamentais
O ponto chave para a paraleliza√ß√£o √© a representa√ß√£o dos inputs como matrizes e o uso de opera√ß√µes matriciais para computar as compara√ß√µes necess√°rias para a autoaten√ß√£o. Em vez de realizar compara√ß√µes token a token sequencialmente, os tokens de entrada s√£o agrupados em uma matriz, que √© ent√£o manipulada para calcular as pontua√ß√µes de aten√ß√£o de uma s√≥ vez.

**Empacotamento dos Embeddings em uma Matriz**
Para iniciar o processo de paraleliza√ß√£o, os embeddings dos *N* tokens de entrada, que s√£o vetores de dimens√£o *d*, s√£o organizados em uma √∫nica matriz *X* de dimens√£o *N x d* [^8].  Cada linha da matriz *X* representa o embedding de um token espec√≠fico. Ou seja, $X \in \mathbb{R}^{N \times d}$.
Essa matriz cont√©m todos os inputs necess√°rios para o c√°lculo da autoaten√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha que temos uma sequ√™ncia de 3 tokens, e que cada embedding tenha dimens√£o 4 (d=4). Assim, N=3. A matriz X resultante poderia ser:
> ```python
> import numpy as np
> X = np.array([
>     [0.1, 0.2, 0.3, 0.4],  # Embedding do primeiro token
>     [0.5, 0.6, 0.7, 0.8],  # Embedding do segundo token
>     [0.9, 1.0, 1.1, 1.2]   # Embedding do terceiro token
> ])
> print(X)
> ```
> Isso resulta na seguinte matriz:
> ```
> [[0.1 0.2 0.3 0.4]
>  [0.5 0.6 0.7 0.8]
>  [0.9 1.  1.1 1.2]]
> ```
> Cada linha representa um token, e cada coluna representa uma dimens√£o do embedding.

**Gera√ß√£o das Matrizes de Chave, Consulta e Valor**
A matriz *X* √© usada para gerar as matrizes de chave (*K*), consulta (*Q*), e valor (*V*). Isso √© feito multiplicando *X* pelas matrizes de peso correspondentes $W^K$, $W^Q$ e $W^V$ de dimens√£o *d x d*, onde $W^Q, W^K, W^V \in \mathbb{R}^{d \times d}$ [^6]. Matematicamente, essas opera√ß√µes s√£o expressas como:

$$Q = XW^Q$$
$$K = XW^K$$
$$V = XW^V$$
As matrizes resultantes $Q$, $K$ e $V$ t√™m todas a dimens√£o *N x d* e cont√™m os vetores de consulta, chave e valor para todos os *N* tokens de entrada simultaneamente [^8].

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, vamos supor que temos matrizes de peso $W^Q$, $W^K$ e $W^V$ (4x4):
> ```python
> WQ = np.array([
>     [0.1, 0.2, 0.3, 0.4],
>     [0.5, 0.6, 0.7, 0.8],
>     [0.9, 1.0, 1.1, 1.2],
>     [1.3, 1.4, 1.5, 1.6]
> ])
> WK = np.array([
>     [0.2, 0.3, 0.4, 0.5],
>     [0.6, 0.7, 0.8, 0.9],
>     [1.0, 1.1, 1.2, 1.3],
>     [1.4, 1.5, 1.6, 1.7]
> ])
> WV = np.array([
>     [0.3, 0.4, 0.5, 0.6],
>     [0.7, 0.8, 0.9, 1.0],
>     [1.1, 1.2, 1.3, 1.4],
>     [1.5, 1.6, 1.7, 1.8]
> ])
>
> Q = np.dot(X, WQ)
> K = np.dot(X, WK)
> V = np.dot(X, WV)
> print("Q:\n", Q)
> print("K:\n", K)
> print("V:\n", V)
> ```
> O c√≥digo acima realiza as multiplica√ß√µes de matrizes para gerar Q, K e V. Os resultados s√£o:
> ```
> Q:
>  [[1.26 1.4  1.54 1.68]
>  [3.06 3.4  3.74 4.08]
>  [4.86 5.4  5.94 6.48]]
> K:
>  [[1.3  1.48 1.66 1.84]
>  [3.1  3.52 3.94 4.36]
>  [4.9  5.56 6.22 6.88]]
> V:
>  [[1.42 1.6  1.78 1.96]
>  [3.42 3.8  4.18 4.56]
>  [5.42 6.   6.58 7.16]]
> ```
> As matrizes Q, K e V agora cont√™m representa√ß√µes transformadas dos embeddings de entrada, preparadas para o c√°lculo da aten√ß√£o.

**Lema 1:** *As transforma√ß√µes lineares representadas pelas matrizes $W^Q$, $W^K$, e $W^V$ projetam os embeddings de entrada em espa√ßos vetoriais distintos, permitindo que cada matriz capture diferentes aspectos sem√¢nticos dos tokens. Isso √© crucial para que o mecanismo de aten√ß√£o possa focar em diferentes rela√ß√µes entre os tokens.*

**Prova do Lema 1:**
I.  As matrizes de peso $W^Q$, $W^K$, e $W^V$ s√£o inicializadas aleatoriamente ou aprendidas durante o treinamento, garantindo que sejam distintas entre si.
II. A multiplica√ß√£o da matriz de embeddings $X$ por cada uma dessas matrizes de peso resulta em transforma√ß√µes lineares diferentes sobre os embeddings.
III. Cada transforma√ß√£o linear projeta os embeddings de entrada em um espa√ßo vetorial diferente, com suas pr√≥prias caracter√≠sticas.
IV. Consequentemente, as matrizes $Q$, $K$ e $V$ representam diferentes aspectos sem√¢nticos da informa√ß√£o contida em *X*. A matriz $Q$ (consulta) √© usada para perguntar "o que eu procuro?", a matriz $K$ (chave) representa "o que eu tenho para oferecer" e a matriz $V$ (valor) fornece o "conte√∫do" a ser agregado. Essa separa√ß√£o permite que a autoaten√ß√£o foque em diferentes rela√ß√µes entre os tokens, como similaridade sem√¢ntica, rela√ß√µes sint√°ticas e depend√™ncias contextuais. Portanto, as transforma√ß√µes lineares permitem que o mecanismo de aten√ß√£o capture diferentes facetas das rela√ß√µes entre os tokens. ‚ñ†

**C√°lculo Simult√¢neo das Pontua√ß√µes de Autoaten√ß√£o**
O c√°lculo das pontua√ß√µes (scores) de autoaten√ß√£o, que antes era feito token a token, agora √© executado de uma vez por meio de uma multiplica√ß√£o de matrizes. A multiplica√ß√£o da matriz *Q* pela transposta de *K*, $K^T$, resulta em uma matriz de pontua√ß√µes de dimens√£o *N x N* [^8]. Cada elemento *(i, j)* dessa matriz corresponde √† pontua√ß√£o da compara√ß√£o da consulta do token *i* com a chave do token *j*. Essa opera√ß√£o √© formalizada como:
$$Q K^T$$
A matriz resultante cont√©m todas as pontua√ß√µes necess√°rias para o c√°lculo da autoaten√ß√£o.
Essa matriz, como visto anteriormente, possui uma m√°scara triangular superior para evitar o vazamento de informa√ß√µes futuras no processo de modelagem de linguagem causal [^8].

> üí° **Exemplo Num√©rico:** Usando as matrizes Q e K do exemplo anterior, podemos calcular a matriz de pontua√ß√µes:
> ```python
> scores = np.dot(Q, K.T)
> print("Scores:\n", scores)
> ```
> Isso resulta em:
> ```
> Scores:
>  [[ 7.796  18.892  29.988]
>  [19.14  46.356  73.572]
>  [30.484  73.82  117.156]]
> ```
> A matriz de scores, neste caso 3x3, indica o quanto cada token deve prestar aten√ß√£o nos outros tokens. Por exemplo, o elemento na posi√ß√£o (1,2) da matriz (valor 18.892) indica o score da aten√ß√£o do primeiro token em rela√ß√£o ao segundo token.

**Observa√ß√£o 1:** *A opera√ß√£o $QK^T$ calcula o produto escalar entre cada par de vetores de consulta e chave. Este produto escalar quantifica a similaridade entre os vetores, e √© a base para a determina√ß√£o de quanto cada token deve "prestar aten√ß√£o" nos outros tokens.*

**Normaliza√ß√£o e C√°lculo da Matriz de Sa√≠da**
Ap√≥s o c√°lculo da matriz de pontua√ß√µes, elas s√£o normalizadas via softmax, e ent√£o multiplicadas pela matriz *V*, resultando na matriz de sa√≠da *A* de dimens√£o *N x d*. Esta matriz representa as representa√ß√µes contextuais dos tokens [^8]. A equa√ß√£o completa para o c√°lculo da autoaten√ß√£o com paraleliza√ß√£o por meio de matrizes √©:

$$A = \text{SelfAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
Nessa equa√ß√£o, $d_k$ √© a dimens√£o das matrizes de consulta e chave [^7].

> üí° **Exemplo Num√©rico:** Vamos calcular a matriz de sa√≠da A. Primeiro, precisamos normalizar os scores. Como d_k = 4 (a dimens√£o dos embeddings), precisamos dividir os scores por sqrt(4) = 2. Em seguida, aplicamos a fun√ß√£o softmax em cada linha. Finalmente, multiplicamos o resultado pela matriz V.
> ```python
> import numpy as np
> from scipy.special import softmax
>
> d_k = 4
> scaled_scores = scores / np.sqrt(d_k)
> attention_weights = softmax(scaled_scores, axis=1) # Softmax aplicado em cada linha
> A = np.dot(attention_weights, V)
> print("Attention Weights:\n", attention_weights)
> print("Output Matrix A:\n", A)
> ```
> O c√≥digo acima calcula as pontua√ß√µes de aten√ß√£o normalizadas e a matriz de sa√≠da A. O resultado √©:
> ```
> Attention Weights:
>  [[9.99988697e-01 1.12980106e-05 1.23903002e-15]
>  [2.86279529e-04 9.99713686e-01 4.58228144e-09]
>  [8.02457405e-08 2.88252991e-04 9.99711747e-01]]
> Output Matrix A:
>  [[ 1.42001258  1.60001427  1.780016   1.96001768]
>  [ 3.42028659  3.80031971  4.18035283  4.56038595]
>  [ 5.42000233  6.00028677  6.57999878  7.15999497]]
> ```
> Observe como a matriz `attention_weights` se parece com uma matriz identidade, onde a maior parte da aten√ß√£o √© dada ao pr√≥prio token, neste caso. Isso √© apenas um exemplo num√©rico para ilustrar o c√°lculo, e em casos reais, os scores e pesos de aten√ß√£o ser√£o muito mais diversos. A matriz A resultante cont√©m representa√ß√µes contextuais dos tokens.

**Teorema 1:** *A opera√ß√£o de autoaten√ß√£o, conforme formulada na equa√ß√£o acima, pode ser interpretada como uma combina√ß√£o ponderada dos vetores de valor (V), onde os pesos s√£o determinados pela fun√ß√£o softmax aplicada √†s pontua√ß√µes normalizadas. Isso significa que o output A para cada token √© uma representa√ß√£o contextualizada que agrega informa√ß√µes relevantes de outros tokens, ponderadas pela similaridade entre suas consultas e chaves.*

**Prova do Teorema 1:**
I. A opera√ß√£o $QK^T$ calcula as pontua√ß√µes de aten√ß√£o entre cada par de tokens.
II. A divis√£o por $\sqrt{d_k}$ escala as pontua√ß√µes, evitando que se tornem muito grandes antes de passar pela fun√ß√£o softmax.
III. A fun√ß√£o softmax normaliza as pontua√ß√µes, transformando-as em pesos que somam 1, o que pode ser interpretado como uma distribui√ß√£o de probabilidade sobre os tokens.
IV. Ao multiplicar a matriz de pesos (softmax dos scores) pela matriz de valor (V), estamos efetuando uma m√©dia ponderada dos vetores de valor. O peso de cada vetor de valor √© dado pela pontua√ß√£o de aten√ß√£o entre seu token correspondente e o token que estamos calculando a sa√≠da para.
V. Portanto, a sa√≠da *A* para cada token √© uma combina√ß√£o ponderada dos vetores de valor, onde a pondera√ß√£o √© determinada pela similaridade entre sua consulta e as chaves dos outros tokens. Isso prova que a autoaten√ß√£o gera representa√ß√µes contextuais dos tokens. ‚ñ†

**Corol√°rio 1.1:** *A divis√£o por $\sqrt{d_k}$ antes da aplica√ß√£o da fun√ß√£o softmax na equa√ß√£o acima, √© uma t√©cnica de escalonamento para garantir que as pontua√ß√µes n√£o se tornem excessivamente grandes, o que poderia levar a gradientes muito pequenos durante o treinamento, pois softmax satura para entradas muito grandes. Essa normaliza√ß√£o ajuda a manter a estabilidade e converg√™ncia do treinamento da rede.*

**Prova do Corol√°rio 1.1:**
I. As pontua√ß√µes $QK^T$ podem ter valores de magnitudes variadas dependendo das dimens√µes e valores dos embeddings, podendo levar a valores muito grandes.
II. A fun√ß√£o softmax tem um comportamento que tende a extremos (0 ou 1) para valores de entrada muito grandes ou pequenos.
III. Se as pontua√ß√µes n√£o forem escalonadas, ap√≥s passar pelo softmax, a distribui√ß√£o das probabilidades tenderia a se concentrar em poucos tokens, levando a uma quase perda de informa√ß√£o, e um gradiente que tende a zero nas fun√ß√µes de perda.
IV. Dividir por $\sqrt{d_k}$ escala essas pontua√ß√µes para uma faixa mais adequada, permitindo que o softmax distribua os pesos de forma mais equitativa sobre todos os tokens.
V. Essa escala garante que o softmax n√£o se sature, e que a informa√ß√£o seja melhor distribu√≠da sobre os tokens, levando a um treinamento mais est√°vel e convergente, j√° que valores mais balanceados evitam gradientes que tendem a zero. Portanto, a divis√£o por $\sqrt{d_k}$ ajuda a manter o treinamento da rede mais eficiente. ‚ñ†

**Visualiza√ß√£o da Paraleliza√ß√£o**
A Figura 10.4 [^8] no texto original, ilustra como a matriz $QK^T$ √© constru√≠da, com a parte triangular superior zerada para garantir a causalidade. Essa figura demonstra como todos os produtos escalares entre consultas e chaves s√£o calculados simultaneamente, permitindo a paraleliza√ß√£o eficiente do processo.

### Conclus√£o
A capacidade de paralelizar o c√°lculo da autoaten√ß√£o por meio de opera√ß√µes matriciais √© fundamental para o desempenho de modelos de linguagem grandes. A transforma√ß√£o das opera√ß√µes em produtos de matrizes permite que o processo de autoaten√ß√£o seja executado de forma muito mais eficiente em hardware moderno, como GPUs, que s√£o projetados para computa√ß√£o paralela. Ao empacotar os embeddings dos tokens em uma matriz e usar a multiplica√ß√£o de matrizes, o modelo √© capaz de calcular as representa√ß√µes contextuais de todos os tokens simultaneamente, levando a uma acelera√ß√£o significativa do processo de treinamento e infer√™ncia. Este passo √© crucial para a constru√ß√£o de modelos capazes de lidar com contextos extensos e complexos.

### Refer√™ncias
[^5]:  Se√ß√£o 10.1.3 do texto original, que introduz o conceito de autoaten√ß√£o causal e sua formula√ß√£o matem√°tica.
[^6]: Se√ß√£o 10.1 do texto original, que introduz as matrizes WQ, WK e WV.
[^7]: Se√ß√£o 10.1.4 do texto original, onde √© introduzida a necessidade de normalizar os scores de autoaten√ß√£o.
[^8]: Se√ß√£o 10.1.5 do texto original, onde √© introduzida a necessidade da m√°scara para evitar vazamento de informa√ß√µes futuras.
<!-- END -->
