## Mascaramento em Modelagem de Linguagem Causal
### Introdu√ß√£o
Em continuidade √† discuss√£o sobre a paraleliza√ß√£o da autoaten√ß√£o via multiplica√ß√£o de matrizes [^1], e o processo de c√°lculo da autoaten√ß√£o [^5], esta se√ß√£o aborda um aspecto crucial para o uso de autoaten√ß√£o em modelos de linguagem causais: o mascaramento. Como vimos anteriormente, o c√°lculo da autoaten√ß√£o envolve comparar cada palavra (token) de uma sequ√™ncia com todas as outras, atrav√©s do produto escalar entre suas representa√ß√µes de consulta e chave [^1]. No entanto, em um contexto de modelagem de linguagem causal, onde o modelo deve prever a pr√≥xima palavra com base apenas no passado, permitir que o modelo tenha acesso ao futuro seria um erro. Para resolver esse problema, aplicamos uma t√©cnica de mascaramento √† matriz de pontua√ß√µes de aten√ß√£o, que garante que o modelo n√£o tenha acesso a informa√ß√µes futuras [^8]. Este cap√≠tulo detalha esse processo e seu impacto no treinamento do modelo.

### Conceitos Fundamentais
O mascaramento √© uma t√©cnica utilizada para evitar que um modelo, durante a modelagem de linguagem causal, acesse informa√ß√µes futuras. Em ess√™ncia, queremos que o modelo, ao prever a pr√≥xima palavra em uma sequ√™ncia, utilize apenas as palavras que vieram antes dela [^8]. Para implementar essa restri√ß√£o no mecanismo de autoaten√ß√£o, que por padr√£o computa rela√ß√µes entre todos os pares de palavras, aplicamos uma m√°scara na matriz de pontua√ß√µes de aten√ß√£o.

**A Matriz de Pontua√ß√µes de Aten√ß√£o**
Como vimos na se√ß√£o anterior, o c√°lculo da autoaten√ß√£o envolve a computa√ß√£o de uma matriz de pontua√ß√µes, obtida pelo produto matricial de Q e KT, ou seja, $QK^T$ [^1].  Essa matriz de dimens√£o *N x N* (onde N √© o n√∫mero de tokens na sequ√™ncia de entrada), cont√©m o resultado de cada produto escalar entre as representa√ß√µes de consulta e chave.  Em particular, o elemento (i,j) dessa matriz representa o quanto a palavra na posi√ß√£o *i* deve prestar aten√ß√£o √† palavra na posi√ß√£o *j*. Em modelos de linguagem causais, o token na posi√ß√£o i s√≥ deve prestar aten√ß√£o em tokens anteriores a ele.

**A Necessidade do Mascaramento**
Sem mascaramento, o mecanismo de autoaten√ß√£o teria acesso a todas as palavras da sequ√™ncia, incluindo aquelas que v√™m depois da palavra sendo analisada. Isso violaria o princ√≠pio da causalidade e resultaria em um modelo que essencialmente estaria "trapaceando" ao prever a pr√≥xima palavra [^8]. Em outras palavras, o modelo estaria usando conhecimento do futuro para prever o presente, o que seria inadequado, pois n√£o poderia ser generalizado para cen√°rios onde o futuro n√£o √© conhecido. O objetivo do mascaramento √© evitar o vazamento de informa√ß√µes futuras, garantindo que o modelo aprenda a gerar texto de forma sequencial, condicionado apenas no contexto passado.

**Implementa√ß√£o do Mascaramento**
A implementa√ß√£o do mascaramento envolve modificar a matriz de pontua√ß√µes, normalmente antes da aplica√ß√£o da fun√ß√£o softmax. Para isso, zeramos (na verdade, definimos como $-\infty$) todos os elementos na por√ß√£o triangular superior da matriz, incluindo a diagonal principal. Isso garante que, para cada palavra na posi√ß√£o *i*, o modelo n√£o tenha acesso a informa√ß√µes das palavras nas posi√ß√µes *j > i*.
A forma mais eficiente de realizar essa opera√ß√£o √© criando uma m√°scara *M*, tamb√©m uma matriz *N x N* onde:

$$ M_{i,j} =  \begin{cases}
0, & \text{se } j \leq i \\
-\infty, & \text{se } j > i
\end{cases} $$
Em seguida, adicionamos essa m√°scara √† matriz de pontua√ß√µes $QK^T$ ,  de forma que a matriz de pontua√ß√µes mascarada  $S_M$ seja dada por:
$$S_M = QK^T + M $$
A adi√ß√£o da matriz *M* a $QK^T$ garante que as pontua√ß√µes correspondentes a posi√ß√µes futuras tenham um valor de $-\infty$. Quando a fun√ß√£o softmax √© aplicada em seguida [^1], as posi√ß√µes com valor $-\infty$ ser√£o transformadas em 0, efetivamente zerando a influ√™ncia desses elementos na sa√≠da da autoaten√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha que temos uma matriz de pontua√ß√£o 3x3 e uma m√°scara correspondente:
> ```python
> import numpy as np
>
> scores = np.array([
>     [1, 2, 3],
>     [4, 5, 6],
>     [7, 8, 9]
> ])
>
> mask = np.array([
>     [0, -np.inf, -np.inf],
>     [0,    0,  -np.inf],
>     [0,    0,     0]
> ])
> masked_scores = scores + mask
> print("Pontua√ß√µes Originais:\n", scores)
> print("M√°scara:\n", mask)
> print("Pontua√ß√µes Mascaradas:\n", masked_scores)
> ```
> O c√≥digo acima gera a m√°scara triangular inferior e a aplica na matriz de scores. O resultado √©:
> ```
> Pontua√ß√µes Originais:
>  [[1 2 3]
>  [4 5 6]
>  [7 8 9]]
> M√°scara:
>  [[ 0. -inf -inf]
>  [ 0.  0. -inf]
>  [ 0.  0.  0.]]
> Pontua√ß√µes Mascaradas:
>  [[  1. -inf -inf]
>  [  4.   5. -inf]
>  [  7.   8.   9.]]
> ```
> Note que os valores na por√ß√£o triangular superior da matriz de pontua√ß√µes s√£o agora -inf.
>
> üí° **Exemplo Num√©rico:** Para visualizar melhor o efeito do mascaramento, considere uma sequ√™ncia de texto com 4 tokens, "A", "B", "C" e "D".  A matriz de pontua√ß√µes $QK^T$ 4x4 (antes do mascaramento) representaria o qu√£o relacionada cada palavra est√° com todas as outras, incluindo o futuro:
> ```mermaid
>  graph LR
>      A[A] --> A_A(A);
>      A --> B_A(B);
>      A --> C_A(C);
>      A --> D_A(D);
>      B[B] --> A_B(A);
>      B --> B_B(B);
>      B --> C_B(C);
>      B --> D_B(D);
>      C[C] --> A_C(A);
>      C --> B_C(B);
>      C --> C_C(C);
>      C --> D_C(D);
>      D[D] --> A_D(A);
>      D --> B_D(B);
>      D --> C_D(C);
>      D --> D_D(D);
>      style A fill:#f9f,stroke:#333,stroke-width:2px
>      style B fill:#ccf,stroke:#333,stroke-width:2px
>      style C fill:#aaf,stroke:#333,stroke-width:2px
>      style D fill:#cca,stroke:#333,stroke-width:2px
> ```
> Cada seta indica uma pontua√ß√£o de aten√ß√£o, onde a palavra "origem" compara sua representa√ß√£o com a representa√ß√£o da palavra "destino".  Ap√≥s a aplica√ß√£o da m√°scara, as pontua√ß√µes das palavras com tokens futuros seriam zeradas, de forma que a palavra "A" s√≥ se relaciona com ela mesma, "B" com "A" e "B", e assim por diante.
> ```mermaid
>  graph LR
>      A[A] --> A_A(A);
>      B[B] --> A_B(A);
>      B --> B_B(B);
>      C[C] --> A_C(A);
>      C --> B_C(B);
>      C --> C_C(C);
>      D[D] --> A_D(A);
>      D --> B_D(B);
>      D --> C_D(C);
>      D --> D_D(D);
>      style A fill:#f9f,stroke:#333,stroke-width:2px
>       style B fill:#ccf,stroke:#333,stroke-width:2px
>      style C fill:#aaf,stroke:#333,stroke-width:2px
>      style D fill:#cca,stroke:#333,stroke-width:2px
>      style D_A fill:#ddd,stroke:#ddd,stroke-width:1px
>      style D_B fill:#ddd,stroke:#ddd,stroke-width:1px
>      style C_A fill:#ddd,stroke:#ddd,stroke-width:1px
>      style C_B fill:#ddd,stroke:#ddd,stroke-width:1px
>      style B_A fill:#ddd,stroke:#ddd,stroke-width:1px
>       style A_A fill:#f9f,stroke:#333,stroke-width:1px
>       linkStyle 0,1,2,3,4,5,6,7,8,9,10,11,12,13 stroke:#ddd,stroke-width:1px
> ```
> As setas com linha pontilhada representam as rela√ß√µes que foram mascaradas, de forma que n√£o tem nenhuma influ√™ncia ap√≥s a aplica√ß√£o da fun√ß√£o softmax.

**Efeito da Fun√ß√£o Softmax**
Ap√≥s a aplica√ß√£o da m√°scara, as pontua√ß√µes mascaradas s√£o passadas pela fun√ß√£o softmax, o que transforma os valores $-\infty$ em 0. Isso garante que as palavras futuras n√£o tenham nenhuma influ√™ncia nas palavras presentes. Matematicamente, a matriz de pesos de aten√ß√£o *A*  √© calculada como:
$$A = \text{softmax}\left(\frac{S_M}{\sqrt{d_k}}\right)$$
Onde $d_k$ √© a dimens√£o dos vetores de consulta e chave, utilizada para escalar as pontua√ß√µes, como vimos anteriormente [^1].

> üí° **Exemplo Num√©rico:** Continuando o exemplo num√©rico acima, vamos aplicar a fun√ß√£o softmax nos scores mascarados e dividi-los por $\sqrt{d_k} = \sqrt{2} \approx 1.414$:
> ```python
> from scipy.special import softmax
>
> d_k = 2
> masked_scaled_scores = masked_scores / np.sqrt(d_k)
> attention_weights = softmax(masked_scaled_scores, axis=1) # Softmax aplicado em cada linha
> print("Pesos de Aten√ß√£o Mascarados:\n", attention_weights)
> ```
> O resultado √©:
> ```
> Pesos de Aten√ß√£o Mascarados:
> [[1.00000000e+00 0.00000000e+00 0.00000000e+00]
>  [2.68941421e-01 7.31058579e-01 0.00000000e+00]
>  [6.87195405e-02 1.87595835e-01 7.43684624e-01]]
> ```
> Note que os elementos correspondentes aos valores $-\infty$ na matriz `masked_scores` (que indicam posi√ß√µes futuras) t√™m agora valor 0 em `attention_weights`. Isso significa que, por exemplo, a primeira palavra s√≥ tem influ√™ncia sobre ela mesma e n√£o sobre as outras. A segunda palavra tem influ√™ncia sobre ela mesma e sobre a primeira palavra, e assim por diante.
>
> üí° **Exemplo Num√©rico:** Suponha que temos uma sequ√™ncia de entrada de 5 tokens, e ap√≥s o c√°lculo da matriz de pontua√ß√£o e aplica√ß√£o da m√°scara, obtemos a seguinte matriz $S_M$, com $d_k = 4$:
>
> ```python
> import numpy as np
> from scipy.special import softmax
>
> scores_masked = np.array([
>     [2.0, -np.inf, -np.inf, -np.inf, -np.inf],
>     [1.5,  3.0, -np.inf, -np.inf, -np.inf],
>     [0.5,  2.0, 4.0, -np.inf, -np.inf],
>     [-0.5, 1.0, 2.5, 3.5, -np.inf],
>     [-1.0, 0.0, 1.5, 2.0, 3.0]
> ])
> d_k = 4
>
> scaled_scores_masked = scores_masked / np.sqrt(d_k)
> attention_probs = softmax(scaled_scores_masked, axis=1)
>
> print("Matriz de Pontua√ß√µes Mascarada e Escalonada:\n", scaled_scores_masked)
> print("Pesos de Aten√ß√£o Ap√≥s Softmax:\n", attention_probs)
> ```
> O c√≥digo produz os seguintes resultados:
>
> ```
> Matriz de Pontua√ß√µes Mascarada e Escalonada:
> [[ 1.         -inf -inf -inf -inf]
>  [ 0.75       1.5        -inf -inf -inf]
>  [ 0.25       1.         2.         -inf -inf]
>  [-0.25       0.5        1.25       1.75       -inf]
>  [-0.5        0.         0.75       1.         1.5       ]]
> Pesos de Aten√ß√£o Ap√≥s Softmax:
> [[1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]
>  [2.18076278e-01 7.81923722e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]
>  [ 5.97992094e-02 2.20074319e-01 7.20126472e-01 0.00000000e+00 0.00000000e+00]
>  [ 2.10903338e-02 8.10260843e-02 2.62218600e-01 6.35664982e-01 0.00000000e+00]
>  [ 1.44650674e-02 2.59507318e-02 1.17106932e-01 2.11783943e-01 6.30693328e-01]]
> ```
>
> Observamos que os valores nas posi√ß√µes futuras foram zerados, e a soma dos pesos de aten√ß√£o em cada linha √© igual a 1. Por exemplo, na terceira linha, o token 3 s√≥ considera tokens 1, 2 e 3 no passado, e a soma de seus pesos de aten√ß√£o √© 1 (0.0597 + 0.2200 + 0.7201 ‚âà 1). Isso demonstra como o mascaramento garante que o modelo de linguagem causal aprenda a gerar texto de forma sequencial.

**Observa√ß√£o 1:** *A aplica√ß√£o da m√°scara e da fun√ß√£o softmax garantem que, ao calcular as representa√ß√µes contextuais de cada token, a influ√™ncia dos tokens futuros seja completamente eliminada. Isso √© crucial para que o modelo aprenda a gerar texto de forma sequencial, sem acesso a informa√ß√µes futuras.*

**Teorema 1:** *A m√°scara aplicada √† matriz de pontua√ß√£o de aten√ß√£o, em conjunto com a fun√ß√£o softmax, garante que o modelo de linguagem causal n√£o use informa√ß√µes futuras para prever o presente. Isso √© essencial para o funcionamento correto de um modelo autorregressivo, que deve gerar texto sequencialmente, dependendo apenas do contexto passado. A matriz de aten√ß√£o resultante, obtida ap√≥s o mascaramento, representa um mecanismo de aten√ß√£o causal, no qual o presente s√≥ depende do passado*

**Prova do Teorema 1:**
I. A matriz de pontua√ß√£o de aten√ß√£o $QK^T$ calcula as pontua√ß√µes entre todos os pares de tokens, incluindo os futuros.
II. A matriz de m√°scara $M$ adicionada a $QK^T$ zera (atribui o valor de $-\infty$) todos os elementos onde o √≠ndice da coluna *j* √© maior que o √≠ndice da linha *i*, o que significa que a pontua√ß√£o √© de um token futuro.
III. Ao passar a matriz de pontua√ß√£o mascarada pelo softmax, os valores de $-\infty$ s√£o transformados em 0, eliminando qualquer influ√™ncia dos tokens futuros.
IV. Isso significa que o peso de aten√ß√£o associado a tokens futuros √© sempre 0, e portanto o modelo n√£o considera informa√ß√µes futuras ao gerar a representa√ß√£o contextual de um token.
V. Portanto, a m√°scara garante que a autoaten√ß√£o seja causal, ou seja, cada token s√≥ √© influenciado por tokens anteriores na sequ√™ncia. ‚ñ†

**Corol√°rio 1.1:** *A remo√ß√£o de informa√ß√£o atrav√©s do mascaramento pode levar a uma performance inferior, j√° que o modelo n√£o pode usar todos os dados dispon√≠veis. No entanto, a garantia de causalidade √© mais importante para modelagem de linguagem, j√° que permite que o modelo seja usado para gera√ß√£o de textos.*

**Prova do Corol√°rio 1.1:**
I. A autoaten√ß√£o, por padr√£o, pode extrair informa√ß√µes de todos os tokens.
II. O mascaramento impede o modelo de usar as palavras futuras para prever o presente, diminuindo a informa√ß√£o dispon√≠vel para cada token.
III. Isso poderia levar a uma performance inferior comparada a um modelo que pode usar todas as informa√ß√µes, j√° que a representa√ß√£o de cada token pode ser menos rica.
IV. No entanto, para tarefas de modelagem de linguagem, a causalidade √© uma restri√ß√£o fundamental, pois o modelo precisa gerar texto sequencialmente sem conhecer o futuro.
V. Portanto, embora o mascaramento possa levar a uma performance inferior em rela√ß√£o a outros modelos, √© crucial para a gera√ß√£o autorregressiva de texto e √© a solu√ß√£o para que a modelagem causal funcione corretamente. ‚ñ†

**Lema 1:** *A m√°scara $M$ definida como:*

$$ M_{i,j} =  \begin{cases}
0, & \text{se } j \leq i \\
-\infty, & \text{se } j > i
\end{cases} $$

*pode ser gerada eficientemente atrav√©s de opera√ß√µes vetorizadas, evitando loops expl√≠citos sobre os √≠ndices i e j.*

**Prova do Lema 1:**

I. A matriz $M$ pode ser constru√≠da usando uma matriz base de uns e aplicando opera√ß√µes de compara√ß√£o e atribui√ß√£o condicionais.
II. Inicialmente, podemos criar uma matriz de dimens√£o *N x N* com todos os elementos iguais a 0, que chamaremos de $M_0$.
III. Em seguida, constru√≠mos um vetor *v* com os √≠ndices de coluna `j` para cada elemento da matriz, onde `j = 0, 1, ..., N-1`.
IV. Podemos construir um vetor *u* contendo o √≠ndice de linha `i` repetido N vezes, onde `i = 0, 1, ..., N-1`.
V. Podemos ent√£o construir uma matriz de √≠ndices de coluna, repetindo *v*  N vezes para cada linha. De forma similar, constru√≠mos uma matriz de √≠ndices de linha repetindo *u* N vezes para cada coluna
VI. Comparamos os √≠ndices de coluna e linha para cada elemento da matriz, atrav√©s de `j > i`, gerando uma matriz booleana.
VII. Finalmente, atribu√≠mos o valor $-\infty$ onde a compara√ß√£o retorna verdadeiro e deixamos os valores originais (0) nas outras posi√ß√µes, obtendo assim a matriz $M$.
VIII. Todas essas opera√ß√µes s√£o vetorizadas, ou seja, aplicadas sobre todos os elementos da matriz em paralelo, evitando loops expl√≠citos. ‚ñ†

**Observa√ß√£o 2:** *O Lema 1 mostra que o mascaramento pode ser implementado de forma eficiente, utilizando opera√ß√µes matriciais, o que √© muito importante para a performance de modelos de linguagem com grandes sequ√™ncias de entrada.*

**Teorema 1.1:** *A matriz de aten√ß√£o causal obtida ap√≥s o mascaramento (A) √© uma matriz estoc√°stica por linha, com a propriedade de que a soma dos pesos de aten√ß√£o correspondentes a tokens futuros √© sempre 0. Essa propriedade √© uma consequ√™ncia da aplica√ß√£o da fun√ß√£o softmax ap√≥s o mascaramento.*

**Prova do Teorema 1.1:**
I. Ap√≥s o mascaramento, a matriz de pontua√ß√£o $S_M$ cont√©m $-\infty$ em todas as posi√ß√µes correspondentes a tokens futuros ($j>i$).
II. A fun√ß√£o softmax, aplicada sobre as linhas de $S_M$, mapeia os valores para o intervalo (0,1), garantindo que a soma dos elementos em cada linha seja 1.
III. Como os valores $-\infty$ s√£o transformados em 0 pelo softmax, a soma dos pesos de aten√ß√£o correspondentes a posi√ß√µes futuras √© sempre 0.
IV. A soma dos pesos de aten√ß√£o para posi√ß√µes anteriores ou iguais ao token de interesse √© sempre igual a 1, o que garante que a matriz de aten√ß√£o seja uma matriz estoc√°stica por linha.
V. Portanto, a matriz de aten√ß√£o *A* ap√≥s mascaramento √© uma matriz estoc√°stica por linha, que distribui o peso de aten√ß√£o apenas sobre os tokens do passado, garantindo que o modelo aprenda a gerar texto de forma causal. ‚ñ†

### Conclus√£o
O mascaramento √© uma etapa essencial na aplica√ß√£o de autoaten√ß√£o para modelos de linguagem causais. Ele garante que o modelo aprenda a prever a pr√≥xima palavra com base apenas no contexto passado, sem acesso a informa√ß√µes futuras. A aplica√ß√£o da m√°scara e da fun√ß√£o softmax √© fundamental para obter um comportamento causal da autoaten√ß√£o. A capacidade de gerar textos de forma autorregressiva √© um dos pilares da modelagem de linguagem com transformadores, e o mascaramento √© essencial para que isso funcione corretamente.

### Refer√™ncias
[^1]: Se√ß√£o anterior do texto, que trata sobre a paraleliza√ß√£o da autoaten√ß√£o.
[^5]: Se√ß√£o 10.1.3 do texto original, que introduz o conceito de autoaten√ß√£o causal.
[^8]: Se√ß√£o 10.1.5 do texto original, onde √© introduzida a necessidade da m√°scara para evitar vazamento de informa√ß√µes futuras.
<!-- END -->
