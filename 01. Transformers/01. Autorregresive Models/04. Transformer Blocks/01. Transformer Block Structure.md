## Transformer Blocks: The Core of Transformer Networks
### Introdu√ß√£o
Este cap√≠tulo mergulha nos detalhes dos **transformer blocks**, os blocos de constru√ß√£o fundamentais das redes Transformer, explorando sua estrutura e o fluxo de informa√ß√µes. Como vimos anteriormente, a arquitetura Transformer revolucionou o processamento de linguagem natural, permitindo a constru√ß√£o de modelos poderosos como os Large Language Models (LLMs). Os transformer blocks, com sua combina√ß√£o de mecanismos de autoaten√ß√£o, camadas feedforward, conex√µes residuais e normaliza√ß√£o, s√£o essenciais para alcan√ßar o desempenho avan√ßado desses modelos [^2].

### Conceitos Fundamentais
Como introduzido no in√≠cio do cap√≠tulo [^2], os **transformers** s√£o arquiteturas neurais que processam informa√ß√µes sequenciais, superando as limita√ß√µes das RNNs. A arquitetura baseia-se em **transformer blocks**, que consistem em v√°rias camadas interconectadas. Cada bloco recebe uma sequ√™ncia de vetores de entrada $(x_1, ..., x_n)$ e produz uma sequ√™ncia de vetores de sa√≠da $(z_1, ..., z_n)$ de mesmo comprimento [^3]. Esses blocos s√£o empilhados para formar redes profundas e complexas. O conceito chave dentro de um transformer block √© o mecanismo de **autoaten√ß√£o**, que permite ao modelo extrair informa√ß√µes de contextos grandes [^3].

#### Autoaten√ß√£o
A **autoaten√ß√£o**, ou *self-attention*, √© o cora√ß√£o dos transformers, um mecanismo que possibilita que o modelo aprenda as rela√ß√µes entre palavras (ou tokens) em uma sequ√™ncia [^2]. Ao contr√°rio das RNNs, que processam as sequ√™ncias de forma sequencial, a autoaten√ß√£o permite que o modelo considere todas as posi√ß√µes na sequ√™ncia simultaneamente [^3]. Isso √© crucial para capturar depend√™ncias de longa dist√¢ncia no texto. 
A autoaten√ß√£o opera atrav√©s de tr√™s vetores derivados da entrada: **query** (consulta), **key** (chave) e **value** (valor) [^6]. A similaridade entre os vetores query e key determina os pesos de aten√ß√£o, que s√£o ent√£o utilizados para ponderar os vetores value. Esse processo produz uma representa√ß√£o contextualizada de cada palavra, permitindo que o modelo entenda seu significado em rela√ß√£o a outras palavras da sequ√™ncia [^3].

**Lema 1** (Representa√ß√£o da Autoaten√ß√£o): Matematicamente, a opera√ß√£o de autoaten√ß√£o para um token $x_i$ pode ser expressa como:
$$Attention(Q_i, K, V) = softmax\left(\frac{Q_iK^T}{\sqrt{d_k}}\right)V$$
onde $Q_i$ representa o vetor query para o token $x_i$, $K$ √© a matriz de vetores key para todos os tokens, $V$ √© a matriz de vetores value para todos os tokens, e $d_k$ √© a dimens√£o dos vetores key. Essa formula√ß√£o mostra como o modelo calcula pesos de aten√ß√£o e os utiliza para gerar uma representa√ß√£o ponderada dos vetores value.

> üí° **Exemplo Num√©rico:** Suponha que temos uma sequ√™ncia de 3 tokens, cada um representado por um vetor de dimens√£o $d_k = 4$. Para o token $x_1$, temos $Q_1 = [1, 0.2, 0.5, 0.1]$. As matrizes K e V (concatenadas) s√£o as seguintes:
>
>  $K = \begin{bmatrix} 0.2 & 0.1 & 0.3 & 0.2 \\ 0.1 & 0.8 & 0.1 & 0.5 \\ 0.4 & 0.2 & 0.7 & 0.1 \end{bmatrix}$, $V = \begin{bmatrix} 0.5 & 0.6 & 0.2 & 0.4 \\ 0.2 & 0.3 & 0.9 & 0.1 \\ 0.8 & 0.1 & 0.7 & 0.3 \end{bmatrix}$
>
>  **Step 1:** Calcular o produto $Q_1K^T$:
>
> $Q_1K^T = [1, 0.2, 0.5, 0.1] \begin{bmatrix} 0.2 & 0.1 & 0.4 \\ 0.1 & 0.8 & 0.2 \\ 0.3 & 0.1 & 0.7 \\ 0.2 & 0.5 & 0.1 \end{bmatrix} = [0.2 + 0.02 + 0.15 + 0.02, 0.1 + 0.16 + 0.05 + 0.05, 0.4+0.04+0.35+0.01] = [0.39, 0.36, 0.8]$
>
> **Step 2:** Dividir por $\sqrt{d_k} = \sqrt{4} = 2$:
>
> $\frac{Q_1K^T}{\sqrt{d_k}} = \frac{[0.39, 0.36, 0.8]}{2} = [0.195, 0.18, 0.4]$
>
> **Step 3:** Aplicar a fun√ß√£o softmax:
>
> $softmax([0.195, 0.18, 0.4]) = [\frac{e^{0.195}}{e^{0.195} + e^{0.18} + e^{0.4}}, \frac{e^{0.18}}{e^{0.195} + e^{0.18} + e^{0.4}}, \frac{e^{0.4}}{e^{0.195} + e^{0.18} + e^{0.4}}] \approx [0.28, 0.27, 0.45]$
>
> **Step 4:** Multiplicar os pesos de aten√ß√£o pelos vetores value:
>
> $Attention(Q_1, K, V) = [0.28, 0.27, 0.45]  \begin{bmatrix} 0.5 & 0.6 & 0.2 & 0.4 \\ 0.2 & 0.3 & 0.9 & 0.1 \\ 0.8 & 0.1 & 0.7 & 0.3 \end{bmatrix} = [0.28*0.5 + 0.27*0.2 + 0.45*0.8, 0.28*0.6 + 0.27*0.3 + 0.45*0.1, 0.28*0.2 + 0.27*0.9 + 0.45*0.7, 0.28*0.4 + 0.27*0.1 + 0.45*0.3] = [0.574, 0.294, 0.616, 0.302]$
>
> O resultado final √© um vetor que representa a informa√ß√£o contextualizada do token $x_1$ levando em conta o restante da sequ√™ncia.

#### Camada Feedforward
Ap√≥s a camada de autoaten√ß√£o, cada transformer block inclui uma **camada feedforward** [^9]. Essa camada √© uma rede neural de duas camadas totalmente conectadas, aplicada de forma independente a cada posi√ß√£o na sequ√™ncia. Embora os par√¢metros sejam os mesmos para todas as posi√ß√µes, eles s√£o diferentes para cada camada do modelo [^10]. Essa camada aumenta a capacidade de modelagem da rede, fornecendo uma transforma√ß√£o n√£o linear das representa√ß√µes contextualizadas geradas pela autoaten√ß√£o.

**Proposi√ß√£o 1** (Camada Feedforward): A camada feedforward pode ser descrita como:
$$FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2$$
onde $W_1$, $b_1$, $W_2$ e $b_2$ s√£o os pesos e vieses da primeira e segunda camadas lineares, respectivamente, e $ReLU$ √© a fun√ß√£o de ativa√ß√£o linear retificada. Essa estrutura permite que a camada aprenda transforma√ß√µes n√£o lineares de forma eficaz. Al√©m disso, para melhor performance, a fun√ß√£o $ReLU$ pode ser substitu√≠da por $GELU$ (Gaussian Error Linear Unit).

> üí° **Exemplo Num√©rico:** Considere a sa√≠da da camada de autoaten√ß√£o como um vetor $x = [0.574, 0.294, 0.616, 0.302]$. Vamos usar uma camada feedforward com $W_1$ de dimens√£o $4 \times 8$, $b_1$ de dimens√£o $8$, $W_2$ de dimens√£o $8 \times 4$ e $b_2$ de dimens√£o 4.
>
>  **Step 1:** Multiplica√ß√£o da entrada pelo primeiro peso e adi√ß√£o do bias:
>
>  $z_1 = xW_1 + b_1$, onde
>
>  $W_1 = \begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 \\ 0.8 & 0.7 & 0.6 & 0.5 & 0.4 & 0.3 & 0.2 & 0.1 \\ 0.2 & 0.4 & 0.6 & 0.8 & 0.1 & 0.3 & 0.5 & 0.7 \\ 0.7 & 0.5 & 0.3 & 0.1 & 0.8 & 0.6 & 0.4 & 0.2 \end{bmatrix}$ e $b_1 = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]$
>
>  Para simplificar, vamos calcular o primeiro elemento de $z_1$:
>
>  $z_{1,1} = 0.574*0.1 + 0.294*0.8 + 0.616*0.2 + 0.302*0.7 + 0.1 = 0.0574 + 0.2352 + 0.1232 + 0.2114 + 0.1 = 0.7272$
>
>  Seguindo para todos os elementos, $z_1 = [0.7272, 0.7608, 0.8208, 0.7072, 0.7272, 0.7936, 0.6984, 0.6816 ]$
>
> **Step 2:** Aplicar a fun√ß√£o de ativa√ß√£o ReLU:
>
>  $ReLU(z_1) = [max(0, 0.7272), max(0, 0.7608), max(0, 0.8208), max(0, 0.7072), max(0, 0.7272), max(0, 0.7936), max(0, 0.6984), max(0, 0.6816)] = [0.7272, 0.7608, 0.8208, 0.7072, 0.7272, 0.7936, 0.6984, 0.6816]$
>
>  **Step 3:** Multiplicar o resultado pelo segundo peso e adicionar o segundo bias:
>
>  $z_2 = ReLU(z_1)W_2 + b_2$, onde
>  $W_2 = \begin{bmatrix} 0.1 & 0.8 & 0.2 & 0.7 \\ 0.2 & 0.7 & 0.4 & 0.5 \\ 0.3 & 0.6 & 0.6 & 0.3 \\ 0.4 & 0.5 & 0.8 & 0.1 \\ 0.5 & 0.4 & 0.1 & 0.8 \\ 0.6 & 0.3 & 0.3 & 0.6 \\ 0.7 & 0.2 & 0.5 & 0.4 \\ 0.8 & 0.1 & 0.7 & 0.2 \end{bmatrix}$ e $b_2 = [0.2, 0.3, 0.4, 0.1]$
>
>  Para simplificar, vamos calcular o primeiro elemento de $z_2$:
>
> $z_{2,1} = 0.7272*0.1 + 0.7608*0.2 + 0.8208*0.3 + 0.7072*0.4 + 0.7272*0.5 + 0.7936*0.6 + 0.6984*0.7 + 0.6816*0.8 + 0.2 = 0.07272 + 0.15216 + 0.24624 + 0.28288 + 0.3636 + 0.47616 + 0.48888 + 0.54528 + 0.2 = 2.82792$
>
> Continuando com os outros elementos, $z_2 = [2.82792, 2.66176, 2.80384, 2.59024]$.
>
>  O resultado final da camada feedforward √© um vetor $z_2 = [2.82792, 2.66176, 2.80384, 2.59024]$, que passa por uma conex√£o residual e uma camada de normaliza√ß√£o.

#### Conex√µes Residuais
As **conex√µes residuais** s√£o um componente essencial dos transformer blocks, permitindo o fluxo de informa√ß√µes diretamente de camadas inferiores para camadas superiores, evitando problemas de desvanecimento ou explos√£o de gradientes [^11]. Cada conex√£o residual adiciona a entrada da camada anterior √† sa√≠da da camada atual. Elas s√£o utilizadas tanto ap√≥s a camada de autoaten√ß√£o quanto ap√≥s a camada feedforward, garantindo que a informa√ß√£o possa fluir por toda a rede sem sofrer perdas significativas [^11].

**Observa√ß√£o 1** (Impacto das Conex√µes Residuais): As conex√µes residuais s√£o uma t√©cnica crucial para o treinamento de redes profundas, especialmente em transformers. Elas permitem que o gradiente flua mais livremente atrav√©s da rede, mitigando o problema de vanishing gradient. Essa caracter√≠stica √© essencial para o aprendizado de depend√™ncias complexas.

> üí° **Exemplo Num√©rico:**
>
>  Suponha que a sa√≠da da camada de autoaten√ß√£o para o token $x_i$ seja $t^1 = [0.6, 0.3, 0.7, 0.2]$ e a entrada original seja $x_i = [0.1, 0.2, 0.3, 0.4]$. A conex√£o residual adiciona diretamente $x_i$ a $t^1$:
>  $t^2 = t^1 + x_i = [0.6, 0.3, 0.7, 0.2] + [0.1, 0.2, 0.3, 0.4] = [0.7, 0.5, 1.0, 0.6]$.
>
>  Este resultado $t^2$ √© ent√£o passado para a camada de normaliza√ß√£o. Similarmente, ap√≥s a camada feedforward, a sa√≠da $t^4$ √© adicionada √† entrada da camada feedforward $t^3$. Por exemplo, se a sa√≠da da camada feedforward for $t^4 = [1.1, 0.9, 1.2, 0.8]$ e a entrada for $t^3 = [0.7, 0.5, 1.0, 0.6]$ (resultado da normaliza√ß√£o da sa√≠da $t^2$ do exemplo anterior), a conex√£o residual resultar√° em $t^5 = t^4 + t^3 = [1.1, 0.9, 1.2, 0.8] + [0.7, 0.5, 1.0, 0.6] = [1.8, 1.4, 2.2, 1.4]$. Este valor $t^5$ ser√°, ent√£o, a entrada para a camada de normaliza√ß√£o final no transformer block.

#### Camadas de Normaliza√ß√£o
As **camadas de normaliza√ß√£o**, geralmente **layer normalization**, s√£o usadas para estabilizar o treinamento, normalizando as sa√≠das de cada camada [^11]. A layer normalization opera normalizando o vetor da camada para cada token separadamente, calculando uma m√©dia e desvio padr√£o por cada vetor, e em seguida normalizando os componentes de cada vetor, permitindo a cada camada ter a sua pr√≥pria distribui√ß√£o [^11]. Isso facilita o treinamento e melhora a converg√™ncia do modelo, acelerando o aprendizado [^11].

**Teorema 1** (Estabilidade da Normaliza√ß√£o de Camada): A layer normalization melhora significativamente a estabilidade do treinamento ao manter a escala das ativa√ß√µes dentro de uma faixa razo√°vel. Isto √© realizado normalizando as ativa√ß√µes para ter uma m√©dia de zero e um desvio padr√£o de um para cada amostra dentro de um lote (batch). Formalmente, um vetor $x$ passa por layer normalization $LN(x)$:
$$LN(x) = \gamma \frac{x-\mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$
onde $\mu$ e $\sigma$ s√£o a m√©dia e o desvio padr√£o de $x$, $\gamma$ e $\beta$ s√£o par√¢metros aprendidos, e $\epsilon$ √© um valor pequeno para evitar divis√£o por zero.

**Prova do Teorema 1:**
Provaremos que Layer Normalization estabiliza o treinamento, mantendo as ativa√ß√µes em uma faixa razo√°vel.

I. **Defini√ß√£o:** A layer normalization para um vetor de ativa√ß√µes $x$ √© definida como:
   $$LN(x) = \gamma \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$
   onde $\mu$ √© a m√©dia de $x$, $\sigma$ √© o desvio padr√£o de $x$, $\gamma$ e $\beta$ s√£o par√¢metros aprendidos, e $\epsilon$ √© um pequeno valor para estabilidade num√©rica.

II. **Normaliza√ß√£o:** O termo $\frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}$ normaliza os elementos de $x$ para ter uma m√©dia pr√≥xima de 0 e desvio padr√£o pr√≥ximo de 1. Isso √© porque:
   * $\mu$ subtrai a m√©dia dos valores de $x$, centralizando-os em torno de zero.
   * A divis√£o por $\sqrt{\sigma^2 + \epsilon}$ escala os valores para terem uma vari√¢ncia unit√°ria (aproximadamente).

III. **Par√¢metros Ajust√°veis:** $\gamma$ e $\beta$ s√£o par√¢metros aprend√≠veis que permitem que a camada ajuste a escala e o deslocamento dos dados normalizados. A adi√ß√£o de $\beta$ permite que o modelo ajuste a m√©dia das ativa√ß√µes, enquanto o $\gamma$ ajusta sua escala.

IV. **Estabilidade:** Ao normalizar as ativa√ß√µes para uma faixa razo√°vel, layer normalization evita que os valores cres√ßam demais ou diminuam demais durante o treinamento. Isto ajuda a manter o gradiente em uma faixa est√°vel.

V. **Conclus√£o:** Portanto, a layer normalization estabiliza o treinamento mantendo a escala das ativa√ß√µes dentro de uma faixa razo√°vel e garantindo que o gradiente flua de forma eficaz durante o treinamento da rede. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Suponha que o vetor de entrada para a camada de normaliza√ß√£o seja $t^2 = [0.7, 0.5, 1.0, 0.6]$.
>
> **Step 1:** Calcular a m√©dia $\mu$:
> $\mu = \frac{0.7 + 0.5 + 1.0 + 0.6}{4} = \frac{2.8}{4} = 0.7$
>
> **Step 2:** Calcular o desvio padr√£o $\sigma$:
> $\sigma^2 = \frac{(0.7-0.7)^2 + (0.5-0.7)^2 + (1.0-0.7)^2 + (0.6-0.7)^2}{4} = \frac{0 + 0.04 + 0.09 + 0.01}{4} = 0.035$
> $\sigma = \sqrt{0.035} \approx 0.187$
>
> **Step 3:** Normalizar cada elemento:
> Usando $\epsilon = 10^{-5}$
> $t^2_{norm} = \frac{t^2-\mu}{\sqrt{\sigma^2 + \epsilon}} = [\frac{0.7-0.7}{\sqrt{0.035 + 10^{-5}}}, \frac{0.5-0.7}{\sqrt{0.035 + 10^{-5}}}, \frac{1.0-0.7}{\sqrt{0.035 + 10^{-5}}}, \frac{0.6-0.7}{\sqrt{0.035 + 10^{-5}}}] = [0, -1.069, 1.604, -0.535]$
>
> **Step 4:** Aplicar os par√¢metros aprend√≠veis:
> Suponha que $\gamma = [0.5, 0.5, 0.5, 0.5]$ e $\beta = [0.1, 0.2, 0.3, 0.4]$.
> $LN(t^2) =  \gamma * t^2_{norm} + \beta = [0*0.5 + 0.1, -1.069*0.5 + 0.2, 1.604*0.5 + 0.3, -0.535*0.5 + 0.4] = [0.1, -0.334, 1.102, 0.132]$
>
> O vetor normalizado $LN(t^2) = [0.1, -0.334, 1.102, 0.132]$ √© a sa√≠da da camada de normaliza√ß√£o, que ser√° usada como entrada da camada seguinte.

#### Fluxo de Informa√ß√£o no Transformer Block
O fluxo de informa√ß√£o dentro de um transformer block, como ilustrado na Fig 10.6 [^10], segue um padr√£o bem definido:
1.  A entrada X passa por uma camada de **autoaten√ß√£o**.
2.  O resultado da autoaten√ß√£o √© adicionado √† entrada original atrav√©s de uma **conex√£o residual**.
3.  Essa soma passa por uma **normaliza√ß√£o de camada** (layer norm).
4.  O resultado √© ent√£o processado por uma **camada feedforward**.
5.  O resultado da camada feedforward √© adicionado √† sa√≠da da normaliza√ß√£o de camada anterior atrav√©s de uma **conex√£o residual**.
6.  Essa soma passa por outra **normaliza√ß√£o de camada**.
Este processo mant√©m a dimensionalidade dos vetores, permitindo que os blocos sejam empilhados de forma consistente [^12].

#### Representa√ß√£o do Fluxo de Informa√ß√£o
O fluxo de informa√ß√£o atrav√©s de um transformer block tamb√©m pode ser visto como um *residual stream* [^13], onde cada token √© processado de forma individual e seu fluxo √© influenciado pela camada de autoaten√ß√£o que considera informa√ß√µes do contexto [^13]. Matematicamente, esse processo √© definido pelas seguintes equa√ß√µes:
$$t^1 = MultiHeadAttention(x_i, [x_1, \ldots, x_N])$$
$$t^2 = t^1 + x_i$$
$$t^3 = LayerNorm(t^2)$$
$$t^4 = FFN(t^3)$$
$$t^5 = t^4 + t^3$$
$$h_i = LayerNorm(t^5)$$

**Teorema 1.1** (Conex√£o Residual e Layer Normalization): A aplica√ß√£o da Layer Normalization ap√≥s cada conex√£o residual garante a estabilidade das ativa√ß√µes e auxilia no aprendizado da rede. A camada de normaliza√ß√£o de camada ajusta o histograma das ativa√ß√µes, estabilizando o gradiente e evitando que as ativa√ß√µes se tornem muito grandes ou muito pequenas, o que poderia prejudicar o treinamento do modelo.

**Prova do Teorema 1.1:**
Provaremos que a Layer Normalization ap√≥s cada conex√£o residual garante a estabilidade das ativa√ß√µes e auxilia o aprendizado da rede.

I. **Conex√£o Residual:** A conex√£o residual adiciona a sa√≠da da camada anterior ($t^1$ ou $t^4$) √† entrada da camada atual ($x_i$ ou $t^3$), ou seja, $t^2 = t^1 + x_i$ e $t^5 = t^4 + t^3$. Essa opera√ß√£o garante que as informa√ß√µes originais fluam atrav√©s da rede sem serem drasticamente alteradas por transforma√ß√µes intermedi√°rias.

II. **Problemas sem Normaliza√ß√£o:** Sem layer normalization ap√≥s a conex√£o residual, as ativa√ß√µes $t^2$ e $t^5$ podem ter uma distribui√ß√£o que varia significativamente de camada para camada. Isso pode levar a instabilidade no treinamento devido a grandes ou pequenos gradientes.

III. **Efeito da Layer Normalization:** A layer normalization, aplicada como $t^3 = LayerNorm(t^2)$ e $h_i = LayerNorm(t^5)$, normaliza as ativa√ß√µes antes de serem usadas em camadas subsequentes. Isto garante que as ativa√ß√µes tenham uma m√©dia pr√≥xima de zero e um desvio padr√£o pr√≥ximo de um.

IV. **Estabilidade:** Ao normalizar as ativa√ß√µes, a layer normalization impede que os valores cres√ßam demais ou diminuam demais, mantendo os gradientes em uma faixa razo√°vel. Isso √© crucial para um treinamento eficiente da rede.

V. **Aux√≠lio no Aprendizado:** Ao estabilizar as ativa√ß√µes e os gradientes, a layer normalization facilita o treinamento da rede. Isso permite que a rede aprenda rela√ß√µes complexas entre os dados sem sofrer problemas de vanishing/exploding gradients.

VI. **Conclus√£o:** Portanto, a aplica√ß√£o da Layer Normalization ap√≥s cada conex√£o residual garante a estabilidade das ativa√ß√µes e auxilia no aprendizado da rede, mantendo as ativa√ß√µes numa faixa adequada. ‚ñ†

### Conclus√£o
Os **transformer blocks** s√£o os componentes essenciais da arquitetura Transformer, combinando de forma eficaz os mecanismos de autoaten√ß√£o, camadas feedforward, conex√µes residuais e normaliza√ß√£o de camada [^9]. Essa combina√ß√£o permite a constru√ß√£o de modelos capazes de capturar depend√™ncias de longo alcance no texto e produzir representa√ß√µes contextuais ricas [^2]. A capacidade de empilhar esses blocos permite a cria√ß√£o de redes profundas e poderosas, que t√™m demonstrado um desempenho not√°vel em diversas tarefas de processamento de linguagem natural, como modelagem de linguagem e tradu√ß√£o autom√°tica, conforme discutido ao longo deste cap√≠tulo [^2].

### Refer√™ncias
[^2]: Cap√≠tulo 10 - Transformers and Large Language Models.
[^3]: 10.1 The Transformer: A Self-Attention Network
[^6]: 10.1.3 Self-attention more formally
[^9]: 10.3 Transformer Blocks
[^10]: Figure 10.6 A transformer block showing all the layers
[^11]: 10.3 Transformer Blocks
[^12]: 10.4 The Residual Stream view of the Transformer Block
[^13]: 10.4 The Residual Stream view of the Transformer Block
<!-- END -->
