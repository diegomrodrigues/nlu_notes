## Autoaten√ß√£o e Feedforward nos Transformer Blocks
### Introdu√ß√£o
Expandindo sobre a arquitetura dos **transformer blocks**, como discutido anteriormente, este cap√≠tulo aprofundar√° o funcionamento das camadas de **autoaten√ß√£o** (self-attention) e **feedforward**, componentes cruciais que capacitam os modelos Transformer a processar e entender a linguagem natural de maneira t√£o eficaz [^2]. Anteriormente, estabelecemos que os transformer blocks s√£o as unidades b√°sicas de constru√ß√£o das redes Transformer, compostas por camadas de autoaten√ß√£o, feedforward, conex√µes residuais e normaliza√ß√£o [^2]. Agora, exploraremos em detalhes como as camadas de autoaten√ß√£o permitem que o modelo foque em partes relevantes da sequ√™ncia de entrada, e como as camadas feedforward aplicam transforma√ß√µes n√£o lineares para cada token, adicionando capacidade de modelagem ao modelo [^2].

### Autoaten√ß√£o: Foco no Contexto Relevante
A camada de **autoaten√ß√£o** √© o mecanismo central que possibilita aos Transformers capturar rela√ß√µes entre palavras em uma sequ√™ncia, superando as limita√ß√µes das arquiteturas de redes neurais recorrentes (RNNs) [^3]. Ao inv√©s de processar a sequ√™ncia de forma sequencial, como as RNNs, a autoaten√ß√£o permite que o modelo considere todas as posi√ß√µes da sequ√™ncia simultaneamente, extraindo informa√ß√µes de contextos mais amplos e relacionamentos de longa dist√¢ncia entre as palavras [^3].

#### Mecanismo da Autoaten√ß√£o
O funcionamento da autoaten√ß√£o envolve a transforma√ß√£o da entrada em tr√™s vetores: **query** ($q_i$), **key** ($k_i$) e **value** ($v_i$) [^6]. Estes vetores s√£o obtidos atrav√©s de proje√ß√µes lineares da representa√ß√£o de entrada $x_i$, utilizando matrizes de pesos $W^Q$, $W^K$ e $W^V$, respectivamente [^6]. Cada token $x_i$ √© transformado nos vetores $q_i$, $k_i$ e $v_i$ usando estas proje√ß√µes, como mostrado nas equa√ß√µes abaixo:
$$q_i = x_iW^Q$$
$$k_i = x_iW^K$$
$$v_i = x_iW^V$$
onde $W^Q \in \mathbb{R}^{d \times d_k}$, $W^K \in \mathbb{R}^{d \times d_k}$ e $W^V \in \mathbb{R}^{d \times d_v}$ s√£o as matrizes de proje√ß√£o, $d$ √© a dimens√£o da representa√ß√£o de entrada, $d_k$ √© a dimens√£o dos vetores *query* e *key*, e $d_v$ √© a dimens√£o dos vetores *value* [^6].

> üí° **Exemplo Num√©rico:**
>
> Considere uma representa√ß√£o de entrada para um token $x_i$ com dimens√£o $d=4$, ou seja, $x_i = [1, 2, 3, 4]$. Vamos usar matrizes de pesos aleat√≥rias para exemplificar as proje√ß√µes, com $d_k=3$ e $d_v=2$.
> ```python
> import numpy as np
>
> # Representa√ß√£o de entrada
> x_i = np.array([1, 2, 3, 4])
> d = 4
> dk = 3
> dv = 2
> # Matrizes de pesos aleat√≥rias
> np.random.seed(42) # Para reproducibilidade
> W_Q = np.random.rand(d, dk)
> W_K = np.random.rand(d, dk)
> W_V = np.random.rand(d, dv)
>
> # C√°lculo dos vetores query, key e value
> q_i = np.dot(x_i, W_Q)
> k_i = np.dot(x_i, W_K)
> v_i = np.dot(x_i, W_V)
>
> print("x_i:", x_i)
> print("W_Q:\n", W_Q)
> print("q_i:", q_i)
> print("W_K:\n", W_K)
> print("k_i:", k_i)
> print("W_V:\n", W_V)
> print("v_i:", v_i)
> ```
> O c√≥digo acima define um vetor de entrada $x_i$ e matrizes de proje√ß√£o aleat√≥rias. Ao multiplicar $x_i$ pelas matrizes, obtemos os vetores $q_i$, $k_i$ e $v_i$. Note que as dimens√µes s√£o $d_k$ para $q_i$ e $k_i$ e $d_v$ para $v_i$. Os valores num√©ricos aqui demonstram as transforma√ß√µes lineares que ocorrem.

A similaridade entre os vetores *query* e *key* √© medida atrav√©s de um produto escalar, que √© ent√£o escalado pela raiz quadrada da dimens√£o do vetor key, $\sqrt{d_k}$ para estabilizar o treinamento [^7]. Esta pontua√ß√£o √© ent√£o usada para calcular os pesos de aten√ß√£o atrav√©s de uma fun√ß√£o softmax, que converte as pontua√ß√µes em probabilidades [^5]:
$$score(x_i,x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}$$
$$a_{ij} = \frac{exp(score(x_i, x_j))}{\sum_{l=1}^{n} exp(score(x_i, x_l))}$$
onde $a_{ij}$ √© o peso de aten√ß√£o para o token $x_j$ em rela√ß√£o ao token $x_i$, e $n$ √© o comprimento da sequ√™ncia. Os pesos de aten√ß√£o s√£o ent√£o usados para ponderar os vetores *value*, produzindo uma representa√ß√£o contextualizada do token $x_i$:
$$a_i = \sum_{j=1}^n a_{ij} v_j$$
Este processo permite que o modelo considere o contexto global da sequ√™ncia ao calcular a representa√ß√£o de cada token.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar uma sequ√™ncia de 3 tokens com os vetores *query* e *key* j√° calculados. Para simplificar, vamos assumir $d_k=3$.
> ```python
> import numpy as np
>
> # Vetores query, key e value (3 tokens, dk=3, dv=2)
> q_i = np.array([0.2, 0.5, 0.8])
> k_1 = np.array([0.1, 0.3, 0.5])
> k_2 = np.array([0.6, 0.2, 0.9])
> k_3 = np.array([0.4, 0.7, 0.1])
> v_1 = np.array([0.1, 0.2])
> v_2 = np.array([0.3, 0.4])
> v_3 = np.array([0.5, 0.6])
>
> K = np.array([k_1, k_2, k_3])
> V = np.array([v_1, v_2, v_3])
>
> dk = 3
>
> # C√°lculo dos scores
> scores = np.dot(q_i, K.T) / np.sqrt(dk)
>
> # C√°lculo dos pesos de aten√ß√£o
> exp_scores = np.exp(scores)
> attention_weights = exp_scores / np.sum(exp_scores)
>
> # C√°lculo da representa√ß√£o contextualizada
> a_i = np.dot(attention_weights, V)
>
> print("Query (q_i):", q_i)
> print("Keys (k_1, k_2, k_3):\n", K)
> print("Values (v_1, v_2, v_3):\n", V)
> print("Scores:", scores)
> print("Attention Weights:", attention_weights)
> print("Contextualized Representation (a_i):", a_i)
> ```
> Neste exemplo, calculamos os scores entre o vetor *query* e todos os vetores *key*, aplicamos o softmax para obter os pesos de aten√ß√£o e ponderamos os vetores *value* usando os pesos para obter a representa√ß√£o contextualizada $a_i$. Os pesos de aten√ß√£o representam a import√¢ncia relativa de cada token da sequ√™ncia para o token $x_i$.

**Lema 2** (Proje√ß√µes Lineares da Autoaten√ß√£o): As proje√ß√µes lineares $W^Q$, $W^K$ e $W^V$ aprendem transforma√ß√µes que mapeiam as representa√ß√µes de entrada para diferentes espa√ßos, permitindo que o modelo capture diferentes aspectos da informa√ß√£o. O uso dessas proje√ß√µes lineares √© fundamental para a flexibilidade e capacidade de modelagem da autoaten√ß√£o.

**Prova do Lema 2:**
Para mostrar como as proje√ß√µes lineares $W^Q$, $W^K$ e $W^V$ permitem ao modelo capturar diferentes aspectos da informa√ß√£o, analisaremos o papel de cada proje√ß√£o.

I. **Matriz de Proje√ß√£o Query ($W^Q$):** A matriz $W^Q$ transforma a representa√ß√£o de entrada $x_i$ no vetor de consulta $q_i$. Essa transforma√ß√£o √© crucial porque o vetor $q_i$ representa a "pergunta" ou o interesse do token $x_i$ ao avaliar outros tokens na sequ√™ncia. Ao aprender a matriz $W^Q$, o modelo aprende a extrair as caracter√≠sticas de $x_i$ que s√£o relevantes para medir sua similaridade com outros tokens.

II. **Matriz de Proje√ß√£o Key ($W^K$):** A matriz $W^K$ transforma a representa√ß√£o de entrada $x_i$ no vetor key $k_i$. O vetor $k_i$ representa a "chave" ou identificador de um token, usado para calcular os scores com o vetor de *query* de outro token. Ao aprender a matriz $W^K$, o modelo aprende a extrair as caracter√≠sticas que melhor identificam cada token para o c√°lculo das pontua√ß√µes de aten√ß√£o.

III. **Matriz de Proje√ß√£o Value ($W^V$):** A matriz $W^V$ transforma a representa√ß√£o de entrada $x_i$ no vetor value $v_i$. O vetor $v_i$ √© o valor que ser√° ponderado pelos scores de aten√ß√£o para criar a representa√ß√£o final do token. Ao aprender a matriz $W^V$, o modelo aprende a extrair a informa√ß√£o que √© mais relevante ser agregada pelos pesos de aten√ß√£o.

IV. **Representa√ß√£o Diferenciada:** O modelo aprende $W^Q$, $W^K$ e $W^V$ independentemente, permitindo que cada token tenha representa√ß√µes diferentes para *query*, *key* e *value*. Isso permite ao modelo diferenciar e capturar as rela√ß√µes entre os tokens da sequ√™ncia de maneira muito mais detalhada do que usar a mesma representa√ß√£o para todos os prop√≥sitos.

V. **Conclus√£o:** Portanto, as proje√ß√µes lineares $W^Q$, $W^K$ e $W^V$ s√£o fundamentais porque permitem que o modelo mapeie a representa√ß√£o de entrada para espa√ßos diferentes, onde ele pode aprender as caracter√≠sticas necess√°rias para capturar as rela√ß√µes entre tokens de maneira flex√≠vel e eficiente. $\blacksquare$

**Lema 2.1** (Independ√™ncia das Proje√ß√µes): As proje√ß√µes lineares $W^Q$, $W^K$, e $W^V$ s√£o independentemente parametrizadas e aprendidas. Esta independ√™ncia √© crucial para permitir que cada uma delas capture diferentes caracter√≠sticas da representa√ß√£o de entrada, sem as restri√ß√µes impostas por par√¢metros compartilhados.

**Prova do Lema 2.1:**
A independ√™ncia das proje√ß√µes $W^Q$, $W^K$, e $W^V$ √© uma caracter√≠stica fundamental da arquitetura da autoaten√ß√£o.

I. **Parametriza√ß√£o Separada:** Cada proje√ß√£o linear √© definida por sua pr√≥pria matriz de peso e, portanto, tem seu pr√≥prio conjunto de par√¢metros trein√°veis.  Especificamente, $W^Q \in \mathbb{R}^{d \times d_k}$, $W^K \in \mathbb{R}^{d \times d_k}$, e $W^V \in \mathbb{R}^{d \times d_v}$ s√£o matrizes distintas, sem quaisquer pesos compartilhados entre elas.

II. **Aprendizado Independente:** Durante o treinamento, o modelo aprende os par√¢metros de $W^Q$, $W^K$, e $W^V$ separadamente. O algoritmo de otimiza√ß√£o (por exemplo, descida do gradiente) ajusta os par√¢metros de cada matriz para minimizar a fun√ß√£o de perda, sem levar em considera√ß√£o ou impor restri√ß√µes sobre os par√¢metros das outras matrizes.

III. **Fun√ß√µes Diferentes:** A independ√™ncia das proje√ß√µes permite que cada matriz desempenhe uma fun√ß√£o espec√≠fica e diferenciada: $W^Q$ transforma a entrada em *query* para consulta de outros tokens; $W^K$ transforma a entrada em *key* para identifica√ß√£o dos tokens; e $W^V$ transforma a entrada em *value* para representar os valores que ser√£o ponderados.

IV. **Flexibilidade na Captura de Informa√ß√£o:** Essa independ√™ncia √© crucial porque permite que o modelo capture diferentes caracter√≠sticas da representa√ß√£o de entrada. Se as proje√ß√µes fossem compartilhadas, a flexibilidade do modelo seria restringida, e ele teria dificuldade em aprender representa√ß√µes distintas para *query*, *key*, e *value*.

V. **Conclus√£o:** Portanto, a independ√™ncia das proje√ß√µes lineares $W^Q$, $W^K$, e $W^V$ √© essencial para a capacidade do modelo de aprender representa√ß√µes diferenciadas e otimizar o processo de autoaten√ß√£o. A parametriza√ß√£o e aprendizado separados permitem que cada matriz capture diferentes aspectos da informa√ß√£o de entrada. $\blacksquare$

### Feedforward Layer: Transforma√ß√µes N√£o Lineares
Ap√≥s a camada de autoaten√ß√£o, cada transformer block inclui uma **camada feedforward** (FFN), que √© aplicada de forma independente a cada posi√ß√£o na sequ√™ncia [^9]. Esta camada consiste em duas proje√ß√µes lineares com uma ativa√ß√£o n√£o linear (ReLU ou GELU) entre elas [^10]. Embora os par√¢metros sejam os mesmos para todas as posi√ß√µes, eles s√£o diferentes para cada camada do modelo [^10]. Isso adiciona n√£o-linearidade e capacidade de modelagem ao modelo, transformando a representa√ß√£o contextualizada produzida pela autoaten√ß√£o [^10].

#### Estrutura da Camada Feedforward
A camada feedforward √© tipicamente estruturada como uma rede neural de duas camadas totalmente conectadas. A primeira camada expande a dimens√£o da representa√ß√£o de entrada $x$, para uma dimens√£o $d_{ff}$, e a segunda camada reduz a dimens√£o de volta para a dimens√£o original $d$ [^10]. A ativa√ß√£o n√£o linear, ReLU ou GELU, √© aplicada entre as duas camadas para introduzir n√£o-linearidade, que √© essencial para que o modelo aprenda rela√ß√µes complexas [^10].
A transforma√ß√£o da camada feedforward pode ser representada como:
$$FFN(x) = GELU(xW_1 + b_1)W_2 + b_2$$
onde $W_1 \in \mathbb{R}^{d \times d_{ff}}$, $W_2 \in \mathbb{R}^{d_{ff} \times d}$ s√£o matrizes de pesos, $b_1 \in \mathbb{R}^{d_{ff}}$ e $b_2 \in \mathbb{R}^d$ s√£o vetores de bias e $GELU$ √© a fun√ß√£o de ativa√ß√£o *Gaussian Error Linear Unit*. A fun√ß√£o GELU √© definida como:
$$GELU(x) = x \Phi(x)$$
onde $\Phi(x)$ √© a fun√ß√£o de distribui√ß√£o cumulativa da distribui√ß√£o normal padr√£o.

> üí° **Exemplo Num√©rico:**
>
> Considere uma representa√ß√£o de entrada $x$ com dimens√£o $d=4$, e vamos utilizar uma dimens√£o interna $d_{ff} = 8$. Vamos usar valores aleat√≥rios para ilustrar as transforma√ß√µes e bias. Para simplificar o c√°lculo do GELU, vamos aproxim√°-lo por $GELU(x) \approx 0.5x(1+tanh[\sqrt{2/\pi}(x + 0.044715x^3)])$.
> ```python
> import numpy as np
>
> # Dimens√µes
> d = 4
> d_ff = 8
>
> # Representa√ß√£o de entrada
> x = np.array([1, 2, 3, 4])
>
> # Matrizes de pesos e bias aleat√≥rios
> np.random.seed(42)
> W1 = np.random.rand(d, d_ff)
> b1 = np.random.rand(d_ff)
> W2 = np.random.rand(d_ff, d)
> b2 = np.random.rand(d)
>
> # Primeira proje√ß√£o linear
> hidden = np.dot(x, W1) + b1
>
> # Fun√ß√£o de ativa√ß√£o GELU (aproxima√ß√£o)
> def gelu(x):
>   return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))
>
> activated_hidden = gelu(hidden)
>
> # Segunda proje√ß√£o linear
> output = np.dot(activated_hidden, W2) + b2
>
> print("Entrada (x):", x)
> print("W1:\n", W1)
> print("Bias 1 (b1):", b1)
> print("Sa√≠da da primeira camada (xW1 + b1):", hidden)
> print("Sa√≠da da ativa√ß√£o GELU:", activated_hidden)
> print("W2:\n", W2)
> print("Bias 2 (b2):", b2)
> print("Sa√≠da da camada Feedforward:", output)
> ```
> O exemplo acima ilustra como a camada feedforward transforma a representa√ß√£o de entrada atrav√©s das proje√ß√µes lineares e da fun√ß√£o de ativa√ß√£o GELU. Observe que a dimens√£o √© expandida de 4 para 8 e depois reduzida de volta para 4.

**Proposi√ß√£o 2** (Camada Feedforward como Transformador N√£o Linear): A camada feedforward dentro de um transformer block serve para introduzir n√£o linearidade e aumentar a capacidade de modelagem da rede, possibilitando o aprendizado de rela√ß√µes complexas nas representa√ß√µes contextuais.

**Prova da Proposi√ß√£o 2:**
Para demonstrar como a camada feedforward introduz n√£o-linearidade e capacidade de modelagem, vamos analisar cada um de seus componentes:

I. **Primeira Proje√ß√£o Linear:** A primeira proje√ß√£o linear, $xW_1 + b_1$, mapeia a representa√ß√£o de entrada $x$ para um espa√ßo de maior dimensionalidade, $d_{ff}$. Essa transforma√ß√£o permite que a camada capture diferentes caracter√≠sticas da entrada.

II. **Fun√ß√£o de Ativa√ß√£o N√£o Linear (GELU):** A fun√ß√£o de ativa√ß√£o n√£o linear $GELU$ √© aplicada ap√≥s a primeira proje√ß√£o linear. A n√£o-linearidade √© crucial porque permite que a rede aprenda rela√ß√µes complexas entre os dados. Sem uma fun√ß√£o de ativa√ß√£o n√£o linear, a rede seria equivalente a uma √∫nica camada linear, incapaz de modelar rela√ß√µes n√£o-lineares.

III. **Segunda Proje√ß√£o Linear:** A segunda proje√ß√£o linear, $GELU(xW_1 + b_1)W_2 + b_2$, mapeia a representa√ß√£o de volta para o espa√ßo da dimens√£o original $d$. Essa transforma√ß√£o combina os resultados da fun√ß√£o de ativa√ß√£o n√£o linear e as caracter√≠sticas extra√≠das na primeira proje√ß√£o.

IV. **Capacidade de Modelagem:** Ao concatenar as proje√ß√µes lineares e a ativa√ß√£o n√£o linear, a camada feedforward introduz transforma√ß√µes n√£o lineares das representa√ß√µes contextuais. Essa arquitetura permite que a rede modele rela√ß√µes n√£o lineares entre os dados, aumentando sua capacidade de aprender padr√µes complexos na linguagem.

V. **Posi√ß√£o-Independente, Par√¢metros Espec√≠ficos da Camada:** As camadas feedforward aplicam redes id√™nticas a cada posi√ß√£o da sequ√™ncia de forma independente e em paralelo, o que √© diferente da autoaten√ß√£o que interage com outras posi√ß√µes. Al√©m disso, os par√¢metros $W_1$, $b_1$, $W_2$ e $b_2$ s√£o espec√≠ficos para cada camada do modelo, adicionando outra dimens√£o de flexibilidade para capturar caracter√≠sticas diferentes em cada n√≠vel da rede.

VI. **Conclus√£o:** Portanto, a camada feedforward, com suas duas proje√ß√µes lineares e ativa√ß√£o n√£o linear, adiciona n√£o-linearidade e capacidade de modelagem ao modelo, transformando as representa√ß√µes de entrada e permitindo o aprendizado de rela√ß√µes complexas na linguagem. $\blacksquare$

**Proposi√ß√£o 2.1** (Transforma√ß√£o por Expans√£o e Redu√ß√£o): A camada feedforward utiliza uma arquitetura de expans√£o e redu√ß√£o da dimensionalidade, onde a representa√ß√£o de entrada de dimens√£o $d$ √© expandida para uma dimens√£o $d_{ff} > d$ antes de ser reduzida de volta para a dimens√£o original $d$. Esta abordagem permite que a camada realize transforma√ß√µes mais complexas.

**Prova da Proposi√ß√£o 2.1:**

I. **Expans√£o da Dimensionalidade:** A primeira proje√ß√£o linear na camada feedforward, $xW_1 + b_1$, transforma a representa√ß√£o de entrada $x$ de dimens√£o $d$ para uma representa√ß√£o de dimens√£o $d_{ff}$. A dimens√£o $d_{ff}$ √© geralmente maior que $d$. Essa expans√£o para um espa√ßo de maior dimens√£o permite que a camada capture mais caracter√≠sticas e detalhes na representa√ß√£o.

II. **Transforma√ß√£o N√£o Linear:** A fun√ß√£o de ativa√ß√£o n√£o linear (GELU neste caso) √© aplicada √† representa√ß√£o expandida. Essa n√£o-linearidade introduz a capacidade de aprender rela√ß√µes complexas entre as caracter√≠sticas expandidas.

III. **Redu√ß√£o da Dimensionalidade:** A segunda proje√ß√£o linear, $GELU(xW_1 + b_1)W_2 + b_2$, transforma a representa√ß√£o de volta para a dimens√£o original $d$. Esta redu√ß√£o da dimens√£o combina as caracter√≠sticas aprendidas no espa√ßo de maior dimens√£o e transforma-as de volta para a dimens√£o original, mas com representa√ß√µes mais ricas.

IV. **Capacidade Aprimorada:** A combina√ß√£o da expans√£o, transforma√ß√£o n√£o linear e redu√ß√£o permite que a camada feedforward execute transforma√ß√µes n√£o lineares mais complexas do que seria poss√≠vel com apenas uma √∫nica transforma√ß√£o linear. Isso capacita a rede a aprender representa√ß√µes mais abstratas e √∫teis para a tarefa em quest√£o.

V. **Analogia:** Uma analogia √∫til √© pensar nesse processo como uma transforma√ß√£o de uma imagem para um espa√ßo de maior dimens√£o, onde caracter√≠sticas como arestas, texturas e formas s√£o identificadas separadamente, antes de serem combinadas em uma nova representa√ß√£o da mesma imagem, mas com representa√ß√µes mais ricas.

VI. **Conclus√£o:** Portanto, a arquitetura de expans√£o e redu√ß√£o da dimensionalidade na camada feedforward desempenha um papel crucial na capacidade do modelo de aprender transforma√ß√µes complexas, permitindo que a rede modele rela√ß√µes n√£o lineares e aprenda representa√ß√µes mais ricas para a linguagem. $\blacksquare$

### Camadas de Autoaten√ß√£o Multi-Cabe√ßas
Na pr√°tica, os Transformers usam **autoaten√ß√£o multi-cabe√ßas**, que consiste em v√°rias camadas de autoaten√ß√£o em paralelo, cada uma com seus pr√≥prios conjuntos de proje√ß√µes $W^Q_i$, $W^K_i$, e $W^V_i$ [^9]. As sa√≠das de cada cabe√ßa s√£o ent√£o concatenadas e projetadas para a dimens√£o original $d$, permitindo que o modelo capture diferentes tipos de rela√ß√µes entre palavras [^9]. Isso √© matematicamente expresso como:
$$MultiHead(Q, K, V) = Concat(head_1, \ldots, head_h)W^O$$
onde $head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)$ s√£o as sa√≠das das $h$ cabe√ßas de aten√ß√£o, e $W^O$ √© uma matriz de proje√ß√£o que transforma a sa√≠da concatenada para a dimens√£o desejada.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um exemplo com duas cabe√ßas ($h=2$), uma dimens√£o de entrada $d=4$ e $d_k=d_v=2$.
> ```python
> import numpy as np
>
> # Par√¢metros
> d = 4
> dk = 2
> dv = 2
> h = 2
>
> # Representa√ß√£o de entrada (apenas para ilustra√ß√£o)
> x_i = np.array([1, 2, 3, 4])
>
> # Matrizes de proje√ß√£o para as cabe√ßas (aleat√≥rias)
> np.random.seed(42)
> W_Q1 = np.random.rand(d, dk)
> W_K1 = np.random.rand(d, dk)
> W_V1 = np.random.rand(d, dv)
> W_Q2 = np.random.rand(d, dk)
> W_K2 = np.random.rand(d, dk)
> W_V2 = np.random.rand(d, dv)
>
> # Matriz de proje√ß√£o da concatena√ß√£o (aleat√≥ria)
> W_O = np.random.rand(h * dv, d)
>
> # C√°lculo de Q, K, V para cada cabe√ßa
> Q1 = np.dot(x_i, W_Q1)
> K1 = np.dot(x_i, W_K1)
> V1 = np.dot(x_i, W_V1)
>
> Q2 = np.dot(x_i, W_Q2)
> K2 = np.dot(x_i, W_K2)
> V2 = np.dot(x_i, W_V2)
>
> # Fun√ß√£o de aten√ß√£o (simplificada sem softmax para demonstra√ß√£o)
> def attention(Q, K, V):
>    return np.dot(Q, K.T) / np.sqrt(dk)
>
> # C√°lculo das sa√≠das de cada cabe√ßa (sem softmax para simplifica√ß√£o)
> head1 = attention(Q1, K1, V1)
> head2 = attention(Q2, K2, V2)
>
> # Concatena√ß√£o das sa√≠das
> concatenated_heads = np.concatenate((head1, head2))
>
> # Proje√ß√£o da sa√≠da concatenada
> output = np.dot(concatenated_heads, W_O)
>
> print("Representa√ß√£o de entrada (x_i):", x_i)
> print("Matrizes de proje√ß√£o da cabe√ßa 1:\nW_Q1:\n", W_Q1,"\nW_K1:\n", W_K1,"\nW_V1:\n", W_V1)
> print("Matrizes de proje√ß√£o da cabe√ßa 2:\nW_Q2:\n", W_Q2,"\nW_K2:\n", W_K2,"\nW_V2:\n", W_V2)
> print("Q1:", Q1)
> print("K1:", K1)
> print("V1:", V1)
> print("Q2:", Q2)
> print("K2:", K2)
> print("V2:", V2)
> print("Sa√≠da da cabe√ßa 1 (sem softmax):", head1)
> print("Sa√≠da da cabe√ßa 2 (sem softmax):", head2)
> print("Sa√≠das concatenadas:", concatenated_heads)
> print("Matriz de proje√ß√£o W_O:\n", W_O)
> print("Sa√≠da da MultiHead:", output)
> ```
> Este exemplo demonstra como as proje√ß√µes e a concatena√ß√£o funcionam em uma arquitetura de multi-cabe√ßas. As sa√≠das de cada cabe√ßa s√£o concatenadas e projetadas de volta para a dimens√£o original.

**Teorema 1** (Expressividade da Autoaten√ß√£o Multi-Cabe√ßas): A autoaten√ß√£o multi-cabe√ßas permite que o modelo capture m√∫ltiplos aspectos e rela√ß√µes entre as palavras em uma sequ√™ncia, aumentando a expressividade e a capacidade de modelagem em compara√ß√£o com a autoaten√ß√£o de cabe√ßa √∫nica. Cada cabe√ßa de aten√ß√£o aprende padr√µes diferentes, que s√£o combinados para formar uma representa√ß√£o contextualizada mais completa.

**Prova do Teorema 1:**
Para demonstrar a expressividade da autoaten√ß√£o multi-cabe√ßas, consideraremos a opera√ß√£o de m√∫ltiplas cabe√ßas de aten√ß√£o e como suas sa√≠das s√£o combinadas:

I. **M√∫ltiplas Cabe√ßas de Aten√ß√£o:** A autoaten√ß√£o multi-cabe√ßas consiste em *h* cabe√ßas de aten√ß√£o, cada uma com suas pr√≥prias matrizes de proje√ß√£o $W^Q_i$, $W^K_i$ e $W^V_i$, para $i = 1, \ldots, h$. Cada cabe√ßa opera independentemente e em paralelo, processando as representa√ß√µes de entrada de forma diferente.

II. **Aprendizado Diversificado:** Cada cabe√ßa de aten√ß√£o aprende transforma√ß√µes diferentes e, portanto, pode se concentrar em diferentes tipos de rela√ß√µes entre as palavras na sequ√™ncia. Por exemplo, uma cabe√ßa pode aprender rela√ß√µes sint√°ticas, enquanto outra pode aprender rela√ß√µes sem√¢nticas.

III. **Concatena√ß√£o das Sa√≠das:** As sa√≠das de cada cabe√ßa, $head_i$, s√£o concatenadas em um √∫nico vetor. A concatena√ß√£o cria uma representa√ß√£o que cont√©m todas as informa√ß√µes extra√≠das pelas diferentes cabe√ßas.

IV. **Proje√ß√£o Final:** A representa√ß√£o concatenada √© ent√£o projetada usando uma matriz de pesos $W^O$ para a dimens√£o desejada. Essa proje√ß√£o final combina as representa√ß√µes de diferentes espa√ßos aprendidos pelas cabe√ßas, gerando a sa√≠da final.

V. **Aumento da Expressividade:** Ao processar e transformar as entradas de m√∫ltiplas perspectivas diferentes, a autoaten√ß√£o multi-cabe√ßas oferece ao modelo uma capacidade muito maior de capturar nuances complexas nas rela√ß√µes entre as palavras. Se a autoaten√ß√£o fosse aplicada em um √∫nico espa√ßo, o modelo ficaria limitado √† captura de um √∫nico tipo de relacionamento.

VI. **Conclus√£o:** Portanto, a autoaten√ß√£o multi-cabe√ßas aumenta a expressividade e capacidade de modelagem porque permite ao modelo aprender e combinar m√∫ltiplos tipos de rela√ß√µes e padr√µes na linguagem, algo que uma √∫nica cabe√ßa de aten√ß√£o n√£o seria capaz de fazer. $\blacksquare$

### Conclus√£o
As camadas de **autoaten√ß√£o** e **feedforward** dentro dos transformer blocks s√£o fundamentais para o sucesso dos modelos Transformer em tarefas de processamento de linguagem natural [^2]. A autoaten√ß√£o possibilita que o modelo capture rela√ß√µes contextuais complexas entre as palavras, enquanto as camadas feedforward introduzem transforma√ß√µes n√£o lineares para a representa√ß√£o de cada token [^3]. Juntas, essas camadas permitem a cria√ß√£o de modelos poderosos, capazes de compreender e gerar linguagem natural com alta precis√£o [^2]. Como discutido, a autoaten√ß√£o pode ser refinada atrav√©s do uso de *multi-head attention* e camadas *feedforward* que, embora aplicadas de forma independente em cada posi√ß√£o, s√£o diferentes para cada camada, adicionando capacidades de modelagem [^9, 10].

### Refer√™ncias
[^2]: Cap√≠tulo 10 - Transformers and Large Language Models.
[^3]: 10.1 The Transformer: A Self-Attention Network
[^5]: 10.1.3 Self-attention more formally
[^6]: 10.1.3 Self-attention more formally
[^7]: 10.1 The Transformer: A Self-Attention Network
[^9]: 10.3 Transformer Blocks
[^10]: Figure 10.6 A transformer block showing all the layers
<!-- END -->
