## Token Embeddings e a Matriz de Embedding em Transformers

### Introdu√ß√£o
Em continuidade ao nosso estudo sobre a arquitetura Transformer e, em particular, sobre a representa√ß√£o da entrada para esses modelos, este cap√≠tulo se aprofunda na cria√ß√£o de **token embeddings** e na fun√ß√£o da **matriz de embedding**. Conforme introduzido anteriormente [^2], os token embeddings s√£o vetores que representam palavras ou subpalavras, armazenados numa matriz que possui uma linha para cada token do vocabul√°rio. A forma como essa matriz √© constru√≠da e como os embeddings s√£o selecionados para cada token da sequ√™ncia de input √© fundamental para o funcionamento do Transformer, permitindo que ele processe e entenda o significado sem√¢ntico das palavras em um texto [^2]. Este cap√≠tulo expande a se√ß√£o 10.5 e os conceitos discutidos na se√ß√£o anterior, detalhando o processo de convers√£o de tokens em √≠ndices de vocabul√°rio e como esses √≠ndices s√£o usados para selecionar as linhas correspondentes da matriz de embedding. Al√©m disso, exploraremos como os token embeddings s√£o combinados com os positional embeddings para formar a entrada do modelo, permitindo que ele interprete tanto a sem√¢ntica quanto a ordem das palavras em um contexto.

### Conceitos Fundamentais

**Vocabul√°rio e √çndices**
O processo de cria√ß√£o de token embeddings come√ßa com a constru√ß√£o de um **vocabul√°rio**, que √© um conjunto de todos os tokens (palavras ou subpalavras) que o modelo ser√° capaz de processar [^2]. Cada token no vocabul√°rio recebe um √≠ndice √∫nico, que √© usado para identific√°-lo. Essa indexa√ß√£o permite que os tokens sejam representados numericamente, o que √© essencial para o processamento computacional.
Por exemplo, um vocabul√°rio poderia ser:
  `["ola", "mundo", "!", "como", "voce", "obrigado", "a", "todos"]`
Nesse vocabul√°rio, "ola" poderia ter o √≠ndice 0, "mundo" o √≠ndice 1, "!" o √≠ndice 2, e assim por diante.

**A Matriz de Embedding**
A **matriz de embedding** ($E$) √© a estrutura central para armazenar os token embeddings. Essa matriz tem dimens√µes $|V| \times d$, onde $|V|$ √© o tamanho do vocabul√°rio e $d$ √© a dimensionalidade do vetor de embedding [^2]. Cada linha da matriz $E$ corresponde ao vetor de embedding de um token espec√≠fico no vocabul√°rio. Inicialmente, os valores nessa matriz podem ser atribu√≠dos aleatoriamente ou por meio de um processo de pr√©-treinamento, e s√£o ajustados durante o treinamento do modelo usando o processo de backpropagation.

> üí° **Exemplo Num√©rico:** Se tivermos um vocabul√°rio de 8 tokens e cada embedding for um vetor de 3 dimens√µes, a matriz de embedding $E$ ser√° uma matriz de 8 linhas por 3 colunas. A primeira linha ($E_0$) pode representar o embedding de "ola", a segunda linha ($E_1$) o embedding de "mundo", e assim sucessivamente. Os valores dentro da matriz s√£o inicialmente aleat√≥rios e aprendidos durante o treinamento.
  ```
     E = [
           [0.1, -0.2, 0.5],    // "ola"
           [0.3,  0.4, -0.2],   // "mundo"
           [-0.1, 0.2, -0.3],    // "!"
           [0.5, -0.3, 0.1],    // "como"
           [-0.2, 0.1, 0.4],   // "voce"
           [0.6, -0.4, 0.2],   // "obrigado"
           [-0.3, 0.5, 0.1],    // "a"
           [0.2, 0.3, -0.4]    // "todos"
          ]
    ```

> üí° **Exemplo Num√©rico:** Para um vocabul√°rio maior, digamos, com 10.000 tokens e embeddings de 512 dimens√µes, a matriz de embedding teria dimens√µes 10.000 x 512. Inicializar essa matriz com valores aleat√≥rios de uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 0.01, por exemplo, pode ser feito com numpy em python da seguinte forma:
```python
import numpy as np

vocab_size = 10000
embedding_dim = 512
E = np.random.normal(loc=0.0, scale=0.01, size=(vocab_size, embedding_dim))
print(E.shape) # Output: (10000, 512)
print(E[0,:5]) # Output: [0.0019, 0.0033, -0.0005, -0.0071, 0.0043] (valores de exemplo)
```
Esta matriz √© o ponto de partida para o aprendizado dos embeddings.

**Lema 1**
A matriz de embedding $E$ pode ser vista como uma fun√ß√£o $E: V \rightarrow \mathbb{R}^d$, que mapeia cada token do vocabul√°rio $V$ para um vetor de embedding em $\mathbb{R}^d$.
*Proof:* 
I. Seja $V = \{v_1, v_2, ..., v_{|V|}\}$ o vocabul√°rio, onde $v_i$ representa o $i$-√©simo token.
II. A matriz de embedding $E$ tem dimens√µes $|V| \times d$, onde cada linha $E_i$ corresponde ao embedding do token $v_i$.
III. Podemos definir uma fun√ß√£o $E: V \rightarrow \mathbb{R}^d$ tal que $E(v_i) = E_i$, onde $E_i$ √© um vetor em $\mathbb{R}^d$.
IV. Portanto, a fun√ß√£o $E$ mapeia cada token $v_i$ do vocabul√°rio $V$ para um vetor $E_i$ em $\mathbb{R}^d$. ‚ñ†

**Observa√ß√£o 1**
A fun√ß√£o $E$ √© um mapeamento que associa um elemento discreto do vocabul√°rio a uma representa√ß√£o cont√≠nua em um espa√ßo vetorial. Esta representa√ß√£o cont√≠nua permite que similaridades sem√¢nticas entre tokens sejam capturadas pelas rela√ß√µes geom√©tricas entre seus vetores correspondentes.

**Cria√ß√£o dos Token Embeddings**

Como discutido anteriormente [^2], os token embeddings s√£o criados usando os √≠ndices do vocabul√°rio para selecionar as linhas correspondentes na matriz de embedding $E$. Dada uma sequ√™ncia de tokens, cada token √© primeiramente convertido em seu √≠ndice correspondente no vocabul√°rio. Esses √≠ndices s√£o ent√£o utilizados para selecionar as linhas correspondentes na matriz de embedding $E$, formando um conjunto de vetores, um para cada token.
Podemos visualizar esse processo como uma busca em uma tabela, onde o √≠ndice do token √© a chave, e o embedding correspondente √© o valor retornado. Alternativamente, podemos pensar em cada token como um vetor one-hot (com um 1 na posi√ß√£o correspondente ao √≠ndice do token e 0 nas demais), e ao multiplicar o vetor one-hot pela matriz de embedding $E$, selecionamos a linha correspondente da matriz $E$. As duas abordagens s√£o equivalentes, conforme demonstrado anteriormente.

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, se "ola" tem √≠ndice 0, "mundo" tem √≠ndice 1, e "!" tem √≠ndice 2, podemos criar uma matriz one-hot `I` para a sequ√™ncia "ola mundo !". A matriz `I` seria uma matriz 3x8 (3 tokens, tamanho do vocabul√°rio 8)
```python
import numpy as np
vocab_size = 8
seq_indices = [0, 1, 2] # indices para "ola", "mundo", "!"
I = np.zeros((len(seq_indices), vocab_size))
for i, index in enumerate(seq_indices):
    I[i, index] = 1
print(I) # Matriz one-hot
```
```
[[1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0.]]
```

**Proposi√ß√£o 1**
A sele√ß√£o de embeddings de uma sequ√™ncia de tokens pode ser expressa por uma multiplica√ß√£o de matrizes. Se $x$ for um vetor de √≠ndices de tokens, e $I$ for uma matriz one-hot, onde cada linha corresponde a um vetor one-hot do √≠ndice de token correspondente em $x$, ent√£o a matriz de embeddings $X$ da sequ√™ncia √© dada por $X = IE$.
*Proof:*
I. Seja $x = [x_1, x_2, \ldots, x_n]$ uma sequ√™ncia de √≠ndices de tokens, onde $x_i$ √© o √≠ndice do $i$-√©simo token.
II. Constru√≠mos uma matriz one-hot $I$ de dimens√£o $n \times |V|$, onde cada linha $I_i$ √© um vetor one-hot de dimens√£o $|V|$ com um 1 na posi√ß√£o $x_i$ e 0 nas demais posi√ß√µes.
III. A matriz de embedding $E$ tem dimens√£o $|V| \times d$.
IV. O produto $I E$ resulta em uma matriz $X$ de dimens√£o $n \times d$, onde a $i$-√©sima linha $X_i$ √© dada por $I_i E$.
V. Dado que $I_i$ √© um vetor one-hot com um 1 na posi√ß√£o $x_i$, o produto $I_i E$ seleciona a linha $E_{x_i}$ da matriz $E$, que √© o embedding do token com √≠ndice $x_i$.
VI. Portanto, a matriz $X = IE$ √© a matriz de embeddings da sequ√™ncia de tokens $x$, com $X_i = E_{x_i}$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Suponha que tenhamos a sequ√™ncia de input "ola mundo !". Os √≠ndices correspondentes a esses tokens no vocabul√°rio acima s√£o 0, 1 e 2 respectivamente. Para obter os embeddings para essa sequ√™ncia, selecionar√≠amos a linha 0, a linha 1 e a linha 2 da matriz $E$:
  ```
     embedding("ola") = E[0] = [0.1, -0.2, 0.5]
     embedding("mundo") = E[1] = [0.3, 0.4, -0.2]
     embedding("!") = E[2] = [-0.1, 0.2, -0.3]
  ```
  Os embeddings para esta sequ√™ncia de tokens formariam ent√£o a matriz:
  ```
  [
    [0.1, -0.2, 0.5],
    [0.3, 0.4, -0.2],
    [-0.1, 0.2, -0.3]
  ]
  ```
Cada linha dessa matriz √© o embedding para um token da sequ√™ncia.

> üí° **Exemplo Num√©rico:** Usando as matrizes `I` (one-hot) e `E` (embedding) dos exemplos anteriores, podemos calcular a matriz de embeddings `X` pela multiplica√ß√£o de matrizes `I @ E` (em Python usando numpy).
```python
import numpy as np
E = np.array([
    [0.1, -0.2, 0.5],
    [0.3,  0.4, -0.2],
    [-0.1, 0.2, -0.3],
    [0.5, -0.3, 0.1],
    [-0.2, 0.1, 0.4],
    [0.6, -0.4, 0.2],
    [-0.3, 0.5, 0.1],
    [0.2, 0.3, -0.4]
])
vocab_size = 8
seq_indices = [0, 1, 2] # indices para "ola", "mundo", "!"
I = np.zeros((len(seq_indices), vocab_size))
for i, index in enumerate(seq_indices):
    I[i, index] = 1

X = I @ E
print(X)
```
```
[[ 0.1 -0.2  0.5]
 [ 0.3  0.4 -0.2]
 [-0.1  0.2 -0.3]]
```
A matriz `X` resultante corresponde √† matriz de embeddings para a sequ√™ncia "ola mundo !".

**Teorema 1**
A matriz de embeddings $X$ de uma sequ√™ncia de tokens $x$, obtida como $X = IE$, pode ser vista como a representa√ß√£o da sequ√™ncia de tokens em um espa√ßo vetorial de dimens√£o $d$.
*Proof:*
I. Da Proposi√ß√£o 1, sabemos que $X = IE$ onde $I$ √© a matriz one-hot correspondente √† sequ√™ncia de tokens $x$ e $E$ √© a matriz de embedding.
II. A matriz $X$ tem dimens√µes $n \times d$, onde $n$ √© o comprimento da sequ√™ncia de tokens e $d$ √© a dimens√£o do embedding.
III. Cada linha $X_i$ de $X$ corresponde ao embedding do token $x_i$, denotado como $E_{x_i}$, que √© um vetor em $\mathbb{R}^d$.
IV. Portanto, a matriz $X$ √© composta por $n$ vetores de dimens√£o $d$, cada um representando um token na sequ√™ncia, e portanto representa a sequ√™ncia no espa√ßo vetorial $\mathbb{R}^d$. ‚ñ†

**A Matriz de Embedding como um Dicion√°rio**
√â √∫til pensar na matriz de embedding como um dicion√°rio. Cada linha da matriz age como um registro nesse dicion√°rio, associando um √≠ndice num√©rico a um vetor denso de valores reais. Essa representa√ß√£o vetorial captura as propriedades sem√¢nticas e sint√°ticas de cada token. Durante o treinamento, os valores nesses vetores s√£o refinados para que tokens com significados semelhantes tenham vetores pr√≥ximos.
A fun√ß√£o da matriz de embedding √© traduzir um token que √© uma entidade discreta para uma representa√ß√£o cont√≠nua e densa que o modelo pode manipular. A matriz √© um par√¢metro do modelo, aprendido durante o treinamento e que √© respons√°vel por codificar a sem√¢ntica dos tokens.

**Lema 2**
O processo de treinamento ajusta a matriz de embedding $E$ de forma a minimizar a perda associada √† tarefa espec√≠fica do modelo.
*Proof:*
I. A matriz de embedding $E$ √© inicializada com valores aleat√≥rios ou pr√©-treinados.
II. Durante o treinamento, o modelo usa esses embeddings para fazer previs√µes e compara as previs√µes com os resultados esperados.
III. A fun√ß√£o de perda quantifica a diferen√ßa entre as previs√µes e os resultados esperados.
IV. O algoritmo de backpropagation ajusta os par√¢metros do modelo, incluindo os valores na matriz de embedding $E$, para minimizar essa fun√ß√£o de perda.
V. Portanto, o processo de treinamento ajusta iterativamente a matriz de embedding $E$ para que os embeddings resultantes sejam mais adequados para a tarefa espec√≠fica do modelo. ‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar como a matriz de embedding √© ajustada, vamos considerar um exemplo simplificado. Suponha que nosso modelo esteja tentando prever a pr√≥xima palavra em uma sequ√™ncia. Ap√≥s algumas itera√ß√µes de treinamento, os embeddings de "rei" e "rainha" podem ter se tornado mais pr√≥ximos, enquanto os embeddings de "rei" e "mesa" se tornaram mais distantes. Inicialmente, os embeddings poderiam ser:
    ```
    E_inicial = {
       "rei": [0.2, -0.1, 0.3],
       "rainha": [0.1, 0.0, 0.2],
       "mesa": [-0.3, 0.4, -0.1]
    }
    ```
   Ap√≥s o treinamento, a matriz de embedding poderia se tornar:
    ```
    E_treinado = {
       "rei": [0.5, -0.2, 0.6],
       "rainha": [0.45, -0.15, 0.55],
       "mesa": [-0.8, 0.7, -0.4]
    }
    ```
   Note que os embeddings de "rei" e "rainha" agora est√£o mais pr√≥ximos (similaridade maior), indicando uma semelhan√ßa sem√¢ntica aprendida, enquanto "mesa" est√° mais distante, representando uma diferen√ßa sem√¢ntica. Isso √© alcan√ßado atrav√©s do ajuste dos valores na matriz $E$ com o backpropagation durante o treinamento.

A combina√ß√£o desses token embeddings com positional embeddings, como descrito no cap√≠tulo anterior, permite que os Transformers interpretem e processem informa√ß√µes de linguagem de forma eficaz, construindo uma representa√ß√£o rica e contextualizada de cada token na sequ√™ncia de input.

### Conclus√£o

Este cap√≠tulo detalhou o processo de cria√ß√£o de token embeddings e o papel da matriz de embedding na arquitetura Transformer. Vimos como os tokens s√£o convertidos em √≠ndices de vocabul√°rio e como esses √≠ndices s√£o utilizados para selecionar os vetores correspondentes na matriz de embedding. A matriz de embedding √© essencial para transformar dados textuais em uma representa√ß√£o num√©rica adequada para processamento computacional. A combina√ß√£o desses token embeddings com positional embeddings, conforme abordado nos cap√≠tulos anteriores, permite que os Transformers capturem tanto o significado sem√¢ntico quanto a ordem das palavras nas sequ√™ncias textuais. O conhecimento desse processo √© fundamental para entender a entrada dos modelos Transformer, que formam a base para as camadas de aten√ß√£o e feedforward que discutiremos nas pr√≥ximas se√ß√µes.
Este entendimento detalhado √© crucial para avan√ßar na compreens√£o de modelos de linguagem mais sofisticados.

### Refer√™ncias
[^2]: Jurafsky, Daniel; Martin, James H. (2023). *Speech and Language Processing*. Draft of February 3, 2024.
<!-- END -->
