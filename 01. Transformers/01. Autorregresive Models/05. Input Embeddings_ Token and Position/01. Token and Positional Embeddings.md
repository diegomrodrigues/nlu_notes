## Input Embeddings: Token and Position in Transformer Models

### Introdu√ß√£o
Em continuidade ao estudo dos modelos de linguagem Transformer, abordaremos neste cap√≠tulo um aspecto fundamental de sua arquitetura: a representa√ß√£o da entrada. Como vimos anteriormente, os Transformers operam sobre sequ√™ncias de tokens, e a forma como esses tokens s√£o representados como vetores √© crucial para o desempenho do modelo [^2]. Este cap√≠tulo detalhar√° a composi√ß√£o da entrada, que envolve tanto **token embeddings**, que capturam o significado sem√¢ntico das palavras, quanto **positional embeddings**, que codificam a ordem das palavras na sequ√™ncia, habilitando o modelo a entender a estrutura da linguagem. Esta se√ß√£o expande sobre o que foi introduzido no cap√≠tulo 10 [^2], especificamente na se√ß√£o 10.5, aprofundando a descri√ß√£o sobre como o modelo computa os embeddings para a sequ√™ncia de input.

### Conceitos Fundamentais
A entrada para um modelo Transformer √© uma sequ√™ncia de tokens, onde cada token corresponde, geralmente, a uma palavra ou subpalavra do vocabul√°rio. Essa sequ√™ncia √© transformada em uma representa√ß√£o vetorial que o modelo pode processar. Essa representa√ß√£o √© composta por duas partes principais: **token embeddings** e **positional embeddings** [^2].

**Token Embeddings**
Os **token embeddings** s√£o representa√ß√µes vetoriais de cada token no vocabul√°rio. Essas representa√ß√µes s√£o aprendidas durante o treinamento do modelo e t√™m como objetivo capturar o significado sem√¢ntico das palavras [^2]. Assim como foi visto no Cap√≠tulo 6, cada palavra pode ser representada por um vetor est√°tico, onde palavras com significados semelhantes ter√£o vetores pr√≥ximos no espa√ßo vetorial.
O modelo mant√©m uma **matriz de embedding** $E$, onde cada linha corresponde a um token do vocabul√°rio $V$. Assim, $E$ possui a forma $|V| \times d$, onde $d$ √© a dimensionalidade do vetor de embedding [^2].
Existem duas formas principais de obter o token embedding de uma palavra:
1.  **Sele√ß√£o Direta da Matriz de Embedding:** Para cada token de input, o √≠ndice correspondente na matriz $E$ √© utilizado para selecionar o vetor de embedding referente a esse token [^2]. Se a palavra "obrigado" possuir o √≠ndice 5 no vocabul√°rio, a quinta linha da matriz $E$ ser√° seu token embedding.
    > üí° **Exemplo Num√©rico:** Suponha que nosso vocabul√°rio seja `["ola", "mundo", "!", "como", "voce", "obrigado"]`, e que a matriz de embedding $E$ tenha dimens√£o $6 \times 3$. Cada linha de $E$ representa o embedding de uma palavra. Digamos que a palavra "obrigado" esteja na posi√ß√£o 5, e a quinta linha de $E$ seja `[0.2, -0.5, 0.8]`. Ao utilizar sele√ß√£o direta, o embedding de "obrigado" √© o vetor `[0.2, -0.5, 0.8]`.
2.  **Multiplica√ß√£o por Vetores One-Hot:** Cada palavra pode ser representada por um **vetor one-hot**, com uma dimens√£o para cada palavra no vocabul√°rio. No caso da palavra "obrigado", com √≠ndice 5, o vetor one-hot ter√° um '1' na quinta posi√ß√£o e '0' nas demais. Ao multiplicar esse vetor one-hot pela matriz de embeddings $E$, o resultado ser√° exatamente a linha correspondente ao embedding da palavra "obrigado" [^2]. Essa abordagem √© ilustrada na Figura 10.10 [^2], repetida abaixo:

    ```
     5
     [ 1  0 0 0 1 0 0 ... 0 0 0 ]  *  E =  [v5]
     
    ```
    > üí° **Exemplo Num√©rico:** Usando o mesmo vocabul√°rio do exemplo anterior, o vetor one-hot para "obrigado" (√≠ndice 5) seria `[0, 0, 0, 0, 0, 1]`. Se multiplicarmos esse vetor pela matriz $E$, que √© de dimens√£o 6x3, o resultado ser√° o vetor `[0.2, -0.5, 0.8]`, que √© a quinta linha de $E$.
    
    A Figura 10.11 [^2] demonstra que essa ideia pode ser extendida para uma sequ√™ncia de tokens:
     ```
     d       
     V       
     [0000100...0000]   
     [0000000...0010]
     [1000000...0000]     *    E    =  N
     ...
     [0000100...0000]
    ```
    
    Onde a sequ√™ncia de inputs $W$ √© representada por uma matriz de one-hot vectors e ao multiplicar pela matrix $E$ temos a matriz com os embeddings de cada token na sequ√™ncia, de tamanho $N$.

**Prova da Equival√™ncia entre Sele√ß√£o Direta e Multiplica√ß√£o por One-Hot:**
Aqui, provaremos formalmente que a sele√ß√£o direta da matriz de embedding √© equivalente √† multiplica√ß√£o da matriz de embedding pelo vetor one-hot correspondente ao token.

I.  Seja $E$ a matriz de embedding de dimens√£o $|V| \times d$, onde $|V|$ √© o tamanho do vocabul√°rio e $d$ √© a dimens√£o dos embeddings. Denotamos a i-√©sima linha de $E$ por $E_i$, que corresponde ao embedding do i-√©simo token no vocabul√°rio.
    
II. Seja $w$ um token com √≠ndice $k$ no vocabul√°rio $V$. O vetor one-hot correspondente a $w$, denotado por $v_k$, √© um vetor de dimens√£o $|V|$, onde o k-√©simo elemento √© 1 e todos os outros s√£o 0. Formalmente, $v_k = [0, 0, \ldots, 1, \ldots, 0]$, onde o 1 est√° na k-√©sima posi√ß√£o.

III. A opera√ß√£o de multiplica√ß√£o da matriz $E$ pelo vetor one-hot $v_k$ resulta em:
    $$E \cdot v_k = \begin{bmatrix}
    E_{11} & E_{12} & \cdots & E_{1k} & \cdots & E_{1d} \\
    E_{21} & E_{22} & \cdots & E_{2k} & \cdots & E_{2d} \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    E_{k1} & E_{k2} & \cdots & E_{kk} & \cdots & E_{kd} \\
    \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
    E_{|V|1} & E_{|V|2} & \cdots & E_{|V|k} & \cdots & E_{|V|d}
    \end{bmatrix} \cdot \begin{bmatrix}
    0 \\ 0 \\ \vdots \\ 1 \\ \vdots \\ 0
    \end{bmatrix}$$

IV. Ao realizar a multiplica√ß√£o, o resultado √© um vetor onde cada elemento √© o produto escalar da linha correspondente de $E$ com o vetor $v_k$. Todos os elementos do vetor resultante ser√£o zero, exceto o k-√©simo elemento que ser√° o produto escalar da k-√©sima linha com $v_k$, que √© exatamente $E_k$, j√° que o √∫nico elemento n√£o nulo em $v_k$ √© o k-√©simo, que vale 1. Logo, $E \cdot v_k = E_k$.

V. Assim, multiplicar $E$ pelo vetor one-hot $v_k$ resulta em $E_k$, que √© o k-√©simo vetor de embedding em $E$, o mesmo resultado obtido ao selecionar diretamente a k-√©sima linha de $E$. Portanto, a sele√ß√£o direta e a multiplica√ß√£o por one-hot s√£o opera√ß√µes equivalentes. ‚ñ†

**Positional Embeddings**
Os **positional embeddings** s√£o vetores que s√£o adicionados aos token embeddings para codificar a posi√ß√£o de cada palavra na sequ√™ncia. Diferentemente dos modelos RNN, os Transformers n√£o possuem um mecanismo intr√≠nseco para lidar com a ordem das palavras [^2]. Os positional embeddings s√£o essenciais para que o modelo consiga diferenciar, por exemplo, uma frase com as palavras em ordem correta da mesma frase com as palavras embaralhadas.
Existem duas abordagens principais para gerar positional embeddings:
1.  **Posi√ß√µes Absolutas:** Nessa abordagem, s√£o criados embeddings distintos para cada posi√ß√£o poss√≠vel na sequ√™ncia. Similar ao que ocorre com as palavras, cada posi√ß√£o teria um vetor de embedding √∫nico aprendido durante o treinamento do modelo [^2]. Esses embeddings podem ser armazenados numa matriz $E_{pos}$ com a forma $1 \times N$, onde N √© o tamanho m√°ximo de tokens considerado pelo modelo. O positional embedding para uma determinada posi√ß√£o $i$, $P[i]$, √© ent√£o adicionado ao token embedding $E[id(i)]$, onde $id(i)$ corresponde ao id do token na posi√ß√£o $i$ [^2].
    > üí° **Exemplo Num√©rico:** Suponha que o tamanho m√°ximo da sequ√™ncia seja $N=10$, e que a matriz de embeddings posicionais aprendida $E_{pos}$ seja de dimens√£o $10 \times 3$. Ent√£o, para a terceira palavra da sequ√™ncia, o positional embedding seria a terceira linha da matriz $E_{pos}$, que poderia ser algo como `[-0.1, 0.7, 0.3]`.
   - Um problema potencial desta abordagem √© que o modelo pode ter menos dados para treinamento nas posi√ß√µes mais distantes do in√≠cio da sequ√™ncia, resultando em embeddings menos precisos [^2].
2.  **Fun√ß√µes Est√°ticas:** Uma alternativa para resolver o problema acima √© usar fun√ß√µes est√°ticas que mapeiam inteiros para vetores. As fun√ß√µes de seno e cosseno com diferentes frequ√™ncias s√£o comumente usadas para gerar esses vetores, onde o intuito √© que vetores referentes a posi√ß√µes pr√≥ximas sejam mais similares entre si. Esta abordagem pode ajudar o modelo a entender a rela√ß√£o entre posi√ß√µes mais longas na sequ√™ncia [^2].
      > üí° **Exemplo Num√©rico:**  Suponha que a dimens√£o do embedding seja $d=4$. Para a posi√ß√£o $p=0$, o positional embedding $P[0]$ ser√°:
        $P[0]_1 = \sin(0/10000^{0/4}) = \sin(0) = 0$
        $P[0]_2 = \cos(0/10000^{1/4}) = \cos(0) = 1$
        $P[0]_3 = \sin(0/10000^{2/4}) = \sin(0) = 0$
        $P[0]_4 = \cos(0/10000^{3/4}) = \cos(0) = 1$
       Portanto, $P[0] = [0, 1, 0, 1]$. Para a posi√ß√£o $p=1$:
        $P[1]_1 = \sin(1/10000^{0/4}) = \sin(1) \approx 0.84$
        $P[1]_2 = \cos(1/10000^{1/4}) = \cos(0.1) \approx 0.995$
        $P[1]_3 = \sin(1/10000^{2/4}) = \sin(0.01) \approx 0.01$
        $P[1]_4 = \cos(1/10000^{3/4}) = \cos(0.001) \approx 0.999$
       Portanto, $P[1] \approx [0.84, 0.995, 0.01, 0.999]$.
       Note que $P[0]$ e $P[1]$ s√£o diferentes, e que as fun√ß√µes seno e cosseno, variando em diferentes frequ√™ncias, garantem que cada posi√ß√£o ter√° um embedding √∫nico.
   - Uma outra abordagem seria a representa√ß√£o de *posi√ß√µes relativas* ao inv√©s de posi√ß√µes absolutas.  Esta t√©cnica √© implementada na camada de aten√ß√£o em vez de ser adicionada como uma camada inicial [^2].

**Combina√ß√£o dos Embeddings**
Finalmente, para cada posi√ß√£o $i$ na sequ√™ncia de input, o token embedding $E[id(i)]$ √© somado ao positional embedding $P[i]$, gerando o vetor de entrada $X_i$ [^2]. A matriz de entrada resultante $X$ tem a forma $N \times d$, onde $N$ √© o comprimento da sequ√™ncia e $d$ √© a dimensionalidade dos embeddings (tanto de token quanto de posi√ß√£o). A Figura 10.12 [^2] ilustra esse processo:

```
X = Composite Embeddings (word + position)
Word Embeddings
Position Embeddings
```
     > üí° **Exemplo Num√©rico:**  Suponha que para a terceira palavra da sequ√™ncia, o token embedding seja `[0.5, -0.2, 0.1]` e o positional embedding (seja aprendido ou calculado por fun√ß√µes) seja `[-0.1, 0.7, 0.3]`. O vetor de entrada para essa posi√ß√£o seria ent√£o a soma dos dois vetores: `[0.5 - 0.1, -0.2 + 0.7, 0.1 + 0.3] = [0.4, 0.5, 0.4]`.
    
  **Observa√ß√£o:** Uma propriedade interessante da representa√ß√£o posicional utilizando senos e cossenos √© que para qualquer deslocamento fixo $k$,  o vetor posicional da posi√ß√£o $p+k$ pode ser obtido por uma transforma√ß√£o linear do vetor posicional da posi√ß√£o $p$. Formalmente, se denotarmos o positional embedding da posi√ß√£o $p$ por $P[p]$, ent√£o existe uma matriz $M_k$ tal que $P[p+k] = M_k P[p]$. Esta propriedade √© decorrente da identidade trigonom√©trica que descreve a soma de √¢ngulos. Este mecanismo permite que o modelo generalise para sequ√™ncias maiores e aprenda a modelar depend√™ncias posicionais entre palavras de forma mais robusta. Al√©m disso, esta propriedade n√£o √© compartilhada pela representa√ß√£o posicional utilizando embeddings aprendidos.

**Prova da Transforma√ß√£o Linear dos Positional Embeddings com Senos e Cossenos:**
Vamos provar que para um deslocamento fixo $k$, o positional embedding da posi√ß√£o $p+k$ pode ser obtido por uma transforma√ß√£o linear do positional embedding da posi√ß√£o $p$, quando utilizamos as fun√ß√µes seno e cosseno.

I. O positional embedding $P[p]$ na posi√ß√£o $p$ √© definido como um vetor de dimens√£o $d$, onde cada componente $j$ √© dada por:
   $$P[p]_j = \begin{cases}
               \sin\left(\frac{p}{10000^{2j/d}}\right), & \text{se } j \text{ √© par} \\
               \cos\left(\frac{p}{10000^{(2j-1)/d}}\right), & \text{se } j \text{ √© √≠mpar}
            \end{cases}$$
   Aqui, $j$ varia de $1$ a $d$, e $d$ √© a dimens√£o do embedding.

II. O positional embedding na posi√ß√£o $p+k$ √© dado por:
    $$P[p+k]_j = \begin{cases}
                \sin\left(\frac{p+k}{10000^{2j/d}}\right), & \text{se } j \text{ √© par} \\
                \cos\left(\frac{p+k}{10000^{(2j-1)/d}}\right), & \text{se } j \text{ √© √≠mpar}
            \end{cases}$$

III. Usando as identidades trigonom√©tricas para a soma de √¢ngulos, podemos reescrever os componentes de $P[p+k]$:
    Para $j$ par:
    $$\sin\left(\frac{p+k}{10000^{2j/d}}\right) = \sin\left(\frac{p}{10000^{2j/d}}\right)\cos\left(\frac{k}{10000^{2j/d}}\right) + \cos\left(\frac{p}{10000^{2j/d}}\right)\sin\left(\frac{k}{10000^{2j/d}}\right)$$
    Para $j$ √≠mpar:
    $$\cos\left(\frac{p+k}{10000^{(2j-1)/d}}\right) = \cos\left(\frac{p}{10000^{(2j-1)/d}}\right)\cos\left(\frac{k}{10000^{(2j-1)/d}}\right) - \sin\left(\frac{p}{10000^{(2j-1)/d}}\right)\sin\left(\frac{k}{10000^{(2j-1)/d}}\right)$$

IV. Essas equa√ß√µes mostram que cada componente de $P[p+k]$ pode ser escrito como uma combina√ß√£o linear dos componentes de $P[p]$. Podemos expressar essa rela√ß√£o em forma matricial. Seja $M_k$ uma matriz $d \times d$, tal que seus elementos $M_k[i,j]$ s√£o dados por:
   $$ M_k[i,j] = \begin{cases}
           \cos\left(\frac{k}{10000^{2j/d}}\right) & \text{se } i=j \text{ e i √© par}\\
           \sin\left(\frac{k}{10000^{2j/d}}\right) & \text{se } j=i-1 \text{ e i √© par} \\
            -\sin\left(\frac{k}{10000^{(2j-1)/d}}\right) & \text{se } j=i+1 \text{ e i √© √≠mpar}\\
           \cos\left(\frac{k}{10000^{(2j-1)/d}}\right) & \text{se } i=j \text{ e i √© √≠mpar}\\
          0 & \text{caso contr√°rio}
         \end{cases} $$
    Note que na matriz $M_k$, temos essencialmente rota√ß√µes com base em k em cada par de coordenadas $i, i+1$.

V. Com essa matriz, podemos escrever:
   $$P[p+k] = M_k P[p]$$
   Onde $M_k$ √© uma matriz que depende apenas do deslocamento $k$ e n√£o de $p$. Portanto, o positional embedding na posi√ß√£o $p+k$ √© uma transforma√ß√£o linear do positional embedding na posi√ß√£o $p$. ‚ñ†

### Conclus√£o
O processo de cria√ß√£o de embeddings de entrada √© um passo essencial na arquitetura do Transformer, permitindo que o modelo processe e entenda sequ√™ncias de texto. A combina√ß√£o de token embeddings e positional embeddings permite que o modelo capture tanto o significado das palavras quanto a ordem em que elas aparecem na sequ√™ncia [^2]. A escolha da abordagem para positional embeddings (absolutas ou fun√ß√µes est√°ticas) pode impactar o desempenho do modelo, e o estudo dessas alternativas √© essencial para o avan√ßo da √°rea de processamento de linguagem natural. As pr√≥ximas se√ß√µes tratar√£o de como esses embeddings s√£o utilizados pelas camadas de aten√ß√£o e de feedforward para construir representa√ß√µes contextuais ricas e como essas representa√ß√µes s√£o ent√£o utilizadas pelo modelo para prever a pr√≥xima palavra na sequ√™ncia.

### Refer√™ncias
[^2]: Jurafsky, Daniel; Martin, James H. (2023). *Speech and Language Processing*. Draft of February 3, 2024.
<!-- END -->
