## Positional Embeddings: Representando a Ordem na Sequ√™ncia em Transformers

### Introdu√ß√£o

Este cap√≠tulo continua a discuss√£o sobre a arquitetura Transformer, focando em como os modelos representam a ordem das palavras nas sequ√™ncias de entrada por meio de **positional embeddings**. Como mencionado nos cap√≠tulos anteriores, diferentemente dos modelos recorrentes como RNNs, os Transformers n√£o possuem um mecanismo intr√≠nseco para processar a ordem das palavras [^2]. Portanto, a inclus√£o de **positional embeddings**, combinados com os **token embeddings**, √© fundamental para que o modelo possa entender a estrutura sequencial da linguagem. Este cap√≠tulo aprofunda a explora√ß√£o das t√©cnicas de **positional embeddings**, especificamente discutindo **embeddings absolutos**, **embeddings aprendidos** em conjunto com outros par√¢metros, m√©todos para criar embeddings utilizando **fun√ß√µes est√°ticas** e **embeddings relativos**. Essas diferentes abordagens permitem que o modelo capture diferentes aspectos da rela√ß√£o entre as posi√ß√µes das palavras em um texto, influenciando diretamente a capacidade do modelo de entender a estrutura e o contexto de um input.

### Conceitos Fundamentais
Conforme introduzido nos cap√≠tulos anteriores, os **positional embeddings** s√£o vetores adicionados aos **token embeddings** para informar ao modelo a posi√ß√£o de cada token na sequ√™ncia. Sem eles, o Transformer trataria a sequ√™ncia de entrada como um "saco de palavras", ignorando a ordem e a estrutura gramatical, sint√°tica e sem√¢ntica do texto [^2].

**Positional Embeddings Absolutos**

A abordagem mais simples para criar **positional embeddings** √© utilizar **embeddings absolutos**. Nesta t√©cnica, um embedding distinto √© criado para cada posi√ß√£o poss√≠vel na sequ√™ncia, de forma similar ao que √© feito com os tokens. Esses embeddings s√£o inicializados aleatoriamente e aprendidos junto com os demais par√¢metros do modelo durante o treinamento [^2].

A matriz de positional embeddings, denotada por $E_{pos}$, tem dimens√µes $N \times d$, onde $N$ √© o comprimento m√°ximo da sequ√™ncia que o modelo suporta e $d$ √© a dimens√£o do embedding. Cada linha da matriz $E_{pos}$ representa o positional embedding de uma posi√ß√£o espec√≠fica na sequ√™ncia. Dado um token na posi√ß√£o $i$ da sequ√™ncia, seu positional embedding $P[i]$ √© obtido selecionando a linha $i$ da matriz $E_{pos}$. Esse embedding $P[i]$ √© ent√£o somado ao token embedding correspondente $E[id(i)]$, onde $id(i)$ √© o √≠ndice do token na posi√ß√£o $i$ [^2].

> üí° **Exemplo Num√©rico:** Suponha que a dimens√£o dos embeddings seja $d=3$ e que o comprimento m√°ximo da sequ√™ncia seja $N=5$. Ent√£o, ter√≠amos uma matriz de positional embeddings $E_{pos}$ de dimens√£o $5\times 3$:

```
E_pos = [
   [0.1, -0.2, 0.3],    // Posi√ß√£o 0
   [0.2, 0.1, -0.1],    // Posi√ß√£o 1
   [-0.1, 0.3, 0.2],   // Posi√ß√£o 2
   [0.3, -0.1, 0.1],   // Posi√ß√£o 3
   [-0.2, 0.2, 0.3]   // Posi√ß√£o 4
]
```
Para um token na terceira posi√ß√£o da sequ√™ncia (√≠ndice 2), o positional embedding selecionado seria o vetor $[-0.1, 0.3, 0.2]$.

**Lema 1**
O m√©todo de embeddings posicionais absolutos define um espa√ßo vetorial de embeddings posicionais, onde cada posi√ß√£o na sequ√™ncia √© representada por um vetor √∫nico de dimens√£o $d$. Formalmente, existe uma fun√ß√£o $P: \{0, 1, \ldots, N-1\} \rightarrow \mathbb{R}^d$ que mapeia cada posi√ß√£o $i$ para um vetor $P[i]$ em $\mathbb{R}^d$.
*Proof:*
I. Seja $N$ o comprimento m√°ximo da sequ√™ncia e $d$ a dimens√£o dos embeddings.
II. A matriz de embeddings posicionais $E_{pos}$ tem dimens√µes $N \times d$, onde cada linha $E_{pos, i}$ corresponde ao embedding da posi√ß√£o $i$.
III. Definimos uma fun√ß√£o $P: \{0, 1, \ldots, N-1\} \rightarrow \mathbb{R}^d$ tal que $P(i) = E_{pos, i}$.
IV. Portanto, a fun√ß√£o $P$ mapeia cada posi√ß√£o $i$ em $\{0, 1, \ldots, N-1\}$ para um vetor $P(i)$ em $\mathbb{R}^d$. ‚ñ†

**Observa√ß√£o 1:**
Embora simples de implementar, essa abordagem pode apresentar limita√ß√µes, especialmente quando o comprimento da sequ√™ncia √© muito grande, ou quando o modelo tem poucos exemplos de treinamento nas posi√ß√µes mais distantes do in√≠cio.

**Proposi√ß√£o 1**
O espa√ßo vetorial definido por embeddings posicionais absolutos tem dimens√£o finita e √© limitado pelo comprimento m√°ximo da sequ√™ncia $N$ e pela dimens√£o do embedding $d$.
*Proof:*
I. Pelo Lema 1, cada posi√ß√£o $i$ √© mapeada para um vetor $P[i] \in \mathbb{R}^d$.
II. O conjunto de todas as posi√ß√µes √© $\{0, 1, \ldots, N-1\}$, que cont√©m $N$ elementos.
III. Portanto, existem no m√°ximo $N$ vetores distintos $P[i]$, cada um com dimens√£o $d$.
IV. O espa√ßo vetorial √© gerado por esses $N$ vetores, e como eles t√™m dimens√£o $d$, o espa√ßo vetorial tem dimens√£o no m√°ximo $N \cdot d$. Como $N$ e $d$ s√£o finitos, o espa√ßo √© finito.
V. Assim, o espa√ßo vetorial de embeddings posicionais absolutos tem dimens√£o finita, limitada por $N$ e $d$. ‚ñ†

**Positional Embeddings Utilizando Fun√ß√µes Est√°ticas**

Uma alternativa √† abordagem de embeddings absolutos aprendidos √© o uso de **fun√ß√µes est√°ticas** para gerar positional embeddings [^2]. Essa t√©cnica emprega fun√ß√µes matem√°ticas, como seno e cosseno, para mapear cada posi√ß√£o da sequ√™ncia em um vetor de embedding. As fun√ß√µes s√£o definidas de modo a que posi√ß√µes pr√≥ximas tenham embeddings similares, e posi√ß√µes mais distantes tenham embeddings mais diferentes. Essa abordagem elimina a necessidade de aprender os positional embeddings durante o treinamento, sendo que eles s√£o criados a priori, antes mesmo de o modelo ser treinado.

As fun√ß√µes de seno e cosseno s√£o usadas com diferentes frequ√™ncias para gerar os diferentes componentes do vetor de embedding posicional. Para um vetor de embedding com dimens√£o $d$, o componente $j$ do positional embedding $P[i]$ para a posi√ß√£o $i$ √© dado por:

 $$P[i]_j = \begin{cases}
               \sin\left(\frac{i}{10000^{2j/d}}\right), & \text{se } j \text{ √© par} \\
               \cos\left(\frac{i}{10000^{(2j-1)/d}}\right), & \text{se } j \text{ √© √≠mpar}
            \end{cases}$$

Onde $j$ varia de $1$ a $d$, e $d$ √© a dimens√£o do embedding. O fator $10000$ √© uma constante que controla a varia√ß√£o das frequ√™ncias e √© um hiperpar√¢metro do modelo.

> üí° **Exemplo Num√©rico:** Suponha que a dimens√£o do embedding seja $d = 4$ e queiramos calcular os embeddings posicionais para as posi√ß√µes 0, 1 e 2:

*   **Para a posi√ß√£o 0:**
    *   $P[0]_1 = \sin(0/10000^0) = \sin(0) = 0$
    *   $P[0]_2 = \cos(0/10000^{1/4}) = \cos(0) = 1$
    *   $P[0]_3 = \sin(0/10000^{1/2}) = \sin(0) = 0$
    *   $P[0]_4 = \cos(0/10000^{3/4}) = \cos(0) = 1$

    Portanto, $P[0] = [0, 1, 0, 1]$.

*   **Para a posi√ß√£o 1:**
    *   $P[1]_1 = \sin(1/10000^0) = \sin(1) \approx 0.841$
    *   $P[1]_2 = \cos(1/10000^{1/4}) = \cos(0.1) \approx 0.995$
    *   $P[1]_3 = \sin(1/10000^{1/2}) = \sin(0.01) \approx 0.010$
    *   $P[1]_4 = \cos(1/10000^{3/4}) = \cos(0.001) \approx 0.999$

    Portanto, $P[1] \approx [0.841, 0.995, 0.010, 0.999]$.

*   **Para a posi√ß√£o 2:**
    *   $P[2]_1 = \sin(2/10000^0) = \sin(2) \approx 0.909$
    *   $P[2]_2 = \cos(2/10000^{1/4}) = \cos(0.2) \approx 0.980$
    *   $P[2]_3 = \sin(2/10000^{1/2}) = \sin(0.02) \approx 0.020$
    *   $P[2]_4 = \cos(2/10000^{3/4}) = \cos(0.002) \approx 0.999$

    Portanto, $P[2] \approx [0.909, 0.980, 0.020, 0.999]$.

    Note que os vetores $P[0], P[1]$ e $P[2]$ s√£o distintos. Al√©m disso, vetores de posi√ß√µes pr√≥ximas s√£o mais similares do que vetores de posi√ß√µes mais distantes.

> üí° **Exemplo Num√©rico:** Vamos visualizar como os positional embeddings gerados pelas fun√ß√µes est√°ticas variam com a posi√ß√£o para uma dimens√£o de embedding $d=20$. Usaremos um gr√°fico para isso.

```python
import numpy as np
import matplotlib.pyplot as plt

def get_positional_encoding(position, d_model):
    angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model) // 2)) / np.float32(d_model))
    pos_encodings = position * angle_rates
    pos_encodings[::2] = np.sin(pos_encodings[::2])
    pos_encodings[1::2] = np.cos(pos_encodings[1::2])
    return pos_encodings

d_model = 20
max_pos = 100
pos_encodings = np.array([get_positional_encoding(pos, d_model) for pos in range(max_pos)])

plt.figure(figsize=(10, 6))
plt.pcolormesh(pos_encodings.T, cmap='viridis')
plt.xlabel("Position")
plt.ylabel("Embedding Dimension")
plt.title("Positional Embeddings Variation")
plt.colorbar()
plt.show()
```

Este c√≥digo gera um heatmap que mostra como os valores do positional embedding variam ao longo das dimens√µes do embedding e das posi√ß√µes. As cores mais claras e escuras representam valores maiores e menores, respectivamente.  Este gr√°fico demonstra visualmente como cada dimens√£o do positional embedding varia de forma senoidal e cossenosoidal com diferentes frequ√™ncias, dependendo da sua posi√ß√£o dentro do vetor, como expresso na f√≥rmula.

**Lema 2**
A abordagem de positional embeddings com fun√ß√µes est√°ticas define uma fun√ß√£o $P: \mathbb{N} \rightarrow \mathbb{R}^d$ que mapeia cada posi√ß√£o inteira $i$ para um vetor $P[i]$ em $\mathbb{R}^d$, onde os vetores $P[i]$ s√£o calculados utilizando as fun√ß√µes seno e cosseno com diferentes frequ√™ncias.
*Proof:*
I. Seja $d$ a dimens√£o do embedding posicional, e a fun√ß√£o $P$ √© definida como:
      $$P[i]_j = \begin{cases}
               \sin\left(\frac{i}{10000^{2j/d}}\right), & \text{se } j \text{ √© par} \\
               \cos\left(\frac{i}{10000^{(2j-1)/d}}\right), & \text{se } j \text{ √© √≠mpar}
            \end{cases}$$
II. Para cada posi√ß√£o inteira $i$, o vetor $P[i]$ √© calculado utilizando as fun√ß√µes seno e cosseno com diferentes frequ√™ncias para cada componente $j$ do vetor, onde $j$ varia de $1$ a $d$.
III. Portanto, a fun√ß√£o $P$ mapeia cada posi√ß√£o inteira $i$ para um vetor $P[i]$ em $\mathbb{R}^d$ baseado nas fun√ß√µes trigonom√©tricas e na dimensionalidade $d$. ‚ñ†

**Observa√ß√£o 2:**
Essa abordagem tem a vantagem de ser mais generaliz√°vel a sequ√™ncias de comprimentos maiores, j√° que as fun√ß√µes seno e cosseno s√£o peri√≥dicas e podem ser avaliadas para qualquer valor inteiro de posi√ß√£o.

**Teorema 1**
Os positional embeddings gerados por fun√ß√µes est√°ticas possuem uma propriedade de deslocamento linear, ou seja, para um deslocamento fixo $k$, existe uma matriz $M_k$ tal que $P[p+k] = M_k P[p]$.
*Proof:*
I.  A componente $j$ do positional embedding na posi√ß√£o $i$ √© dada por:
   $$P[i]_j = \begin{cases}
               \sin\left(\frac{i}{10000^{2j/d}}\right), & \text{se } j \text{ √© par} \\
               \cos\left(\frac{i}{10000^{(2j-1)/d}}\right), & \text{se } j \text{ √© √≠mpar}
            \end{cases}$$
II.  Pelas identidades trigonom√©tricas, sabemos que:
    $$\sin(a + b) = \sin(a)\cos(b) + \cos(a)\sin(b)$$
    $$\cos(a + b) = \cos(a)\cos(b) - \sin(a)\sin(b)$$
III.  Assim, para cada componente $j$ e um deslocamento $k$, podemos escrever $P[p+k]_j$ como uma combina√ß√£o linear de $\sin$ e $\cos$ do termo $\frac{p}{10000^{\alpha}}$, onde $\alpha$ √© $2j/d$ ou $(2j-1)/d$ dependendo se $j$ √© par ou √≠mpar, e uma combina√ß√£o linear de $\sin$ e $\cos$ do termo $\frac{k}{10000^{\alpha}}$.
IV. Isso significa que existe uma matriz $M_k$ que transforma o vetor $P[p]$ em $P[p+k]$.
V. Portanto, $P[p+k] = M_k P[p]$ para um deslocamento fixo $k$. ‚ñ†

**Propriedade da Transforma√ß√£o Linear**

Uma propriedade interessante dos positional embeddings gerados com fun√ß√µes de seno e cosseno √© que, para um deslocamento fixo $k$, o embedding da posi√ß√£o $p+k$ pode ser obtido por uma transforma√ß√£o linear do embedding da posi√ß√£o $p$, i.e., existe uma matriz $M_k$ tal que $P[p+k] = M_k P[p]$. Essa propriedade permite que o modelo aprenda a modelar depend√™ncias posicionais relativas entre tokens. A prova dessa propriedade foi detalhada no cap√≠tulo anterior, e √© consequ√™ncia das identidades trigonom√©tricas para a soma de √¢ngulos [^2].
A relev√¢ncia pr√°tica dessa propriedade reside na capacidade do modelo de aprender padr√µes de depend√™ncia posicional que se mant√™m consistentes para diferentes posi√ß√µes ao longo da sequ√™ncia. Essa generaliza√ß√£o permite que o modelo processe sequ√™ncias longas com maior efici√™ncia, j√° que n√£o precisa reaprender padr√µes para cada posi√ß√£o.

**Positional Embeddings Relativos**

Ao inv√©s de codificar posi√ß√µes absolutas, algumas arquiteturas Transformer usam **positional embeddings relativos**. Nesta abordagem, o embedding posicional depende da diferen√ßa entre a posi√ß√£o de dois tokens, ao inv√©s da posi√ß√£o absoluta de cada token.  Essa abordagem geralmente √© implementada na camada de aten√ß√£o, em vez de ser adicionada como parte do input [^2].
> üí° **Exemplo Num√©rico:** Para dois tokens nas posi√ß√µes $i$ e $j$, o positional embedding relativo dependeria da dist√¢ncia $|i - j|$, e n√£o das posi√ß√µes $i$ e $j$ isoladamente. Assim, se a dist√¢ncia fosse 2, o embedding posicional seria sempre o mesmo, independentemente se os tokens estivessem nas posi√ß√µes 1 e 3, ou 5 e 7.

Essa t√©cnica permite que o modelo aprenda a modelar rela√ß√µes de proximidade entre os tokens, em vez de focar apenas em suas posi√ß√µes absolutas.

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde temos tr√™s tokens em uma sequ√™ncia.  Os tokens est√£o nas posi√ß√µes 1, 3 e 4. Usando embeddings relativos, o modelo calcularia embeddings baseados nas dist√¢ncias:
>  * Dist√¢ncia entre token 1 e token 3: $|1 - 3| = 2$
>  * Dist√¢ncia entre token 1 e token 4: $|1 - 4| = 3$
>  * Dist√¢ncia entre token 3 e token 4: $|3 - 4| = 1$
>
> Suponha que os embeddings correspondentes a essas dist√¢ncias sejam:
>   * $R[2] = [0.5, -0.2, 0.1]$
>   * $R[3] = [-0.1, 0.3, 0.4]$
>   * $R[1] = [0.2, 0.1, -0.3]$
>
>   Na camada de aten√ß√£o, esses embeddings seriam usados para ponderar as rela√ß√µes entre os tokens, dando mais peso para as rela√ß√µes de dist√¢ncia menor, por exemplo.

**Lema 3**
Os positional embeddings relativos podem ser formalmente definidos como uma fun√ß√£o $R: \mathbb{Z} \rightarrow \mathbb{R}^d$ que mapeia a diferen√ßa entre as posi√ß√µes $i$ e $j$ para um vetor $R[i-j]$ em $\mathbb{R}^d$.
*Proof:*
I. Seja $R$ uma fun√ß√£o que mapeia a diferen√ßa entre posi√ß√µes para embeddings posicionais relativos.
II. A fun√ß√£o $R$ recebe como entrada um inteiro $i-j$, que representa a dist√¢ncia relativa entre duas posi√ß√µes $i$ e $j$.
III. Para cada diferen√ßa $i-j$, a fun√ß√£o $R$ retorna um vetor $R[i-j]$ em $\mathbb{R}^d$, onde $d$ √© a dimens√£o do embedding.
IV. Portanto, a fun√ß√£o $R$ formalmente mapeia diferen√ßas de posi√ß√µes para embeddings posicionais relativos no espa√ßo $\mathbb{R}^d$. ‚ñ†

**Combina√ß√£o com Token Embeddings**

Independentemente da abordagem utilizada, os positional embeddings s√£o combinados com os token embeddings por meio de adi√ß√£o. Essa soma permite que o modelo processe simultaneamente as informa√ß√µes sem√¢nticas das palavras e suas posi√ß√µes na sequ√™ncia.
Assim, dada uma sequ√™ncia de tokens, com seus respectivos token embeddings $E[id(i)]$ e positional embeddings $P[i]$, a representa√ß√£o da entrada $X$ para o modelo √© dada por:
    $$X = E[id(i)] + P[i]$$
Onde $X$ √© a representa√ß√£o combinada do token e sua posi√ß√£o, que ser√° utilizada pelas camadas de aten√ß√£o do Transformer.

> üí° **Exemplo Num√©rico:** Imagine que temos um token com token embedding $E[id(i)] = [0.7, 0.2, -0.5]$ e o positional embedding correspondente para a posi√ß√£o desse token seja $P[i] = [0.1, -0.2, 0.3]$. A representa√ß√£o combinada da entrada seria:

> $X = E[id(i)] + P[i] = [0.7, 0.2, -0.5] + [0.1, -0.2, 0.3] = [0.8, 0.0, -0.2]$.

>  O vetor resultante $X$ √© agora o input para as camadas de aten√ß√£o do Transformer.

**Observa√ß√£o 3:**
A soma dos positional embeddings com os token embeddings √© uma opera√ß√£o simples, mas eficaz. Essa combina√ß√£o preserva informa√ß√µes sem√¢nticas dos tokens e informa√ß√µes posicionais. Opera√ß√µes mais complexas, como concatena√ß√£o, poderiam ser usadas, mas geralmente a adi√ß√£o resulta em melhor performance na pr√°tica.

### Conclus√£o

Neste cap√≠tulo, exploramos as diferentes abordagens para a cria√ß√£o de positional embeddings em modelos Transformer. Vimos que tanto os embeddings absolutos aprendidos quanto os embeddings calculados por fun√ß√µes est√°ticas e as abordagens relativas desempenham um papel crucial na capacidade do modelo de entender a ordem e a estrutura das sequ√™ncias de entrada. A escolha do m√©todo de positional embedding mais adequado depender√° da tarefa espec√≠fica e das caracter√≠sticas do conjunto de dados. A compreens√£o dessas diferentes t√©cnicas de codifica√ß√£o posicional √© essencial para construir modelos de linguagem eficazes e para explorar diferentes arquiteturas de Transformers. Nos pr√≥ximos cap√≠tulos, investigaremos como essas representa√ß√µes de entrada s√£o processadas pelas camadas de aten√ß√£o do Transformer, e como essa arquitetura possibilita um modelo de linguagem poderoso.

### Refer√™ncias
[^2]: Jurafsky, Daniel; Martin, James H. (2023). *Speech and Language Processing*. Draft of February 3, 2024.
<!-- END -->
