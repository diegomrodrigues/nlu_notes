## Estimativa do N√∫mero de Par√¢metros em Modelos Transformer

### Introdu√ß√£o
Este cap√≠tulo aprofunda o entendimento sobre as **scaling laws** e sua rela√ß√£o com a arquitetura dos modelos **Transformer**, focando especificamente na estimativa do n√∫mero de par√¢metros n√£o-embedding [^1, ^27]. Nos cap√≠tulos anteriores, exploramos como as scaling laws descrevem a rela√ß√£o entre o desempenho de um LLM e o tamanho do modelo, a quantidade de dados de treinamento e o *compute budget*. Agora, investigaremos como o n√∫mero de par√¢metros pode ser estimado atrav√©s da an√°lise da arquitetura do modelo, fornecendo *insights* sobre as propriedades de escalabilidade e os requisitos computacionais.

### Estimativa do N√∫mero de Par√¢metros N√£o-Embedding
Em um modelo **Transformer**, o n√∫mero de par√¢metros n√£o-embedding √© crucial para determinar a capacidade de aprendizado e a complexidade do modelo [^27]. Esses par√¢metros, juntamente com a quantidade de dados de treinamento e o *compute budget*, desempenham um papel fundamental no desempenho final do modelo. O n√∫mero de par√¢metros n√£o-embedding $N$ pode ser estimado atrav√©s de uma an√°lise das dimens√µes das camadas, sendo o resultado dessa an√°lise expresso como:

$$N \approx 2d \cdot n_{\text{layer}}(2d_{\text{attn}} + d_{\text{ff}})$$

Onde:
- $d$ representa a dimensionalidade das embeddings de entrada e sa√≠da do modelo [^1, ^27].
- $n_{\text{layer}}$ √© o n√∫mero de camadas no modelo Transformer [^1, ^27].
- $d_{\text{attn}}$ √© a dimensionalidade das camadas de autoaten√ß√£o [^1, ^27].
- $d_{\text{ff}}$ √© a dimensionalidade da camada feedforward [^1, ^27].

√â comum, em arquiteturas Transformer, utilizar a seguinte rela√ß√£o entre essas dimens√µes:

$$d_{\text{attn}} = \frac{d_{\text{ff}}}{4} = d$$

Com essa rela√ß√£o, a f√≥rmula para estimar o n√∫mero de par√¢metros pode ser simplificada para:

$$N \approx 12n_{\text{layer}}d^2$$

Esta f√≥rmula simplificada destaca que o n√∫mero de par√¢metros aumenta linearmente com o n√∫mero de camadas ($n_{\text{layer}}$) e quadraticamente com a dimensionalidade das embeddings ($d$) [^27].

**Lema 8** (Impacto das Dimens√µes das Camadas) A f√≥rmula simplificada $N \approx 12n_{\text{layer}}d^2$ demonstra que a dimensionalidade das embeddings ($d$) tem um impacto quadr√°tico no n√∫mero de par√¢metros do modelo, enquanto o n√∫mero de camadas ($n_{layer}$) tem um impacto linear.

> üí° **Exemplo Num√©rico:**
>
> Consideremos um modelo Transformer com as seguintes caracter√≠sticas:
>
> - Dimensionalidade das embeddings $(d) = 768$
> - N√∫mero de camadas $(n_{layer}) = 12$
>
> Usando a f√≥rmula simplificada, podemos estimar o n√∫mero de par√¢metros:
>
> $$ N \approx 12 \cdot 12 \cdot 768^2 \approx 85 \text{ milh√µes}$$
>
> Agora, vamos dobrar a dimensionalidade das embeddings para $d = 1536$, mantendo o mesmo n√∫mero de camadas:
>
> $$ N \approx 12 \cdot 12 \cdot 1536^2 \approx 340 \text{ milh√µes}$$
>
> O n√∫mero de par√¢metros aumentou aproximadamente 4 vezes.
>
> Agora, vamos dobrar o n√∫mero de camadas para $n_{layer} = 24$, mantendo a dimensionalidade original $d = 768$:
>
> $$ N \approx 12 \cdot 24 \cdot 768^2 \approx 170 \text{ milh√µes}$$
>
> O n√∫mero de par√¢metros dobrou.
>
> Este exemplo ilustra como o impacto no n√∫mero de par√¢metros √© maior ao aumentar a dimensionalidade das embeddings ($d$) em compara√ß√£o com o aumento do n√∫mero de camadas ($n_{layer}$), o que demonstra a natureza quadr√°tica da rela√ß√£o com d e a rela√ß√£o linear com $n_{layer}$.

**Proposi√ß√£o 9** (Impacto Relativo dos Componentes na Complexidade do Modelo)
O n√∫mero de par√¢metros n√£o-embedding $N$ em um modelo Transformer √© diretamente proporcional ao n√∫mero de camadas ($n_{layer}$) e ao quadrado da dimensionalidade das embeddings ($d$). Isso implica que a dimensionalidade das embeddings tem um impacto muito maior na complexidade do modelo do que o n√∫mero de camadas, demonstrando que o desempenho do modelo √© mais sens√≠vel a mudan√ßas na dimensionalidade das embeddings.

*Prova*:
Provaremos que o impacto de aumentar a dimensionalidade das embeddings ($d$) no n√∫mero de par√¢metros $N$ √© quadr√°tico, enquanto o impacto de aumentar o n√∫mero de camadas ($n_{layer}$) √© linear.

I.  A f√≥rmula para o n√∫mero de par√¢metros √© dada por $N \approx 12n_{\text{layer}}d^2$.

II.  Se aumentarmos a dimensionalidade $d$ por um fator $k$, ou seja, $d' = kd$, o novo n√∫mero de par√¢metros $N'$ ser√°:
    $$N' \approx 12n_{\text{layer}}(kd)^2 = 12n_{\text{layer}}k^2d^2 = k^2(12n_{\text{layer}}d^2) = k^2N$$
    Portanto, o n√∫mero de par√¢metros aumenta por um fator $k^2$.

III. Se aumentarmos o n√∫mero de camadas $n_{\text{layer}}$ por um fator $k$, ou seja, $n'_{\text{layer}} = kn_{\text{layer}}$, o novo n√∫mero de par√¢metros $N''$ ser√°:
    $$N'' \approx 12(kn_{\text{layer}})d^2 = k(12n_{\text{layer}}d^2) = kN$$
    Portanto, o n√∫mero de par√¢metros aumenta por um fator $k$.
IV.  Comparando os resultados de II e III, o aumento em $d$ por um fator $k$ leva a um aumento em $N$ por um fator $k^2$, enquanto o aumento em $n_{\text{layer}}$ por um fator $k$ leva a um aumento em $N$ por um fator $k$.

V.  Como $k^2 > k$ para $k > 1$, o aumento na dimensionalidade $d$ tem um impacto desproporcionalmente maior no n√∫mero de par√¢metros $N$ do que o aumento no n√∫mero de camadas $n_{\text{layer}}$.

VI. Portanto, a complexidade do modelo √© mais sens√≠vel a mudan√ßas na dimensionalidade das embeddings do que em mudan√ßas no n√∫mero de camadas. ‚ñ†

**Lema 9.1** (Impacto da Varia√ß√£o Simult√¢nea de $d$ e $n_{layer}$) Se aumentarmos a dimensionalidade $d$ por um fator $k_d$ e o n√∫mero de camadas $n_{layer}$ por um fator $k_n$, o novo n√∫mero de par√¢metros $N'$ ser√°:
    $$N' \approx 12(k_n n_{\text{layer}})(k_d d)^2 = 12 k_n n_{\text{layer}} k_d^2 d^2 = k_n k_d^2 (12n_{\text{layer}}d^2) = k_n k_d^2 N$$
    Isso demonstra que o aumento no n√∫mero de par√¢metros √© dado pelo produto do fator linear de aumento das camadas com o quadrado do fator de aumento da dimensionalidade das embeddings.

> üí° **Exemplo Num√©rico:**
>
> Considere um modelo com $d = 512$ e $n_{layer} = 6$, resultando em:
>
> $N \approx 12 \cdot 6 \cdot 512^2 = 18874368$
>
> Agora, aumentemos $d$ por um fator de $k_d = 1.5$ ($d'=768$) e $n_{layer}$ por um fator de $k_n = 2$ ($n'_{layer}=12$). O novo n√∫mero de par√¢metros ser√°:
>
> $N' \approx 12 \cdot (2 \cdot 6) \cdot (1.5 \cdot 512)^2 = 85032960$
>
> Calculando o fator de aumento:
>
> $k_n k_d^2 = 2 \cdot 1.5^2 = 4.5$
>
> E, de fato, $N' \approx 4.5 \cdot 18874368 \approx 85034656$
>
> Este exemplo mostra o aumento combinado dos dois fatores e sua rela√ß√£o com o aumento total no n√∫mero de par√¢metros, conforme o Lema 9.1.

**Corol√°rio 9.2** (Rela√ß√£o entre Dimens√µes e N√∫mero de Par√¢metros)  Para atingir um aumento espec√≠fico no n√∫mero de par√¢metros, √© mais eficiente aumentar o n√∫mero de camadas em menor propor√ß√£o e a dimensionalidade em uma maior propor√ß√£o, devido √† rela√ß√£o quadr√°tica entre a dimensionalidade das embeddings e o n√∫mero total de par√¢metros.

### Rela√ß√£o com Scaling Laws
A estimativa do n√∫mero de par√¢metros √© crucial para entender as scaling laws, pois o n√∫mero de par√¢metros √© um dos tr√™s fatores que determinam o desempenho de um LLM, como visto nos cap√≠tulos anteriores [^27]. As scaling laws mostram que a performance de um modelo (i.e., a redu√ß√£o da perda) segue uma *power law* com o n√∫mero de par√¢metros, com a quantidade de dados de treinamento e com o *compute budget*, ou seja:

$$L(N) \propto N^{\alpha_N}$$

$$L(D) \propto D^{\alpha_D}$$

$$L(C) \propto C^{\alpha_C}$$

onde $\alpha_N, \alpha_D$ e $\alpha_C$ s√£o os expoentes espec√≠ficos para cada fator [^27].
As rela√ß√µes entre n√∫mero de par√¢metros e desempenho, expressas pelas scaling laws, implicam que:
- Aumentar o n√∫mero de par√¢metros em um modelo, mantendo os outros fatores constantes, geralmente leva a uma melhora no desempenho, ou seja, a uma redu√ß√£o do *loss*. No entanto, os ganhos marginais tornam-se cada vez menores.
- O n√∫mero de par√¢metros deve ser considerado em conjunto com o tamanho do dataset e com o *compute budget* para otimizar o desempenho de um modelo.
- Modelos muito grandes podem demandar um *compute budget* e datasets proporcionalmente maiores para atingir todo o seu potencial, o que pode ser invi√°vel para algumas aplica√ß√µes.

**Corol√°rio 9.1** (Impacto do N√∫mero de Par√¢metros no Desempenho)
Apesar do aumento do n√∫mero de par√¢metros levar a um aumento da capacidade de aprendizado e, consequentemente, redu√ß√£o da perda, √© importante notar que o impacto desse aumento diminui com o aumento do pr√≥prio n√∫mero de par√¢metros, devido √† rela√ß√£o de power law entre o n√∫mero de par√¢metros e o loss. Isso significa que o ganho em performance torna-se cada vez menor ao aumentar o n√∫mero de par√¢metros.

> üí° **Exemplo Num√©rico:**
>
>  Suponha que para um modelo pequeno, com $N_1 = 10^7$ par√¢metros, se observe uma perda de $L_1 = 0.5$. Ap√≥s aumentar o n√∫mero de par√¢metros para $N_2 = 10^8$, ou seja, 10 vezes maior, a perda observada √© de $L_2 = 0.25$. Agora, ao aumentar novamente para $N_3 = 10^9$ (mais 10 vezes), a perda cai para $L_3 = 0.15$. O aumento de $10^7$ para $10^8$ reduziu a perda em 0.25, enquanto o aumento de $10^8$ para $10^9$ reduziu a perda em apenas 0.1. Isso ilustra a diminui√ß√£o dos ganhos marginais com o aumento do n√∫mero de par√¢metros, como demonstrado pelo Corol√°rio 9.1 e scaling laws.

**Teorema 1** (Rela√ß√£o entre Perda, Par√¢metros, Dados e Computa√ß√£o) A perda $L$ de um modelo de linguagem √© uma fun√ß√£o do n√∫mero de par√¢metros $N$, do tamanho do conjunto de dados de treinamento $D$, e do *compute budget* $C$, dada por
$$L(N,D,C) \propto N^{\alpha_N} D^{\alpha_D} C^{\alpha_C}$$
onde $\alpha_N, \alpha_D$ e $\alpha_C$ s√£o os expoentes espec√≠ficos para cada fator, que em geral s√£o negativos. Este teorema encapsula as scaling laws, demonstrando a rela√ß√£o inversa entre a perda e cada um dos tr√™s fatores.

**Observa√ß√£o 1** A rela√ß√£o de *power law* entre a perda e o n√∫mero de par√¢metros, o tamanho do dataset e o *compute budget* sugere que, para obter melhorias significativas no desempenho, √© necess√°rio aumentar os tr√™s fatores. No entanto, como os expoentes s√£o geralmente menores que 1 (em m√≥dulo), os ganhos marginais s√£o decrescentes.

### Aplica√ß√µes Pr√°ticas e Implica√ß√µes
A estimativa do n√∫mero de par√¢metros atrav√©s da arquitetura do modelo e da aplica√ß√£o das scaling laws tem diversas aplica√ß√µes pr√°ticas no treinamento de LLMs:
- **Previs√£o de Desempenho:** Permite estimar o desempenho de um modelo maior com base no treinamento de um modelo menor, auxiliando a prever o *loss* de modelos maiores sem a necessidade de trein√°-los completamente, o que foi explorado em detalhes no cap√≠tulo anterior [^27].
- **Planejamento de Recursos:** Auxilia no planejamento de recursos computacionais, permitindo estimar o custo de treinamento de um determinado modelo.
- **Otimiza√ß√£o da Arquitetura:** Ajuda a otimizar a arquitetura de um modelo, escolhendo a combina√ß√£o ideal entre dimensionalidade das embeddings, n√∫mero de camadas e outros par√¢metros, de modo a equilibrar capacidade de aprendizado e custo computacional.
- **Compara√ß√£o de Modelos:** Permite comparar diferentes modelos, usando o n√∫mero de par√¢metros como m√©trica para comparar a complexidade e a capacidade de aprendizado de diferentes arquiteturas, embora outras considera√ß√µes tamb√©m devam ser feitas.

**Proposi√ß√£o 10** (Trade-off entre Tamanho e Custo)
Embora aumentar o n√∫mero de par√¢metros de um modelo geralmente leve a um melhor desempenho, √© crucial considerar o trade-off entre o tamanho do modelo e o custo computacional. O ponto √≥timo √© alcan√ßado quando o ganho marginal em desempenho √© proporcional ao aumento no custo computacional.

*Prova*:
Provaremos que a decis√£o de aumentar o n√∫mero de par√¢metros deve ser guiada por um equil√≠brio entre o aumento de desempenho e o aumento de custo associado, devido √† lei de retornos decrescentes, representada pela scaling law do n√∫mero de par√¢metros.

I.  A scaling law para o n√∫mero de par√¢metros indica que o *loss* ($L$) √© inversamente proporcional ao n√∫mero de par√¢metros $N$ elevado a um expoente $\alpha_N$, onde $0 < \alpha_N < 1$:
     $$L \propto N^{\alpha_N}$$

II. O aumento no n√∫mero de par√¢metros $N$ tem um custo computacional $c_N$ associado ao treinamento, que, em geral, aumenta com o n√∫mero de par√¢metros.

III. O objetivo √© minimizar a perda $L$ com o menor custo computacional poss√≠vel. A decis√£o de aumentar ou n√£o o n√∫mero de par√¢metros deve considerar um trade-off entre esses dois fatores, ou seja, √© preciso aumentar o n√∫mero de par√¢metros at√© o ponto em que o ganho de performance justifique o aumento de custo.

IV. A decis√£o √≥tima ocorre quando o ganho marginal em desempenho por unidade de custo √© equilibrado. Formalmente,  o trade-off entre tamanho e custo deve ser avaliado de modo a que
    $$ \frac{\partial L}{\partial N} \cdot \frac{1}{c_N} \approx 0$$
    ou seja, o ganho marginal de redu√ß√£o da perda por unidade de custo seja aproximadamente zero, indicando que aumentos maiores de N n√£o justificam o custo.

V. Portanto, n√£o √© suficiente maximizar o n√∫mero de par√¢metros sem considerar o aumento de custo associado, pois isso levar√° a retornos decrescentes de desempenho. A decis√£o √≥tima requer um equil√≠brio entre o desempenho e o custo, para um treinamento eficiente. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Suponha que um modelo com $N = 10^8$ par√¢metros tenha um *loss* $L=0.2$ e um custo de treinamento $c_N = 100$ unidades de custo. Ao aumentar o modelo para $N' = 2 \cdot 10^8$ par√¢metros, o *loss* cai para $L'=0.15$, e o custo aumenta para $c_N' = 250$.
>
> O ganho marginal no *loss* √© $\Delta L = 0.2 - 0.15 = 0.05$, enquanto o aumento de custo √© $\Delta c_N = 250 - 100 = 150$.
>
> O ganho de *loss* por unidade de custo √© $\frac{\Delta L}{\Delta c_N} = \frac{0.05}{150} \approx 0.00033$.
>
> Se aumentar novamente o n√∫mero de par√¢metros para $N''=4 \cdot 10^8$  e o *loss* cair para $L''=0.12$ com um custo $c_N''=500$, o ganho marginal em *loss* seria $\Delta L' = 0.15-0.12 = 0.03$ e o aumento em custo $\Delta c_N' = 500-250 = 250$, resultando em um ganho de *loss* por unidade de custo de $\frac{\Delta L'}{\Delta c_N'} = \frac{0.03}{250} = 0.00012$.
>
>Este exemplo mostra que, √† medida que o modelo cresce, o ganho marginal no *loss* por unidade de custo decresce, o que refor√ßa a Proposi√ß√£o 10. A decis√£o de aumentar o modelo deve equilibrar o ganho em *loss* com o aumento de custo.

**Corol√°rio 10.1** A escolha da arquitetura de um modelo Transformer deve levar em considera√ß√£o n√£o apenas o desempenho esperado (estimado por meio das scaling laws e do n√∫mero de par√¢metros), mas tamb√©m o custo computacional do treinamento. Em alguns casos, modelos menores com arquiteturas mais eficientes podem oferecer um desempenho compar√°vel a modelos maiores, com um custo computacional menor.

**Teorema 2** (Efici√™ncia do Treinamento e Arquitetura) A efici√™ncia do treinamento de um modelo Transformer, medida em termos de desempenho alcan√ßado por unidade de custo computacional, depende n√£o apenas do n√∫mero total de par√¢metros, mas tamb√©m da distribui√ß√£o desses par√¢metros entre dimensionalidade das embeddings ($d$) e n√∫mero de camadas ($n_{layer}$). O ponto √≥timo √© alcan√ßado quando a arquitetura √© otimizada de forma a equilibrar o ganho em desempenho e o custo computacional, levando em considera√ß√£o as rela√ß√µes de *power law* e o trade-off entre tamanho do modelo e custo.

**Proposi√ß√£o 11** (Trade-off entre Tamanho e Tempo de Treinamento) Em geral, modelos maiores tendem a levar mais tempo para treinar do que modelos menores, mesmo com o mesmo *compute budget*. Isso ocorre devido ao aumento da complexidade computacional e ao n√∫mero de opera√ß√µes realizadas durante cada etapa de treinamento. Portanto, o tempo de treinamento tamb√©m deve ser considerado na escolha de uma arquitetura, em conjunto com o desempenho esperado e o custo computacional.

### Conclus√£o
A estimativa do n√∫mero de par√¢metros n√£o-embedding em modelos **Transformer** √© essencial para o entendimento das **scaling laws** e para o desenvolvimento de **Large Language Models** mais eficientes [^27]. A f√≥rmula $N \approx 12n_{\text{layer}}d^2$ e a rela√ß√£o de *power law* com o desempenho, demonstrada pelas scaling laws, permitem prever o impacto de mudan√ßas na arquitetura do modelo no seu desempenho, auxiliando na aloca√ß√£o de recursos computacionais e de dados. A compreens√£o do trade-off entre tamanho do modelo, custo computacional e desempenho √© crucial para o desenvolvimento de modelos de linguagem cada vez mais avan√ßados.

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^27]: Kaplan, J., S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. 2020. Scaling laws for neural language models. ArXiv preprint.
<!-- END -->
