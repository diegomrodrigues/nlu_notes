## Large Language Models e Aquisi√ß√£o de Conhecimento
### Introdu√ß√£o
Este cap√≠tulo explora aprofundadamente como os **Large Language Models (LLMs)** adquirem conhecimento substancial a partir de grandes volumes de texto por meio de *pretraining*. Este processo permite que os LLMs demonstrem desempenhos not√°veis em diversas tarefas de linguagem natural, especialmente na gera√ß√£o de texto. A aquisi√ß√£o de conhecimento √© baseada na **hip√≥tese distribucional**, um conceito central que j√° foi apresentado no Cap√≠tulo 6 [^1], onde o significado das palavras √© aprendido atrav√©s de padr√µes de co-ocorr√™ncia no texto, sem necessidade de instru√ß√£o direta ou ancoragem no mundo real [^1]. O processo de aquisi√ß√£o de vocabul√°rio, um fen√¥meno complexo, ocorre principalmente de forma indireta atrav√©s da leitura e do processamento de texto, enfatizando a import√¢ncia do aprendizado contextual e da hip√≥tese distribucional [^1]. Este cap√≠tulo tamb√©m abordar√° o papel central da arquitetura *transformer* e de seus mecanismos de *self-attention* nesse processo de aquisi√ß√£o.

### Conceitos Fundamentais
#### Aquisi√ß√£o de Vocabul√°rio e a Hip√≥tese Distribucional
A aquisi√ß√£o de vocabul√°rio por humanos √© um processo notavelmente eficiente e cont√≠nuo, em que se estima que as crian√ßas aprendem entre 7 a 10 palavras por dia para atingir o tamanho do vocabul√°rio de um adulto [^1]. Este crescimento n√£o √© primariamente resultado de instru√ß√£o direta em escolas, mas ocorre principalmente como um subproduto da leitura e do processamento de texto [^1]. Este fen√¥meno √© fundamental para a compreens√£o da efic√°cia dos LLMs, que tamb√©m utilizam o mesmo princ√≠pio, de acordo com a hip√≥tese distribucional [^1].

A hip√≥tese distribucional, mencionada no cap√≠tulo 6 [^1], afirma que o significado de uma palavra pode ser inferido pelas palavras com as quais ela co-ocorre, formando associa√ß√µes complexas no texto [^1]. Essas associa√ß√µes permitem que o conhecimento adquirido seja utilizado muito tempo depois da sua aquisi√ß√£o inicial. Este princ√≠pio √© crucial para a capacidade dos LLMs de compreender e gerar texto coerente, mesmo sem uma compreens√£o intr√≠nseca do mundo real [^1].
> üí° **Exemplo Num√©rico:** Considere a palavra "banco". Em um contexto como "Vou ao banco sacar dinheiro", a palavra "banco" co-ocorre com "sacar" e "dinheiro", sugerindo a sua acep√ß√£o como uma institui√ß√£o financeira. Em um outro contexto como "O banco do jardim estava cheio de flores", a palavra "banco" co-ocorre com "jardim" e "flores", indicando seu significado como um assento. A hip√≥tese distribucional afirma que o modelo aprende esses diferentes significados atrav√©s de padr√µes de co-ocorr√™ncia, sem que explicitamente lhe seja ensinado a diferen√ßa entre essas duas acep√ß√µes.

**Lema 1**
A capacidade de um modelo de linguagem em generalizar e reutilizar conhecimento adquirido atrav√©s da hip√≥tese distribucional √© diretamente proporcional √† diversidade e ao volume de dados de treinamento. Uma maior variedade de contextos de co-ocorr√™ncia leva a representa√ß√µes mais robustas e adapt√°veis.

*Proof strategy.* Este lema estabelece uma conex√£o direta entre a qualidade dos dados de *pretraining* e o desempenho do modelo. A intui√ß√£o √© que quanto mais variados forem os contextos nos quais uma palavra √© observada, mais o modelo pode entender e generalizar seu significado, capturando nuances que podem passar despercebidas com dados limitados.

#### *Pretraining* e *Large Language Models*
O *pretraining* formaliza a ideia de aprender sobre a linguagem e o mundo a partir de grandes quantidades de texto [^2]. Os LLMs, que s√£o resultado desse processo, exibem capacidades not√°veis em v√°rias tarefas de linguagem natural devido ao conhecimento adquirido durante o *pretraining* [^2]. Eles s√£o particularmente transformadores em tarefas de gera√ß√£o de texto como sumariza√ß√£o, tradu√ß√£o autom√°tica, resposta a perguntas e chatbots [^2].

A arquitetura padr√£o para construir LLMs √© o *transformer*, que utiliza um mecanismo denominado **self-attention** [^2]. O *self-attention*, que se desenvolveu a partir da ideia de *attention* introduzida para RNNs no Cap√≠tulo 9 [^2], permite que o modelo construa representa√ß√µes contextuais do significado de uma palavra, integrando informa√ß√µes de palavras vizinhas [^2]. Essa capacidade de integrar informa√ß√µes de longos trechos de texto ajuda os LLMs a entender como as palavras se relacionam em contextos mais amplos [^2].

**Teorema 1**
O *pretraining* de LLMs com arquitetura *transformer*, utilizando o mecanismo de *self-attention* e alimentado por grandes volumes de texto, induz a emerg√™ncia de representa√ß√µes sem√¢nticas contextuais que capturam rela√ß√µes sint√°ticas e sem√¢nticas complexas.

*Proof strategy.*  Este teorema afirma que o *pretraining* com *transformers* leva a representa√ß√µes contextuais sofisticadas. A prova se baseia em estudos emp√≠ricos que mostram que as camadas internas de um *transformer* aprendem a representar hierarquias sint√°ticas e rela√ß√µes sem√¢nticas, com *self-attention* desempenhando um papel crucial na captura dessas rela√ß√µes.

#### O Mecanismo de *Self-Attention*
A intui√ß√£o por tr√°s do *transformer* √© a constru√ß√£o de representa√ß√µes contextuais mais ricas e complexas dos significados das palavras ou *tokens* de entrada atrav√©s de uma s√©rie de camadas [^3]. Em cada camada, a representa√ß√£o de uma palavra √© combinada com informa√ß√µes das representa√ß√µes de palavras vizinhas da camada anterior, visando produzir uma representa√ß√£o contextualizada para cada posi√ß√£o [^3]. O mecanismo de *self-attention* permite que o modelo avalie e combine as representa√ß√µes de diferentes palavras no contexto, considerando sua relev√¢ncia e conex√µes lingu√≠sticas.

Por exemplo, no trecho *(10.1) The keys to the cabinet are on the table.* [^3], o mecanismo de *self-attention* permite que o modelo identifique que "keys" √© o sujeito e que a concord√¢ncia com o verbo "are" √© necess√°ria. Em *(10.2) The chicken crossed the road because it wanted to get to the other side.* [^3], o pronome "it" √© associado a "chicken", estabelecendo a correfer√™ncia. Em *(10.3) I walked along the pond, and noticed that one of the trees along the bank had fallen into the water after the storm.* [^3], o contexto, incluindo "pond" e "water," define o sentido da palavra "bank". Essas rela√ß√µes contextuais s√£o cruciais para o entendimento e gera√ß√£o de texto coerente.
> üí° **Exemplo Num√©rico:** Vamos ilustrar como o *self-attention* pode operar em uma frase simplificada. Considere a frase "O gato preto dorme." Suponha que a representa√ß√£o inicial (embedding) de cada palavra seja um vetor bidimensional. Vamos usar valores simplificados para ilustrar o conceito:
> - $x_{gato} = [1, 0]$
> - $x_{preto} = [0, 1]$
> - $x_{dorme} = [1, 1]$
>
>  Vamos simplificar ainda mais, considerando que $W^Q$, $W^K$, e $W^V$ s√£o matrizes identidade. Assim, $q_i = x_i$, $k_i = x_i$, e $v_i = x_i$.
>
>  1.  **C√°lculo dos *scores***: Para cada palavra, calculamos o *dot product* com todas as outras palavras.
>    - $score(gato, gato) = [1, 0] \cdot [1, 0] = 1$
>    - $score(gato, preto) = [1, 0] \cdot [0, 1] = 0$
>    - $score(gato, dorme) = [1, 0] \cdot [1, 1] = 1$
>   - $score(preto, gato) = [0, 1] \cdot [1, 0] = 0$
>   - $score(preto, preto) = [0, 1] \cdot [0, 1] = 1$
>   - $score(preto, dorme) = [0, 1] \cdot [1, 1] = 1$
>  - $score(dorme, gato) = [1, 1] \cdot [1, 0] = 1$
>   - $score(dorme, preto) = [1, 1] \cdot [0, 1] = 1$
>   - $score(dorme, dorme) = [1, 1] \cdot [1, 1] = 2$
>
>  2. **Softmax**: Aplicamos softmax aos scores para obter os pesos de aten√ß√£o.
>  - Para "gato": $\alpha_{gato} = softmax([1, 0, 1]) \approx [0.42, 0.16, 0.42]$
>  - Para "preto": $\alpha_{preto} = softmax([0, 1, 1]) \approx [0.16, 0.42, 0.42]$
> - Para "dorme": $\alpha_{dorme} = softmax([1, 1, 2]) \approx [0.10, 0.10, 0.80]$
>
>  3. **Soma ponderada**: Calculamos a representa√ß√£o final como a soma ponderada dos *values* (que neste caso s√£o os pr√≥prios embeddings).
>
>  -  $a_{gato} = 0.42*[1,0] + 0.16*[0,1] + 0.42*[1,1] = [0.84, 0.58]$
> - $a_{preto} = 0.16*[1,0] + 0.42*[0,1] + 0.42*[1,1] = [0.58, 0.84]$
> - $a_{dorme} = 0.10*[1,0] + 0.10*[0,1] + 0.80*[1,1] = [0.9, 0.9]$
>
>  A representa√ß√£o contextualizada de "gato" agora inclui informa√ß√µes de "dorme", dado que ambos t√™m uma alta pontua√ß√£o. Do mesmo modo, a representa√ß√£o contextualizada de "dorme" enfatiza sua rela√ß√£o consigo mesma e com as outras palavras do contexto. Em um modelo real, essas representa√ß√µes seriam muito maiores e mais ricas em significado. Este exemplo num√©rico ilustra a ideia b√°sica de como o mecanismo de *self-attention* combina informa√ß√µes de diferentes partes da senten√ßa, ponderando a relev√¢ncia de cada palavra.
Formalmente, o *self-attention* compara um item de interesse a outros itens no contexto para revelar sua relev√¢ncia [^5]. A compara√ß√£o utiliza o *dot product* entre os vetores de palavras, calculando um *score* que indica a similaridade entre elas [^5]. Esses *scores* s√£o normalizados atrav√©s de uma fun√ß√£o *softmax*, criando um vetor de pesos que representa a import√¢ncia de cada palavra no contexto [^5]. A representa√ß√£o final de uma palavra √© obtida por meio de uma soma ponderada das representa√ß√µes de palavras anteriores, com os pesos indicando a relev√¢ncia de cada uma.

Para capturar diferentes fun√ß√µes das palavras no contexto, o *transformer* usa matrizes de peso ($W^Q$, $W^K$, e $W^V$) que projetam vetores de entrada em representa√ß√µes de *query*, *key*, e *value*, respectivamente [^6]. O *score* entre uma palavra de interesse ($x_i$) e as outras palavras no contexto ($x_j$) √© calculado como o *dot product* do vetor *query* ($q_i$) e do vetor *key* ($k_j$), $score(x_i, x_j) = q_i \cdot k_j$ [^6]. A sa√≠da do *self-attention* √© uma soma ponderada dos vetores *value* ($v_j$), $a_i = \sum_j \alpha_{ij}v_j$ [^6], em que $\alpha_{ij}$ s√£o os pesos calculados via *softmax* [^5]. O *softmax* assegura que os pesos sejam normalizados e que a contribui√ß√£o de cada palavra para a representa√ß√£o final seja proporcional √† sua relev√¢ncia no contexto [^5].

O processo de *self-attention* √© escal√°vel, o que permite a constru√ß√£o de modelos cada vez maiores e mais capazes. A aplica√ß√£o de *self-attention* em todo o contexto √© feita de forma paralela, aproveitando a capacidade de processamento simult√¢neo de m√∫ltiplas unidades de processamento [^7].

##### Aten√ß√£o Causal e *Masking*
O *self-attention* pode ser aplicado de duas maneiras: causal e bidirecional [^4]. No contexto do modelamento da linguagem, usa-se aten√ß√£o causal ou retroativa, em que o contexto para cada palavra inclui apenas as palavras anteriores [^4]. Na aten√ß√£o bidirecional, o contexto pode incluir palavras futuras [^4], mas esta √© explorada no Cap√≠tulo 11 [^4]. Para garantir que o modelo de linguagem n√£o tenha acesso a informa√ß√µes futuras durante o treinamento, utiliza-se uma t√©cnica de *masking*, que zera as entradas da matriz de *scores* para as palavras que est√£o √† frente da atual [^8]. Essa t√©cnica impede que o modelo utilize informa√ß√µes futuras durante a predi√ß√£o de palavras.
> üí° **Exemplo Num√©rico:** Considere a frase "O gato preto". Ao calcular o *self-attention* para a palavra "gato" em um modelo causal, os scores entre "gato" e as palavras futuras seriam mascarados:
>
> |       | O    | gato | preto |
> | :---- | :--- | :--- | :---- |
> | **O**   |  $s_{OO}$  | $s_{Og}$  | $s_{Op}$   |
> | **gato** | $s_{gO}$ | $s_{gg}$  |    $0$    |
> | **preto**| $s_{pO}$ | $s_{pg}$ |  $s_{pp}$  |
>
> Os valores $s_{ij}$ representam o score entre as palavras. Veja que o *score* entre "gato" e "preto" ($s_{gp}$) √© zerado pelo *masking*, impedindo que a palavra "gato" tenha acesso √† informa√ß√£o da palavra "preto" ao calcular a sua representa√ß√£o. Ao processar a palavra "preto", a matriz completa de scores estar√° dispon√≠vel, e sua representa√ß√£o ser√° constru√≠da considerando as palavras "O" e "gato".
>
> ```mermaid
>  graph LR
>      A[O] --> B(gato)
>      B --> C(preto)
>      style B fill:#f9f,stroke:#333,stroke-width:2px
> ```
> A seta representa a ordem temporal e a informa√ß√£o dispon√≠vel ao computar a representa√ß√£o contextual de uma palavra. A seta entre "gato" e "preto" n√£o existiria no caso de aten√ß√£o causal, ilustrando o papel do *masking*.

**Proposi√ß√£o 1**
A t√©cnica de *masking* na aten√ß√£o causal √© essencial para o treinamento adequado de modelos de linguagem. Ela garante que o modelo n√£o aprenda a prever palavras futuras a partir do seu pr√≥prio futuro, mas sim a partir do seu passado e o contexto anterior, mantendo a propriedade de causalidade.

*Proof strategy.* Esta proposi√ß√£o destaca a necessidade do *masking* para evitar "olhar para o futuro". Sem essa t√©cnica, o modelo tenderia a "colar" informa√ß√£o das palavras √† frente, comprometendo a capacidade de realmente modelar a distribui√ß√£o de probabilidade das pr√≥ximas palavras.

*Prova da Proposi√ß√£o 1:*
I. Considere um modelo de linguagem que tem como objetivo prever a pr√≥xima palavra em uma sequ√™ncia.
II. Se o modelo, durante o treinamento, tiver acesso √†s palavras futuras, ele simplesmente "copiaria" essa informa√ß√£o, n√£o aprendendo a rela√ß√£o entre o contexto anterior e a palavra seguinte.
III. O *masking* impede que o modelo utilize informa√ß√µes das palavras futuras ao zerar as entradas correspondentes na matriz de *scores* durante o c√°lculo do *self-attention*.
IV. Desta forma, o modelo √© for√ßado a aprender a prever a pr√≥xima palavra com base apenas no contexto anterior.
V. Isso garante que o modelo aprenda a rela√ß√£o condicional $P(w_t | w_1, w_2, \ldots, w_{t-1})$, que √© a base do modelamento de linguagem causal.
VI. Portanto, o *masking* preserva a propriedade de causalidade e permite que o modelo seja treinado adequadamente. ‚ñ†

#### *Multi-Head Attention*
Os *transformers* empregam *multi-head self-attention* para capturar as diferentes rela√ß√µes entre as palavras de um texto [^9]. Cada *head* em uma camada de *self-attention* tem seu pr√≥prio conjunto de matrizes de peso, permitindo que diferentes aspectos das rela√ß√µes entre as entradas sejam aprendidos de forma paralela [^9]. As sa√≠das de cada *head* s√£o concatenadas e projetadas para a dimens√£o original [^9], produzindo uma representa√ß√£o rica e diversificada da entrada.
> üí° **Exemplo Num√©rico:** Imagine que temos uma frase simples, "o grande c√£o corre rapidamente". Em um modelo *multi-head attention* com dois *heads*, um *head* pode se concentrar em rela√ß√µes sint√°ticas como a rela√ß√£o entre "c√£o" e "corre" (sujeito-verbo), enquanto o outro *head* pode se concentrar em modificadores, como a rela√ß√£o entre "grande" e "c√£o" ou "rapidamente" e "corre". Cada *head* tem suas pr√≥prias matrizes $W^Q$, $W^K$, e $W^V$, que s√£o treinadas separadamente para capturar diferentes aspectos das rela√ß√µes entre as palavras. As representa√ß√µes aprendidas pelos dois *heads* s√£o combinadas, permitindo que o modelo capture m√∫ltiplas facetas das rela√ß√µes entre as palavras.

**Teorema 1.1**
A *multi-head attention* aumenta a expressividade do modelo ao permitir que ele capture diferentes rela√ß√µes lingu√≠sticas simultaneamente. Cada *head* funciona como um sub-modelo que aprende uma representa√ß√£o espec√≠fica de aten√ß√£o, e a combina√ß√£o dessas representa√ß√µes proporciona um entendimento mais robusto e completo do texto.

*Proof strategy.* Este teorema expande o Teorema 1, explicando que a *multi-head attention* permite que o modelo aprenda diversas facetas das rela√ß√µes entre as palavras. A combina√ß√£o das representa√ß√µes produzidas por cada *head* leva a uma representa√ß√£o mais abrangente do contexto.

*Prova do Teorema 1.1:*
I.  Considere um modelo *transformer* com uma √∫nica camada de *self-attention* (single-head).
II.  Esta camada tem um √∫nico conjunto de matrizes de proje√ß√£o $W^Q$, $W^K$, e $W^V$.
III.  Portanto, a camada *single-head* √© capaz de aprender apenas um √∫nico conjunto de rela√ß√µes entre as palavras.
IV. Agora, considere um modelo *transformer* com *multi-head attention*, onde existem $h$ *heads*.
V.  Cada *head* possui seu pr√≥prio conjunto de matrizes de proje√ß√£o $W^Q_i$, $W^K_i$, e $W^V_i$ para $i = 1, \ldots, h$.
VI. Cada *head* aprende um conjunto diferente de rela√ß√µes entre as palavras.
VII.  As sa√≠das de cada *head* s√£o concatenadas e projetadas novamente para a dimens√£o original, produzindo uma representa√ß√£o que integra todas as rela√ß√µes aprendidas pelos diferentes *heads*.
VIII. Assim, a *multi-head attention* permite que o modelo aprenda m√∫ltiplos aspectos das rela√ß√µes entre as palavras simultaneamente.
IX.  Este aprendizado paralelo aumenta a expressividade do modelo, permitindo que ele capture rela√ß√µes lingu√≠sticas mais complexas.
X.  Portanto, a *multi-head attention* leva a uma compreens√£o mais robusta e completa do texto do que uma √∫nica camada de *self-attention* (single-head). ‚ñ†

#### *Transformer Blocks*
O c√°lculo de *self-attention* √© o n√∫cleo do **transformer block**, que tamb√©m inclui outros tipos de camadas: *feedforward layers*, conex√µes residuais e camadas de normaliza√ß√£o [^9]. As *feedforward layers* s√£o redes totalmente conectadas que processam cada posi√ß√£o de forma independente [^10]. As conex√µes residuais permitem que informa√ß√µes de camadas inferiores alcancem camadas superiores, melhorando o aprendizado [^11]. As camadas de normaliza√ß√£o, como a *layer norm*, ajudam a manter as ativa√ß√µes em uma escala que facilite o treinamento [^11]. Esses blocos s√£o empilhados para criar redes *transformers* mais profundas e poderosas [^12].
> üí° **Exemplo Num√©rico:** Um bloco *transformer* pode receber como entrada um tensor de dimens√£o (batch_size, sequence_length, embedding_dimension). Suponha que temos um batch de 3 frases, cada frase com 10 tokens, e a dimens√£o dos embeddings seja 512. A entrada seria um tensor de dimens√µes (3, 10, 512). O *self-attention* opera dentro de cada sequ√™ncia, gerando um tensor de mesma dimens√£o. Em seguida, a *feedforward layer* processa cada token individualmente, tamb√©m preservando a dimens√£o (3, 10, 512). As conex√µes residuais somam a entrada original com a sa√≠da da *feedforward layer*, e a normaliza√ß√£o garante que a escala da sa√≠da permane√ßa adequada. Essa sa√≠da √© ent√£o passada para o pr√≥ximo *transformer block*.

#### O Papel do *Residual Stream*
Uma perspectiva alternativa para entender os *transformers* √© atrav√©s do **residual stream**, um fluxo de representa√ß√µes d-dimensionais que atravessa as camadas de um modelo [^13]. Cada *token* de entrada possui um vetor de *embedding* que flui atrav√©s das camadas, com conex√µes residuais e *feedforward networks* adicionando novas perspectivas sobre essa representa√ß√£o. A *self-attention* √© o √∫nico componente que recebe informa√ß√µes de outros *tokens* no contexto [^13].

**Lema 2**
O *residual stream* facilita o fluxo de informa√ß√£o atrav√©s das camadas, permitindo que o modelo mantenha acesso √†s representa√ß√µes iniciais dos *tokens* enquanto aprende representa√ß√µes mais abstratas nas camadas superiores. Isso auxilia na mitiga√ß√£o do problema de desvanecimento do gradiente em modelos profundos.

*Proof strategy.* Este lema detalha o funcionamento do *residual stream*, explicando que ele age como um atalho para a informa√ß√£o. Ele tamb√©m explica como a passagem direta da representa√ß√£o inicial para as camadas superiores mitiga o desvanecimento do gradiente, um desafio comum em redes muito profundas.

*Prova do Lema 2:*
I. Considere um *transformer* com v√°rias camadas, onde a sa√≠da da camada $l$ √© $h_l$.
II. Sem conex√µes residuais, a sa√≠da da camada $l+1$ seria fun√ß√£o apenas de $h_l$, ou seja, $h_{l+1} = F(h_l)$, onde $F$ representa o processamento de uma camada.
III. Com as conex√µes residuais, a sa√≠da da camada $l+1$ √© $h_{l+1} = F(h_l) + h_l$.
IV. A representa√ß√£o da camada $l$ ($h_l$) √© adicionada diretamente √† sa√≠da da camada $l+1$, criando um "atalho" para a informa√ß√£o.
V. Isso permite que o modelo mantenha acesso √†s representa√ß√µes das camadas inferiores mesmo nas camadas superiores.
VI. Durante o treinamento, o gradiente da fun√ß√£o de perda √© propagado de volta atrav√©s das camadas.
VII. As conex√µes residuais facilitam a passagem do gradiente sem desvanecer ou explodir, pois a derivada da fun√ß√£o de soma √© sempre 1.
VIII. Essa propaga√ß√£o mais eficiente permite que camadas mais profundas sejam treinadas de forma eficaz.
IX. Portanto, o *residual stream* auxilia na mitiga√ß√£o do desvanecimento do gradiente e na manuten√ß√£o do acesso √†s representa√ß√µes iniciais dos *tokens*. ‚ñ†

#### Arquiteturas *Prenorm* e *Postnorm*
Existem duas arquiteturas comuns para a organiza√ß√£o das camadas de normaliza√ß√£o nos *transformers*: *prenorm* e *postnorm* [^13]. Na arquitetura *prenorm*, a *layer norm* √© aplicada antes da camada de *self-attention* e antes da camada *feedforward* [^14]. Na arquitetura *postnorm* (a descrita inicialmente), a normaliza√ß√£o ocorre ap√≥s essas camadas. A arquitetura *prenorm* frequentemente resulta em melhor desempenho em diversas aplica√ß√µes.
> üí° **Exemplo Num√©rico:** Imagine um bloco *transformer* com a entrada $X$. Na arquitetura *prenorm*, a ordem das opera√ß√µes seria:
>
> 1.  $X_{norm} = LayerNorm(X)$
> 2.  $X_{attention} = SelfAttention(X_{norm})$
> 3.  $X_{residual} = X + X_{attention}$
> 4.  $X_{norm2} = LayerNorm(X_{residual})$
> 5.  $X_{feedforward} = FeedForward(X_{norm2})$
> 6.  $Output = X_{residual} + X_{feedforward}$
>
> Na arquitetura *postnorm*, a ordem seria:
>
> 1. $X_{attention} = SelfAttention(X)$
> 2. $X_{residual} = X + X_{attention}$
> 3. $X_{norm} = LayerNorm(X_{residual})$
> 4. $X_{feedforward} = FeedForward(X_{norm})$
> 5. $Output = X_{norm} + X_{feedforward}$
>
> A diferen√ßa principal est√° no momento em que a normaliza√ß√£o ocorre. Na arquitetura *prenorm*, ela ocorre *antes* do processamento por *self-attention* e *feedforward*, enquanto na arquitetura *postnorm*, ela ocorre *depois*. A arquitetura *prenorm* tende a ter melhor desempenho em alguns casos.

#### *Embeddings* de *Tokens* e Posi√ß√£o
A entrada de um *transformer* √© uma matriz X, que cont√©m um *embedding* para cada palavra no contexto [^14]. Esse *embedding* √© composto por dois componentes: o *embedding* do *token* e o *embedding* da posi√ß√£o [^14]. O *embedding* do *token* captura o significado da palavra, enquanto o *embedding* da posi√ß√£o captura a ordem das palavras no contexto. Existem diversas formas de modelar o *embedding* de posi√ß√£o, desde *embeddings* aprendidos para cada posi√ß√£o at√© fun√ß√µes fixas que codificam a rela√ß√£o entre as posi√ß√µes [^16].
> üí° **Exemplo Num√©rico:** Suponha que a representa√ß√£o de cada palavra (token) √© um vetor de 100 dimens√µes. Para a frase "O gato corre", ter√≠amos tr√™s vetores de 100 dimens√µes, um para cada palavra. Adicionalmente, um vetor de *embedding* de posi√ß√£o √© somado ao *embedding* de cada palavra. No caso de usar *embeddings* aprendidos, podemos ter uma matriz de *embeddings* de posi√ß√£o de tamanho (max_sequence_length, embedding_dim), ou seja, se o tamanho m√°ximo de sequ√™ncia for 1024 e a dimens√£o dos embeddings for 100, ter√≠amos uma matriz de (1024, 100). O vetor de *embedding* de posi√ß√£o para a primeira palavra seria o vetor de √≠ndice 0, para a segunda, o de √≠ndice 1, e assim por diante. No caso de embeddings posicionais fixos, poder√≠amos usar senos e cossenos de diferentes frequ√™ncias para representar as diferentes posi√ß√µes.
>
> Exemplo simplificado (vetores de 2 dimens√µes):
> -  *Token embeddings*: $e_O=[1,0]$, $e_{gato}=[0,1]$, $e_{corre}=[1,1]$
> - *Positional embeddings*: $p_1=[0.1, 0.1]$, $p_2=[0.2, 0.2]$, $p_3=[0.3, 0.3]$
> - *Input embeddings*: $x_1 = e_O + p_1 = [1.1, 0.1]$, $x_2 = e_{gato} + p_2 = [0.2, 1.2]$, $x_3 = e_{corre} + p_3 = [1.3, 1.3]$.
>
> Note que o *embedding* final cont√©m informa√ß√µes tanto sobre a palavra quanto sobre sua posi√ß√£o na sequ√™ncia.

**Proposi√ß√£o 2**
A inclus√£o de *embeddings* de posi√ß√£o √© fundamental para que *transformers* capturem informa√ß√µes sequenciais no texto. Sem essa informa√ß√£o, o modelo trataria todas as palavras como permut√°veis, perdendo a informa√ß√£o da ordem, que √© essencial para o entendimento da linguagem.

*Proof strategy.* Esta proposi√ß√£o destaca a import√¢ncia de *embeddings* de posi√ß√£o. Sem essa informa√ß√£o, o *transformer* n√£o conseguiria diferenciar entre "o gato comeu o rato" e "o rato comeu o gato", j√° que ambas as frases cont√©m as mesmas palavras.

*Prova da Proposi√ß√£o 2:*
I. Considere um *transformer* que recebe como entrada uma sequ√™ncia de palavras $w_1, w_2, \ldots, w_n$.
II. Cada palavra $w_i$ √© convertida em um vetor de *embedding* $e_i$.
III. Se o modelo n√£o tiver *embeddings* de posi√ß√£o, a entrada para o modelo seria simplesmente a sequ√™ncia de *embeddings* $e_1, e_2, \ldots, e_n$.
IV. Neste caso, a ordem das palavras n√£o estaria codificada na entrada, ou seja, a permuta√ß√£o da ordem das palavras n√£o alteraria a entrada do modelo.
V. No entanto, a ordem das palavras √© crucial para o entendimento da linguagem. Por exemplo, as senten√ßas "o gato comeu o rato" e "o rato comeu o gato" t√™m significados opostos.
VI. Para que o modelo capture essa informa√ß√£o, a entrada deve codificar a posi√ß√£o de cada palavra na sequ√™ncia.
VII. Os *embeddings* de posi√ß√£o s√£o vetores $p_1, p_2, \ldots, p_n$ que codificam a posi√ß√£o de cada palavra.
VIII. A entrada final para o modelo √© ent√£o a soma dos *embeddings* de *token* e os *embeddings* de posi√ß√£o: $e_1 + p_1, e_2 + p_2, \ldots, e_n + p_n$.
IX. Portanto, a inclus√£o de *embeddings* de posi√ß√£o permite que o modelo capture informa√ß√µes sequenciais no texto, que s√£o essenciais para o entendimento da linguagem. ‚ñ†

#### *Language Modeling Head*
O componente final para um modelo de linguagem baseado em *transformers* √© o **language modeling head** [^16]. Este componente transforma o *embedding* final da √∫ltima camada do *transformer* em uma distribui√ß√£o de probabilidade sobre as palavras do vocabul√°rio [^17]. O *language modeling head* geralmente inclui uma camada linear seguida por uma fun√ß√£o *softmax* [^17]. A matriz linear do *language modeling head* √© frequentemente amarrada √† matriz de *embedding* do *token*, usando a mesma matriz para as duas transforma√ß√µes [^17].
> üí° **Exemplo Num√©rico:** Considere um vocabul√°rio de 10000 palavras. A sa√≠da do *transformer* para um token √© um vetor de tamanho 512 (por exemplo). O *language modeling head* multiplica esse vetor por uma matriz de peso $W$ de tamanho (512, 10000) e ent√£o aplica a fun√ß√£o *softmax*. A sa√≠da √© um vetor de 10000 elementos, em que cada elemento representa a probabilidade do token ser a pr√≥xima palavra na sequ√™ncia. A matriz de peso $W$ √© muitas vezes compartilhada com a matriz de *embedding* dos tokens, ou seja, ela √© usada para transformar o *embedding* de tokens em representa√ß√µes intermedi√°rias dentro do *transformer*, al√©m de transformar as representa√ß√µes finais em probabilidade de palavras.

### Conclus√£o
Os LLMs, constru√≠dos sobre a arquitetura *transformer*, demonstram uma capacidade impressionante de adquirir conhecimento a partir de grandes volumes de texto [^2]. A chave para essa capacidade reside na hip√≥tese distribucional, que permite que o significado das palavras seja aprendido a partir de seus padr√µes de co-ocorr√™ncia, e no mecanismo de *self-attention*, que possibilita que o modelo construa representa√ß√µes contextuais ricas e complexas [^1]. Esses modelos s√£o capazes de gerar texto coerente e relevante, al√©m de resolver uma variedade de tarefas de linguagem natural [^2]. Os detalhes da arquitetura, o *pretraining*, os algoritmos de amostragem, e as t√©cnicas de treinamento ser√£o abordados com mais profundidade nos pr√≥ximos cap√≠tulos [^2].

### Refer√™ncias
[^1]: Cap√≠tulo 6.
[^2]: [^2]
[^3]: [^3]
[^4]: [^4]
[^5]: [^5]
[^6]: [^6]
[^7]: [^7]
[^8]: [^8]
[^9]: [^9]
[^10]: [^10]
[^11]: [^11]
[^12]: [^12]
[^13]: [^13]
[^14]: [^14]
[^16]: [^16]
[^17]: [^17]
<!-- END -->
