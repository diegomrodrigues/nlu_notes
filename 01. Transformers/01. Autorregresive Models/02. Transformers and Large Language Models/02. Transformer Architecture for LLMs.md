## Arquitetura Transformer e Modelagem de Linguagem
### Introdu√ß√£o
Como vimos anteriormente, os *Large Language Models (LLMs)* obt√™m seu conhecimento por meio de *pretraining* em vastos conjuntos de dados textuais, o que lhes permite desempenhar uma ampla gama de tarefas de linguagem natural [^2]. Este processo de aquisi√ß√£o de conhecimento √© fundamentado na hip√≥tese distribucional, na qual os significados das palavras s√£o aprendidos a partir de seus padr√µes de co-ocorr√™ncia no texto [^1]. Este cap√≠tulo ir√° aprofundar a arquitetura *Transformer*, demonstrando como ela se tornou o padr√£o para a constru√ß√£o de LLMs e explorando seus mecanismos de *self-attention* e como eles capturam depend√™ncias de longo alcance, sem as limita√ß√µes dos modelos recorrentes [^2]. Vamos nos concentrar tamb√©m como os *transformers* s√£o empregados em modelos de linguagem, particularmente na arquitetura decoder-only.

### Conceitos Fundamentais
#### Arquitetura *Transformer* e *Self-Attention*
A arquitetura *Transformer*, introduzida no Cap√≠tulo 10, √© uma rede neural que n√£o se baseia em conex√µes recorrentes, mas sim em um mecanismo chamado *self-attention* [^2]. O *self-attention* permite que o modelo avalie e combine as representa√ß√µes de diferentes palavras no contexto, considerando sua relev√¢ncia e conex√µes lingu√≠sticas [^3]. Isso possibilita que o modelo capture depend√™ncias de longo alcance no texto sem as limita√ß√µes de processamento sequencial dos modelos recorrentes [^3].

Os *transformers* s√£o compostos por pilhas de *transformer blocks*, cada um dos quais inclui uma camada de *self-attention*, uma *feedforward layer*, conex√µes residuais e camadas de normaliza√ß√£o [^3, 9, 10, 11]. Essa arquitetura modular permite a constru√ß√£o de redes mais profundas e complexas, que podem ser treinadas de forma eficiente devido √† natureza paraleliz√°vel do *self-attention* [^3, 7].

**Corol√°rio 1.1**
A capacidade de paraleliza√ß√£o do *self-attention* e a arquitetura modular dos *transformers* permitem o treinamento de modelos muito maiores e com mais camadas, sem as limita√ß√µes de treinamento de modelos recorrentes. O corol√°rio se deriva do fato de que opera√ß√µes com matrizes podem ser otimizadas para processamento em GPUs e outros hardwares de alto desempenho.

**Lema 1.1**
A camada de *self-attention* pode ser expressa como uma s√©rie de opera√ß√µes matriciais, que s√£o altamente paraleliz√°veis. Especificamente, o c√°lculo da aten√ß√£o envolve tr√™s matrizes: *Query (Q), Key (K), e Value (V)*, que s√£o obtidas da proje√ß√£o linear da entrada. A aten√ß√£o √© ent√£o calculada como *Attention(Q, K, V) = softmax(QK<sup>T</sup>/‚àöd<sub>k</sub>)V*, onde d<sub>k</sub> √© a dimens√£o das matrizes *Q* e *K*.
*Proof strategy.* O lema se baseia na formula√ß√£o matem√°tica do mecanismo de self-attention, que envolve a multiplica√ß√£o de matrizes e a aplica√ß√£o da fun√ß√£o softmax, opera√ß√µes que podem ser eficientemente implementadas em hardware de processamento paralelo.

> üí° **Exemplo Num√©rico:**
> Considere o processamento de uma frase como "O c√£o corre rapidamente pelo parque". Em um modelo recorrente (como um LSTM), o processamento seria sequencial, palavra por palavra:
>
> *   "O" ‚Üí estado oculto 1
> *   estado oculto 1 + "c√£o" ‚Üí estado oculto 2
> *   estado oculto 2 + "corre" ‚Üí estado oculto 3
> *   ... e assim por diante.
>
> O c√°lculo do estado oculto no tempo *t* depende do estado oculto no tempo *t-1*. Isso impede o processamento paralelo.
>
> Em um *transformer*, cada palavra √© transformada em um vetor e esses vetores s√£o processados em paralelo:
>
> * "O" ‚Üí vetor 1
> * "c√£o" ‚Üí vetor 2
> * "corre" ‚Üí vetor 3
> * ... e assim por diante.
>
> O mecanismo de *self-attention* permite que cada vetor "atenda" aos outros vetores da sequ√™ncia, de forma paralela. As opera√ß√µes de multiplica√ß√£o matricial podem ser executadas em paralelo, permitindo um uso mais eficiente do hardware de processamento. Isso significa que um *transformer* pode ser treinado muito mais rapidamente do que um modelo recorrente para a mesma quantidade de dados.
>
>Para demonstrar o c√°lculo de self-attention de forma mais concreta, vamos considerar um exemplo simplificado com apenas 3 palavras e dimens√µes reduzidas. Vamos supor que temos a seguinte sequ√™ncia de palavras tokenizadas e j√° convertidas em embeddings de dimens√£o 4:
>
> "O gato comeu"
>
> Que foram convertidas nos seguintes embeddings:
>
> $w_1 = \begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}, w_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \\ 1 \end{bmatrix}, w_3 = \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}$
>
>Para este exemplo, assumiremos que as matrizes de proje√ß√£o para Q, K e V s√£o as seguintes, cada uma de dimens√£o 4x3 (note que em um cen√°rio real, essas matrizes seriam aprendidas durante o treinamento e teriam uma dimens√£o muito maior):
>
> $W_Q = \begin{bmatrix} 1 & 0 & 0 & 1 \\ 0 & 1 & 1 & 0 \\ 1 & 0 & 1 & 0 \end{bmatrix}$,  $W_K = \begin{bmatrix} 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 1 & 0 \end{bmatrix}$,  $W_V = \begin{bmatrix} 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix}$
>
> **Passo 1: Calcular Q, K, V**
>
> Primeiro, calculamos as matrizes Q, K, e V multiplicando cada embedding pelas matrizes de proje√ß√£o correspondentes:
>
> $Q = W_Q \cdot W =  \begin{bmatrix} 1 & 0 & 0 & 1 \\ 0 & 1 & 1 & 0 \\ 1 & 0 & 1 & 0 \end{bmatrix}  \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 0 & 1 \\ 2 & 1 & 1\end{bmatrix}$
>
> $K = W_K \cdot W = \begin{bmatrix} 0 & 1 & 0 & 1 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 1 & 0 \end{bmatrix}  \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}= \begin{bmatrix} 0 & 2 & 1 \\ 2 & 0 & 1 \\ 0 & 1 & 1\end{bmatrix}$
>
> $V = W_V \cdot W = \begin{bmatrix} 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 1 \\ 1 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}  = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 1 & 0 \\ 2 & 0 & 1\end{bmatrix}$
>
> **Passo 2: Calcular a matriz de aten√ß√£o**
>
> A matriz de aten√ß√£o √© calculada por $QK^T$, onde $K^T$ √© a matriz transposta de $K$. Como a dimens√£o das matrizes $Q$ e $K$ s√£o 3x3, o produto $QK^T$ ser√° 3x3.
>
> $QK^T = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 0 & 1 \\ 2 & 1 & 1\end{bmatrix} \begin{bmatrix} 0 & 2 & 0 \\ 2 & 0 & 1 \\ 1 & 1 & 1\end{bmatrix} = \begin{bmatrix} 3 & 3 & 2 \\ 1 & 3 & 1 \\ 3 & 5 & 2\end{bmatrix}$
>
> **Passo 3: Escalar e aplicar softmax**
>
> Em seguida, dividimos a matriz resultante pela raiz quadrada da dimens√£o de Q e K, que √© $\sqrt{3} \approx 1.73$. Depois, aplicamos a fun√ß√£o softmax a cada linha:
>
> $A_{scaled} = \frac{QK^T}{\sqrt{3}} = \begin{bmatrix} 1.73 & 1.73 & 1.15 \\ 0.57 & 1.73 & 0.57 \\ 1.73 & 2.88 & 1.15\end{bmatrix}$
>
> Aplicando a fun√ß√£o softmax em cada linha de $A_{scaled}$, teremos a matriz de pesos de aten√ß√£o:
>
> $AttentionWeights = softmax(A_{scaled}) = \begin{bmatrix} 0.38 & 0.38 & 0.24 \\ 0.21 & 0.60 & 0.21 \\ 0.21 & 0.70 & 0.09 \end{bmatrix}$
>
> **Passo 4: Calcular a sa√≠da da self-attention**
>
> Finalmente, calculamos a sa√≠da da camada de self-attention, multiplicando a matriz de pesos de aten√ß√£o pela matriz $V$.
>
> $Attention(Q,K,V) = AttentionWeights \cdot V = \begin{bmatrix} 0.38 & 0.38 & 0.24 \\ 0.21 & 0.60 & 0.21 \\ 0.21 & 0.70 & 0.09 \end{bmatrix} \begin{bmatrix} 1 & 1 & 2 \\ 1 & 1 & 0 \\ 2 & 0 & 1\end{bmatrix} = \begin{bmatrix} 1.24 & 0.76 & 1.00 \\ 1.03 & 0.81 & 1.03 \\ 0.99 & 0.91 & 0.53\end{bmatrix}$
>
> Cada linha da matriz resultante representa o embedding contextualizado de cada palavra na sequ√™ncia original. √â crucial notar que este √© um exemplo simplificado e, em um cen√°rio real, a dimens√£o dos embeddings e das matrizes de proje√ß√£o s√£o muito maiores. A fun√ß√£o softmax tamb√©m √© fundamental para normalizar os pesos de aten√ß√£o, garantindo que eles sejam n√£o-negativos e somem 1, representando uma distribui√ß√£o de probabilidade.

#### Arquitetura *Encoder-Decoder* e *Decoder-Only*
Existem dois tipos principais de arquitetura *Transformer*: *encoder-decoder* e *decoder-only*. A arquitetura *encoder-decoder*, tamb√©m discutida no Cap√≠tulo 13 [^2], √© composta por um *encoder*, que processa a sequ√™ncia de entrada e a transforma em um estado interno, e um *decoder*, que gera a sequ√™ncia de sa√≠da a partir desse estado. Essa arquitetura √© tipicamente usada em tarefas como tradu√ß√£o autom√°tica, em que h√° uma sequ√™ncia de entrada em uma l√≠ngua e uma sequ√™ncia de sa√≠da em outra [^2].

A arquitetura *decoder-only*, por outro lado, √© usada em modelos de linguagem, como os da fam√≠lia GPT [^2]. Nesta arquitetura, a entrada √© uma sequ√™ncia de *tokens*, e a sa√≠da √© a predi√ß√£o do pr√≥ximo *token* [^2]. Esta arquitetura √© mais adequada para modelagem de linguagem, pois ela opera de forma autoregressiva, predizendo cada palavra a partir das palavras anteriores [^2, 22]. A sa√≠da do *transformer* √© uma sequ√™ncia de *embeddings* contextuais, que s√£o ent√£o processados por um *language modeling head* para produzir a distribui√ß√£o de probabilidade sobre o vocabul√°rio [^16, 17].

**Defini√ß√£o 1**
Um modelo *decoder-only* √© um tipo de modelo *transformer* que processa uma sequ√™ncia de entrada para produzir uma representa√ß√£o contextualizada de cada *token* e ent√£o utiliza essa representa√ß√£o para predizer o pr√≥ximo *token* na sequ√™ncia.

**Corol√°rio 1.2**
O uso da arquitetura *decoder-only* em modelos de linguagem permite uma gera√ß√£o de texto autoregressiva, em que a predi√ß√£o de cada palavra depende das palavras anteriores, resultando em texto coerente e contextual.

*Proof strategy.* O corol√°rio deriva da natureza autoregressiva do modelo decoder-only. Dado que cada palavra √© predita com base nas palavras anteriores, isso cria uma cadeia causal que garante que o texto gerado seja semanticamente e sintaticamente coerente.

*Prova do Corol√°rio 1.2:*
I. Um modelo *decoder-only* recebe como entrada uma sequ√™ncia de *tokens* $w_1, w_2, \ldots, w_n$.
II. O modelo *transformer* processa essa sequ√™ncia, gerando uma representa√ß√£o contextualizada para cada *token*, incluindo um *embedding* $h_n$ para o √∫ltimo *token* $w_n$.
III. O *language modeling head* transforma o *embedding* $h_n$ em uma distribui√ß√£o de probabilidade sobre o vocabul√°rio $P(w_{n+1} | w_1, w_2, \ldots, w_n)$.
IV. O pr√≥ximo *token* $w_{n+1}$ √© selecionado a partir dessa distribui√ß√£o, seja atrav√©s de *greedy decoding*, *top-k sampling* ou outro m√©todo de amostragem [^21, 22, 23, 24].
V. Essa sa√≠da torna-se parte da sequ√™ncia de entrada na pr√≥xima itera√ß√£o, ou seja a sequ√™ncia passa a ser $w_1, w_2, \ldots, w_n, w_{n+1}$.
VI.  Assim, a predi√ß√£o do pr√≥ximo *token* $w_{n+2}$ depende de todos os *tokens* anteriores $w_1, w_2, \ldots, w_{n+1}$.
VII. Essa natureza autoregressiva garante que cada palavra seja predita com base no contexto anterior, o que leva √† gera√ß√£o de texto coerente e semanticamente consistente. ‚ñ†

**Observa√ß√£o 1**
A arquitetura *decoder-only* pode ser vista como um caso especial da arquitetura *encoder-decoder* onde o *encoder* e o *decoder* compartilham a mesma estrutura e o *encoder* √© simplesmente omitido. Na pr√°tica, a entrada para o *decoder* √© a pr√≥pria sequ√™ncia de entrada que se pretende modelar, sem necessidade de uma codifica√ß√£o separada.

#### Processamento de Sequ√™ncias com *Transformers*
Em um modelo de linguagem, a entrada para um *transformer* √© uma sequ√™ncia de palavras ou *tokens*, que s√£o convertidos em *embeddings* [^3, 15]. O *transformer* usa o mecanismo de *self-attention* para ponderar a import√¢ncia de cada *token* na sequ√™ncia, permitindo que o modelo capture rela√ß√µes de longo alcance [^3, 5]. Essa capacidade de capturar depend√™ncias de longo alcance √© uma das principais vantagens dos *transformers* em rela√ß√£o aos modelos recorrentes, como LSTMs, que t√™m dificuldade em manter informa√ß√µes de longa dist√¢ncia no contexto [^3].

A sa√≠da de um *transformer* √© uma sequ√™ncia de *embeddings* contextuais, que representam o significado de cada palavra na sequ√™ncia, considerando o seu contexto [^3]. Para modelos de linguagem, a √∫ltima camada de *embedding* √© ent√£o passada para um *language modeling head*, que prediz o pr√≥ximo *token* [^16, 17].

**Teorema 2**
A arquitetura *Transformer*, com seu mecanismo de *self-attention* e processamento paralelo, supera as limita√ß√µes de modelos recorrentes em rela√ß√£o √† captura de depend√™ncias de longo alcance em sequ√™ncias de texto.

*Proof strategy.*  Este teorema estabelece a superioridade dos transformers sobre os modelos recorrentes. A prova se baseia em estudos emp√≠ricos que mostram que os transformers conseguem capturar rela√ß√µes sint√°ticas e sem√¢nticas em longos trechos de texto, algo que os modelos recorrentes, devido ao processamento sequencial e problemas de vanishing gradient, n√£o conseguem fazer t√£o bem.

*Prova do Teorema 2:*
I. Considere um modelo recorrente (como um LSTM) processando uma sequ√™ncia de *tokens* $w_1, w_2, \ldots, w_n$.
II. O modelo processa os *tokens* de forma sequencial, mantendo um estado oculto que √© atualizado a cada passo.
III. A informa√ß√£o das palavras anteriores √© armazenada no estado oculto, que √© propagado ao longo da sequ√™ncia.
IV. No entanto, a informa√ß√£o de *tokens* distantes na sequ√™ncia pode ser perdida ou desvanecer √† medida que o estado oculto √© atualizado repetidamente.
V. Isso limita a capacidade de modelos recorrentes de capturar depend√™ncias de longo alcance.
VI. Agora, considere um modelo *transformer* processando a mesma sequ√™ncia de *tokens*.
VII. O mecanismo de *self-attention* permite que cada *token* tenha acesso a todos os outros *tokens* da sequ√™ncia, sem processamento sequencial.
VIII. A representa√ß√£o contextualizada de cada *token* √© constru√≠da a partir da combina√ß√£o de informa√ß√µes de todos os *tokens* na sequ√™ncia.
IX. Isso permite que o modelo capture depend√™ncias entre *tokens* distantes na sequ√™ncia, sem perder informa√ß√£o.
X. Portanto, o *transformer*, com seu mecanismo de *self-attention* e processamento paralelo, supera as limita√ß√µes de modelos recorrentes em rela√ß√£o √† captura de depend√™ncias de longo alcance em sequ√™ncias de texto. ‚ñ†

**Teorema 2.1**
O mecanismo de *self-attention* permite que os *transformers* aprendam representa√ß√µes contextuais ricas e diversificadas para cada *token* na sequ√™ncia, ao ponderar a import√¢ncia de cada *token* em rela√ß√£o aos outros.

*Proof strategy.* Este teorema expande o Teorema 2, detalhando o mecanismo de self-attention como a chave para a representa√ß√£o contextual rica. A prova se baseia na an√°lise do funcionamento interno do mecanismo de *self-attention*, como descrito no Lema 1.1.

*Prova do Teorema 2.1:*
I. De acordo com o Lema 1.1, cada *token* na sequ√™ncia √© transformado em vetores *Query (Q), Key (K), e Value (V)*.
II. O mecanismo de *self-attention* calcula os pesos de aten√ß√£o como $softmax(QK^T/\sqrt{d_k})$, onde $d_k$ √© a dimens√£o das matrizes *Q* e *K*.
III. Esses pesos de aten√ß√£o indicam a import√¢ncia de cada *token* em rela√ß√£o aos outros.
IV. A sa√≠da da camada de *self-attention* √© uma combina√ß√£o ponderada dos vetores *Value (V)*, onde os pesos s√£o os pesos de aten√ß√£o calculados.
V. Cada *token* recebe uma representa√ß√£o contextualizada que √© uma combina√ß√£o de informa√ß√µes de todos os outros *tokens* na sequ√™ncia, ponderada por sua import√¢ncia.
VI. Assim, o mecanismo de *self-attention* permite que os *transformers* aprendam representa√ß√µes contextuais ricas e diversificadas para cada *token* na sequ√™ncia. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Para ilustrar como um modelo *decoder-only* processa uma sequ√™ncia e prediz o pr√≥ximo *token*, vamos considerar um exemplo simplificado. Suponha que o modelo j√° tenha processado a sequ√™ncia "O rato comeu" e precisa predizer o pr√≥ximo *token*.
>
> 1.  **Entrada:** A sequ√™ncia de entrada √© "O rato comeu". Cada palavra √© convertida em um *embedding* e passada para o *transformer*.
> 2.  **Processamento pelo *Transformer*:** O *transformer* usa o mecanismo de *self-attention* para criar representa√ß√µes contextuais para cada palavra. Por exemplo, o *embedding* para "comeu" √© influenciado pelos *embeddings* de "O" e "rato".
> 3.  **Language Modeling Head:** O *embedding* contextualizado para "comeu" √© passado para um *language modeling head*, que √© essencialmente uma camada linear seguida por uma fun√ß√£o *softmax*. Esta camada mapeia o *embedding* para um vetor de probabilidades sobre todo o vocabul√°rio.
> 4. **Predi√ß√£o:** A fun√ß√£o *softmax* produz uma distribui√ß√£o de probabilidade para cada *token* no vocabul√°rio. Suponha que, nesse exemplo, as probabilidades sejam:
>
>     | Token     | Probabilidade |
>     |-----------|---------------|
>     | "queijo"  | 0.6           |
>     | "p√£o"     | 0.2           |
>     | "tudo"    | 0.1           |
>     | "feliz"   | 0.05          |
>     | ...       | ...           |
>
> 5. **Gera√ß√£o:** Usando amostragem, *greedy decoding* ou outro m√©todo, o modelo seleciona o pr√≥ximo *token*. Nesse caso, "queijo" √© o mais prov√°vel e √© selecionado como o pr√≥ximo *token*.
>
> 6. **Autoregress√£o:** O modelo agora considera "O rato comeu queijo" como a nova entrada e repete o processo para predizer o pr√≥ximo *token*, e assim por diante, gerando texto de forma autoregressiva.
>
> Este exemplo ilustra a natureza autoregressiva do modelo *decoder-only*, onde a sa√≠da de cada passo √© usada como parte da entrada para o pr√≥ximo passo, garantindo consist√™ncia e coer√™ncia no texto gerado.

### Conclus√£o
A arquitetura *Transformer*, com seu mecanismo de *self-attention* e processamento paralelo, revolucionou o campo da modelagem de linguagem, tornando poss√≠vel a constru√ß√£o de LLMs que conseguem capturar depend√™ncias de longo alcance, gerar texto coerente e resolver uma variedade de tarefas de linguagem natural [^2]. Os *transformers* s√£o tipicamente empregados em uma arquitetura decoder-only em modelos de linguagem, predizendo o pr√≥ximo *token* em uma sequ√™ncia, o que permite a gera√ß√£o de texto autoregressiva [^2]. A combina√ß√£o desses elementos tem permitido a cria√ß√£o de modelos de linguagem com um desempenho impressionante e √© a base para os *Large Language Models* atuais. Nos pr√≥ximos cap√≠tulos, exploraremos como treinar esses modelos, como us√°-los para gerar texto de forma eficiente e outros aspectos importantes do uso e da implementa√ß√£o dos *transformers*.

### Refer√™ncias
[^2]: [^2]
[^3]: [^3]
[^5]: [^5]
[^7]: [^7]
[^9]: [^9]
[^10]: [^10]
[^11]: [^11]
[^15]: [^15]
[^16]: [^16]
[^17]: [^17]
[^21]: [^21]
[^22]: [^22]
[^23]: [^23]
[^24]: [^24]
<!-- END -->
