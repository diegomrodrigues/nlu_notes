## O Fluxo Residual e os Blocos Transformer
### Introdu√ß√£o
Como vimos nos cap√≠tulos anteriores, os *Large Language Models (LLMs)* s√£o constru√≠dos utilizando a arquitetura *Transformer*, que emprega mecanismos de *self-attention* para capturar depend√™ncias de longo alcance no texto, superando as limita√ß√µes dos modelos recorrentes [^2, 3]. A arquitetura *Transformer*, composta por blocos *encoder-decoder* ou *decoder-only*, tornou-se o padr√£o para LLMs devido √† sua efici√™ncia e capacidade de processamento paralelo [^2]. Modelos de linguagem tipicamente usam uma arquitetura *decoder-only*, onde a entrada √© uma sequ√™ncia de *tokens* e a sa√≠da √© uma predi√ß√£o do pr√≥ximo *token*, juntamente com *embeddings* contextuais [^2, 22]. Este cap√≠tulo se aprofundar√° em um componente chave dos *transformers*: o **fluxo residual**, uma s√©rie de representa√ß√µes *d-dimensionais* que s√£o continuamente refinadas √† medida que passam pelas camadas. Tamb√©m exploraremos os **blocos *transformer***, unidades fundamentais da arquitetura, que s√£o compostos por camadas de *self-attention*, redes *feedforward*, conex√µes residuais e normaliza√ß√£o de camada, repetidamente empilhados para construir redes profundas [^3, 9, 10, 11, 12].

### Conceitos Fundamentais
#### O Papel do Fluxo Residual (*Residual Stream*)
O **fluxo residual** (*residual stream*) √© um conceito fundamental para entender como a informa√ß√£o flui atrav√©s de um *transformer* [^13]. Ele consiste em uma s√©rie de representa√ß√µes *d-dimensionais* (onde *d* √© a dimens√£o dos *embeddings*) que s√£o continuamente refinadas ao passar pelas camadas do modelo [^13]. Cada *token* de entrada possui um *embedding* inicial que serve como ponto de partida para o fluxo residual. √Ä medida que esse *embedding* passa pelas camadas do *transformer*, as sa√≠das das camadas de *feedforward* e *attention* s√£o adicionadas ao fluxo residual, criando um fluxo constante de informa√ß√£o que se mant√©m ao longo da rede [^13]. Essa adi√ß√£o de sa√≠das das camadas ao fluxo residual, por meio de **conex√µes residuais**, permite que informa√ß√µes de camadas inferiores alcancem camadas superiores diretamente, sem desvanecimento, e mant√©m a consist√™ncia dimensional em toda a rede [^11, 13].

**Lema 2.1**
O *residual stream* atua como um condutor de informa√ß√£o, garantindo que a representa√ß√£o original dos *tokens* seja preservada e que novas informa√ß√µes contextuais sejam adicionadas ao longo das camadas do *transformer*. A dimens√£o *d* das representa√ß√µes √© mantida consistente em todas as camadas, permitindo que a informa√ß√£o flua de forma cont√≠nua e paralela, j√° que cada *token* possui seu pr√≥prio fluxo independente.

*Prova do Lema 2.1:*
I. Considere um *transformer* com *n* camadas, onde cada camada processa uma sequ√™ncia de *embeddings* de dimens√£o *d*.
II. Inicialmente, cada *token* de entrada $w_i$ √© convertido em um *embedding* $x_i$ de dimens√£o *d*, que √© o ponto de partida do *residual stream* para esse *token*.
III. Em cada camada *l*, a representa√ß√£o do *token* $x_i$ passa pela camada de *self-attention*, gerando um novo *embedding* $a_i^l$, tamb√©m de dimens√£o *d*.
IV. A sa√≠da da camada de *self-attention* √© ent√£o adicionada ao *residual stream*: $x_i^l = x_i^{l-1} + a_i^l$ (conex√£o residual).
V. Em seguida, o *embedding* $x_i^l$ passa pela camada *feedforward*, gerando outro *embedding* $f_i^l$, tamb√©m de dimens√£o *d*.
VI. A sa√≠da da camada *feedforward* √© adicionada ao *residual stream*: $x_i^{l+1} = x_i^l + f_i^l$ (conex√£o residual).
VII. As camadas de normaliza√ß√£o tamb√©m operam em vetores de dimens√£o *d*, preservando a dimens√£o do fluxo.
VIII. Esse processo se repete em todas as *n* camadas do *transformer*, com o fluxo de informa√ß√£o mantendo a dimens√£o *d* e sendo continuamente refinado pelas opera√ß√µes de cada camada.
IX. A consist√™ncia dimensional garante que as opera√ß√µes de adi√ß√£o podem ser realizadas corretamente em todos os n√≠veis.
X. Portanto, o *residual stream* garante que as representa√ß√µes originais dos *tokens* sejam preservadas e que novas informa√ß√µes contextuais sejam adicionadas ao longo das camadas, com a dimens√£o *d* sendo mantida consistente. $\blacksquare$

**Observa√ß√£o 2:**
O fluxo residual √© um conceito abstrato; na implementa√ß√£o, o que acontece √© que cada *token* possui um fluxo de *embeddings* *d*-dimensionais. Essa abstra√ß√£o √© √∫til para entender o fluxo de informa√ß√£o ao longo do *transformer*.

> üí° **Exemplo Num√©rico:**
> Considere um *transformer* com duas camadas e um *embedding* inicial para uma palavra $w$ de dimens√£o 3, representado como $x = [1, 2, 3]$.
>
> **Camada 1:**
> 1.  **Self-Attention:** A camada de *self-attention* processa $x$ e retorna um vetor $a_1 = [0.5, 1.5, 0.5]$.
> 2.  **Conex√£o Residual:** O resultado da *self-attention* √© adicionado ao fluxo residual: $x_1 = x + a_1 = [1, 2, 3] + [0.5, 1.5, 0.5] = [1.5, 3.5, 3.5]$.
> 3. **Feedforward:** A camada *feedforward* processa $x_1$ e retorna um vetor $f_1 = [0.2, 0.8, 0.2]$.
> 4. **Conex√£o Residual:** O resultado da *feedforward* √© adicionado ao fluxo residual: $x_2 = x_1 + f_1 = [1.5, 3.5, 3.5] + [0.2, 0.8, 0.2] = [1.7, 4.3, 3.7]$.
>
> **Camada 2:**
> 1.  **Self-Attention:** A segunda camada de *self-attention* processa $x_2$ e retorna um vetor $a_2 = [0.1, 0.9, 0.1]$.
> 2.  **Conex√£o Residual:** O resultado da *self-attention* √© adicionado ao fluxo residual: $x_3 = x_2 + a_2 = [1.7, 4.3, 3.7] + [0.1, 0.9, 0.1] = [1.8, 5.2, 3.8]$.
> 3.  **Feedforward:** A camada *feedforward* processa $x_3$ e retorna um vetor $f_2 = [0.05, 0.4, 0.05]$.
> 4.  **Conex√£o Residual:** O resultado da *feedforward* √© adicionado ao fluxo residual: $x_4 = x_3 + f_2 = [1.8, 5.2, 3.8] + [0.05, 0.4, 0.05] = [1.85, 5.6, 3.85]$.
>
> O *residual stream* mant√©m a representa√ß√£o inicial da palavra ao longo do processamento, permitindo que informa√ß√µes de camadas anteriores alcancem camadas superiores. Em cada camada, as sa√≠das da *self-attention* e *feedforward* s√£o adicionadas, refinando a representa√ß√£o do *token* com informa√ß√µes contextuais e transforma√ß√µes aprendidas pelo modelo.
>
> ```mermaid
>  graph LR
>      A[x] -->|Self-Attention| B(a1)
>      B --> |Add| C(x1)
>      C --> |Feedforward| D(f1)
>      D --> |Add| E(x2)
>       E --> |Self-Attention| F(a2)
>       F --> |Add| G(x3)
>      G --> |Feedforward| H(f2)
>       H --> |Add| I(x4)
>        style A fill:#f9f,stroke:#333,stroke-width:2px
>       style B fill:#ccf,stroke:#333,stroke-width:2px
>         style D fill:#ccf,stroke:#333,stroke-width:2px
>
> ```
>
> As setas indicam o fluxo de informa√ß√£o e os r√≥tulos em cima da seta indicam a opera√ß√£o realizada. Note que a representa√ß√£o original $x$ (fluxo residual) sempre passa adiante e novas informa√ß√µes, atrav√©s das sa√≠das das camadas *self-attention* e *feedforward* s√£o adicionadas. As opera√ß√µes de adi√ß√£o indicam o papel das conex√µes residuais.

**Lema 2.2**
As conex√µes residuais, por meio da adi√ß√£o das sa√≠das das camadas ao fluxo residual, garantem que o gradiente durante o treinamento n√£o se dissipe, permitindo que a informa√ß√£o flua atrav√©s de camadas profundas e facilita o aprendizado de redes mais profundas.

*Prova do Lema 2.2:*
I. Durante o treinamento, o *backpropagation* calcula o gradiente do erro em rela√ß√£o aos par√¢metros do modelo.
II. Em redes profundas sem conex√µes residuais, o gradiente pode se tornar muito pequeno ou muito grande ao passar pelas camadas (problema do desvanecimento ou explos√£o do gradiente).
III. As conex√µes residuais, que adicionam a sa√≠da de cada camada √† sua entrada, permitem que o gradiente flua diretamente atrav√©s das conex√µes, criando um "atalho" para o gradiente.
IV.  Em outras palavras, o gradiente da camada $l$ √© adicionado ao gradiente das camadas anteriores $l-1, l-2$, etc., evitando que o gradiente se dissipe.
V. Isso facilita a propaga√ß√£o do gradiente por toda a rede, mesmo em redes muito profundas, permitindo que o modelo aprenda de forma mais eficaz e est√°vel.
VI. Portanto, as conex√µes residuais atuam como uma forma de regulariza√ß√£o, estabilizando o treinamento e permitindo a constru√ß√£o de redes mais profundas. $\blacksquare$

#### Blocos *Transformer* (*Transformer Blocks*)
Os **blocos *transformer*** s√£o as unidades b√°sicas de constru√ß√£o dos *transformers* [^9]. Cada bloco *transformer* √© composto por:
1.  Uma camada de *self-attention* (ou *multi-head self-attention*) que calcula a import√¢ncia de cada *token* no contexto [^3, 9].
2. Uma rede *feedforward*, que aplica transforma√ß√µes n√£o lineares √† representa√ß√£o de cada *token* individualmente [^10].
3. Conex√µes residuais, que adicionam a entrada de cada camada √† sua sa√≠da, permitindo que a informa√ß√£o flua mais facilmente atrav√©s das camadas [^11].
4. Camadas de normaliza√ß√£o, como a *layer normalization*, que estabilizam o treinamento [^11].
Estes blocos s√£o empilhados repetidamente para formar redes profundas, permitindo que o modelo aprenda hierarquias complexas de representa√ß√µes [^12]. Os blocos *transformer* s√£o projetados para manter a dimens√£o *d* da entrada consistente com a dimens√£o da sa√≠da, permitindo que os blocos sejam empilhados em sequ√™ncia sem a necessidade de transforma√ß√µes dimensionais adicionais [^12]. A repeti√ß√£o e o empilhamento desses blocos permite que o modelo aprenda representa√ß√µes cada vez mais abstratas e complexas do texto [^12].

**Defini√ß√£o 2**
Um bloco *transformer* √© uma unidade computacional que transforma uma sequ√™ncia de representa√ß√µes contextuais de dimens√£o *d* em outra sequ√™ncia de representa√ß√µes contextuais de mesma dimens√£o *d*, preservando as dimens√µes e realizando opera√ß√µes de *self-attention*, *feedforward*, conex√µes residuais e normaliza√ß√£o de camadas.

**Corol√°rio 2.1**
A natureza modular e a manuten√ß√£o da consist√™ncia dimensional dos blocos *transformer* permitem que eles sejam empilhados repetidamente, construindo redes profundas que s√£o capazes de capturar rela√ß√µes complexas entre as palavras e criar representa√ß√µes de texto cada vez mais abstratas.

*Prova do Corol√°rio 2.1:*
I. Um bloco *transformer* recebe como entrada uma sequ√™ncia de representa√ß√µes contextuais de dimens√£o *d*, representadas por um vetor $x$.
II. Internamente, o bloco aplica opera√ß√µes de *self-attention*, *feedforward*, conex√µes residuais e normaliza√ß√£o de camada, preservando a dimens√£o *d* da sa√≠da, ou seja, gera um vetor de sa√≠da $h$ de dimens√£o *d*.
III. Como a dimens√£o da sa√≠da $h$ √© igual √† dimens√£o da entrada $x$, a sa√≠da de um bloco pode ser usada como entrada para o bloco seguinte.
IV. A repeti√ß√£o desse processo permite que os blocos sejam empilhados sequencialmente, criando uma rede *transformer* com v√°rias camadas, sem restri√ß√µes de dimensionalidade.
V. A arquitetura modular permite a constru√ß√£o de redes cada vez mais profundas.
VI. Redes mais profundas conseguem aprender representa√ß√µes cada vez mais complexas e abstratas do texto.
VII. Portanto, a natureza modular dos blocos e a consist√™ncia dimensional permitem a constru√ß√£o de modelos poderosos e altamente flex√≠veis. $\blacksquare$

> üí° **Exemplo Num√©rico:**
> Considere um bloco *transformer* que recebe uma sequ√™ncia de *embeddings* com dimens√£o *d* = 512, representada como uma matriz de tamanho (N, 512), onde N √© o n√∫mero de *tokens* na sequ√™ncia.
>
> 1.  **Self-Attention/Multi-Head Attention:** A camada de *self-attention* (ou *multi-head self-attention*) processa a matriz (N, 512) e gera uma nova matriz de tamanho (N, 512). A camada de *multi-head attention* √© composta por m√∫ltiplos *heads* de *self-attention* que operam em paralelo, concatenando as sa√≠das e projetando-as de volta para uma dimens√£o 512. Para fins ilustrativos, vamos considerar que a sa√≠da de cada head tenha dimens√£o 64, e que haja 8 heads, totalizando uma dimens√£o 512 ap√≥s a concatena√ß√£o e proje√ß√£o.
> 2.  **Conex√£o Residual:** A sa√≠da da camada de *self-attention* √© adicionada √† matriz de entrada original, resultando em uma nova matriz de tamanho (N, 512).
> 3.  **Normaliza√ß√£o de Camada:** A *layer norm* √© aplicada √† matriz resultante, garantindo que a escala das ativa√ß√µes seja adequada. A matriz continua com tamanho (N, 512).
> 4.  **Feedforward Network:** A rede *feedforward* processa cada *token* da sequ√™ncia de forma independente, transformando a matriz de tamanho (N, 512) em outra matriz de mesmo tamanho. A rede *feedforward* √© composta por duas camadas lineares, com uma ativa√ß√£o ReLU entre elas, e a dimens√£o intermedi√°ria √© tipicamente maior que a dimens√£o da entrada. Por exemplo, a primeira camada linear poderia mapear de 512 para 2048 e a segunda de 2048 para 512. A dimensionalidade da sa√≠da √© 512.
> 5.  **Conex√£o Residual:** A sa√≠da da rede *feedforward* √© adicionada √† matriz que entrou na camada *feedforward*, resultando em uma matriz final de tamanho (N, 512).
> 6.  **Normaliza√ß√£o de Camada:** Uma segunda *layer norm* √© aplicada √† matriz resultante, finalizando o processamento do bloco. A sa√≠da final tamb√©m tem tamanho (N, 512).
>
> A sa√≠da do bloco *transformer* √© uma matriz de mesmo tamanho que a entrada (N, 512), permitindo que ela seja usada como entrada para outro bloco *transformer* em uma rede mais profunda.
>
> ```mermaid
>  graph LR
>      A[Input (N, 512)] -->|Self-Attention| B(N, 512)
>      B --> |Add| C(N, 512)
>       C -->|Layer Norm| D(N, 512)
>      D -->|Feedforward| E(N, 512)
>      E --> |Add| F(N, 512)
>        F -->|Layer Norm| G(N, 512)
>     style A fill:#f9f,stroke:#333,stroke-width:2px
>         style B fill:#ccf,stroke:#333,stroke-width:2px
>           style E fill:#ccf,stroke:#333,stroke-width:2px
>
> ```
>
> As setas indicam o fluxo de informa√ß√£o e os r√≥tulos acima da seta indicam a opera√ß√£o realizada. Note que a dimens√£o da entrada √© igual √† da sa√≠da. O diagrama ilustra um bloco *transformer*. Numa rede com m√∫ltiplos blocos, a sa√≠da de um bloco torna-se a entrada do seguinte.

**Proposi√ß√£o 2.1**
Os blocos *transformer* podem ser vistos como uma fun√ß√£o $T$ que mapeia uma sequ√™ncia de *embeddings* $X$ de dimens√£o $(N, d)$ para outra sequ√™ncia de *embeddings* $T(X)$ de mesma dimens√£o $(N, d)$, onde $N$ √© o n√∫mero de *tokens* e *d* a dimens√£o dos *embeddings*.
*Prova da Proposi√ß√£o 2.1:*
I. Seja $X$ a sequ√™ncia de *embeddings* de entrada do bloco *transformer*, com dimens√£o $(N, d)$.
II. O bloco *transformer* aplica uma sequ√™ncia de opera√ß√µes de *self-attention*, *feedforward*, conex√µes residuais e normaliza√ß√£o de camadas.
III. Cada uma dessas opera√ß√µes preserva a dimens√£o dos *embeddings* $(N, d)$.
IV. Portanto, a sa√≠da do bloco *transformer*, que chamamos de $T(X)$, tamb√©m ter√° dimens√£o $(N, d)$.
V. Assim, o bloco *transformer* pode ser visto como uma fun√ß√£o $T$ que mapeia uma sequ√™ncia de *embeddings* de dimens√£o $(N, d)$ para outra sequ√™ncia de mesma dimens√£o. $\blacksquare$

#### Modelagem da Linguagem com *Transformers*
Modelos de linguagem baseados em *transformers* utilizam uma arquitetura *decoder-only*, onde a entrada √© uma sequ√™ncia de *tokens* e a sa√≠da √© uma predi√ß√£o da pr√≥xima palavra, usando uma camada *language modeling head* [^16, 17, 22]. O *transformer* processa a sequ√™ncia, gerando uma representa√ß√£o contextualizada de cada *token*, e o *language modeling head* usa a representa√ß√£o do √∫ltimo *token* para predizer a pr√≥xima palavra, de forma autoregressiva [^16, 17, 22]. O mecanismo de *self-attention* permite que o modelo capture depend√™ncias de longo alcance, usando longos contextos para modelar a linguagem [^3, 22]. As representa√ß√µes intermedi√°rias e a dimensionalidade do fluxo residual s√£o as mesmas para cada *token* da sequ√™ncia de entrada, permitindo o processamento em paralelo e a combina√ß√£o das informa√ß√µes contextuais [^7, 13, 22].

> üí° **Exemplo Num√©rico:**
> Suponha que temos a sequ√™ncia de *tokens* "The quick brown fox". Cada *token* √© convertido em um *embedding* de dimens√£o 512, resultando em uma matriz de entrada de tamanho (4, 512). Essa matriz passa pelo *transformer*, que consiste em v√°rios blocos *transformer*, e o fluxo residual √© mantido em cada bloco, refinando a representa√ß√£o contextual de cada token. No final, a camada *language modeling head* recebe a representa√ß√£o do √∫ltimo *token* "fox" e gera um vetor de probabilidades sobre o vocabul√°rio, permitindo predizer o pr√≥ximo *token*. Por exemplo, se o vocabul√°rio tiver 10000 *tokens*, a sa√≠da do *language modeling head* ser√° um vetor de tamanho 10000. Suponha que a palavra "jumps" tenha √≠ndice 345 no vocabul√°rio. Se a probabilidade associada ao √≠ndice 345 for alta, o modelo prediz que a pr√≥xima palavra ser√° "jumps". No processo autoregressivo, a sequ√™ncia de *tokens* se torna "The quick brown fox jumps" e o processo se repete para predizer a pr√≥xima palavra depois de "jumps", e assim por diante.
>
> Suponha que a representa√ß√£o do token "fox" no fluxo residual depois de passar por todos os blocos *transformer* seja $r_{fox} = [0.1, -0.2, 0.3, \ldots, 0.2]$, um vetor de dimens√£o 512. A camada *language modeling head* (que √© uma camada linear) aplica uma transforma√ß√£o linear $W$ (uma matriz de tamanho 10000x512) e um *bias* $b$ (um vetor de tamanho 10000) √† representa√ß√£o $r_{fox}$. O c√°lculo √© $p = W \cdot r_{fox} + b$, onde $p$ √© um vetor de tamanho 10000. Em seguida, aplica-se a fun√ß√£o *softmax* a $p$, que transforma esse vetor em um vetor de probabilidades. Se $p_{345}$ for a maior probabilidade, o modelo prediz que a pr√≥xima palavra ser√° "jumps".

### Conclus√£o
O **fluxo residual** e os **blocos *transformer*** s√£o elementos fundamentais na arquitetura dos LLMs atuais [^13, 9]. O fluxo residual garante que a informa√ß√£o inicial dos *tokens* seja preservada e refinada ao longo das camadas, enquanto os blocos *transformer* proporcionam a estrutura modular necess√°ria para o processamento eficiente das sequ√™ncias de texto e a captura de depend√™ncias de longo alcance [^3, 11, 12]. A combina√ß√£o desses componentes permite a constru√ß√£o de modelos de linguagem poderosos e flex√≠veis, capazes de gerar texto coerente e resolver tarefas de linguagem natural de forma not√°vel [^2]. Nos pr√≥ximos cap√≠tulos, exploraremos como treinar esses modelos, como gerar texto de forma eficiente e outros aspectos importantes do uso e implementa√ß√£o dos *transformers*.

### Refer√™ncias
[^2]: [^2]
[^3]: [^3]
[^7]: [^7]
[^9]: [^9]
[^10]: [^10]
[^11]: [^11]
[^12]: [^12]
[^13]: [^13]
[^16]: [^16]
[^17]: [^17]
[^22]: [^22]
<!-- END -->
