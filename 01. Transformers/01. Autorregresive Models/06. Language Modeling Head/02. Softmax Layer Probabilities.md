## O Papel da FunÃ§Ã£o Softmax na Modelagem de Linguagem com Transformers

### IntroduÃ§Ã£o
Como vimos anteriormente [^1], o *language modeling head* Ã© crucial para transformar as representaÃ§Ãµes aprendidas por modelos Transformers em uma distribuiÃ§Ã£o de probabilidade sobre um vocabulÃ¡rio, permitindo a previsÃ£o da prÃ³xima palavra em uma sequÃªncia. A funÃ§Ã£o *softmax* desempenha um papel central nesse processo, atuando como a etapa final que converte *logits* em probabilidades. Neste capÃ­tulo, exploraremos em detalhes a funÃ§Ã£o *softmax*, seu funcionamento matemÃ¡tico, e sua importÃ¢ncia para a modelagem de linguagem. Expandindo a explicaÃ§Ã£o anterior, focaremos em como a funÃ§Ã£o *softmax* normaliza os *logits* para gerar uma distribuiÃ§Ã£o de probabilidade vÃ¡lida, esmagando *scores* em um intervalo probabilÃ­stico (0-1), enfatizando pontuaÃ§Ãµes mais altas e desvalorizando pontuaÃ§Ãµes mais baixas.

### Conceitos Fundamentais
A funÃ§Ã£o *softmax* Ã© uma ferramenta essencial para a modelagem de linguagem, e mais amplamente, para qualquer problema de classificaÃ§Ã£o em *deep learning*. A funÃ§Ã£o Ã© usada para converter um vetor de *logits* (pontuaÃ§Ãµes nÃ£o normalizadas) em uma distribuiÃ§Ã£o de probabilidade [^1].

Como visto anteriormente [^1], apÃ³s o vetor de embedding da Ãºltima camada do Transformer ($h_N$) ser projetado linearmente para o vetor de logits $u$ atravÃ©s de $u = h_N W^T$, a funÃ§Ã£o softmax entra em cena. Formalmente, dado um vetor de *logits* $u = [u_1, u_2, ..., u_{|V|}]$, onde cada $u_i$ representa uma pontuaÃ§Ã£o para a palavra $i$ do vocabulÃ¡rio, a funÃ§Ã£o *softmax* calcula a probabilidade $y_i$ de cada palavra $i$ da seguinte forma:
$$ y_i = \frac{e^{u_i}}{\sum_{j=1}^{|V|} e^{u_j}} $$

Onde:
*   $y_i$ Ã© a probabilidade da palavra $i$ ser a prÃ³xima palavra.
*   $u_i$ Ã© o *logit* (pontuaÃ§Ã£o nÃ£o normalizada) correspondente Ã  palavra $i$.
*   $|V|$ Ã© o tamanho do vocabulÃ¡rio.

A funÃ§Ã£o *softmax* realiza dois papÃ©is cruciais:

1.  **NormalizaÃ§Ã£o:** Ela garante que todas as probabilidades sejam positivas e somem 1, criando uma distribuiÃ§Ã£o de probabilidade vÃ¡lida. Isso Ã© crucial, uma vez que uma probabilidade deve ser sempre um valor entre 0 e 1, e as probabilidades de todas as opÃ§Ãµes devem somar 1 [^1].
2.  **ExponenciaÃ§Ã£o e ÃŠnfase:** A funÃ§Ã£o exponencial $e^{u_i}$ enfatiza pontuaÃ§Ãµes mais altas, enquanto suprime pontuaÃ§Ãµes mais baixas. Isso ocorre porque a funÃ§Ã£o exponencial aumenta rapidamente quando o valor de $u_i$ aumenta, e diminui rapidamente quando o valor de $u_i$ diminui. Isso significa que pequenas diferenÃ§as nos *logits* sÃ£o amplificadas, transformando diferenÃ§as em *scores* em diferenÃ§as mais expressivas nas probabilidades.

> **Exemplo NumÃ©rico Detalhado:** Vamos considerar um vocabulÃ¡rio pequeno de 4 palavras: "o", "gato", "estÃ¡", "feliz". Suponha que o vetor de *logits* $u$ seja [-1.0, 2.0, 0.5, -0.5]. O tamanho do vocabulÃ¡rio, $|V|$, Ã© 4.

   **Step 1:** Calcular $e^{u_i}$ para cada palavra:
    $$e^{-1.0} \approx 0.368 \\
    e^{2.0} \approx 7.389 \\
    e^{0.5} \approx 1.649 \\
    e^{-0.5} \approx 0.607
    $$

    **Step 2:** Calcular a soma de todos os valores exponenciados:
    $$  \sum_{j=1}^{4} e^{u_j} \approx 0.368 + 7.389 + 1.649 + 0.607 \approx 10.013 $$

    **Step 3:** Calcular a probabilidade $y_i$ para cada palavra:
    $$ y_1 = \frac{0.368}{10.013} \approx 0.037 \\
    y_2 = \frac{7.389}{10.013} \approx 0.738 \\
    y_3 = \frac{1.649}{10.013} \approx 0.165 \\
    y_4 = \frac{0.607}{10.013} \approx 0.061
    $$

    O vetor de probabilidades resultante $y$ Ã© aproximadamente [0.037, 0.738, 0.165, 0.061]. Isso significa que:

    *   A probabilidade da prÃ³xima palavra ser "o" Ã© de 0.037.
    *   A probabilidade da prÃ³xima palavra ser "gato" Ã© de 0.738.
    *   A probabilidade da prÃ³xima palavra ser "estÃ¡" Ã© de 0.165.
    *   A probabilidade da prÃ³xima palavra ser "feliz" Ã© de 0.061.

    Note que as probabilidades somam aproximadamente 1 (0.037 + 0.738 + 0.165 + 0.061 = 1.001), e as pontuaÃ§Ãµes mais altas (logit 2.0) correspondem a uma probabilidade muito maior (0.738).

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos agora considerar um caso com um vocabulÃ¡rio maior, digamos com 5 palavras, e com logits um pouco mais prÃ³ximos para ilustrar como o softmax normaliza e amplifica diferenÃ§as: Suponha que o vetor de logits seja $u$ = [-2, -1, 0, 1, 1.5].
   
   **Step 1:** Calcular $e^{u_i}$:
     $$ e^{-2} \approx 0.135 \\
     e^{-1} \approx 0.368 \\
     e^{0} = 1 \\
     e^{1} \approx 2.718 \\
     e^{1.5} \approx 4.482 $$
     
   **Step 2:** Calcular a soma dos exponenciados:
     $$\sum_{j=1}^{5} e^{u_j} \approx 0.135 + 0.368 + 1 + 2.718 + 4.482 = 8.703 $$
   
   **Step 3:** Calcular as probabilidades usando softmax:
      $$ y_1 = \frac{0.135}{8.703} \approx 0.0155 \\
      y_2 = \frac{0.368}{8.703} \approx 0.0423 \\
      y_3 = \frac{1}{8.703} \approx 0.1149 \\
      y_4 = \frac{2.718}{8.703} \approx 0.3123 \\
      y_5 = \frac{4.482}{8.703} \approx 0.5150 $$
   
    O vetor de probabilidades resultante $y$ Ã© aproximadamente [0.0155, 0.0423, 0.1149, 0.3123, 0.5150]. Mesmo com logits relativamente prÃ³ximos (diferenÃ§as de no mÃ¡ximo 3.5), as probabilidades finais mostram uma clara distinÃ§Ã£o. A palavra correspondente ao logit de 1.5 tem uma probabilidade muito maior do que a palavra com logit -2, demonstrando a capacidade da softmax de amplificar as diferenÃ§as e produzir uma distribuiÃ§Ã£o de probabilidades Ãºtil para seleÃ§Ã£o de palavras. AlÃ©m disso, a soma das probabilidades Ã© aproximadamente 1, o que valida a propriedade de normalizaÃ§Ã£o do softmax.

**ProposiÃ§Ã£o 2.** *A funÃ§Ã£o softmax Ã© uma funÃ§Ã£o diferenciÃ¡vel.*

*Prova*:
   I. Para demonstrar que a funÃ§Ã£o *softmax* Ã© diferenciÃ¡vel, devemos mostrar que a derivada de cada saÃ­da $y_i$ em relaÃ§Ã£o a cada entrada $u_j$ existe.
   II. A saÃ­da da funÃ§Ã£o *softmax* Ã© dada por:
   $$ y_i = \frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}} $$
   III. Vamos calcular a derivada parcial de $y_i$ em relaÃ§Ã£o a $u_j$. Dividiremos em dois casos: quando $i=j$ e quando $i \ne j$.

   **Caso 1:** Quando $i = j$.
   $$ \frac{\partial y_i}{\partial u_j} = \frac{\partial}{\partial u_i} \left(\frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}}\right) $$
    
   IV. Usando a regra do quociente:
   $$ \frac{\partial y_i}{\partial u_i} =  \frac{e^{u_i} \sum_{k=1}^{|V|} e^{u_k} - e^{u_i}e^{u_i}}{(\sum_{k=1}^{|V|} e^{u_k})^2} =
    \frac{e^{u_i}}{(\sum_{k=1}^{|V|} e^{u_k})} \left(1 - \frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}}\right)
    $$

   V. Relembrando que $y_i = \frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}}$, podemos simplificar para:

   $$ \frac{\partial y_i}{\partial u_i} = y_i(1 - y_i) $$

   **Caso 2:** Quando $i \ne j$.
    $$ \frac{\partial y_i}{\partial u_j} = \frac{\partial}{\partial u_j} \left(\frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}}\right) $$

    VI. Note que a derivada de $e^{u_i}$ em relaÃ§Ã£o a $u_j$ Ã© 0 quando $i \ne j$, pois sÃ£o variÃ¡veis independentes:
      $$ \frac{\partial y_i}{\partial u_j} = \frac{-e^{u_i}e^{u_j}}{(\sum_{k=1}^{|V|} e^{u_k})^2} $$

     VII.  Simplificando a expressÃ£o:

       $$ \frac{\partial y_i}{\partial u_j} = - y_i y_j $$
   VIII.  JÃ¡ que a derivada existe em ambos os casos, a funÃ§Ã£o softmax Ã© diferenciÃ¡vel.  Essa propriedade Ã© essencial para a otimizaÃ§Ã£o de modelos de *deep learning* usando mÃ©todos de gradiente descendente.
    â– 

A propriedade de diferenciabilidade Ã© essencial para o treinamento de modelos de *deep learning*, pois permite calcular o gradiente da funÃ§Ã£o de perda em relaÃ§Ã£o aos parÃ¢metros do modelo. Esse gradiente Ã© usado para atualizar os parÃ¢metros do modelo usando mÃ©todos como o gradiente descendente, otimizando o modelo para que ele faÃ§a melhores previsÃµes da prÃ³xima palavra.

> ðŸ’¡ **Exemplo NumÃ©rico: CÃ¡lculo de Gradiente:** Suponha que, para as palavras "o", "gato", "estÃ¡", "feliz", os logits sejam [-1, 2, 0.5, -0.5] e as probabilidades calculadas pelo softmax sejam aproximadamente [0.037, 0.738, 0.165, 0.061]. Se quisermos calcular o gradiente da probabilidade da palavra "gato" (Ã­ndice 2) em relaÃ§Ã£o ao seu logit (Ã­ndice 2), usamos a derivada $\frac{\partial y_2}{\partial u_2} = y_2(1 - y_2)$. Portanto, $\frac{\partial y_2}{\partial u_2} = 0.738 * (1 - 0.738) = 0.738 * 0.262 \approx 0.193$. Esse valor do gradiente indica o quÃ£o sensÃ­vel a probabilidade da palavra "gato" Ã© a mudanÃ§as em seu logit.
> Da mesma forma, o gradiente da probabilidade da palavra "o" (Ã­ndice 1) em relaÃ§Ã£o ao logit da palavra "gato" (Ã­ndice 2) seria $\frac{\partial y_1}{\partial u_2} = -y_1y_2 = -0.037 * 0.738 \approx -0.027$. Esse gradiente negativo indica que aumentar o logit da palavra "gato" diminui a probabilidade da palavra "o". Esses cÃ¡lculos sÃ£o usados em *backpropagation* para ajustar os pesos do modelo.

**Lema 2.1** *A funÃ§Ã£o softmax Ã© invariante Ã  adiÃ§Ã£o de uma constante a todos os logits.*

*Prova:*
Seja $c$ uma constante. Considere o vetor de logits modificado $u' = [u_1 + c, u_2 + c, ..., u_{|V|} + c]$. As probabilidades $y'_i$ calculadas com $u'$ serÃ£o:

$$y'_i = \frac{e^{u_i + c}}{\sum_{j=1}^{|V|} e^{u_j + c}} = \frac{e^{u_i}e^c}{\sum_{j=1}^{|V|} e^{u_j}e^c} = \frac{e^c e^{u_i}}{e^c \sum_{j=1}^{|V|} e^{u_j}} = \frac{e^{u_i}}{\sum_{j=1}^{|V|} e^{u_j}} = y_i$$

Assim, $y'_i = y_i$ para todo $i$. Isso demonstra que a funÃ§Ã£o *softmax* Ã© invariante Ã  adiÃ§Ã£o de uma constante aos logits, pois as probabilidades resultantes sÃ£o as mesmas. Esta propriedade Ã© crucial em termos computacionais pois a subtraÃ§Ã£o de uma constante grande dos logits pode evitar o overflow da exponenciaÃ§Ã£o. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Se aos logits [-1, 2, 0.5, -0.5] adicionarmos uma constante, digamos 10, teremos os novos logits [9, 12, 10.5, 9.5]. Ao calcular o softmax com esses novos valores, obteremos exatamente as mesmas probabilidades que com os valores originais. Isso demonstra a invariÃ¢ncia da funÃ§Ã£o softmax Ã  adiÃ§Ã£o de uma constante aos logits. Essa propriedade Ã© relevante na prÃ¡tica para melhorar a estabilidade numÃ©rica do cÃ¡lculo do softmax.

### A Softmax e o Sampling
A funÃ§Ã£o *softmax*, quando combinada com tÃ©cnicas de *sampling*, habilita a geraÃ§Ã£o de texto nos modelos de linguagem. Conforme discutido anteriormente [^1], gerar texto em modelos de linguagem envolve iterativamente amostrar a prÃ³xima palavra com base na distribuiÃ§Ã£o de probabilidade produzida pela funÃ§Ã£o *softmax*. A escolha de como amostrar as palavras, seja por *greedy decoding*, *top-k sampling*, ou *temperature sampling*, impacta a diversidade e a qualidade do texto gerado [^1]. A funÃ§Ã£o *softmax* garante que, independente do mÃ©todo de *sampling* utilizado, cada palavra do vocabulÃ¡rio tem uma probabilidade definida de ser escolhida, com probabilidades que somam 1.

**ObservaÃ§Ã£o 3.** *A distribuiÃ§Ã£o de probabilidade gerada pela funÃ§Ã£o softmax Ã© uma distribuiÃ§Ã£o categÃ³rica.*
Uma distribuiÃ§Ã£o categÃ³rica Ã© uma distribuiÃ§Ã£o de probabilidade discreta que descreve a probabilidade de observar cada uma das *k* possÃ­veis categorias (neste caso, as palavras do vocabulÃ¡rio). Como cada $y_i$ da saÃ­da da softmax Ã© uma probabilidade nÃ£o negativa e a soma de todos os $y_i$ Ã© 1, a saÃ­da da softmax atende Ã  definiÃ§Ã£o de uma distribuiÃ§Ã£o categÃ³rica. Assim, a operaÃ§Ã£o de *sampling* pode ser entendida como o ato de sortear uma categoria de acordo com a distribuiÃ§Ã£o de probabilidades definida pela softmax.

### ConclusÃ£o
A funÃ§Ã£o *softmax* Ã© um componente fundamental nos modelos Transformers e na modelagem de linguagem em geral. Ao transformar um vetor de *logits* em uma distribuiÃ§Ã£o de probabilidade normalizada, ela permite que o modelo avalie e escolha a prÃ³xima palavra em uma sequÃªncia, com as probabilidades refletindo as relaÃ§Ãµes complexas aprendidas pelo modelo. AlÃ©m de garantir uma distribuiÃ§Ã£o de probabilidade vÃ¡lida, a funÃ§Ã£o *softmax* amplifica as diferenÃ§as entre as pontuaÃ§Ãµes, o que Ã© essencial para tomar decisÃµes durante o *sampling* e para guiar o aprendizado do modelo, devido a sua propriedade de diferenciabilidade. A compreensÃ£o detalhada da funÃ§Ã£o *softmax* Ã© fundamental para entender como os modelos de linguagem operam e como podemos refinÃ¡-los para gerar textos de alta qualidade, bem como compreender os mecanismos internos de cada camada por meio da *logit lens*.

### ReferÃªncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright Â© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
