## O Papel da Fun√ß√£o Softmax na Modelagem de Linguagem com Transformers

### Introdu√ß√£o
Como vimos anteriormente [^1], o *language modeling head* √© crucial para transformar as representa√ß√µes aprendidas por modelos Transformers em uma distribui√ß√£o de probabilidade sobre um vocabul√°rio, permitindo a previs√£o da pr√≥xima palavra em uma sequ√™ncia. A fun√ß√£o *softmax* desempenha um papel central nesse processo, atuando como a etapa final que converte *logits* em probabilidades. Neste cap√≠tulo, exploraremos em detalhes a fun√ß√£o *softmax*, seu funcionamento matem√°tico, e sua import√¢ncia para a modelagem de linguagem. Expandindo a explica√ß√£o anterior, focaremos em como a fun√ß√£o *softmax* normaliza os *logits* para gerar uma distribui√ß√£o de probabilidade v√°lida, esmagando *scores* em um intervalo probabil√≠stico (0-1), enfatizando pontua√ß√µes mais altas e desvalorizando pontua√ß√µes mais baixas.

### Conceitos Fundamentais
A fun√ß√£o *softmax* √© uma ferramenta essencial para a modelagem de linguagem, e mais amplamente, para qualquer problema de classifica√ß√£o em *deep learning*. A fun√ß√£o √© usada para converter um vetor de *logits* (pontua√ß√µes n√£o normalizadas) em uma distribui√ß√£o de probabilidade [^1].

Como visto anteriormente [^1], ap√≥s o vetor de embedding da √∫ltima camada do Transformer ($h_N$) ser projetado linearmente para o vetor de logits $u$ atrav√©s de $u = h_N W^T$, a fun√ß√£o softmax entra em cena. Formalmente, dado um vetor de *logits* $u = [u_1, u_2, ..., u_{|V|}]$, onde cada $u_i$ representa uma pontua√ß√£o para a palavra $i$ do vocabul√°rio, a fun√ß√£o *softmax* calcula a probabilidade $y_i$ de cada palavra $i$ da seguinte forma:
$$ y_i = \frac{e^{u_i}}{\sum_{j=1}^{|V|} e^{u_j}} $$

Onde:
*   $y_i$ √© a probabilidade da palavra $i$ ser a pr√≥xima palavra.
*   $u_i$ √© o *logit* (pontua√ß√£o n√£o normalizada) correspondente √† palavra $i$.
*   $|V|$ √© o tamanho do vocabul√°rio.

A fun√ß√£o *softmax* realiza dois pap√©is cruciais:

1.  **Normaliza√ß√£o:** Ela garante que todas as probabilidades sejam positivas e somem 1, criando uma distribui√ß√£o de probabilidade v√°lida. Isso √© crucial, uma vez que uma probabilidade deve ser sempre um valor entre 0 e 1, e as probabilidades de todas as op√ß√µes devem somar 1 [^1].
2.  **Exponencia√ß√£o e √änfase:** A fun√ß√£o exponencial $e^{u_i}$ enfatiza pontua√ß√µes mais altas, enquanto suprime pontua√ß√µes mais baixas. Isso ocorre porque a fun√ß√£o exponencial aumenta rapidamente quando o valor de $u_i$ aumenta, e diminui rapidamente quando o valor de $u_i$ diminui. Isso significa que pequenas diferen√ßas nos *logits* s√£o amplificadas, transformando diferen√ßas em *scores* em diferen√ßas mais expressivas nas probabilidades.

> **Exemplo Num√©rico Detalhado:** Vamos considerar um vocabul√°rio pequeno de 4 palavras: "o", "gato", "est√°", "feliz". Suponha que o vetor de *logits* $u$ seja [-1.0, 2.0, 0.5, -0.5]. O tamanho do vocabul√°rio, $|V|$, √© 4.

   **Step 1:** Calcular $e^{u_i}$ para cada palavra:
    $$e^{-1.0} \approx 0.368 \\
    e^{2.0} \approx 7.389 \\
    e^{0.5} \approx 1.649 \\
    e^{-0.5} \approx 0.607
    $$

    **Step 2:** Calcular a soma de todos os valores exponenciados:
    $$  \sum_{j=1}^{4} e^{u_j} \approx 0.368 + 7.389 + 1.649 + 0.607 \approx 10.013 $$

    **Step 3:** Calcular a probabilidade $y_i$ para cada palavra:
    $$ y_1 = \frac{0.368}{10.013} \approx 0.037 \\
    y_2 = \frac{7.389}{10.013} \approx 0.738 \\
    y_3 = \frac{1.649}{10.013} \approx 0.165 \\
    y_4 = \frac{0.607}{10.013} \approx 0.061
    $$

    O vetor de probabilidades resultante $y$ √© aproximadamente [0.037, 0.738, 0.165, 0.061]. Isso significa que:

    *   A probabilidade da pr√≥xima palavra ser "o" √© de 0.037.
    *   A probabilidade da pr√≥xima palavra ser "gato" √© de 0.738.
    *   A probabilidade da pr√≥xima palavra ser "est√°" √© de 0.165.
    *   A probabilidade da pr√≥xima palavra ser "feliz" √© de 0.061.

    Note que as probabilidades somam aproximadamente 1 (0.037 + 0.738 + 0.165 + 0.061 = 1.001), e as pontua√ß√µes mais altas (logit 2.0) correspondem a uma probabilidade muito maior (0.738).

> üí° **Exemplo Num√©rico:** Vamos agora considerar um caso com um vocabul√°rio maior, digamos com 5 palavras, e com logits um pouco mais pr√≥ximos para ilustrar como o softmax normaliza e amplifica diferen√ßas: Suponha que o vetor de logits seja $u$ = [-2, -1, 0, 1, 1.5].
   
   **Step 1:** Calcular $e^{u_i}$:
     $$ e^{-2} \approx 0.135 \\
     e^{-1} \approx 0.368 \\
     e^{0} = 1 \\
     e^{1} \approx 2.718 \\
     e^{1.5} \approx 4.482 $$
     
   **Step 2:** Calcular a soma dos exponenciados:
     $$\sum_{j=1}^{5} e^{u_j} \approx 0.135 + 0.368 + 1 + 2.718 + 4.482 = 8.703 $$
   
   **Step 3:** Calcular as probabilidades usando softmax:
      $$ y_1 = \frac{0.135}{8.703} \approx 0.0155 \\
      y_2 = \frac{0.368}{8.703} \approx 0.0423 \\
      y_3 = \frac{1}{8.703} \approx 0.1149 \\
      y_4 = \frac{2.718}{8.703} \approx 0.3123 \\
      y_5 = \frac{4.482}{8.703} \approx 0.5150 $$
   
    O vetor de probabilidades resultante $y$ √© aproximadamente [0.0155, 0.0423, 0.1149, 0.3123, 0.5150]. Mesmo com logits relativamente pr√≥ximos (diferen√ßas de no m√°ximo 3.5), as probabilidades finais mostram uma clara distin√ß√£o. A palavra correspondente ao logit de 1.5 tem uma probabilidade muito maior do que a palavra com logit -2, demonstrando a capacidade da softmax de amplificar as diferen√ßas e produzir uma distribui√ß√£o de probabilidades √∫til para sele√ß√£o de palavras. Al√©m disso, a soma das probabilidades √© aproximadamente 1, o que valida a propriedade de normaliza√ß√£o do softmax.

**Proposi√ß√£o 2.** *A fun√ß√£o softmax √© uma fun√ß√£o diferenci√°vel.*

*Prova*:
   I. Para demonstrar que a fun√ß√£o *softmax* √© diferenci√°vel, devemos mostrar que a derivada de cada sa√≠da $y_i$ em rela√ß√£o a cada entrada $u_j$ existe.
   II. A sa√≠da da fun√ß√£o *softmax* √© dada por:
   $$ y_i = \frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}} $$
   III. Vamos calcular a derivada parcial de $y_i$ em rela√ß√£o a $u_j$. Dividiremos em dois casos: quando $i=j$ e quando $i \ne j$.

   **Caso 1:** Quando $i = j$.
   $$ \frac{\partial y_i}{\partial u_j} = \frac{\partial}{\partial u_i} \left(\frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}}\right) $$
    
   IV. Usando a regra do quociente:
   $$ \frac{\partial y_i}{\partial u_i} =  \frac{e^{u_i} \sum_{k=1}^{|V|} e^{u_k} - e^{u_i}e^{u_i}}{(\sum_{k=1}^{|V|} e^{u_k})^2} =
    \frac{e^{u_i}}{(\sum_{k=1}^{|V|} e^{u_k})} \left(1 - \frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}}\right)
    $$

   V. Relembrando que $y_i = \frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}}$, podemos simplificar para:

   $$ \frac{\partial y_i}{\partial u_i} = y_i(1 - y_i) $$

   **Caso 2:** Quando $i \ne j$.
    $$ \frac{\partial y_i}{\partial u_j} = \frac{\partial}{\partial u_j} \left(\frac{e^{u_i}}{\sum_{k=1}^{|V|} e^{u_k}}\right) $$

    VI. Note que a derivada de $e^{u_i}$ em rela√ß√£o a $u_j$ √© 0 quando $i \ne j$, pois s√£o vari√°veis independentes:
      $$ \frac{\partial y_i}{\partial u_j} = \frac{-e^{u_i}e^{u_j}}{(\sum_{k=1}^{|V|} e^{u_k})^2} $$

     VII.  Simplificando a express√£o:

       $$ \frac{\partial y_i}{\partial u_j} = - y_i y_j $$
   VIII.  J√° que a derivada existe em ambos os casos, a fun√ß√£o softmax √© diferenci√°vel.  Essa propriedade √© essencial para a otimiza√ß√£o de modelos de *deep learning* usando m√©todos de gradiente descendente.
    ‚ñ†

A propriedade de diferenciabilidade √© essencial para o treinamento de modelos de *deep learning*, pois permite calcular o gradiente da fun√ß√£o de perda em rela√ß√£o aos par√¢metros do modelo. Esse gradiente √© usado para atualizar os par√¢metros do modelo usando m√©todos como o gradiente descendente, otimizando o modelo para que ele fa√ßa melhores previs√µes da pr√≥xima palavra.

> üí° **Exemplo Num√©rico: C√°lculo de Gradiente:** Suponha que, para as palavras "o", "gato", "est√°", "feliz", os logits sejam [-1, 2, 0.5, -0.5] e as probabilidades calculadas pelo softmax sejam aproximadamente [0.037, 0.738, 0.165, 0.061]. Se quisermos calcular o gradiente da probabilidade da palavra "gato" (√≠ndice 2) em rela√ß√£o ao seu logit (√≠ndice 2), usamos a derivada $\frac{\partial y_2}{\partial u_2} = y_2(1 - y_2)$. Portanto, $\frac{\partial y_2}{\partial u_2} = 0.738 * (1 - 0.738) = 0.738 * 0.262 \approx 0.193$. Esse valor do gradiente indica o qu√£o sens√≠vel a probabilidade da palavra "gato" √© a mudan√ßas em seu logit.
> Da mesma forma, o gradiente da probabilidade da palavra "o" (√≠ndice 1) em rela√ß√£o ao logit da palavra "gato" (√≠ndice 2) seria $\frac{\partial y_1}{\partial u_2} = -y_1y_2 = -0.037 * 0.738 \approx -0.027$. Esse gradiente negativo indica que aumentar o logit da palavra "gato" diminui a probabilidade da palavra "o". Esses c√°lculos s√£o usados em *backpropagation* para ajustar os pesos do modelo.

**Lema 2.1** *A fun√ß√£o softmax √© invariante √† adi√ß√£o de uma constante a todos os logits.*

*Prova:*
Seja $c$ uma constante. Considere o vetor de logits modificado $u' = [u_1 + c, u_2 + c, ..., u_{|V|} + c]$. As probabilidades $y'_i$ calculadas com $u'$ ser√£o:

$$y'_i = \frac{e^{u_i + c}}{\sum_{j=1}^{|V|} e^{u_j + c}} = \frac{e^{u_i}e^c}{\sum_{j=1}^{|V|} e^{u_j}e^c} = \frac{e^c e^{u_i}}{e^c \sum_{j=1}^{|V|} e^{u_j}} = \frac{e^{u_i}}{\sum_{j=1}^{|V|} e^{u_j}} = y_i$$

Assim, $y'_i = y_i$ para todo $i$. Isso demonstra que a fun√ß√£o *softmax* √© invariante √† adi√ß√£o de uma constante aos logits, pois as probabilidades resultantes s√£o as mesmas. Esta propriedade √© crucial em termos computacionais pois a subtra√ß√£o de uma constante grande dos logits pode evitar o overflow da exponencia√ß√£o. ‚ñ†

> üí° **Exemplo Num√©rico:** Se aos logits [-1, 2, 0.5, -0.5] adicionarmos uma constante, digamos 10, teremos os novos logits [9, 12, 10.5, 9.5]. Ao calcular o softmax com esses novos valores, obteremos exatamente as mesmas probabilidades que com os valores originais. Isso demonstra a invari√¢ncia da fun√ß√£o softmax √† adi√ß√£o de uma constante aos logits. Essa propriedade √© relevante na pr√°tica para melhorar a estabilidade num√©rica do c√°lculo do softmax.

### A Softmax e o Sampling
A fun√ß√£o *softmax*, quando combinada com t√©cnicas de *sampling*, habilita a gera√ß√£o de texto nos modelos de linguagem. Conforme discutido anteriormente [^1], gerar texto em modelos de linguagem envolve iterativamente amostrar a pr√≥xima palavra com base na distribui√ß√£o de probabilidade produzida pela fun√ß√£o *softmax*. A escolha de como amostrar as palavras, seja por *greedy decoding*, *top-k sampling*, ou *temperature sampling*, impacta a diversidade e a qualidade do texto gerado [^1]. A fun√ß√£o *softmax* garante que, independente do m√©todo de *sampling* utilizado, cada palavra do vocabul√°rio tem uma probabilidade definida de ser escolhida, com probabilidades que somam 1.

**Observa√ß√£o 3.** *A distribui√ß√£o de probabilidade gerada pela fun√ß√£o softmax √© uma distribui√ß√£o categ√≥rica.*
Uma distribui√ß√£o categ√≥rica √© uma distribui√ß√£o de probabilidade discreta que descreve a probabilidade de observar cada uma das *k* poss√≠veis categorias (neste caso, as palavras do vocabul√°rio). Como cada $y_i$ da sa√≠da da softmax √© uma probabilidade n√£o negativa e a soma de todos os $y_i$ √© 1, a sa√≠da da softmax atende √† defini√ß√£o de uma distribui√ß√£o categ√≥rica. Assim, a opera√ß√£o de *sampling* pode ser entendida como o ato de sortear uma categoria de acordo com a distribui√ß√£o de probabilidades definida pela softmax.

### Conclus√£o
A fun√ß√£o *softmax* √© um componente fundamental nos modelos Transformers e na modelagem de linguagem em geral. Ao transformar um vetor de *logits* em uma distribui√ß√£o de probabilidade normalizada, ela permite que o modelo avalie e escolha a pr√≥xima palavra em uma sequ√™ncia, com as probabilidades refletindo as rela√ß√µes complexas aprendidas pelo modelo. Al√©m de garantir uma distribui√ß√£o de probabilidade v√°lida, a fun√ß√£o *softmax* amplifica as diferen√ßas entre as pontua√ß√µes, o que √© essencial para tomar decis√µes durante o *sampling* e para guiar o aprendizado do modelo, devido a sua propriedade de diferenciabilidade. A compreens√£o detalhada da fun√ß√£o *softmax* √© fundamental para entender como os modelos de linguagem operam e como podemos refin√°-los para gerar textos de alta qualidade, bem como compreender os mecanismos internos de cada camada por meio da *logit lens*.

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
