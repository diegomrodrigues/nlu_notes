## O Cabe√ßalho de Modelagem de Linguagem em Transformers: Uma An√°lise Detalhada

### Introdu√ß√£o
Este cap√≠tulo aprofunda o conceito de **cabe√ßalho de modelagem de linguagem** (*language modeling head*), um componente crucial para transformar a sa√≠da de um modelo Transformer em uma distribui√ß√£o de probabilidade sobre um vocabul√°rio, permitindo que o modelo preveja a pr√≥xima palavra em uma sequ√™ncia. Como visto anteriormente [^1], modelos de linguagem, incluindo aqueles baseados em Transformers, s√£o essencialmente *preditores de palavras*. O *language modeling head* serve como a ponte final, conectando a rica representa√ß√£o contextualizada aprendida pelo modelo Transformer √† tarefa de prever a pr√≥xima palavra.

### Conceitos Fundamentais

O *language modeling head* opera sobre a sa√≠da da √∫ltima camada do bloco Transformer [^1], um vetor de embedding de dimens√£o $d$ representando o token final da sequ√™ncia de entrada. O objetivo principal deste cabe√ßalho √© transformar esse vetor em uma distribui√ß√£o de probabilidade sobre todas as palavras poss√≠veis no vocabul√°rio $V$. Este processo √© alcan√ßado atrav√©s de duas opera√ß√µes principais: uma proje√ß√£o linear e a fun√ß√£o softmax.

1.  **Proje√ß√£o Linear:**
    O primeiro passo envolve projetar o vetor de embedding da √∫ltima camada do Transformer ($h_N$) para um vetor de *logits*, $u$, usando uma transforma√ß√£o linear. Este vetor $u$ tem uma pontua√ß√£o para cada palavra no vocabul√°rio $V$. Essa transforma√ß√£o linear √© dada por:
    $$ u = h_N W^T $$
    Onde $W^T$ √© a transposta da matriz de embeddings $E$ do modelo [^1], com dimens√µes $d$ x $|V|$, e $d$ √© a dimens√£o do embedding e $|V|$ o tamanho do vocabul√°rio. O vetor de *logits* $u$ tem dimens√µes 1 √ó $|V|$. √â crucial notar que essa matriz de proje√ß√£o $W$ √© muitas vezes vinculada (weight tying) √† matriz de embeddings $E$ [^1].
    
    > üí° **Exemplo Num√©rico:** Suponha que a dimens√£o do embedding $d$ seja 512 e o tamanho do vocabul√°rio $|V|$ seja 10000. O vetor de sa√≠da da √∫ltima camada do Transformer, $h_N$, √© um vetor de dimens√£o 1x512. A matriz de embeddings $E$ tem dimens√µes 10000x512. Portanto, a matriz de proje√ß√£o $W$ (antes da transposi√ß√£o) tem dimens√µes 512x10000. A transposta de $W$, ou seja,  $W^T$, tem dimens√£o 10000x512. A multiplica√ß√£o de $h_N$ por $W^T$ resulta num vetor de *logits* $u$ de dimens√£o 1x10000. Este vetor cont√©m um *logit* para cada palavra do vocabul√°rio, representando a pontua√ß√£o n√£o normalizada de cada palavra ser a pr√≥xima na sequ√™ncia.

2.  **Vincula√ß√£o de Pesos (Weight Tying):**
    Como mencionado, em muitos casos, a matriz de proje√ß√£o linear $W$ √© vinculada √† matriz de embedding $E$ do modelo. Isso significa que $W$ √© igual √† transposta da matriz de embeddings $E$, ou seja, $W=E^T$. Essa t√©cnica de *weight tying* implica que os embeddings de entrada e a proje√ß√£o para o espa√ßo de logits usam os mesmos pesos [^1]. Essa pr√°tica ajuda a otimizar o processo de aprendizado e reduzir o n√∫mero de par√¢metros no modelo, e tamb√©m serve como ferramenta de interpretabilidade, como a *logit lens* que ser√° abordada mais adiante [^1]. Al√©m disso, a vincula√ß√£o de pesos pode ter um efeito regularizador, melhorando a generaliza√ß√£o do modelo.

    **Lema 1.** *A vincula√ß√£o de pesos reduz o n√∫mero total de par√¢metros trein√°veis no modelo.*

    *Prova*:
    I. Seja $N_E$ o n√∫mero de par√¢metros na matriz de embeddings $E$. O n√∫mero de par√¢metros √© igual ao produto das dimens√µes da matriz, portanto, se $E$ tem dimens√£o $|V| \times d$, ent√£o $N_E = |V| \times d$.
    II. Se $W$ fosse uma matriz de proje√ß√£o distinta, o n√∫mero de par√¢metros adicionais seria $N_W = d \times |V|$.
    III. Com a vincula√ß√£o de pesos ($W = E^T$), temos que $W$ tem dimens√£o $d \times |V|$,  e $W = E^T$. Portanto, $N_W = |V| \times d$ e os par√¢metros da proje√ß√£o s√£o compartilhados com os da matriz de embeddings ($N_W = N_E$).
    IV. Como o n√∫mero de par√¢metros adicionais passa a ser zero ao se usar *weight tying*, o n√∫mero total de par√¢metros trein√°veis do modelo √© reduzido ao usar a vincula√ß√£o de pesos.
    ‚ñ†

    > üí° **Exemplo Num√©rico:** Usando o exemplo anterior com $d$=512 e $|V|$=10000, sem *weight tying*, a matriz de embeddings $E$ teria 512 * 10000 = 5.120.000 par√¢metros, e a matriz de proje√ß√£o $W$ teria tamb√©m 512 * 10000 = 5.120.000 par√¢metros, totalizando 10.240.000 par√¢metros. Com *weight tying*, a matriz $W$ √© igual a $E^T$ e, portanto, ambas compartilham os mesmos 5.120.000 par√¢metros, reduzindo √† metade o n√∫mero de par√¢metros no modelo.

3.  **Fun√ß√£o Softmax:**
    O vetor de *logits* $u$ representa pontua√ß√µes n√£o normalizadas para cada palavra no vocabul√°rio. Para transformar essas pontua√ß√µes em probabilidades, usamos a fun√ß√£o *softmax*. Essa fun√ß√£o garante que todas as probabilidades sejam positivas e somem 1, criando uma distribui√ß√£o de probabilidade v√°lida sobre o vocabul√°rio. A fun√ß√£o *softmax* √© definida como:
    $$ y_i = \frac{e^{u_i}}{\sum_{j=1}^{|V|} e^{u_j}} $$
    Onde $y$ √© o vetor de probabilidade de tamanho $|V|$, $u_i$ √© o i-√©simo elemento do vetor de *logits* $u$ e $|V|$ √© o tamanho do vocabul√°rio. O resultado $y$ √© uma distribui√ß√£o de probabilidade onde cada valor representa a probabilidade de uma palavra espec√≠fica ser a pr√≥xima palavra da sequ√™ncia [^1].

     > üí° **Exemplo Num√©rico:** Suponha que temos um vocabul√°rio de 3 palavras: "gato", "cachorro", e "p√°ssaro". O vetor de logits $u$ (1x3) resultante da proje√ß√£o linear √© [2.0, 1.0, 0.5]. Aplicamos a fun√ß√£o softmax para transformar esses logits em probabilidades.

    $\text{Step 1: } e^{u_i}$:
     $$ e^{2.0} \approx 7.389 \\ e^{1.0} \approx 2.718 \\ e^{0.5} \approx 1.649 $$

    $\text{Step 2: } \sum_{j=1}^{|V|} e^{u_j} = 7.389 + 2.718 + 1.649 = 11.756 $

    $\text{Step 3: } y_i = \frac{e^{u_i}}{\sum_{j=1}^{|V|} e^{u_j}}$
    $$ y_1 = \frac{7.389}{11.756} \approx 0.628 \\ y_2 = \frac{2.718}{11.756} \approx 0.231 \\ y_3 = \frac{1.649}{11.756} \approx 0.140 $$

    O vetor de probabilidades resultante $y$ √© aproximadamente [0.628, 0.231, 0.140], onde a probabilidade de a pr√≥xima palavra ser "gato" √© 0.628, "cachorro" √© 0.231 e "p√°ssaro" √© 0.140. A soma das probabilidades √© aproximadamente 1 (0.628 + 0.231 + 0.140 = 0.999).

    **Proposi√ß√£o 1.** *A fun√ß√£o softmax transforma um vetor de logits em uma distribui√ß√£o de probabilidade v√°lida.*

    *Prova*:
    I. Pela defini√ß√£o da fun√ß√£o exponencial, $e^{u_i}$ √© sempre positivo para qualquer valor real de $u_i$. Portanto, para cada $i$,  $e^{u_i}$ > 0, o que garante que $y_i$ seja sempre positivo.
    II. Para mostrar que a soma das probabilidades √© 1, somamos $y_i$ para todos os valores de $i$ de 1 a $|V|$:

    $$ \sum_{i=1}^{|V|} y_i = \sum_{i=1}^{|V|} \frac{e^{u_i}}{\sum_{j=1}^{|V|} e^{u_j}} $$

    III. A soma do denominador n√£o depende de $i$, e podemos colocar para fora do somat√≥rio:

    $$  \sum_{i=1}^{|V|} y_i =  \frac{1}{\sum_{j=1}^{|V|} e^{u_j}} \sum_{i=1}^{|V|} e^{u_i} $$

     IV. Os somat√≥rios no numerador e denominador s√£o iguais, portanto:
     $$ \sum_{i=1}^{|V|} y_i = \frac{\sum_{i=1}^{|V|} e^{u_i}}{\sum_{j=1}^{|V|} e^{u_j}} = 1 $$

    V. Portanto, a fun√ß√£o *softmax* mapeia o vetor de *logits* $u$ em um vetor de probabilidade $y$ onde cada entrada $y_i$ est√° entre 0 e 1, e a soma de todas as entradas √© igual a 1, resultando em uma distribui√ß√£o de probabilidade v√°lida.
    ‚ñ†

Em resumo, o *language modeling head* [^1] transforma a sa√≠da da √∫ltima camada do Transformer, que √© um vetor denso de representa√ß√£o contextualizada, para um espa√ßo onde podemos aplicar uma fun√ß√£o *softmax* para obter a probabilidade de cada palavra do vocabul√°rio ser a pr√≥xima na sequ√™ncia. Esse processo transforma representa√ß√µes abstratas em probabilidades pr√°ticas, que direcionam a gera√ß√£o de texto por meio de t√©cnicas de *sampling* [^1].

### A Lente de Logits (Logit Lens)
O conceito de *logit lens* [^1] oferece uma forma de interpretar os estados internos de um modelo Transformer. Ao pegar um vetor de qualquer camada do Transformer e multiplic√°-lo pela matriz de *unembedding* (transposta da matriz de embeddings), obt√™m-se *logits*, que podem ser transformados em uma distribui√ß√£o de probabilidade sobre o vocabul√°rio. A *logit lens* funciona como uma lupa sobre a representa√ß√£o interna do modelo [^1], permitindo verificar quais palavras a representa√ß√£o de uma dada camada "pensa" que devem vir em seguida, mesmo que essa camada n√£o tenha sido treinada para desempenhar essa fun√ß√£o. √â importante notar que, se a matriz de proje√ß√£o $W$ for vinculada √† matriz de embedding $E$ ($W=E^T$), a opera√ß√£o da *logit lens* √© equivalente √† multiplica√ß√£o do vetor da camada pela matriz de proje√ß√£o que seria utilizada no cabe√ßalho de modelagem de linguagem.

    > üí° **Exemplo Num√©rico:**  Considere um modelo Transformer com uma matriz de embedding $E$ (10000 x 512), um vetor $h_{layer}$ (1 x 512) de uma camada intermedi√°ria. Se aplicarmos a *logit lens*, ou seja, multiplicarmos  $h_{layer}$ por $E^T$ (512 x 10000), obtemos um vetor de logits de dimens√£o 1x10000. Ao aplicar softmax a esse vetor, obtemos uma distribui√ß√£o de probabilidade sobre o vocabul√°rio de 10000 palavras. Se as maiores probabilidades forem para palavras como 'o', 'e', e 'a', isso indicaria que, mesmo nessa camada intermedi√°ria, o modelo j√° est√° reconhecendo informa√ß√µes sobre as palavras mais comuns no contexto da sequ√™ncia atual. Isso pode ser utilizado para entender em qual n√≠vel de abstra√ß√£o o modelo est√° trabalhando em cada camada.

### Conclus√£o
O *language modeling head* √© o componente final que permite a um Transformer funcionar como um modelo de linguagem completo. Ele pega as representa√ß√µes ricas aprendidas pelo modelo Transformer e as transforma em um formato que permite prever a pr√≥xima palavra numa sequ√™ncia com probabilidades v√°lidas. Atrav√©s da proje√ß√£o linear (com *weight tying*, em muitos casos) e a fun√ß√£o *softmax*, esse componente √© fundamental para a gera√ß√£o de texto e outras tarefas de processamento de linguagem natural. Compreender o funcionamento do *language modeling head* √© essencial para dominar o funcionamento dos modelos Transformers e para explorar t√©cnicas de gera√ß√£o de texto mais avan√ßadas como *sampling* [^1].

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
