## O Mapeamento do Cabe√ßalho de Modelagem de Linguagem para Distribui√ß√µes de Probabilidade e a Import√¢ncia do *Weight Tying*

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre o *language modeling head* [^1] e a fun√ß√£o *softmax* [^2], este cap√≠tulo abordar√° o processo de mapeamento da sa√≠da do √∫ltimo bloco Transformer para uma distribui√ß√£o de probabilidade sobre todas as palavras do vocabul√°rio. Exploraremos o uso da matriz de embeddings transposta, tamb√©m conhecida como *unembedding layer*, para projetar a sa√≠da do Transformer em *logits*. Adicionalmente, aprofundaremos a pr√°tica comum do '*weight tying*', onde os pesos na proje√ß√£o do *language modeling head* s√£o vinculados (iguais) √† matriz de embeddings de entrada. Analisaremos como o '*weight tying*' utiliza a transposta da matriz de embeddings de entrada como a camada linear de sa√≠da, garantindo que o modelo seja proficiente em ambos os mapeamentos, e como isso impacta a efici√™ncia de par√¢metros e o aprendizado do modelo.

### Mapeamento para Distribui√ß√£o de Probabilidade
Como vimos nos cap√≠tulos anteriores [^1, ^2], o objetivo do *language modeling head* √© transformar o vetor de embedding da √∫ltima camada do Transformer ($h_N$), que tem dimens√£o $d$ [^1], em uma distribui√ß√£o de probabilidade sobre as $|V|$ palavras do vocabul√°rio. O mapeamento desse vetor para uma distribui√ß√£o de probabilidade ocorre em duas etapas principais:

1.  **Proje√ß√£o Linear para Logits:** A primeira etapa √© uma transforma√ß√£o linear, onde o vetor $h_N$ √© multiplicado pela transposta da matriz de embeddings ($E^T$), tamb√©m conhecida como *unembedding layer* [^1], para obter um vetor de *logits* $u$. Formalmente, esta opera√ß√£o √© definida como:
    $$ u = h_N E^T $$
    Onde:
    *   $u$ √© o vetor de *logits*, de dimens√£o 1 x $|V|$.
    *   $h_N$ √© o vetor de embedding de sa√≠da da √∫ltima camada do Transformer, de dimens√£o 1 x $d$.
    *   $E^T$ √© a transposta da matriz de embeddings $E$, tamb√©m conhecida como *unembedding layer*, de dimens√£o $d$ x $|V|$.
    Essa opera√ß√£o mapeia o vetor de embedding de dimens√£o $d$ para um vetor de *logits* de dimens√£o $|V|$, onde cada entrada representa a pontua√ß√£o n√£o normalizada para cada palavra do vocabul√°rio.

> üí° **Exemplo Num√©rico:** Vamos supor que o vetor $h_N$ seja um vetor 1x5 de valores [0.2, -0.1, 0.5, 0.3, -0.2], e a matriz de embeddings $E$ tenha dimens√£o 5x3, representando um vocabul√°rio de 3 palavras com embeddings de dimens√£o 5. A transposta de $E$, $E^T$, teria dimens√£o 3x5. Vamos definir $E$ como:
> ```
> E = [[0.1, 0.2, 0.3, 0.4, 0.5],
>      [0.6, 0.7, 0.8, 0.9, 1.0],
>      [1.1, 1.2, 1.3, 1.4, 1.5]]
> ```
> Ent√£o, $E^T$ seria:
> ```
> E^T = [[0.1, 0.6, 1.1],
>        [0.2, 0.7, 1.2],
>        [0.3, 0.8, 1.3],
>        [0.4, 0.9, 1.4],
>        [0.5, 1.0, 1.5]]
> ```
> O c√°lculo de $u = h_N E^T$ seria:
>
> $\text{Step 1: } u_1 = (0.2 * 0.1) + (-0.1 * 0.6) + (0.5 * 1.1) + (0.3 * 0.4) + (-0.2 * 0.5) = 0.55$
>
> $\text{Step 2: } u_2 = (0.2 * 0.2) + (-0.1 * 0.7) + (0.5 * 1.2) + (0.3 * 0.9) + (-0.2 * 1.0) = 0.64$
>
> $\text{Step 3: } u_3 = (0.2 * 0.3) + (-0.1 * 0.8) + (0.5 * 1.3) + (0.3 * 1.4) + (-0.2 * 1.5) = 0.73$
>
> Logo, $u = [0.55, 0.64, 0.73]$. Este √© o vetor de logits.
> Este vetor de logits indica as pontua√ß√µes n√£o normalizadas para cada palavra do vocabul√°rio.
>
>
2. **Softmax para Probabilidades:** O vetor de *logits* $u$ √© ent√£o usado como entrada para a fun√ß√£o *softmax* [^2]. Esta fun√ß√£o normaliza os *logits*, produzindo uma distribui√ß√£o de probabilidade sobre o vocabul√°rio, onde cada valor $y_i$ representa a probabilidade da palavra $i$ ser a pr√≥xima palavra na sequ√™ncia. A fun√ß√£o *softmax* √© definida como:
    $$ y_i = \frac{e^{u_i}}{\sum_{j=1}^{|V|} e^{u_j}} $$
    Onde:
    * $y_i$ √© a probabilidade da palavra $i$.
    * $u_i$ √© o *logit* correspondente √† palavra $i$.
    * $|V|$ √© o tamanho do vocabul√°rio.
   Como demonstrado anteriormente [^2], a fun√ß√£o softmax garante que as probabilidades somem 1 e sejam valores n√£o negativos, resultando em uma distribui√ß√£o de probabilidade v√°lida [^2].

> üí° **Exemplo Num√©rico (continua√ß√£o):** Usando os logits calculados no exemplo anterior, $u = [0.55, 0.64, 0.73]$, e aplicando a fun√ß√£o *softmax*, temos:
>
> $\text{Step 1: } e^{u_1} = e^{0.55} \approx 1.733$
>
> $\text{Step 2: } e^{u_2} = e^{0.64} \approx 1.896$
>
> $\text{Step 3: } e^{u_3} = e^{0.73} \approx 2.075$
>
> $\text{Step 4: } \sum_{j=1}^{3} e^{u_j} = 1.733 + 1.896 + 2.075 \approx 5.704$
>
> $\text{Step 5: } y_1 = \frac{1.733}{5.704} \approx 0.304$
>
> $\text{Step 6: } y_2 = \frac{1.896}{5.704} \approx 0.332$
>
> $\text{Step 7: } y_3 = \frac{2.075}{5.704} \approx 0.364$
>
>  O vetor de probabilidades resultante √©, aproximadamente, $y = [0.304, 0.332, 0.364]$. A soma das probabilidades √© aproximadamente 1 (0.304 + 0.332 + 0.364 = 1.0). Isso significa que a terceira palavra do vocabul√°rio tem a maior probabilidade de ser a pr√≥xima palavra na sequ√™ncia, de acordo com este modelo.
>

### *Weight Tying* e a Camada de *Unembedding*

Uma pr√°tica comum na constru√ß√£o de modelos de linguagem baseados em Transformers √© o '*weight tying*' [^1]. Nessa t√©cnica, a matriz de proje√ß√£o linear utilizada no *language modeling head* √© vinculada (ou seja, igual) √† transposta da matriz de embeddings de entrada, usada no in√≠cio da arquitetura do Transformer. Em outras palavras, a matriz de pesos $W$ da proje√ß√£o linear ($u= h_N W^T$) √© definida como a transposta da matriz de embeddings $E$, ou seja, $W=E^T$. Assim, $W^T$ passa a ser igual √† matriz de embeddings original $E$. A principal motiva√ß√£o para usar '*weight tying*' √© a de melhorar a efici√™ncia de par√¢metros e a regulariza√ß√£o do modelo.

**Lema 3.** *O uso de weight tying implica que o modelo aprende representa√ß√µes de palavras que s√£o √∫teis tanto para a entrada quanto para a sa√≠da do modelo.*

*Prova:*
I. Na entrada do modelo, a matriz de embeddings $E$ √© utilizada para transformar um vetor one-hot de dimens√£o $|V|$ em uma representa√ß√£o densa da palavra com dimens√£o $d$, onde $d$ √© a dimens√£o de embedding.
II. Na sa√≠da do modelo, durante a modelagem de linguagem, a matriz de proje√ß√£o $W^T$ (que, com *weight tying*, √© igual a $E$) √© usada para transformar uma representa√ß√£o densa do contexto (o vetor $h_N$) em *logits*, que indicam a pontua√ß√£o para cada palavra do vocabul√°rio.
III. Ao usar o *weight tying* ($W=E^T$), o modelo √© treinado para aprender uma representa√ß√£o de palavra que seja √∫til tanto para o mapeamento de um vetor one-hot para um vetor de embedding (representa√ß√£o de entrada), quanto para o mapeamento de um vetor de embedding para um vetor de *logits* (representa√ß√£o de sa√≠da).
IV. Como a matriz de embeddings $E$ serve para mapear *one-hot vectors* para *embeddings*, e a matriz de proje√ß√£o $E^T$ mapeia *embeddings* para *logits*, o modelo aprende representa√ß√µes de palavras que podem ser usadas tanto para representar a palavra como entrada, quanto para predizer a probabilidade de uma palavra como sa√≠da. Isso melhora a efici√™ncia do modelo, pois os mesmos par√¢metros s√£o usados para ambos os processos.
‚ñ†
**Lema 3.1.** *A opera√ß√£o de proje√ß√£o linear com weight tying pode ser interpretada como um c√°lculo de similaridade entre o embedding contextualizado h_N e os embeddings de todas as palavras do vocabul√°rio.*

*Prova:*
I. A proje√ß√£o linear com *weight tying* √© dada por $u = h_N E^T$.
II. Essa opera√ß√£o pode ser reescrita como $u_i = h_N E_i^T$, onde $u_i$ √© o logit para a palavra $i$ e $E_i$ √© o embedding da palavra $i$ (a i-√©sima coluna de E).
III. A express√£o $h_N E_i^T$ √© equivalente ao produto escalar entre o vetor $h_N$ e o vetor $E_i$, que mede a similaridade entre os dois vetores no espa√ßo vetorial.
IV. Portanto, o logit $u_i$ pode ser interpretado como uma medida da similaridade entre a representa√ß√£o contextualizada $h_N$ e a representa√ß√£o da palavra $i$.
V. Ao realizar esse c√°lculo para todas as palavras do vocabul√°rio, o modelo encontra as palavras que s√£o mais semanticamente pr√≥ximas ao contexto $h_N$.
‚ñ†

> üí° **Exemplo Num√©rico:** Considere um vocabul√°rio de 10000 palavras, com cada palavra tendo um embedding de dimens√£o 512. Sem *weight tying*, ter√≠amos uma matriz de embeddings $E$ com dimens√µes 10000x512 (5.120.000 par√¢metros) e uma matriz de proje√ß√£o $W$ com dimens√µes 512x10000 (5.120.000 par√¢metros), totalizando 10.240.000 par√¢metros. Com *weight tying*, a matriz de proje√ß√£o $W$ passa a ser $E^T$, e tanto $E$ quanto $E^T$ compartilham os mesmos 5.120.000 par√¢metros. Isso reduz o n√∫mero total de par√¢metros a serem treinados e garante que o modelo aprenda representa√ß√µes de palavras que sejam eficazes tanto para a entrada quanto para a sa√≠da.
>
> üí° **Exemplo Num√©rico:** Para visualizar o efeito do *weight tying* no c√°lculo da similaridade, considere o exemplo anterior com um vocabul√°rio de 3 palavras, onde $h_N = [0.2, -0.1, 0.5, 0.3, -0.2]$ e a matriz $E$ foi definida como:
> ```
> E = [[0.1, 0.2, 0.3, 0.4, 0.5],
>      [0.6, 0.7, 0.8, 0.9, 1.0],
>      [1.1, 1.2, 1.3, 1.4, 1.5]]
> ```
>
> Os logits, calculados anteriormente, $u = [0.55, 0.64, 0.73]$, representam o produto escalar entre $h_N$ e cada embedding das palavras em $E$. Em ess√™ncia, $u_1 = h_N . E_1$, onde $E_1$ √© a primeira linha de E, e assim por diante. Logits maiores indicam maior similaridade entre $h_N$ e os embeddings das palavras, ou seja, a palavra correspondente tem maior probabilidade de ocorrer no contexto.

**Benef√≠cios do *Weight Tying***

O *weight tying* oferece m√∫ltiplos benef√≠cios:

1.  **Efici√™ncia de Par√¢metros:** Como a mesma matriz de pesos √© usada para os embeddings de entrada e a proje√ß√£o linear de sa√≠da, o n√∫mero total de par√¢metros trein√°veis no modelo √© reduzido. Isso diminui o risco de overfitting, especialmente em modelos com vocabul√°rios grandes.
2.  **Regulariza√ß√£o:** Compartilhar pesos entre diferentes partes do modelo atua como uma forma de regulariza√ß√£o, j√° que o modelo aprende representa√ß√µes de palavras que s√£o √∫teis tanto para a entrada quanto para a sa√≠da. Isso melhora a generaliza√ß√£o do modelo, fazendo com que ele tenha melhor desempenho com dados nunca vistos.
3.  **Simetria:** O *weight tying* imp√µe uma certa simetria ao processo de aprendizado. Como a matriz de proje√ß√£o de sa√≠da √© a transposta da matriz de embedding, ela est√°, de certa forma, *desfazendo* a opera√ß√£o realizada no in√≠cio do modelo. Isso pode fazer com que a representa√ß√£o final do texto seja mais *semanticamente* alinhada com a representa√ß√£o de entrada.
4.  **Interpretabilidade:** A t√©cnica tamb√©m tem vantagens em termos de interpretabilidade. A matriz de embedding, que antes era usada apenas como representa√ß√£o de entrada, passa a ser √∫til como uma matriz para analisar os resultados do modelo. A *logit lens* √© um exemplo de como podemos usar essa propriedade para analisar a informa√ß√£o interna do modelo [^1].
5.  **Simplicidade:** A implementa√ß√£o do *weight tying* adiciona pouca ou nenhuma complexidade ao c√≥digo do modelo, sendo implementada atrav√©s de uma simples atribui√ß√£o de par√¢metros.

### O Papel do *Unembedding Layer*

O termo '*unembedding layer*' refere-se √† transposta da matriz de embeddings ($E^T$), que √© utilizada para projetar o vetor de sa√≠da do Transformer ($h_N$) para o espa√ßo de *logits*. Essa camada realiza o inverso do embedding de entrada, mapeando de um vetor de embedding para um vetor de pontua√ß√µes (logits), e √© tamb√©m chamada de *camada de decodifica√ß√£o* ou *camada de proje√ß√£o para o vocabul√°rio*.

O *unembedding layer*, com *weight tying*, desempenha um papel fundamental em:

1.  **Mapeamento para o Espa√ßo de Logits:** Projeta a representa√ß√£o contextualizada do Transformer ($h_N$) para o espa√ßo de *logits*, onde cada valor corresponde √† pontua√ß√£o de uma palavra no vocabul√°rio [^1]. Essa proje√ß√£o √© essencial para transformar as representa√ß√µes internas do modelo em uma forma que possa ser usada para fazer uma previs√£o de qual ser√° a pr√≥xima palavra.
2.  **Gera√ß√£o de Texto:** Permite que o modelo gere texto ao converter representa√ß√µes internas em probabilidades sobre um vocabul√°rio [^1]. O modelo pode usar essas probabilidades para selecionar a pr√≥xima palavra em um processo iterativo.
3. **An√°lise da Representa√ß√£o:** Permite a utiliza√ß√£o da *logit lens* [^1], que possibilita a an√°lise das representa√ß√µes internas do modelo. Ao mapear os vetores de diferentes camadas com a *unembedding layer* para obter *logits*, podemos ver quais palavras o modelo "acha" que devem vir em seguida.

**Proposi√ß√£o 4.** *O uso da unembedding layer com weight tying permite interpretar os logits como medidas de similaridade, o que √© fundamental para a gera√ß√£o de texto e an√°lise da representa√ß√£o interna do modelo.*

*Prova:*
I. Conforme o Lema 3.1, a proje√ß√£o linear na sa√≠da do modelo, quando utilizado *weight tying*,  √© equivalente a calcular a similaridade entre a representa√ß√£o contextualizada $h_N$ e as representa√ß√µes das palavras do vocabul√°rio.
II. A *unembedding layer* √© a transposta da matriz de embeddings, e com *weight tying* ela serve tanto para proje√ß√£o dos *embeddings* para *logits* quanto para converter um vetor one-hot em um *embedding*.
III. Ao aplicar a *unembedding layer* no vetor $h_N$, o modelo est√°, efetivamente, comparando a representa√ß√£o contextualizada $h_N$ com as representa√ß√µes de todas as palavras do vocabul√°rio, retornando os *logits* que indicam o quanto cada palavra se encaixa no contexto.
IV.  Essa interpreta√ß√£o dos *logits* como medidas de similaridade √© crucial, pois permite que o modelo selecione a palavra mais apropriada para a sequ√™ncia, com base no grau de similaridade. Al√©m disso, essa rela√ß√£o permite que as representa√ß√µes internas do modelo sejam analisadas usando a *logit lens*.
‚ñ†

### Conclus√£o

O mapeamento da sa√≠da do Transformer para uma distribui√ß√£o de probabilidade sobre o vocabul√°rio √© crucial para a modelagem de linguagem. O uso da *unembedding layer* (a transposta da matriz de embeddings) e a t√©cnica de '*weight tying*' s√£o pr√°ticas comuns que melhoram a efici√™ncia de par√¢metros, regularizam o modelo e garantem que o modelo aprenda representa√ß√µes de palavras que sejam √∫teis tanto na entrada quanto na sa√≠da do modelo. Ao vincular os pesos da proje√ß√£o da *unembedding layer* com os da matriz de embeddings, o modelo aprende representa√ß√µes de palavras que podem ser usadas para mapear *one-hot vectors* para *embeddings* e *embeddings* de volta para *logits*, desempenhando um papel essencial na tarefa de previs√£o da pr√≥xima palavra.

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^2]: O Papel da Fun√ß√£o Softmax na Modelagem de Linguagem com Transformers
<!-- END -->
