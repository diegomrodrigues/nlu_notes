## Leis de Escala: Determinantes do Desempenho de LLMs

### Introdu√ß√£o
Este cap√≠tulo aprofunda a discuss√£o sobre o treinamento de *Large Language Models* (LLMs), focando nas chamadas **leis de escala**. Como vimos nos cap√≠tulos anteriores, o treinamento de LLMs √© um processo complexo, que envolve a auto-supervis√£o, o uso da perda cross-entropy, a t√©cnica de *teacher forcing* e otimizadores como o Adam [^28, 29, 31]. Nesta se√ß√£o, exploraremos como o desempenho dos LLMs √© influenciado por tr√™s fatores fundamentais: o tamanho do modelo (n√∫mero de par√¢metros), o tamanho do *dataset* de treinamento e a quantidade de computa√ß√£o utilizada para o treinamento. As rela√ß√µes entre esses fatores e o desempenho do modelo s√£o descritas pelas chamadas *scaling laws*, que nos ajudam a compreender como treinar esses modelos de forma mais eficiente.

### Os Tr√™s Pilares do Desempenho de LLMs

O desempenho de um LLM n√£o depende apenas de sua arquitetura, mas tamb√©m de tr√™s fatores que atuam em conjunto [^32]:

1.  **Tamanho do Modelo (N√∫mero de Par√¢metros):** O n√∫mero de par√¢metros de um modelo, que inclui as matrizes de pesos, os *biases* e outros par√¢metros que s√£o aprendidos durante o treinamento, influencia a capacidade do modelo de capturar a complexidade da linguagem. Modelos maiores t√™m mais capacidade de memoriza√ß√£o e generaliza√ß√£o, mas tamb√©m exigem mais dados e poder computacional para serem treinados.
2.  **Tamanho do Dataset de Treinamento:** O tamanho do *dataset* de treinamento, medido em *tokens*, afeta a capacidade do modelo de aprender representa√ß√µes robustas da linguagem. *Datasets* maiores proporcionam mais exemplos para o modelo aprender, reduzindo o *overfitting* e melhorando a capacidade de generaliza√ß√£o para dados n√£o vistos.
3.  **Or√ßamento Computacional (Compute Budget):** A quantidade de computa√ß√£o usada no treinamento, medida em opera√ß√µes de ponto flutuante (FLOPs) por segundo ou dias de processamento (petaflop-days), influencia a qualidade do modelo. Um maior or√ßamento computacional permite que o modelo seja treinado por mais itera√ß√µes, com *batches* maiores e usando algoritmos de otimiza√ß√£o mais eficientes, levando a um melhor desempenho final.

**Teorema 9:** O desempenho de um LLM, medido pela sua capacidade de prever a pr√≥xima palavra (ou pela perda cross-entropy), aumenta com o aumento do tamanho do modelo, o tamanho do *dataset* e o or√ßamento computacional utilizado no treinamento, conforme indicado pelas *scaling laws*.

*Prova:*
I.  O tamanho do modelo (n√∫mero de par√¢metros) define a capacidade do modelo de aprender as rela√ß√µes complexas existentes em um *dataset* de treinamento.
II. O tamanho do *dataset* de treinamento determina a quantidade de exemplos de aprendizagem dispon√≠veis para o modelo, evitando o *overfitting* e melhorando a generaliza√ß√£o.
III. O or√ßamento computacional (tempo ou recursos dedicados ao treinamento) define o n√∫mero de itera√ß√µes e ajustes do modelo, afetando sua capacidade de atingir um √≥timo local (ou global) na fun√ß√£o de perda.
IV. Modelos maiores, com mais dados de treino e mais itera√ß√µes de treino convergem mais rapidamente, aprendem representa√ß√µes mais robustas e generalizam melhor.
V.  As *scaling laws* quantificam essas rela√ß√µes, mostrando que o desempenho do modelo melhora √† medida que esses fatores aumentam.
VI. Logo, o desempenho de um LLM aumenta com o tamanho do modelo, o tamanho do *dataset* e o or√ßamento computacional.
$\blacksquare$

### Scaling Laws: Rela√ß√µes de Lei de Pot√™ncia

As **leis de escala** ( *scaling laws* ) descrevem como o desempenho de um LLM, tipicamente expresso em termos da perda de treinamento, se relaciona com cada um dos tr√™s fatores mencionados acima, mantendo os outros dois fatores constantes [^32]. Essas rela√ß√µes foram observadas empiricamente e mostram que o desempenho (perda) escala de acordo com uma lei de pot√™ncia em rela√ß√£o ao tamanho do modelo ($N$), ao tamanho do *dataset* ($D$) e ao or√ßamento computacional ($C$). As *scaling laws* s√£o representadas pelas seguintes equa√ß√µes:

1.  **Rela√ß√£o com o tamanho do modelo:**
    $$L(N) = \left( \frac{N_c}{N} \right)^{\alpha_N}$$
    onde $L(N)$ √© a perda, $N$ √© o n√∫mero de par√¢metros do modelo, $N_c$ √© uma constante e $\alpha_N$ √© um expoente que define a inclina√ß√£o da rela√ß√£o. Essa equa√ß√£o mostra que a perda diminui √† medida que o n√∫mero de par√¢metros do modelo aumenta.
2.  **Rela√ß√£o com o tamanho do dataset:**
    $$L(D) = \left( \frac{D_c}{D} \right)^{\alpha_D}$$
    onde $L(D)$ √© a perda, $D$ √© o tamanho do *dataset*, $D_c$ √© uma constante e $\alpha_D$ √© um expoente que define a inclina√ß√£o da rela√ß√£o. Essa equa√ß√£o mostra que a perda diminui √† medida que o tamanho do *dataset* aumenta.
3.  **Rela√ß√£o com o or√ßamento computacional:**
     $$L(C) = \left( \frac{C_c}{C} \right)^{\alpha_C}$$
     onde $L(C)$ √© a perda, $C$ √© o or√ßamento computacional, $C_c$ √© uma constante e $\alpha_C$ √© um expoente que define a inclina√ß√£o da rela√ß√£o. Essa equa√ß√£o mostra que a perda diminui √† medida que o or√ßamento computacional aumenta.
> üí° **Exemplo Num√©rico:** Suponha que um modelo inicial com 10 milh√µes de par√¢metros (N1) tenha uma perda de 0.5. Se o modelo for aumentado para 100 milh√µes de par√¢metros (N2), podemos aplicar a rela√ß√£o de lei de pot√™ncia ($L(N) = (N_c/N)^{\alpha_N}$) para prever a nova perda. Se soubermos que, para esta arquitetura, a constante $N_c$ √© 1 milh√£o, e o expoente $\alpha_N$ √© 0.07, podemos calcular:
> $$ L(N_1) = (\frac{10^6}{10^7})^{0.07} = 0.86 $$
> $$ L(N_2) = (\frac{10^6}{10^8})^{0.07} = 0.69 $$
> Isso mostra que, ao aumentar o tamanho do modelo de 10 para 100 milh√µes de par√¢metros, a perda diminui de 0.86 para 0.69. Note que, nessa rela√ß√£o, usamos a perda normalizada e n√£o a perda absoluta, pois os valores de $N_c$ e $\alpha$ s√£o definidos com o modelo e *dataset* base como refer√™ncia.
> üí° **Exemplo Num√©rico:** Considere um modelo com uma perda inicial de 0.9 quando treinado com 1 bilh√£o de tokens (D1). Se dobrarmos o tamanho do *dataset* para 2 bilh√µes de tokens (D2), mantendo o tamanho do modelo e o or√ßamento computacional constantes, podemos usar a lei de pot√™ncia para o tamanho do *dataset*. Suponha que $D_c$ seja 500 milh√µes e $\alpha_D$ seja 0.05.
>
>$$ L(D_1) = \left( \frac{5 \times 10^8}{1 \times 10^9} \right)^{0.05} = (0.5)^{0.05} \approx 0.966 $$
>
>$$ L(D_2) = \left( \frac{5 \times 10^8}{2 \times 10^9} \right)^{0.05} = (0.25)^{0.05} \approx 0.933 $$
>
>O exemplo mostra uma diminui√ß√£o na perda ao dobrar o tamanho do *dataset*, ainda que em uma magnitude menor que o aumento nos par√¢metros.

As constantes $N_c$, $D_c$, $C_c$, e os expoentes $\alpha_N$, $\alpha_D$ e $\alpha_C$ variam dependendo da arquitetura do modelo, da natureza dos dados e do processo de otimiza√ß√£o. No entanto, a rela√ß√£o de lei de pot√™ncia se mant√©m v√°lida na grande maioria dos casos, e essas rela√ß√µes s√£o empiricamente bem estabelecidas.

**Lema 9.1:** As rela√ß√µes de lei de pot√™ncia nas *scaling laws* mostram que o aumento no tamanho do modelo, o tamanho do *dataset* ou o or√ßamento computacional produzem ganhos decrescentes no desempenho do modelo.

*Prova:*
I.  As *scaling laws* mostram que o desempenho de um LLM melhora como uma fun√ß√£o de lei de pot√™ncia em rela√ß√£o aos tr√™s fatores. Isso implica que o aumento no desempenho √© proporcional √† raiz do aumento do fator, e n√£o ao aumento linear no fator.
II. A forma geral da lei de pot√™ncia √© $L(x) \propto x^\alpha$ onde $\alpha$ √© menor do que 1. Como $\alpha < 1$, a inclina√ß√£o da fun√ß√£o diminui √† medida que $x$ aumenta.
III. Isso significa que, √† medida que aumentamos os recursos de treinamento (tamanho do modelo, *dataset* e or√ßamento computacional), os ganhos de desempenho ficam menores a cada acr√©scimo do recurso, o que revela um ganho decrescente.
IV. Portanto, as *scaling laws* mostram que, apesar do desempenho melhorar com maiores recursos, a efici√™ncia em termos de ganho por unidade adicional de recurso diminui com o aumento dos recursos.
$\blacksquare$

**Corol√°rio 9.1:** As *scaling laws* permitem prever o desempenho de LLMs com base nos recursos utilizados no treinamento, ajudando a tomar decis√µes sobre a aloca√ß√£o de recursos para obter o melhor compromisso entre desempenho e custo.

*Prova:*
I.  As *scaling laws* estabelecem rela√ß√µes matem√°ticas entre o tamanho do modelo, o tamanho do *dataset*, o or√ßamento computacional e o desempenho do modelo.
II. Essas rela√ß√µes permitem estimar o desempenho de um LLM com base nos recursos dispon√≠veis.
III. As previs√µes baseadas nas *scaling laws* ajudam na aloca√ß√£o de recursos de treinamento, permitindo identificar o ponto √≥timo onde o ganho em desempenho justifica o custo adicional.
IV. Portanto, as *scaling laws* s√£o √∫teis para a tomada de decis√µes no treinamento de LLMs, permitindo a aloca√ß√£o eficiente dos recursos.
$\blacksquare$
**Observa√ß√£o 1:** √â importante notar que as *scaling laws* s√£o modelos emp√≠ricos e, portanto, aproxima√ß√µes que podem n√£o se manter v√°lidas em todos os cen√°rios. No entanto, essas leis fornecem uma compreens√£o valiosa sobre como o desempenho dos LLMs √© afetado por diferentes fatores, e s√£o um guia √∫til no treinamento e desenvolvimento desses modelos.

**Teorema 9.1** A combina√ß√£o √≥tima de tamanho de modelo, tamanho do dataset e or√ßamento computacional para um dado desempenho desejado n√£o √© √∫nica.
*Prova:*
I.  As scaling laws, como expresso em $L(N) = (N_c/N)^{\alpha_N}$, $L(D) = (D_c/D)^{\alpha_D}$ e $L(C) = (C_c/C)^{\alpha_C}$, indicam que o desempenho do modelo √© influenciado por esses tr√™s fatores.
II.  A perda $L$ pode ser alcan√ßada atrav√©s de v√°rias combina√ß√µes de $N$, $D$ e $C$ que resultam no mesmo valor de perda.
III. A forma de lei de pot√™ncia das scaling laws implica que uma redu√ß√£o na perda pode ser alcan√ßada tanto aumentando o tamanho do modelo, o tamanho do dataset ou o or√ßamento computacional (ou uma combina√ß√£o dos tr√™s).
IV. Portanto, n√£o existe uma √∫nica combina√ß√£o √≥tima de $N$, $D$, e $C$ para atingir um dado desempenho, mas sim um conjunto de trade-offs entre esses tr√™s fatores.
$\blacksquare$

### Implica√ß√µes Pr√°ticas das Scaling Laws

As *scaling laws* t√™m v√°rias implica√ß√µes pr√°ticas no treinamento de LLMs:

1.  **Previs√£o de Desempenho:** Elas permitem prever o desempenho de um LLM com base nos recursos utilizados no treinamento, auxiliando na decis√£o sobre a aloca√ß√£o de recursos para obter um determinado desempenho. Isso √© √∫til para estimar a necessidade de GPUs, *datasets* e tempo de treinamento.
2.  **Otimiza√ß√£o da Aloca√ß√£o de Recursos:** As *scaling laws* guiam a aloca√ß√£o eficiente de recursos computacionais, mostrando que existe um balan√ßo entre o tamanho do modelo, o tamanho do *dataset* e o tempo de treinamento. Por exemplo, elas podem nos ajudar a decidir se √© mais eficiente aumentar o tamanho do modelo ou aumentar o tamanho do *dataset* de treino para um dado or√ßamento computacional.
3.  **Guiando o Treinamento:** As rela√ß√µes de lei de pot√™ncia nas *scaling laws* permitem prever como o desempenho do modelo evolui com o aumento de cada um dos fatores de treinamento, mostrando que o benef√≠cio obtido com aumentos sucessivos de um fator diminui a cada passo. Isso permite planejar o treinamento, evitando gastos desnecess√°rios em um √∫nico fator.
4.  **Preven√ß√£o de Overfitting:** O aumento do tamanho do *dataset* √©, frequentemente, mais eficaz para reduzir o *overfitting* do que o aumento do tamanho do modelo, pois o modelo treinado com mais dados generaliza melhor para dados n√£o vistos.
> üí° **Exemplo Num√©rico:** Suponha que um modelo com 100 milh√µes de par√¢metros treinado com 500GB de texto atinja uma dada perda. Com um or√ßamento de computa√ß√£o fixo, as *scaling laws* nos permitem avaliar se seria melhor aumentar o modelo para 200 milh√µes de par√¢metros ou o dataset para 1000GB. Utilizando as leis de pot√™ncia, podemos modelar a perda com os dois cen√°rios e comparar qual delas apresenta maior redu√ß√£o da perda para o mesmo custo computacional. Em geral, aumentar o dataset traz mais benef√≠cios para evitar *overfitting*.

### Estimativa do N√∫mero de Par√¢metros

Uma das m√©tricas fundamentais nas *scaling laws* √© o n√∫mero de par√¢metros do modelo ($N$). Em um transformer, o n√∫mero de par√¢metros pode ser estimado por:

$$ N \approx 2d n_{layer} (2d_{attn} + d_{ff})$$
Onde $d$ √© a dimens√£o de entrada e sa√≠da do modelo, $n_{layer}$ √© o n√∫mero de camadas, $d_{attn}$ √© a dimens√£o das *queries*, *keys* e *values* da camada de aten√ß√£o e $d_{ff}$ √© a dimens√£o da camada *feedforward*. Em muitos modelos, $d_{attn}$ e $d_{ff}$ est√£o relacionadas com $d$, e podemos aproximar a equa√ß√£o como:
$$ N \approx 12 n_{layer} d^2 $$
Essa equa√ß√£o √© √∫til para ter uma estimativa da escala de par√¢metros de um modelo. Para um modelo como o GPT-3, com $n_{layer} = 96$ e $d=12288$, temos $N\approx175$ bilh√µes de par√¢metros [^32].

> üí° **Exemplo Num√©rico:** Suponha que temos um modelo com $n_{layer} = 20$, $d=1024$, $d_{attn} = 128$ e $d_{ff}=2048$. Podemos calcular o n√∫mero aproximado de par√¢metros como:
>  $$ N \approx 2 * 1024 * 20 * (2 * 128 + 2048)  \approx 92.274.176 \text{ (cerca de 92 milh√µes)} $$
> Podemos tamb√©m aproximar usando a segunda formula:
>  $$N \approx 12 * 20 * 1024^2 = 251.658.240  \text{ (cerca de 250 milh√µes)} $$
>  Esses dois valores s√£o estimativas grosseiras, mas nos d√£o uma ideia da escala de par√¢metros de um modelo.
> üí° **Exemplo Num√©rico:** Consideremos dois modelos: Modelo A com $n_{layer} = 12$ e $d=768$ e Modelo B com $n_{layer} = 24$ e $d = 512$. Usando a aproxima√ß√£o $N \approx 12 n_{layer} d^2$:
> Modelo A: $N_A \approx 12 * 12 * 768^2 \approx 85$ milh√µes de par√¢metros
> Modelo B: $N_B \approx 12 * 24 * 512^2 \approx 75$ milh√µes de par√¢metros
> Apesar de B ter mais camadas, o valor de d menor resulta em um modelo menor. Isso exemplifica como diferentes combina√ß√µes de $n_{layer}$ e $d$ podem resultar em tamanhos de modelo similares, ressaltando que esta √© uma aproxima√ß√£o.

**Lema 9.2:** A estimativa do n√∫mero de par√¢metros do modelo, $N$, pode ser usada como um proxy para o poder de modelagem do modelo, embora n√£o seja uma medida exata.

*Prova:*
I.  A f√≥rmula $N \approx 2d n_{layer} (2d_{attn} + d_{ff})$ ou sua aproxima√ß√£o $N \approx 12 n_{layer} d^2$ dependem de par√¢metros arquiteturais do modelo, que influenciam sua capacidade de aprender.
II. Modelos com maior $N$ tendem a ter mais capacidade de memoriza√ß√£o e generaliza√ß√£o, o que, em geral, leva a um melhor desempenho.
III. No entanto, o n√∫mero de par√¢metros n√£o captura a totalidade da capacidade de modelagem, pois depende tamb√©m da arquitetura espec√≠fica, dos dados usados no treinamento e da estrat√©gia de otimiza√ß√£o.
IV. Portanto, embora $N$ seja um bom proxy para o poder de modelagem, n√£o √© uma medida perfeita, e dois modelos com o mesmo $N$ podem ter desempenhos diferentes.
$\blacksquare$

### Conclus√£o

As *scaling laws* s√£o fundamentais para o treinamento de LLMs, mostrando como o desempenho do modelo melhora em rela√ß√£o ao tamanho do modelo, ao tamanho do *dataset* e ao or√ßamento computacional. Essas leis guiam a aloca√ß√£o de recursos, permitem prever o desempenho do modelo e entender os compromissos entre custo e desempenho. Ao entender e utilizar as *scaling laws*, √© poss√≠vel treinar LLMs de forma mais eficiente, alcan√ßando resultados not√°veis em uma ampla gama de tarefas de processamento de linguagem natural.

### Refer√™ncias
[^28]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^29]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^31]: Kingma, Diederik P., and Jimmy Ba. "Adam: A method for stochastic optimization." arXiv preprint arXiv:1412.6980 (2014).
[^32]: Kaplan, Jared, et al. "Scaling laws for neural language models." *arXiv preprint arXiv:2001.08361* (2020).
<!-- END -->
