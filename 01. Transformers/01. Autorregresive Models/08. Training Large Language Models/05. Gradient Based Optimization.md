## Otimiza√ß√£o Baseada em Gradiente e Paraleliza√ß√£o no Treinamento de Transformers

### Introdu√ß√£o
Em continuidade aos t√≥picos sobre o treinamento de *Large Language Models* (LLMs) com transformers [^28, 29], este cap√≠tulo se concentra na otimiza√ß√£o dos pesos da rede e na capacidade de paraleliza√ß√£o inerente √† arquitetura do transformer. Como vimos, o treinamento de LLMs envolve o uso de auto-supervis√£o, *teacher forcing*, grandes *datasets* e processamento paralelo, e aqui discutiremos como otimizadores baseados em gradiente e a independ√™ncia de processamento de cada token s√£o cruciais para o aprendizado eficiente desses modelos. Exploraremos como os otimizadores ajustam os pesos da rede e como a arquitetura do transformer permite um treinamento mais r√°pido e eficiente, especialmente em compara√ß√£o com modelos recorrentes.

### Otimiza√ß√£o Baseada em Gradiente

O processo de treinamento de redes neurais, incluindo os transformers, √© guiado por **otimizadores baseados em gradiente**. Esses otimizadores ajustam os pesos das redes para minimizar a fun√ß√£o de perda, que no caso de modelos de linguagem √© a perda cross-entropy [^28].  Essencialmente, o objetivo √© encontrar os pesos que fazem com que o modelo preveja as pr√≥ximas palavras com a maior precis√£o poss√≠vel, e esta procura √© realizada de forma iterativa pelos otimizadores.

#### Otimizador Adam

Um dos otimizadores mais utilizados no treinamento de LLMs √© o **Adam** (*Adaptive Moment Estimation*) [^31]. O Adam √© um otimizador adaptativo que combina as vantagens de dois m√©todos de otimiza√ß√£o diferentes: o *Momentum* e o RMSprop. Ele ajusta a taxa de aprendizado individualmente para cada peso, o que acelera a converg√™ncia e permite que o modelo aprenda de forma mais eficiente.

##### Como o Adam Funciona
O Adam atualiza os pesos do modelo com base nas estimativas de momentos de primeira ordem (m√©dia) e segunda ordem (vari√¢ncia) dos gradientes:

1.  **C√°lculo do Gradiente:** Em cada passo de treinamento, o otimizador Adam calcula o gradiente da fun√ß√£o de perda em rela√ß√£o aos pesos do modelo.
2.  **Estimativa dos Momentos:** O Adam calcula as m√©dias m√≥veis dos gradientes ($m_t$) e dos gradientes quadrados ($v_t$):
    $$ m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
     $$ v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$
     onde $g_t$ √© o gradiente no tempo *t*, e $\beta_1$ e $\beta_2$ s√£o os fatores de decaimento exponencial para os momentos de primeira e segunda ordem, respectivamente.
3.  **Corre√ß√£o do Vi√©s:** As m√©dias m√≥veis $m_t$ e $v_t$ s√£o corrigidas para mitigar o vi√©s inicial:
    $$ \hat{m}_t = \frac{m_t}{1 - \beta_1^t} $$
     $$ \hat{v}_t = \frac{v_t}{1 - \beta_2^t} $$
4.  **Atualiza√ß√£o dos Pesos:** Os pesos do modelo s√£o atualizados na dire√ß√£o do gradiente, usando uma taxa de aprendizado adaptativa:
    $$ w_t = w_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} $$
    onde $\alpha$ √© a taxa de aprendizado e $\epsilon$ √© um pequeno valor para evitar divis√µes por zero.

**Teorema 7:** O otimizador Adam converge para um m√≠nimo local da fun√ß√£o de perda em problemas n√£o convexos, como o treinamento de redes neurais, ajustando as taxas de aprendizagem de cada peso de forma adaptativa, com base nos momentos dos gradientes.

*Prova:*
I. Adam combina as vantagens do *momentum* (que acelera o aprendizado na dire√ß√£o do gradiente) e do RMSprop (que adapta as taxas de aprendizado por peso).
II. O c√°lculo dos momentos de primeira e segunda ordem dos gradientes ($m_t$ e $v_t$) permite ao Adam adaptar-se √† topografia da fun√ß√£o de perda, ajustando a magnitude da taxa de aprendizado de forma din√¢mica para cada peso, com base nas varia√ß√µes e na intensidade do gradiente.
III. A corre√ß√£o de vi√©s das m√©dias m√≥veis ($m_t$ e $v_t$) assegura que o otimizador n√£o seja enviesado pelas primeiras itera√ß√µes do treinamento, levando a uma melhor adapta√ß√£o √† fun√ß√£o de perda.
IV. Estudos emp√≠ricos mostram que Adam converge para um m√≠nimo local da fun√ß√£o de perda para fun√ß√µes n√£o convexas, como as de treinamento de redes neurais.
V. Logo, o otimizador Adam converge para um m√≠nimo da fun√ß√£o de perda de forma eficiente.
‚ñ†
> üí° **Exemplo Num√©rico:** Vamos supor que um peso do modelo tem um gradiente constante de 0.1 nas primeiras itera√ß√µes. Com o Adam, os momentos $m_t$ e $v_t$ acumular√£o gradientes para aquele peso espec√≠fico, resultando numa taxa de aprendizagem menor para aquele peso. Em outro peso, com um gradiente que varia muito, o Adam manter√° a taxa de aprendizagem maior. Isso acontece de forma autom√°tica, atrav√©s da computa√ß√£o dos momentos de primeira e segunda ordem, permitindo que cada peso aprenda com a sua pr√≥pria din√¢mica.
>
> Para ilustrar, suponha que temos um peso $w_0 = 0.5$, taxa de aprendizagem $\alpha = 0.001$, $\beta_1 = 0.9$, $\beta_2 = 0.999$, e $\epsilon = 10^{-8}$. Inicialmente, $m_0 = 0$ e $v_0 = 0$.
>
> **Itera√ß√£o 1:**
> Suponha que o gradiente $g_1 = 0.1$.
>
> 1.  Calculamos os momentos:
>     $m_1 = 0.9 * 0 + (1 - 0.9) * 0.1 = 0.01$
>     $v_1 = 0.999 * 0 + (1 - 0.999) * 0.1^2 = 0.00001$
>
> 2.  Corre√ß√£o do vi√©s:
>     $\hat{m}_1 = \frac{0.01}{1 - 0.9^1} = 0.1$
>     $\hat{v}_1 = \frac{0.00001}{1 - 0.999^1} = 0.01$
>
> 3.  Atualiza√ß√£o dos pesos:
>     $w_1 = 0.5 - 0.001 * \frac{0.1}{\sqrt{0.01} + 10^{-8}} \approx 0.5 - 0.001 * \frac{0.1}{0.1} = 0.5 - 0.001 = 0.499$
>
> **Itera√ß√£o 2:**
> Suponha que o gradiente $g_2 = 0.2$.
>
> 1.  Calculamos os momentos:
>     $m_2 = 0.9 * 0.01 + (1 - 0.9) * 0.2 = 0.029$
>      $v_2 = 0.999 * 0.00001 + (1 - 0.999) * 0.2^2 = 0.0000409$
>
> 2.  Corre√ß√£o do vi√©s:
>     $\hat{m}_2 = \frac{0.029}{1 - 0.9^2} \approx  0.1526$
>     $\hat{v}_2 = \frac{0.0000409}{1 - 0.999^2} \approx  0.0204$
>
> 3.  Atualiza√ß√£o dos pesos:
>      $w_2 = 0.499 - 0.001 * \frac{0.1526}{\sqrt{0.0204} + 10^{-8}} \approx 0.499 - 0.001* 0.1526/0.1428 \approx 0.4979$
>
> Notamos que o Adam ajusta a atualiza√ß√£o dos pesos, considerando o hist√≥rico dos gradientes. A taxa de aprendizado adaptativa permite uma atualiza√ß√£o mais controlada e eficiente.

**Lema 7.1:** A escolha adequada dos par√¢metros $\beta_1$, $\beta_2$ e $\alpha$ influencia significativamente o desempenho do Adam. Uma taxa de aprendizagem $\alpha$ muito alta pode levar a oscila√ß√µes e dificuldade de converg√™ncia, enquanto um valor muito baixo pode tornar o treinamento excessivamente lento. Similarmente, valores inadequados para $\beta_1$ e $\beta_2$ podem prejudicar a adapta√ß√£o dos momentos e, consequentemente, a efici√™ncia da otimiza√ß√£o.

*Prova:*
I. Os par√¢metros $\beta_1$ e $\beta_2$ controlam a taxa de decaimento exponencial das m√©dias m√≥veis dos gradientes e dos gradientes quadrados, respectivamente. Valores muito altos tornam o otimizador "m√≠ope", dando menor peso aos gradientes mais recentes.
II. O par√¢metro $\alpha$ define a magnitude da atualiza√ß√£o dos pesos, controlando o tamanho dos passos em dire√ß√£o ao m√≠nimo da fun√ß√£o de perda.
III. A escolha inadequada desses par√¢metros leva a um ajuste sub√≥timo da taxa de aprendizado adaptativa, e consequentemente a um desempenho sub√≥timo do otimizador.
IV. Portanto, a escolha adequada desses par√¢metros √© essencial para uma converg√™ncia eficiente e um bom desempenho do Adam.
‚ñ†
##### Outros Otimizadores
Outros otimizadores tamb√©m podem ser utilizados, como o SGD (*Stochastic Gradient Descent*) com *momentum* e o RMSprop, mas o Adam √© frequentemente preferido devido √† sua capacidade de adapta√ß√£o e converg√™ncia mais r√°pida [^31].

> üí° **Exemplo Num√©rico:** Vamos comparar a atualiza√ß√£o de um peso usando SGD com *momentum* e Adam. Suponha que temos um peso inicial $w=0.5$ e um gradiente $g = 0.2$, com uma taxa de aprendizagem $\alpha=0.01$ e um fator de *momentum* $\beta=0.9$.
>
> **SGD com *momentum*:**
>
> 1.  Calculamos o *momentum*:
>     $$v_t = \beta v_{t-1} + (1-\beta)g = 0.9 * 0 + 0.1 * 0.2 = 0.02$$
> 2.  Atualizamos o peso:
>     $$w_t = w_{t-1} - \alpha * v_t = 0.5 - 0.01 * 0.02 = 0.4998$$
>
> **Adam:**
> 1.  Calculamos os momentos $m_t$ e $v_t$ (com $\beta_1=0.9$, $\beta_2=0.999$ e inicializando $m_0 = 0$, $v_0 = 0$).
>     $$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g = 0.9 * 0 + 0.1 * 0.2 = 0.02$$
>      $$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g^2 = 0.999 * 0 + 0.001 * 0.2^2 = 0.00004$$
> 2.  Corrigimos o vi√©s:
>      $$ \hat{m}_t = \frac{m_t}{1-\beta_1^t} = \frac{0.02}{1-0.9} = 0.2 $$
>     $$ \hat{v}_t = \frac{v_t}{1-\beta_2^t} = \frac{0.00004}{1-0.999} = 0.04$$
> 3.  Atualizamos o peso ($\epsilon=10^{-8}$):
>     $$w_t = w_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} = 0.5 - 0.01 * \frac{0.2}{\sqrt{0.00004} + 10^{-8}} = 0.5 - 0.01 * \frac{0.2}{0.0063} \approx  0.468 $$
>
>   Como podemos observar, Adam fez uma atualiza√ß√£o maior ao peso do que SGD com *momentum*, ajustando a taxa de aprendizagem de forma adaptativa √† fun√ß√£o de perda.
>
> A tabela abaixo resume as diferen√ßas na atualiza√ß√£o dos pesos:
>
> | Otimizador          | Atualiza√ß√£o do Peso ($w_t$) |
> |----------------------|------------------------------|
> | SGD com *momentum*   | 0.4998                       |
> | Adam                 | 0.468                        |
>
> Isso ilustra como o Adam, ao usar momentos e corre√ß√£o de vi√©s, ajusta a taxa de aprendizagem de forma mais din√¢mica e eficiente.

### Paraleliza√ß√£o no Treinamento de Transformers

Como discutido anteriormente, uma das principais vantagens da arquitetura do transformer √© a sua capacidade de paraleliza√ß√£o [^28, 29]. Ao contr√°rio dos modelos recorrentes (RNNs), onde as computa√ß√µes em cada passo de tempo dependem dos c√°lculos nos passos anteriores, os transformers processam cada *token* da sequ√™ncia de entrada de forma independente, o que permite um treinamento mais r√°pido, especialmente quando processados com GPUs.

#### Independ√™ncia de Token
A **auto-aten√ß√£o** dentro do transformer permite que cada *token* compute sua representa√ß√£o com base em todas as outras posi√ß√µes na sequ√™ncia, e n√£o de forma sequencial. A fun√ß√£o de *self-attention* permite que todos os *tokens* calculem suas representa√ß√µes contextuais sem depender das representa√ß√µes calculadas nos *tokens* anteriores. Cada posi√ß√£o na sequ√™ncia pode calcular seu pr√≥prio estado (representa√ß√£o) de forma independente das outras posi√ß√µes, o que permite o processamento paralelo de todos os elementos de uma sequ√™ncia, utilizando *batches* de tokens.
> üí° **Exemplo Num√©rico:**
> Suponha uma sequ√™ncia de entrada com 4 *tokens*: "O", "gato", "preto", "dorme". Em uma RNN, a representa√ß√£o de "dorme" dependeria da representa√ß√£o de "preto", que por sua vez dependeria de "gato", e assim por diante, o que impede o processamento paralelo. Em um transformer, as representa√ß√µes de todos os 4 *tokens* podem ser calculadas ao mesmo tempo. A representa√ß√£o de "dorme" depende de todos os 4 *tokens*, mas cada um deles √© calculado de forma independente.
>
> ```mermaid
> graph LR
>     A[Sequ√™ncia de Tokens: O gato preto dorme] --> B(RNN - Processamento Sequencial)
>     A --> C(Transformer - Processamento Paralelo)
>     B --> D[Sa√≠das Sequenciais]
>     C --> E[Sa√≠das Paralelas]
>      style B fill:#f9f,stroke:#333,stroke-width:2px
>      style C fill:#ccf,stroke:#333,stroke-width:2px
> ```
>
> Vamos considerar um exemplo simplificado da computa√ß√£o da autoaten√ß√£o com 3 tokens:
>
> Token 1: "O"
> Token 2: "gato"
> Token 3: "preto"
>
> **Representa√ß√µes Iniciais (Embeddings):**
>
>  Token 1:  $x_1 = [0.1, 0.2]$
>  Token 2:  $x_2 = [0.3, 0.4]$
>  Token 3:  $x_3 = [0.5, 0.6]$
>
>  Suponha matrizes de proje√ß√£o (para simplificar):
>
>  $W_Q = [[0.1, 0.2], [0.3, 0.4]]$
>  $W_K = [[0.5, 0.6], [0.7, 0.8]]$
>  $W_V = [[0.9, 0.1], [0.2, 0.3]]$
>
>  **C√°lculo de Queries, Keys e Values:**
>
>  $q_1 = x_1 * W_Q = [0.1*0.1 + 0.2*0.3 , 0.1*0.2 + 0.2*0.4] = [0.07, 0.1]$
>  $k_1 = x_1 * W_K = [0.1*0.5 + 0.2*0.7, 0.1*0.6 + 0.2*0.8] = [0.19, 0.22]$
>  $v_1 = x_1 * W_V = [0.1*0.9 + 0.2*0.2, 0.1*0.1 + 0.2*0.3] = [0.13, 0.07]$
>
> De forma similar, calculamos:
>
>  $q_2 = x_2 * W_Q = [0.15, 0.22]$
>  $k_2 = x_2 * W_K = [0.39, 0.46]$
>  $v_2 = x_2 * W_V = [0.31, 0.15]$
>
>  $q_3 = x_3 * W_Q = [0.23, 0.34]$
>  $k_3 = x_3 * W_K = [0.59, 0.7]$
>  $v_3 = x_3 * W_V = [0.51, 0.23]$
>
>  **C√°lculo dos Scores de Aten√ß√£o:**
>
>  $score_{11} = q_1 \cdot k_1 = 0.07 * 0.19 + 0.1 * 0.22 = 0.0353$
>  $score_{12} = q_1 \cdot k_2 = 0.07 * 0.39 + 0.1 * 0.46 = 0.0733$
>  $score_{13} = q_1 \cdot k_3 = 0.07 * 0.59 + 0.1 * 0.7 = 0.1113$
>
> De forma similar, calculamos os demais scores. A autoaten√ß√£o permite que a representa√ß√£o de cada token seja influenciada pelos demais, de forma paralela. Todos esses c√°lculos (queries, keys, values e scores) podem ser computados em paralelo para cada token.

**Proposi√ß√£o 1:** A arquitetura do transformer, com sua capacidade de paraleliza√ß√£o, escala de maneira mais eficaz com o aumento do tamanho do *batch* em compara√ß√£o com modelos recorrentes.

*Prova:*
I.  Modelos recorrentes, devido √† natureza sequencial do processamento, n√£o conseguem aproveitar o aumento do tamanho do *batch* para reduzir significativamente o tempo de processamento. O tempo de processamento por *batch* em RNNs aumenta quase linearmente com o tamanho da sequ√™ncia.
II. Em contraste, a capacidade de processamento paralelo do transformer permite um aumento consider√°vel de efici√™ncia com o aumento do tamanho do *batch*, j√° que mais opera√ß√µes podem ser executadas simultaneamente nas unidades de processamento.
III. O ganho de efici√™ncia com o aumento do tamanho do *batch* nos transformers √© limitado principalmente pela capacidade de mem√≥ria da GPU e outros recursos computacionais dispon√≠veis, e n√£o pelo tempo de processamento sequencial, como nas RNNs.
IV. Portanto, a arquitetura do transformer escala de maneira mais eficaz com o aumento do tamanho do *batch*, o que contribui significativamente para sua efici√™ncia no treinamento.
‚ñ†

**Teorema 8:** A independ√™ncia de processamento dos *tokens* na arquitetura do transformer permite a paraleliza√ß√£o do treinamento, resultando em um treinamento mais r√°pido quando comparado com modelos recorrentes, que necessitam de processamento sequencial.

*Prova:*
I.  Em modelos recorrentes, a computa√ß√£o da sa√≠da de um token no tempo $t$ depende da computa√ß√£o das sa√≠das em todos os tempos anteriores $t-1, t-2$, etc.
II.  Esta depend√™ncia sequencial impede a paraleliza√ß√£o do processamento, que deve ser executado passo a passo.
III.  Na arquitetura transformer, a camada de *self-attention* permite que todos os *tokens* da sequ√™ncia calculem suas representa√ß√µes de forma independente e simult√¢nea.
IV.  Esta independ√™ncia de processamento permite que todas as camadas do modelo, incluindo as camadas de *self-attention* e *feedforward*, processem os diferentes *tokens* de um *batch* em paralelo, o que acelera o processo de treino e aumenta a efici√™ncia do uso de GPUs.
V. Logo, a independ√™ncia de processamento dos *tokens* permite o treinamento paralelo dos transformers, resultando em maior velocidade e eficiencia do que modelos recorrentes.
‚ñ†

**Lema 8.1:** A arquitetura do transformer, ao permitir a paraleliza√ß√£o dos c√°lculos, reduz o tempo necess√°rio para processar um *batch* de dados de treino, o que leva a um treinamento mais r√°pido e eficiente.

*Prova:*
I. Modelos recorrentes processam os *tokens* de uma sequ√™ncia sequencialmente, e o tempo necess√°rio para processar um *batch* √© diretamente proporcional ao comprimento da sequ√™ncia.
II. Na arquitetura do transformer, todos os *tokens* de um *batch* s√£o processados em paralelo, o que significa que o tempo necess√°rio para processar um *batch* n√£o √© diretamente afetado pelo comprimento da sequ√™ncia (dentro dos limites da janela de contexto).
III. Logo, a paraleliza√ß√£o dos c√°lculos reduz drasticamente o tempo necess√°rio para processar um *batch*, levando a um treinamento mais r√°pido e eficiente.
‚ñ†

#### Paraleliza√ß√£o e *Teacher Forcing*
Mesmo com a paraleliza√ß√£o, a t√©cnica de *teacher forcing* ainda √© utilizada para garantir a estabilidade do treinamento, alimentando o modelo com o hist√≥rico correto. Como o hist√≥rico correto √© conhecido, a camada do transformer pode computar as sa√≠das de todos os elementos da sequ√™ncia independentemente e em paralelo, pois o c√°lculo n√£o depende das predi√ß√µes anteriores do modelo.

### Conclus√£o

O treinamento de LLMs com transformers √© um processo complexo que requer otimizadores eficientes e arquiteturas de modelos que possam ser treinadas de forma paralela. O uso de otimizadores baseados em gradiente, como o Adam, e a capacidade de paraleliza√ß√£o inerente √† arquitetura do transformer s√£o cruciais para o aprendizado eficiente desses modelos. O Adam ajusta a taxa de aprendizagem de cada peso, enquanto a paraleliza√ß√£o permite que o modelo seja treinado em grandes *datasets* com alta efici√™ncia. Esta combina√ß√£o de otimiza√ß√£o e paraleliza√ß√£o permite que LLMs aprendam a modelar a linguagem de forma eficaz, e s√£o aspectos centrais do sucesso destes modelos em tarefas de processamento de linguagem natural.

### Refer√™ncias
[^28]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^29]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^31]: Kingma, Diederik P., and Jimmy Ba. "Adam: A method for stochastic optimization." arXiv preprint arXiv:1412.6980 (2014).
<!-- END -->
