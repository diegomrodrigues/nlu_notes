## Teacher Forcing: Estabilizando o Treinamento e Evitando o Ac√∫mulo de Erros

### Introdu√ß√£o
Expandindo nossa an√°lise sobre o treinamento de modelos de linguagem grandes (LLMs) e o uso da perda cross-entropy [^28], este cap√≠tulo se dedica a explorar a t√©cnica de **teacher forcing**. Como vimos, a auto-supervis√£o permite que os LLMs aprendam a partir de dados n√£o rotulados, utilizando a pr√≥pria sequ√™ncia de palavras como sinal de treinamento. No entanto, como garantir que o modelo n√£o se desvie do caminho correto durante o treinamento? √â aqui que a t√©cnica de *teacher forcing* se torna fundamental, fornecendo ao modelo o hist√≥rico correto de tokens, estabilizando o treinamento e evitando o ac√∫mulo de erros.

### O Problema da Propaga√ß√£o de Erros
Durante o treinamento, um modelo de linguagem deve aprender a prever o pr√≥ximo token numa sequ√™ncia. Uma abordagem ing√™nua seria usar a pr√≥pria previs√£o do modelo para o passo *$t$* como entrada para prever o token no passo *$t+1$*. No entanto, isso pode levar a um problema: se o modelo fizer uma previs√£o incorreta no passo *$t$*, esta previs√£o errada ser√° usada como entrada para o passo *$t+1$*, levando potencialmente a previs√µes ainda piores e ao ac√∫mulo de erros. Esse problema √© particularmente grave nos primeiros est√°gios do treinamento, quando o modelo ainda n√£o aprendeu a prever com precis√£o.

A *teacher forcing* surge como solu√ß√£o para esse problema.

### A Solu√ß√£o: Teacher Forcing
A **teacher forcing** √© uma t√©cnica de treinamento que consiste em alimentar o modelo, em cada passo de tempo *$t$*, com a sequ√™ncia correta de tokens $w_{1:t}$ at√© aquele ponto para prever o pr√≥ximo token $w_{t+1}$, e n√£o com sua pr√≥pria previs√£o anterior [^28]. Em outras palavras, durante o treinamento, o modelo n√£o utiliza a sua pr√≥pria previs√£o para a palavra anterior como entrada na etapa seguinte, mas sim a palavra verdadeira do conjunto de dados de treino. Esta abordagem assegura que o modelo esteja sempre aprendendo com base no hist√≥rico de texto correto, evitando a propaga√ß√£o de erros e estabilizando o processo de treinamento. Ao utilizar o hist√≥rico correto, o modelo se concentra em aprender a prever a probabilidade correta da pr√≥xima palavra, dada a sequ√™ncia de texto correta.

> üí° **Exemplo Num√©rico:** Considere a sequ√™ncia "O p√°ssaro voou alto". Durante o treinamento, o modelo recebe:
>
> -  "O" para prever "p√°ssaro".
> -  "O p√°ssaro" para prever "voou".
> -  "O p√°ssaro voou" para prever "alto".
>
>  Se o modelo previsse incorretamente "gato" em vez de "p√°ssaro", esta previs√£o errada *n√£o* seria usada como entrada para a previs√£o seguinte. O modelo receber√° a entrada correta "O p√°ssaro" para aprender a prever a palavra "voou". Essa abordagem previne a acumula√ß√£o de erros ao longo da sequ√™ncia.
>
> ```mermaid
> graph LR
>     A[Sequ√™ncia de Texto: O p√°ssaro voou alto] --> B(Teacher Forcing)
>     B --> C[O -> Prev√™ 'p√°ssaro']
>     C --> D[O p√°ssaro -> Prev√™ 'voou']
>     D --> E[O p√°ssaro voou -> Prev√™ 'alto']
>      style C fill:#ccf,stroke:#333,stroke-width:2px
>      style D fill:#ccf,stroke:#333,stroke-width:2px
>      style E fill:#ccf,stroke:#333,stroke-width:2px
> ```
>
> üí° **Exemplo Num√©rico:** Vamos supor que temos um vocabul√°rio com 5 palavras, codificadas como {0: "o", 1: "p√°ssaro", 2: "voou", 3: "alto", 4: "<pad>"}. A sequ√™ncia "o p√°ssaro voou alto" seria representada numericamente como [0, 1, 2, 3]. Vamos analisar como *teacher forcing* operaria em um passo espec√≠fico.
>
> Digamos que o modelo est√° na etapa onde o input √© "o p√°ssaro", que corresponde a [0, 1]. O modelo, neste momento, deve prever a palavra seguinte, que √© "voou" (representado por 2). O *teacher forcing* garante que a entrada para esta etapa seja [0, 1], e n√£o a previs√£o do modelo para o token anterior.
>
> Suponha que a sa√≠da do modelo antes da fun√ß√£o softmax (os logits) para a previs√£o da pr√≥xima palavra seja algo como:
>
>   `logits = [0.1, 0.2, 3.0, 0.1, 0.05]`
>
> Aplicando a fun√ß√£o softmax, obtemos as probabilidades de cada token:
>
>  ```python
>  import numpy as np
>  def softmax(x):
>      e_x = np.exp(x - np.max(x))
>      return e_x / e_x.sum()
>
>  logits = np.array([0.1, 0.2, 3.0, 0.1, 0.05])
>  probs = softmax(logits)
>  print(probs) # Output: [0.023, 0.025, 0.948, 0.023, 0.005]
>  ```
>
> O vetor `probs` nos diz que o modelo est√° prevendo "voou" (√≠ndice 2) com uma probabilidade de 0.948. A fun√ß√£o de perda cross-entropy ir√° comparar essa distribui√ß√£o de probabilidade com a distribui√ß√£o *one-hot* da palavra correta (neste caso, [0, 0, 1, 0, 0]) para calcular a perda e atualizar os pesos do modelo.
>
> A perda cross-entropy seria calculada como:
>
>   $$\text{Loss} = - \sum_{i} y_i \log(\hat{y}_i)$$
>
>   Onde $y_i$ √© a distribui√ß√£o *one-hot* da palavra correta e $\hat{y}_i$ √© a distribui√ß√£o de probabilidades predita pelo modelo. Neste caso:
>
>  $$\text{Loss} = -(0*\log(0.023) + 0*\log(0.025) + 1*\log(0.948) + 0*\log(0.023) + 0*\log(0.005)) \approx 0.053$$
>
> A diferen√ßa entre a probabilidade prevista e a real √© ent√£o utilizada para ajustar os par√¢metros do modelo usando retropropaga√ß√£o.
>

#### Implica√ß√µes no C√°lculo da Perda

Ao utilizar *teacher forcing*, a perda √© calculada com base na previs√£o do modelo para a pr√≥xima palavra *correta* em rela√ß√£o ao hist√≥rico correto fornecido como entrada [^28]. Isso significa que em cada passo, a perda cross-entropy √© calculada comparando a distribui√ß√£o de probabilidade predita pelo modelo com a distribui√ß√£o *one-hot* da palavra correta que o modelo deveria ter previsto, dadas as palavras anteriores corretas na sequ√™ncia.

**Teorema 3:** Teacher forcing promove um treinamento mais est√°vel e eficiente porque impede que erros de predi√ß√£o se propaguem ao longo da sequ√™ncia, permitindo que o modelo aprenda a prever a pr√≥xima palavra usando o contexto correto.

*Prova:*
I. Em um treinamento sem *teacher forcing*, a previs√£o do modelo no tempo $t$  $\hat{w}_{t}$ √© utilizada como entrada no passo seguinte, no tempo $t+1$. Se $\hat{w}_{t}$ for incorreta, o contexto utilizado na etapa $t+1$ estar√° errado e isso vai afetar as pr√≥ximas previs√µes, propagando o erro.
II. Com *teacher forcing*, o modelo recebe como entrada no tempo $t+1$ a sequ√™ncia correta de tokens at√© ao tempo $t$ ($w_{1:t}$), independentemente das previs√µes do modelo. Esta entrada garante que o contexto fornecido para previs√£o da pr√≥xima palavra ($w_{t+1}$) seja sempre o correto.
III. Como o modelo sempre utiliza como input a sequ√™ncia correta, o aprendizado fica mais est√°vel e o modelo converge para um m√≠nimo da fun√ß√£o de perda com maior rapidez, evitando acumula√ß√£o de erros ao longo da sequ√™ncia.
IV. Logo, *teacher forcing* estabiliza o treinamento e promove um aprendizado mais eficiente, ao assegurar que o modelo aprenda com o contexto correto.
‚ñ†
**Teorema 3.1:** A utiliza√ß√£o de *teacher forcing* durante o treinamento de modelos de linguagem, resulta em um gradiente de perda mais consistente e menos ruidoso, o que facilita a otimiza√ß√£o.

*Prova:*
I.  Em um cen√°rio sem *teacher forcing*, o gradiente da fun√ß√£o de perda √© influenciado n√£o apenas pela capacidade do modelo de prever o token atual, mas tamb√©m pela qualidade das previs√µes anteriores. Isso introduz um componente de ru√≠do no gradiente, tornando-o mais inst√°vel e dificultando a converg√™ncia.
II.  Com *teacher forcing*, o gradiente da perda √© calculado exclusivamente com base na discrep√¢ncia entre a previs√£o atual do modelo e o token correto, dado o contexto correto. Isso resulta em um gradiente mais consistente, pois a cada passo o modelo √© avaliado em rela√ß√£o ao contexto verdadeiro, n√£o a suas previs√µes anteriores.
III. A consist√™ncia do gradiente facilita a aplica√ß√£o de algoritmos de otimiza√ß√£o como o gradiente descendente, permitindo que o modelo avance de forma mais eficiente na dire√ß√£o do m√≠nimo da fun√ß√£o de perda, levando a uma converg√™ncia mais r√°pida e est√°vel.
IV. Portanto, *teacher forcing* contribui para um treinamento mais est√°vel e eficiente, n√£o apenas por evitar a propaga√ß√£o de erros, mas tamb√©m por gerar um gradiente de perda mais consistente.
‚ñ†

### Como Teacher Forcing Estabiliza o Treinamento?

A utiliza√ß√£o de *teacher forcing* oferece v√°rias vantagens para o treinamento de modelos de linguagem:

1.  **Converg√™ncia mais r√°pida**: Ao utilizar o hist√≥rico correto, o modelo evita desviar-se para regi√µes do espa√ßo de solu√ß√µes menos acuradas, o que acelera a converg√™ncia do treinamento.
2.  **Estabilidade do treinamento:** O treinamento se torna mais est√°vel, pois o modelo n√£o est√° sujeito a erros cumulativos, o que o torna mais robusto a ru√≠dos e erros iniciais no processo.
3.  **Foco no aprendizado**: Com *teacher forcing*, o modelo pode focar no aprendizado das rela√ß√µes entre o contexto e a pr√≥xima palavra, sem se preocupar em corrigir seus pr√≥prios erros no in√≠cio do treinamento.

**Lema 3.1:** Com *teacher forcing*, o c√°lculo da perda √© desvinculado do hist√≥rico de previs√µes do modelo, mas continua a depender do hist√≥rico correto de texto fornecido.

*Prova:*
I. Sem *teacher forcing*, a perda no tempo *$t+1$* depende da previs√£o do modelo no tempo *$t$*, que pode estar incorreta e acumular erros.
II. Com *teacher forcing*, a perda no tempo *$t+1$* depende da probabilidade que o modelo atribui √† palavra correta $w_{t+1}$ em rela√ß√£o √† sequ√™ncia de palavras corretas $w_{1:t}$.
III. O hist√≥rico de previs√µes do modelo $\hat{w}_{1:t}$ n√£o influencia o c√°lculo da perda, pois o modelo sempre recebe o hist√≥rico correto como entrada.
IV. Portanto, *teacher forcing* desvincula a perda da trajet√≥ria de previs√µes do modelo, assegurando que a aprendizagem seja est√°vel e n√£o seja prejudicada pela propaga√ß√£o de erros.
‚ñ†
**Lema 3.2:** Apesar de *teacher forcing* ser eficaz no treinamento, ele cria uma discrep√¢ncia entre o treinamento e o uso real do modelo, onde o modelo deve gerar o texto baseado nas suas pr√≥prias previs√µes.

*Prova:*
I. Durante o treinamento com *teacher forcing*, o modelo recebe sempre o hist√≥rico correto da sequ√™ncia como entrada e aprende a prever o pr√≥ximo token com base nesse contexto perfeito.
II. No entanto, no uso real do modelo (infer√™ncia), o modelo gera texto de forma autoregressiva, utilizando as suas pr√≥prias previs√µes anteriores como input para gerar novos tokens.
III. Essa diferen√ßa entre treinamento e infer√™ncia pode levar a problemas de desempenho durante a infer√™ncia, pois o modelo n√£o est√° exposto a seu pr√≥prio feedback durante o treinamento.
IV. Portanto, *teacher forcing* cria uma discrep√¢ncia entre as condi√ß√µes de treinamento e infer√™ncia, o que deve ser considerado na avalia√ß√£o e otimiza√ß√£o do modelo.
‚ñ†
**Proposi√ß√£o 1:** *Teacher forcing* pode levar a uma exposi√ß√£o limitada do modelo a estados de entrada que s√£o resultados de suas pr√≥prias previs√µes, o que pode afetar sua robustez durante a infer√™ncia.

*Prova:*
I. Com *teacher forcing*, o modelo √© consistentemente exposto a entradas que s√£o parte da sequ√™ncia verdadeira, evitando que ele experimente inputs resultantes de suas pr√≥prias previs√µes (mesmo as erradas).
II. Essa falta de exposi√ß√£o √†s suas pr√≥prias distribui√ß√µes de sa√≠da pode levar o modelo a n√£o se adaptar bem a cen√°rios onde as previs√µes anteriores s√£o incorretas ou inesperadas.
III. Em cen√°rios de infer√™ncia real, o modelo tem de lidar com um ambiente din√¢mico, no qual os seus erros podem se propagar, impactando o desempenho futuro. A falta dessa experi√™ncia no treinamento pode reduzir a robustez do modelo em tais condi√ß√µes.
IV. Portanto, a natureza determin√≠stica do *teacher forcing* limita a exposi√ß√£o do modelo aos seus pr√≥prios erros, o que pode afetar sua capacidade de lidar com inputs inesperados ou resultados de previs√µes incorretas durante a infer√™ncia.
‚ñ†

### Teacher Forcing e o Processamento Paralelo
Como discutido anteriormente, a arquitetura dos transformers permite o processamento paralelo, o que acelera o treinamento. Mesmo em modelos paralelos, o *teacher forcing* √© usado para calcular a perda em cada etapa de tempo. Como o modelo recebe sempre a sequ√™ncia de entrada correta, as sa√≠das de cada elemento da sequ√™ncia s√£o calculadas independentemente, o que permite um treinamento mais r√°pido e eficiente, mesmo usando *teacher forcing*.

### Conclus√£o

A t√©cnica de *teacher forcing* √© crucial no treinamento de LLMs. Ao alimentar o modelo com o hist√≥rico correto de tokens, *teacher forcing* evita a acumula√ß√£o de erros, estabiliza o treinamento e promove uma converg√™ncia mais r√°pida. Essa t√©cnica permite que o modelo se concentre em aprender as rela√ß√µes entre o contexto e a pr√≥xima palavra correta, maximizando o seu potencial de generaliza√ß√£o. Combinado com auto-supervis√£o e a fun√ß√£o de perda cross-entropy, o *teacher forcing* desempenha um papel fundamental nas capacidades de modelagem de linguagem dos LLMs.

### Refer√™ncias
[^28]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
