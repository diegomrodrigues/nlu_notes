## Treinamento de Modelos de Linguagem Grandes (LLMs) com Transformers

### IntroduÃ§Ã£o
Este capÃ­tulo aborda o treinamento de modelos de linguagem grandes (LLMs) baseados em transformers, explorando os mecanismos e tÃ©cnicas que permitem que esses modelos aprendam a partir de vastos conjuntos de dados textuais. Em particular, focaremos na **auto-supervisÃ£o**, uma abordagem chave para o treinamento de LLMs, que elimina a necessidade de rÃ³tulos manuais, aproveitando a sequÃªncia natural de palavras em um corpus de texto como sinal de supervisÃ£o. Como vimos anteriormente, os LLMs  possuem uma capacidade notÃ¡vel de modelar a linguagem, aprendendo representaÃ§Ãµes ricas e contextualizadas de palavras e frases. Esta seÃ§Ã£o aprofunda os detalhes do processo de treinamento que tornam isso possÃ­vel.

### Auto-supervisÃ£o no Treinamento de Transformers
A **auto-supervisÃ£o** (ou auto-treinamento) Ã© o paradigma de treinamento central para LLMs [^25]. Em vez de usar dados rotulados manualmente, os modelos aprendem a partir da estrutura inerente dos dados textuais. Especificamente, a tarefa de treinamento Ã© projetada de forma que a prÃ³pria sequÃªncia de palavras dentro de um texto forneÃ§a o sinal de supervisÃ£o. Isso Ã© crucial, pois permite o aproveitamento de grandes quantidades de texto nÃ£o rotulado, que sÃ£o mais abundantes e fÃ¡ceis de obter do que os dados rotulados.

A ideia bÃ¡sica Ã© utilizar um corpus de texto como material de treinamento e, em cada etapa de tempo $t$, treinar o modelo para prever a prÃ³xima palavra da sequÃªncia. Em outras palavras, dada uma sequÃªncia de palavras $w_1, w_2, ..., w_t$, o objetivo do modelo Ã© prever a prÃ³xima palavra $w_{t+1}$. O modelo Ã© entÃ£o avaliado com base em quÃ£o bem ele previu a palavra correta. [^25] Essa abordagem de treinamento Ã© *auto-supervisionada* porque a prÃ³pria sequÃªncia de palavras em um texto serve como uma forma de supervisÃ£o, sem necessidade de rÃ³tulos adicionais ou manuais. O modelo aprende atravÃ©s da tentativa de prever os prÃ³ximos tokens do texto, com base no contexto precedente.

#### FunÃ§Ã£o de Perda Cross-Entropy
Para quantificar quÃ£o bem o modelo estÃ¡ prevendo a prÃ³xima palavra, usamos a **perda cross-entropy** [^25]. A perda cross-entropy mede a diferenÃ§a entre a distribuiÃ§Ã£o de probabilidade prevista pelo modelo e a distribuiÃ§Ã£o de probabilidade real (a prÃ³xima palavra correta). Em termos matemÃ¡ticos, a perda cross-entropy Ã© definida como:
$$ L_{CE} = - \sum_{w \in V} y_t[w] \log \hat{y}_t[w]$$
onde $V$ Ã© o vocabulÃ¡rio, $y_t$ Ã© a distribuiÃ§Ã£o real da prÃ³xima palavra (um vetor *one-hot* onde apenas o Ã­ndice da palavra correta tem valor 1, e os demais sÃ£o 0), e $\hat{y}_t$ Ã© a distribuiÃ§Ã£o prevista pelo modelo.
No contexto do modelagem de linguagem, esta equaÃ§Ã£o se simplifica para:
$$ L_{CE} (\hat{y}_t, y_t) = - \log \hat{y}_t[w_{t+1}]$$
Ou seja, a perda Ã© o logaritmo negativo da probabilidade que o modelo atribui a prÃ³xima palavra correta $w_{t+1}$. O objetivo do treinamento Ã© minimizar essa perda ao longo de todas as etapas do processo.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos um vocabulÃ¡rio $V = \{ \text{o}, \text{rato}, \text{comeu}, \text{queijo} \}$ e o modelo estÃ¡ analisando a sequÃªncia "o rato comeu". O prÃ³ximo token correto $w_{t+1}$ Ã© "queijo". O modelo, apÃ³s analisar "o rato comeu", gera uma distribuiÃ§Ã£o de probabilidade $\hat{y}_t$ sobre todo o vocabulÃ¡rio, digamos:
>
>  $\hat{y}_t = [P(\text{o})=0.05, P(\text{rato})=0.10, P(\text{comeu})=0.15, P(\text{queijo})=0.70]$
>
>  O vetor *one-hot* $y_t$ seria:
>
>   $y_t = [0, 0, 0, 1]$
>
>  A perda cross-entropy Ã© entÃ£o:
>
>   $L_{CE} = - \log(\hat{y}_t[\text{queijo}]) = - \log(0.70) \approx 0.356$.
>
>  Se o modelo, em outra iteraÃ§Ã£o, gerasse uma distribuiÃ§Ã£o $\hat{y}_t = [0.01, 0.01, 0.01, 0.97]$, a perda seria:
>
>   $L_{CE} = - \log(0.97) \approx 0.03$, que Ã© menor. O objetivo do treinamento Ã©, portanto, atualizar os pesos do modelo para que ele gere distribuiÃ§Ãµes de probabilidade mais prÃ³ximas da resposta correta (distribuiÃ§Ã£o *one-hot*), minimizando a perda.

**Lema 1**  A minimizaÃ§Ã£o da perda cross-entropy Ã© equivalente a maximizar a probabilidade da palavra correta, dada a sequÃªncia anterior.
*Prova:*
I. A funÃ§Ã£o de perda cross-entropy Ã© definida como:
    $$L_{CE} (\hat{y}_t, y_t) = - \log \hat{y}_t[w_{t+1}]$$
II. Minimizar a funÃ§Ã£o $L_{CE}$ significa encontrar o valor de $\hat{y}_t$ que torna a perda o menor possÃ­vel.
III. Como o logaritmo Ã© uma funÃ§Ã£o monÃ³tona crescente, minimizar o negativo do logaritmo Ã© equivalente a maximizar o argumento do logaritmo.
IV. Portanto, minimizar $- \log \hat{y}_t[w_{t+1}]$ Ã© equivalente a maximizar $\hat{y}_t[w_{t+1}]$, que Ã© a probabilidade que o modelo atribui Ã  palavra correta $w_{t+1}$.
V. Logo, a minimizaÃ§Ã£o da perda cross-entropy Ã© equivalente a maximizar a probabilidade da palavra correta, dada a sequÃªncia anterior.
â– 

#### *Teacher Forcing*
Durante o processo de treinamento, usamos uma tÃ©cnica chamada *teacher forcing* [^25]. Em vez de alimentar o modelo com suas prÃ³prias previsÃµes (que podem conter erros), alimentamos o modelo com a sequÃªncia correta de tokens atÃ© o momento $t$ para prever o prÃ³ximo token $w_{t+1}$. Isso acelera a convergÃªncia do modelo e evita que erros se propaguem ao longo da sequÃªncia, o que poderia desviar o modelo do treinamento. Em outras palavras, ignoramos a previsÃ£o do modelo para o prÃ³ximo token e usamos a sequÃªncia de tokens correta atÃ© aquele ponto para prever o prÃ³ximo token. Este processo de treinamento Ã© ilustrado na Figura 10.18 [^26].

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere a sequÃªncia "O gato preto dorme". Durante o treinamento com *teacher forcing*, o modelo recebe como entrada "O" para prever "gato". Em seguida, recebe "O gato" para prever "preto", e assim por diante. O modelo nÃ£o usa a palavra que ele mesmo previu, mas sim a palavra correta, para treinar a previsÃ£o seguinte. Isso impede o acÃºmulo de erros. Se o modelo, em algum momento, previsse incorretamente "cachorro" em vez de "gato", este erro nÃ£o se propagaria para as previsÃµes seguintes, pois o modelo recebe a palavra correta "gato" como entrada para prever o prÃ³ximo token.

O modelo recebe a sequÃªncia correta de tokens $w_{1:t}$ como entrada e produz uma distribuiÃ§Ã£o de probabilidade sobre o vocabulÃ¡rio $V$. A perda cross-entropy Ã© calculada usando a probabilidade atribuÃ­da pelo modelo Ã  prÃ³xima palavra correta. As matrizes de peso na rede sÃ£o atualizadas por meio do algoritmo de backpropagation para minimizar a perda. Este processo Ã© repetido para todas as palavras na sequÃªncia. Para cada item da sequÃªncia, uma distribuiÃ§Ã£o de saÃ­da do transformer sobre o vocabulÃ¡rio Ã© gerada.

A perda para uma sequÃªncia de treinamento Ã© definida como a perda cross-entropy mÃ©dia sobre toda a sequÃªncia. Os pesos da rede sÃ£o ajustados para minimizar a perda mÃ©dia sobre a sequÃªncia, usando descida de gradiente [^25].

**ObservaÃ§Ã£o 1** A descida de gradiente Ã© um algoritmo iterativo de otimizaÃ§Ã£o que atualiza os pesos do modelo na direÃ§Ã£o do negativo do gradiente da funÃ§Ã£o de perda.  Em cada iteraÃ§Ã£o, os pesos sÃ£o ajustados para reduzir o erro entre as prediÃ§Ãµes do modelo e os valores reais. A taxa de aprendizagem define o tamanho dos passos dados na direÃ§Ã£o do gradiente.

#### Processamento Paralelo
Uma vantagem chave dos transformers Ã© que os itens de treinamento podem ser processados em paralelo. Em contraste com modelos baseados em RNNs, onde os cÃ¡lculos de saÃ­das e perdas dependem de cÃ¡lculos em passos anteriores, os transformers calculam as saÃ­das para cada elemento da sequÃªncia simultaneamente [^26]. Isso aumenta a velocidade de treinamento.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos uma sequÃªncia de 100 palavras. Em uma RNN, a saÃ­da para a palavra 50 depende do cÃ¡lculo das saÃ­das das 49 palavras anteriores. Isso significa que essas operaÃ§Ãµes precisam ser executadas sequencialmente. Em um transformer, o cÃ¡lculo da saÃ­da de cada uma das 100 palavras Ã© feito de forma independente, e essas 100 operaÃ§Ãµes podem ser executadas em paralelo, usando GPUs. Se uma GPU tem 100 unidades de processamento paralelas, o cÃ¡lculo seria, no caso do transformer, aproximadamente 100 vezes mais rÃ¡pido do que em uma RNN.
>
> ```mermaid
> graph LR
>     A[SequÃªncia de Texto] --> B(RNN - Processamento Sequencial)
>     A --> C(Transformer - Processamento Paralelo)
>     B --> D[SaÃ­das Sequenciais]
>     C --> E[SaÃ­das Paralelas]
>     style B fill:#f9f,stroke:#333,stroke-width:2px
>     style C fill:#ccf,stroke:#333,stroke-width:2px
> ```

**Teorema 1** O processamento paralelo em transformers, possÃ­vel graÃ§as Ã  arquitetura de atenÃ§Ã£o, reduz o tempo de treinamento em relaÃ§Ã£o a modelos sequenciais como RNNs.
*Prova:*
I. Em modelos RNNs, a computaÃ§Ã£o da saÃ­da no instante $t$ depende da saÃ­da no instante $t-1$, ou seja, da sequÃªncia de cÃ¡lculos atÃ© o momento $t-1$.
II. Isso significa que a computaÃ§Ã£o para cada instante da sequÃªncia deve ser realizada de forma sequencial, limitando a possibilidade de paralelizaÃ§Ã£o.
III. Em transformers, o mecanismo de atenÃ§Ã£o permite que a saÃ­da para cada token da sequÃªncia seja calculada simultaneamente.
IV. Essa computaÃ§Ã£o paralela Ã© possÃ­vel porque a atenÃ§Ã£o calcula as relaÃ§Ãµes entre todos os tokens da sequÃªncia, sem dependÃªncia direta dos cÃ¡lculos nos instantes anteriores.
V. Com o uso de GPUs, a computaÃ§Ã£o paralela dos transformers reduz significativamente o tempo total de treinamento, em comparaÃ§Ã£o com a computaÃ§Ã£o sequencial das RNNs.
â– 

### ConclusÃ£o
O treinamento de LLMs com transformers Ã© um processo iterativo e de larga escala. O uso de auto-supervisÃ£o Ã© crucial para o aproveitamento de grandes quantidades de dados de texto nÃ£o rotulado. Usando a funÃ§Ã£o de perda cross-entropy, o modelo aprende a prever o prÃ³ximo token da sequÃªncia, construindo uma compreensÃ£o da linguagem. A tÃ©cnica de *teacher forcing* auxilia no processo, alimentando o modelo com a sequÃªncia correta de tokens durante o treinamento. A capacidade de processamento paralelo dos transformers aumenta a velocidade do treinamento. A combinaÃ§Ã£o dessas tÃ©cnicas permite que LLMs alcancem um desempenho notÃ¡vel em vÃ¡rias tarefas de processamento de linguagem natural.

### ReferÃªncias
[^25]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright Â© 2023. All rights reserved. Draft of February 3, 2024.
[^26]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright Â© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
