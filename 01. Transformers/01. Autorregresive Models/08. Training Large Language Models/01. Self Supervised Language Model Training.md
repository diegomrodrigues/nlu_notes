## Treinamento de Modelos de Linguagem Grandes (LLMs) com Transformers

### Introdu√ß√£o
Este cap√≠tulo aborda o treinamento de modelos de linguagem grandes (LLMs) baseados em transformers, explorando os mecanismos e t√©cnicas que permitem que esses modelos aprendam a partir de vastos conjuntos de dados textuais. Em particular, focaremos na **auto-supervis√£o**, uma abordagem chave para o treinamento de LLMs, que elimina a necessidade de r√≥tulos manuais, aproveitando a sequ√™ncia natural de palavras em um corpus de texto como sinal de supervis√£o. Como vimos anteriormente, os LLMs  possuem uma capacidade not√°vel de modelar a linguagem, aprendendo representa√ß√µes ricas e contextualizadas de palavras e frases. Esta se√ß√£o aprofunda os detalhes do processo de treinamento que tornam isso poss√≠vel.

### Auto-supervis√£o no Treinamento de Transformers
A **auto-supervis√£o** (ou auto-treinamento) √© o paradigma de treinamento central para LLMs [^25]. Em vez de usar dados rotulados manualmente, os modelos aprendem a partir da estrutura inerente dos dados textuais. Especificamente, a tarefa de treinamento √© projetada de forma que a pr√≥pria sequ√™ncia de palavras dentro de um texto forne√ßa o sinal de supervis√£o. Isso √© crucial, pois permite o aproveitamento de grandes quantidades de texto n√£o rotulado, que s√£o mais abundantes e f√°ceis de obter do que os dados rotulados.

A ideia b√°sica √© utilizar um corpus de texto como material de treinamento e, em cada etapa de tempo $t$, treinar o modelo para prever a pr√≥xima palavra da sequ√™ncia. Em outras palavras, dada uma sequ√™ncia de palavras $w_1, w_2, ..., w_t$, o objetivo do modelo √© prever a pr√≥xima palavra $w_{t+1}$. O modelo √© ent√£o avaliado com base em qu√£o bem ele previu a palavra correta. [^25] Essa abordagem de treinamento √© *auto-supervisionada* porque a pr√≥pria sequ√™ncia de palavras em um texto serve como uma forma de supervis√£o, sem necessidade de r√≥tulos adicionais ou manuais. O modelo aprende atrav√©s da tentativa de prever os pr√≥ximos tokens do texto, com base no contexto precedente.

#### Fun√ß√£o de Perda Cross-Entropy
Para quantificar qu√£o bem o modelo est√° prevendo a pr√≥xima palavra, usamos a **perda cross-entropy** [^25]. A perda cross-entropy mede a diferen√ßa entre a distribui√ß√£o de probabilidade prevista pelo modelo e a distribui√ß√£o de probabilidade real (a pr√≥xima palavra correta). Em termos matem√°ticos, a perda cross-entropy √© definida como:
$$ L_{CE} = - \sum_{w \in V} y_t[w] \log \hat{y}_t[w]$$
onde $V$ √© o vocabul√°rio, $y_t$ √© a distribui√ß√£o real da pr√≥xima palavra (um vetor *one-hot* onde apenas o √≠ndice da palavra correta tem valor 1, e os demais s√£o 0), e $\hat{y}_t$ √© a distribui√ß√£o prevista pelo modelo.
No contexto do modelagem de linguagem, esta equa√ß√£o se simplifica para:
$$ L_{CE} (\hat{y}_t, y_t) = - \log \hat{y}_t[w_{t+1}]$$
Ou seja, a perda √© o logaritmo negativo da probabilidade que o modelo atribui a pr√≥xima palavra correta $w_{t+1}$. O objetivo do treinamento √© minimizar essa perda ao longo de todas as etapas do processo.

> üí° **Exemplo Num√©rico:** Suponha que temos um vocabul√°rio $V = \{ \text{o}, \text{rato}, \text{comeu}, \text{queijo} \}$ e o modelo est√° analisando a sequ√™ncia "o rato comeu". O pr√≥ximo token correto $w_{t+1}$ √© "queijo". O modelo, ap√≥s analisar "o rato comeu", gera uma distribui√ß√£o de probabilidade $\hat{y}_t$ sobre todo o vocabul√°rio, digamos:
>
>  $\hat{y}_t = [P(\text{o})=0.05, P(\text{rato})=0.10, P(\text{comeu})=0.15, P(\text{queijo})=0.70]$
>
>  O vetor *one-hot* $y_t$ seria:
>
>   $y_t = [0, 0, 0, 1]$
>
>  A perda cross-entropy √© ent√£o:
>
>   $L_{CE} = - \log(\hat{y}_t[\text{queijo}]) = - \log(0.70) \approx 0.356$.
>
>  Se o modelo, em outra itera√ß√£o, gerasse uma distribui√ß√£o $\hat{y}_t = [0.01, 0.01, 0.01, 0.97]$, a perda seria:
>
>   $L_{CE} = - \log(0.97) \approx 0.03$, que √© menor. O objetivo do treinamento √©, portanto, atualizar os pesos do modelo para que ele gere distribui√ß√µes de probabilidade mais pr√≥ximas da resposta correta (distribui√ß√£o *one-hot*), minimizando a perda.

**Lema 1**  A minimiza√ß√£o da perda cross-entropy √© equivalente a maximizar a probabilidade da palavra correta, dada a sequ√™ncia anterior.
*Prova:*
I. A fun√ß√£o de perda cross-entropy √© definida como:
    $$L_{CE} (\hat{y}_t, y_t) = - \log \hat{y}_t[w_{t+1}]$$
II. Minimizar a fun√ß√£o $L_{CE}$ significa encontrar o valor de $\hat{y}_t$ que torna a perda o menor poss√≠vel.
III. Como o logaritmo √© uma fun√ß√£o mon√≥tona crescente, minimizar o negativo do logaritmo √© equivalente a maximizar o argumento do logaritmo.
IV. Portanto, minimizar $- \log \hat{y}_t[w_{t+1}]$ √© equivalente a maximizar $\hat{y}_t[w_{t+1}]$, que √© a probabilidade que o modelo atribui √† palavra correta $w_{t+1}$.
V. Logo, a minimiza√ß√£o da perda cross-entropy √© equivalente a maximizar a probabilidade da palavra correta, dada a sequ√™ncia anterior.
‚ñ†

#### *Teacher Forcing*
Durante o processo de treinamento, usamos uma t√©cnica chamada *teacher forcing* [^25]. Em vez de alimentar o modelo com suas pr√≥prias previs√µes (que podem conter erros), alimentamos o modelo com a sequ√™ncia correta de tokens at√© o momento $t$ para prever o pr√≥ximo token $w_{t+1}$. Isso acelera a converg√™ncia do modelo e evita que erros se propaguem ao longo da sequ√™ncia, o que poderia desviar o modelo do treinamento. Em outras palavras, ignoramos a previs√£o do modelo para o pr√≥ximo token e usamos a sequ√™ncia de tokens correta at√© aquele ponto para prever o pr√≥ximo token. Este processo de treinamento √© ilustrado na Figura 10.18 [^26].

> üí° **Exemplo Num√©rico:** Considere a sequ√™ncia "O gato preto dorme". Durante o treinamento com *teacher forcing*, o modelo recebe como entrada "O" para prever "gato". Em seguida, recebe "O gato" para prever "preto", e assim por diante. O modelo n√£o usa a palavra que ele mesmo previu, mas sim a palavra correta, para treinar a previs√£o seguinte. Isso impede o ac√∫mulo de erros. Se o modelo, em algum momento, previsse incorretamente "cachorro" em vez de "gato", este erro n√£o se propagaria para as previs√µes seguintes, pois o modelo recebe a palavra correta "gato" como entrada para prever o pr√≥ximo token.

O modelo recebe a sequ√™ncia correta de tokens $w_{1:t}$ como entrada e produz uma distribui√ß√£o de probabilidade sobre o vocabul√°rio $V$. A perda cross-entropy √© calculada usando a probabilidade atribu√≠da pelo modelo √† pr√≥xima palavra correta. As matrizes de peso na rede s√£o atualizadas por meio do algoritmo de backpropagation para minimizar a perda. Este processo √© repetido para todas as palavras na sequ√™ncia. Para cada item da sequ√™ncia, uma distribui√ß√£o de sa√≠da do transformer sobre o vocabul√°rio √© gerada.

A perda para uma sequ√™ncia de treinamento √© definida como a perda cross-entropy m√©dia sobre toda a sequ√™ncia. Os pesos da rede s√£o ajustados para minimizar a perda m√©dia sobre a sequ√™ncia, usando descida de gradiente [^25].

**Observa√ß√£o 1** A descida de gradiente √© um algoritmo iterativo de otimiza√ß√£o que atualiza os pesos do modelo na dire√ß√£o do negativo do gradiente da fun√ß√£o de perda.  Em cada itera√ß√£o, os pesos s√£o ajustados para reduzir o erro entre as predi√ß√µes do modelo e os valores reais. A taxa de aprendizagem define o tamanho dos passos dados na dire√ß√£o do gradiente.

#### Processamento Paralelo
Uma vantagem chave dos transformers √© que os itens de treinamento podem ser processados em paralelo. Em contraste com modelos baseados em RNNs, onde os c√°lculos de sa√≠das e perdas dependem de c√°lculos em passos anteriores, os transformers calculam as sa√≠das para cada elemento da sequ√™ncia simultaneamente [^26]. Isso aumenta a velocidade de treinamento.

> üí° **Exemplo Num√©rico:** Suponha que temos uma sequ√™ncia de 100 palavras. Em uma RNN, a sa√≠da para a palavra 50 depende do c√°lculo das sa√≠das das 49 palavras anteriores. Isso significa que essas opera√ß√µes precisam ser executadas sequencialmente. Em um transformer, o c√°lculo da sa√≠da de cada uma das 100 palavras √© feito de forma independente, e essas 100 opera√ß√µes podem ser executadas em paralelo, usando GPUs. Se uma GPU tem 100 unidades de processamento paralelas, o c√°lculo seria, no caso do transformer, aproximadamente 100 vezes mais r√°pido do que em uma RNN.
>
> ```mermaid
> graph LR
>     A[Sequ√™ncia de Texto] --> B(RNN - Processamento Sequencial)
>     A --> C(Transformer - Processamento Paralelo)
>     B --> D[Sa√≠das Sequenciais]
>     C --> E[Sa√≠das Paralelas]
>     style B fill:#f9f,stroke:#333,stroke-width:2px
>     style C fill:#ccf,stroke:#333,stroke-width:2px
> ```

**Teorema 1** O processamento paralelo em transformers, poss√≠vel gra√ßas √† arquitetura de aten√ß√£o, reduz o tempo de treinamento em rela√ß√£o a modelos sequenciais como RNNs.
*Prova:*
I. Em modelos RNNs, a computa√ß√£o da sa√≠da no instante $t$ depende da sa√≠da no instante $t-1$, ou seja, da sequ√™ncia de c√°lculos at√© o momento $t-1$.
II. Isso significa que a computa√ß√£o para cada instante da sequ√™ncia deve ser realizada de forma sequencial, limitando a possibilidade de paraleliza√ß√£o.
III. Em transformers, o mecanismo de aten√ß√£o permite que a sa√≠da para cada token da sequ√™ncia seja calculada simultaneamente.
IV. Essa computa√ß√£o paralela √© poss√≠vel porque a aten√ß√£o calcula as rela√ß√µes entre todos os tokens da sequ√™ncia, sem depend√™ncia direta dos c√°lculos nos instantes anteriores.
V. Com o uso de GPUs, a computa√ß√£o paralela dos transformers reduz significativamente o tempo total de treinamento, em compara√ß√£o com a computa√ß√£o sequencial das RNNs.
‚ñ†

### Conclus√£o
O treinamento de LLMs com transformers √© um processo iterativo e de larga escala. O uso de auto-supervis√£o √© crucial para o aproveitamento de grandes quantidades de dados de texto n√£o rotulado. Usando a fun√ß√£o de perda cross-entropy, o modelo aprende a prever o pr√≥ximo token da sequ√™ncia, construindo uma compreens√£o da linguagem. A t√©cnica de *teacher forcing* auxilia no processo, alimentando o modelo com a sequ√™ncia correta de tokens durante o treinamento. A capacidade de processamento paralelo dos transformers aumenta a velocidade do treinamento. A combina√ß√£o dessas t√©cnicas permite que LLMs alcancem um desempenho not√°vel em v√°rias tarefas de processamento de linguagem natural.

### Refer√™ncias
[^25]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^26]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
<!-- END -->
