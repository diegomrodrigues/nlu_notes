## O Mecanismo de Autoaten√ß√£o em Detalhe: Computando Representa√ß√µes Contextuais

### Introdu√ß√£o
Este cap√≠tulo aprofunda a discuss√£o sobre o mecanismo de **autoaten√ß√£o**, que √© essencial para o funcionamento dos **Transformers**, conforme introduzido no cap√≠tulo anterior [^1]. Exploraremos como a autoaten√ß√£o computa representa√ß√µes contextuais, combinando a representa√ß√£o de uma palavra na camada anterior com as representa√ß√µes de palavras vizinhas, usando pesos que determinam o grau de integra√ß√£o [^3]. Al√©m disso, detalharemos o uso dos vetores *query*, *key* e *value* derivados das *embeddings* de entrada para ponderar e combinar representa√ß√µes de diferentes palavras, permitindo que o modelo capture as rela√ß√µes lingu√≠sticas dentro do contexto [^6]. Conforme j√° apresentado, as palavras de entrada desempenham tr√™s pap√©is na autoaten√ß√£o: *query* (foco atual), *key* (entrada precedente para compara√ß√£o) e *value* (usado para computar a sa√≠da) [^6].

### Conceitos Fundamentais
Como vimos anteriormente, o mecanismo de **autoaten√ß√£o** √© respons√°vel por construir representa√ß√µes contextuais ricas para cada *token* em uma sequ√™ncia de texto [^3]. Para isso, ele integra informa√ß√µes de palavras vizinhas, ponderando-as de acordo com sua relev√¢ncia para o *token* de interesse [^4]. Essa pondera√ß√£o √© alcan√ßada atrav√©s do uso de vetores de *query*, *key* e *value* que s√£o derivados das *embeddings* de entrada [^6].

A intui√ß√£o b√°sica √© que a representa√ß√£o de cada palavra n√£o √© est√°tica, mas sim influenciada pelo contexto em que ela aparece [^3]. As palavras t√™m rela√ß√µes complexas com outras palavras, mesmo aquelas que est√£o distantes no texto [^3]. Por exemplo, em "O gato preto correu rapidamente", a palavra "preto" est√° relacionada a "gato", enquanto "rapidamente" est√° relacionado a "correu". O mecanismo de autoaten√ß√£o permite que o modelo capture essas rela√ß√µes, ponderando as representa√ß√µes de palavras relevantes e combinando-as para produzir uma representa√ß√£o contextualizada para cada palavra [^4].

Formalmente, a autoaten√ß√£o pode ser vista como um mecanismo que compara um item de interesse com uma cole√ß√£o de outros itens, revelando a sua relev√¢ncia no contexto atual [^5]. No caso da linguagem, o conjunto de compara√ß√µes √© feito com outras palavras dentro de uma sequ√™ncia [^5]. O resultado dessas compara√ß√µes √© ent√£o usado para calcular a sequ√™ncia de sa√≠da para a sequ√™ncia de entrada [^5]. A relev√¢ncia de cada palavra vizinha √© quantificada atrav√©s de um produto escalar (dot product) entre a representa√ß√£o *query* da palavra de foco ($q_i$) e a representa√ß√£o *key* de cada palavra vizinha ($k_j$) [^6]:
$$score(x_i, x_j) = q_i \cdot k_j$$
onde $x_i$ √© a representa√ß√£o da palavra de foco, e $x_j$ √© a representa√ß√£o da palavra vizinha. Os vetores *query* e *key* s√£o derivados das *embeddings* de entrada atrav√©s de matrizes de peso ($W^Q$ e $W^K$), permitindo que o modelo aprenda diferentes representa√ß√µes para cada palavra dependendo do seu papel no mecanismo de autoaten√ß√£o [^6]:
$$q_i = x_iW^Q; \quad k_i = x_iW^K$$
A representa√ß√£o *value* ($v_i$), derivada atrav√©s de uma matriz de peso separada ($W^V$):
$$v_i = x_iW^V$$
√© usada para construir a representa√ß√£o de sa√≠da, ponderando-a com a relev√¢ncia contextual.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar uma sequ√™ncia de entrada simplificada de tr√™s palavras com *embeddings* de dimens√£o 4:
>
>  $x_1 = \begin{bmatrix} 1 \\ 0 \\ 2 \\ 1 \end{bmatrix}$,  $x_2 = \begin{bmatrix} 0 \\ 2 \\ 1 \\ 1 \end{bmatrix}$, $x_3 = \begin{bmatrix} 1 \\ 1 \\ 0 \\ 2 \end{bmatrix}$
>
>  E as matrizes de pesos (inicializadas aleatoriamente) para a proje√ß√£o em *query*, *key* e *value*:
>
> $W^Q = \begin{bmatrix} 0.1 & 0.2 & -0.1 & 0.3 \\ 0.2 & 0.1 & 0.1 & -0.2 \\ -0.1 & 0.3 & 0.2 & 0.1 \\ 0.3 & -0.2 & 0.1 & 0.2 \end{bmatrix}$, $W^K = \begin{bmatrix} 0.2 & -0.1 & 0.3 & 0.1 \\ 0.1 & 0.2 & -0.2 & 0.3 \\ 0.3 & 0.1 & 0.1 & -0.1 \\ -0.1 & 0.3 & 0.2 & 0.2 \end{bmatrix}$, $W^V = \begin{bmatrix} 0.1 & 0.3 & 0.2 & -0.1 \\ 0.2 & -0.2 & 0.1 & 0.3 \\ -0.1 & 0.1 & 0.3 & 0.2 \\ 0.3 & 0.2 & -0.1 & 0.1 \end{bmatrix}$
>
> **Passo 1: Proje√ß√£o para Query, Key e Value:**
> Calculamos os vetores $q_i$, $k_i$ e $v_i$ para cada $x_i$:
>
> $q_1 = x_1W^Q = \begin{bmatrix} 1 \\ 0 \\ 2 \\ 1 \end{bmatrix} \begin{bmatrix} 0.1 & 0.2 & -0.1 & 0.3 \\ 0.2 & 0.1 & 0.1 & -0.2 \\ -0.1 & 0.3 & 0.2 & 0.1 \\ 0.3 & -0.2 & 0.1 & 0.2 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.8 \\ 0.6 \\ 0.8 \end{bmatrix}$
>
> $k_1 = x_1W^K = \begin{bmatrix} 1 \\ 0 \\ 2 \\ 1 \end{bmatrix} \begin{bmatrix} 0.2 & -0.1 & 0.3 & 0.1 \\ 0.1 & 0.2 & -0.2 & 0.3 \\ 0.3 & 0.1 & 0.1 & -0.1 \\ -0.1 & 0.3 & 0.2 & 0.2 \end{bmatrix} = \begin{bmatrix} 0.7 \\ 0.6 \\ 0.9 \\ 0.2 \end{bmatrix}$
>
> $v_1 = x_1W^V = \begin{bmatrix} 1 \\ 0 \\ 2 \\ 1 \end{bmatrix} \begin{bmatrix} 0.1 & 0.3 & 0.2 & -0.1 \\ 0.2 & -0.2 & 0.1 & 0.3 \\ -0.1 & 0.1 & 0.3 & 0.2 \\ 0.3 & 0.2 & -0.1 & 0.1 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.8 \\ 0.7 \\ 0.4 \end{bmatrix}$
>
> De forma semelhante, podemos calcular $q_2, k_2, v_2$ e $q_3, k_3, v_3$.
>
> **Passo 2: C√°lculo dos Scores (para a primeira palavra):**
> O score de $x_1$ em rela√ß√£o a si pr√≥prio √©:
>
> $score(x_1, x_1) = q_1 \cdot k_1 = \begin{bmatrix} 0.2 \\ 0.8 \\ 0.6 \\ 0.8 \end{bmatrix} \cdot \begin{bmatrix} 0.7 \\ 0.6 \\ 0.9 \\ 0.2 \end{bmatrix} = 0.2*0.7 + 0.8*0.6 + 0.6*0.9 + 0.8*0.2 = 1.4$
>
> Para $x_2$:
> $q_2 =  \begin{bmatrix} 0.1 \\ 0.6 \\ 0.9 \\ 0.8 \end{bmatrix}$, $k_1 = \begin{bmatrix} 0.7 \\ 0.6 \\ 0.9 \\ 0.2 \end{bmatrix}$, $k_2 = \begin{bmatrix} 0.4 \\ 0.9 \\ 0.1 \\ 0.4 \end{bmatrix}$
>
>  $score(x_2, x_1) = q_2 \cdot k_1 = 1.22 $
>  $score(x_2, x_2) = q_2 \cdot k_2 = 1.32$
>
> Para $x_3$:
> $q_3 = \begin{bmatrix} 0.4 \\ 0.3 \\ 0.5 \\ 1.0 \end{bmatrix}$, $k_1 = \begin{bmatrix} 0.7 \\ 0.6 \\ 0.9 \\ 0.2 \end{bmatrix}$, $k_2 = \begin{bmatrix} 0.4 \\ 0.9 \\ 0.1 \\ 0.4 \end{bmatrix}$, $k_3 = \begin{bmatrix} 0.6 \\ 0.4 \\ 0.6 \\ 0.5 \end{bmatrix}$
>
>  $score(x_3, x_1) = q_3 \cdot k_1 = 1.16$
>  $score(x_3, x_2) = q_3 \cdot k_2 = 0.89$
>  $score(x_3, x_3) = q_3 \cdot k_3 = 1.04$
>
>  **Passo 3: Normaliza√ß√£o dos Scores (Softmax):**
>
>  Para $x_1$:
>  $\alpha_{11} = \frac{e^{1.4}}{e^{1.4}} = 1$
>
>  Para $x_2$:
>  $\alpha_{21} = \frac{e^{1.22}}{e^{1.22} + e^{1.32}} = 0.47$,  $\alpha_{22} = \frac{e^{1.32}}{e^{1.22} + e^{1.32}} = 0.53$
>
>  Para $x_3$:
>   $\alpha_{31} = \frac{e^{1.16}}{e^{1.16} + e^{0.89} + e^{1.04}} = 0.37$, $\alpha_{32} = \frac{e^{0.89}}{e^{1.16} + e^{0.89} + e^{1.04}} = 0.28$, $\alpha_{33} = \frac{e^{1.04}}{e^{1.16} + e^{0.89} + e^{1.04}} = 0.35$
>
> **Passo 4: Combina√ß√£o dos Values:**
>
>  $a_1 = \alpha_{11} * v_1 = 1 * \begin{bmatrix} 0.2 \\ 0.8 \\ 0.7 \\ 0.4 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.8 \\ 0.7 \\ 0.4 \end{bmatrix}$
>
>  $a_2 = \alpha_{21} * v_1 + \alpha_{22} * v_2= 0.47 * \begin{bmatrix} 0.2 \\ 0.8 \\ 0.7 \\ 0.4 \end{bmatrix} + 0.53 * \begin{bmatrix} 0.7 \\ 0.1 \\ 0.6 \\ 0.9 \end{bmatrix} = \begin{bmatrix} 0.46 \\ 0.45 \\ 0.64 \\ 0.67 \end{bmatrix}$
>
> $a_3 = \alpha_{31}*v_1 + \alpha_{32}*v_2 + \alpha_{33}*v_3 = 0.37 * \begin{bmatrix} 0.2 \\ 0.8 \\ 0.7 \\ 0.4 \end{bmatrix} + 0.28 * \begin{bmatrix} 0.7 \\ 0.1 \\ 0.6 \\ 0.9 \end{bmatrix} + 0.35 * \begin{bmatrix} 0.5 \\ 0.4 \\ 0.8 \\ 0.3 \end{bmatrix} =  \begin{bmatrix} 0.45 \\ 0.53 \\ 0.71 \\ 0.46 \end{bmatrix}$
>
>  Assim, $a_1$, $a_2$ e $a_3$ s√£o as representa√ß√µes contextuais de $x_1$, $x_2$ e $x_3$ respectivamente. Note que $a_1$ √© simplesmente a representa√ß√£o inicial ponderada por 1 (s√≥ h√° auto-aten√ß√£o), $a_2$ leva em conta $x_1$ e $x_2$, e $a_3$ as tr√™s palavras anteriores.

> üí° **Relembrando o exemplo num√©rico anterior**:
>
> No cap√≠tulo anterior, usamos um exemplo num√©rico simplificado para ilustrar o processo de autoaten√ß√£o, incluindo as matrizes de proje√ß√£o $W^Q$, $W^K$ e $W^V$ e como os scores, pesos e vetores de sa√≠da s√£o calculados. A se√ß√£o seguinte detalha ainda mais como esses vetores s√£o usados para ponderar e combinar as representa√ß√µes das palavras no contexto.

O score resultante do produto escalar √© normalizado utilizando a fun√ß√£o *softmax* para produzir um vetor de pesos $\alpha_{ij}$ que representa a relev√¢ncia proporcional de cada palavra vizinha $j$ para a palavra foco $i$ [^5]:
$$\alpha_{ij} = \frac{exp(score(x_i, x_j))}{\sum_{k=1}^i exp(score(x_i, x_k))}, \forall j \leq i$$
Este vetor de pesos √© ent√£o usado para ponderar os vetores de *value* ($v_j$), resultando em um valor de sa√≠da ponderada que representa a palavra *i* no contexto:
$$a_i = \sum_{j=1}^i \alpha_{ij} v_j$$

O resultado deste processo √© uma representa√ß√£o contextualizada $a_i$ para a palavra $i$, refletindo a influ√™ncia das palavras vizinhas, combinando suas representa√ß√µes com base em pesos que refletem a sua import√¢ncia no contexto.

> üí° **Formaliza√ß√£o da Autoaten√ß√£o**
>
> Vamos agora formalizar o processo de autoaten√ß√£o passo a passo, usando os conceitos introduzidos acima:
>
> 1. **Proje√ß√£o das Embeddings:** Para cada palavra $x_i$ na sequ√™ncia de entrada, calcule os vetores *query* ($q_i$), *key* ($k_i$) e *value* ($v_i$) atrav√©s de multiplica√ß√µes com as matrizes de peso $W^Q$, $W^K$ e $W^V$:
>
> $$q_i = x_iW^Q$$
>
> $$k_i = x_iW^K$$
>
> $$v_i = x_iW^V$$
>
> 2. **C√°lculo dos Scores:** Para cada palavra $x_i$, calcule o *score* com todas as palavras precedentes $x_j$ (incluindo a pr√≥pria $x_i$):
>
> $$score(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}$$
>
> onde $d_k$ √© a dimens√£o dos vetores *key* e *query*. A divis√£o por $\sqrt{d_k}$ ajuda a estabilizar o treinamento.
>
> 3. **Normaliza√ß√£o dos Scores:** Aplique a fun√ß√£o *softmax* nos scores para obter um vetor de pesos $\alpha_{ij}$ que representa a relev√¢ncia proporcional de cada palavra vizinha $x_j$ para a palavra foco $x_i$:
>
> $$\alpha_{ij} = \frac{exp(score(x_i, x_j))}{\sum_{k=1}^i exp(score(x_i, x_k))}, \forall j \leq i$$
>
> 4. **Combina√ß√£o dos Values:** Calcule a sa√≠da ponderada $a_i$ para a palavra $x_i$, combinando os vetores de *value* ($v_j$) com os pesos $\alpha_{ij}$:
>
> $$a_i = \sum_{j=1}^i \alpha_{ij} v_j$$
>
> O resultado $a_i$ √© a representa√ß√£o contextualizada da palavra $x_i$, que ser√° passada para a pr√≥xima camada do Transformer.

√â crucial entender que as matrizes de pesos $W^Q$, $W^K$ e $W^V$ s√£o par√¢metros que o modelo aprende durante o treinamento [^6]. Ao otimizar esses par√¢metros, o modelo √© capaz de aprender a projetar as *embeddings* de entrada de forma a capturar as rela√ß√µes contextuais mais relevantes para a tarefa em quest√£o.

A autoaten√ß√£o computa essas representa√ß√µes contextuais por meio de uma combina√ß√£o de opera√ß√µes lineares (as proje√ß√µes com $W^Q$, $W^K$, $W^V$) e n√£o lineares (*softmax*) [^3]. O produto escalar entre os vetores *query* e *key* quantifica a similaridade entre palavras, enquanto o *softmax* converte esses *scores* em pesos, representando uma distribui√ß√£o de probabilidade sobre as palavras precedentes [^5]. Esses pesos s√£o utilizados para ponderar os vetores de *value*, permitindo que o modelo combine as representa√ß√µes de palavras vizinhas de forma contextualmente relevante [^6].

√â importante notar tamb√©m que este mecanismo de autoaten√ß√£o √© computado em paralelo para todas as posi√ß√µes da sequ√™ncia de entrada, o que torna o Transformer muito mais eficiente do que modelos recorrentes [^3]. A etapa de mascaramento, que impede que o modelo "veja" as palavras futuras, √© crucial para o treinamento de modelos auto-regressivos, que predizem palavras em uma sequ√™ncia [^8].

> üí° **Observa√ß√µes Importantes**
>
> *   **Fun√ß√£o do *softmax*:** A fun√ß√£o *softmax* garante que os pesos $\alpha_{ij}$ somem 1, criando uma distribui√ß√£o de probabilidade sobre as palavras vizinhas. Isso permite que o modelo aprenda a distribuir sua "aten√ß√£o" entre as palavras mais relevantes no contexto.
> *   **Pondera√ß√£o dos *values*:** Os vetores de *value* representam a informa√ß√£o a ser passada adiante. Ao ponderar os *values* com os pesos $\alpha_{ij}$, o modelo √© capaz de combinar as representa√ß√µes de palavras vizinhas, dando mais import√¢ncia √†quelas que s√£o mais relevantes para a palavra foco.
> *   **Paraleliza√ß√£o:** A capacidade de calcular as representa√ß√µes contextuais em paralelo √© uma das principais vantagens do Transformer em rela√ß√£o a modelos recorrentes.
> *   **Mascaramento:** O mascaramento garante que o modelo n√£o "cole" o texto de treinamento, e sim aprenda a modelar as depend√™ncias entre as palavras.

**Lema 1.** *A autoaten√ß√£o √© invariante por transla√ß√£o no espa√ßo de embeddings, desde que as matrizes de peso $W^Q$, $W^K$ e $W^V$ sejam aplicadas ap√≥s a transla√ß√£o.*

*Prova.*
I.  Seja $x_i$ a representa√ß√£o original de um *token* e $t$ um vetor de transla√ß√£o. Seja $x_i' = x_i + t$ a representa√ß√£o transladada.
II. Os vetores *query*, *key* e *value* originais s√£o:
    $$q_i = x_iW^Q; \quad k_i = x_iW^K; \quad v_i = x_iW^V$$
III. Os vetores *query*, *key* e *value* transladados, aplicados corretamente ap√≥s a transla√ß√£o, s√£o:
    $$q_i' = x_i'W^Q = (x_i + t)W^Q = x_iW^Q + tW^Q = q_i + tW^Q$$
    $$k_i' = x_i'W^K = (x_i + t)W^K = x_iW^K + tW^K = k_i + tW^K$$
    $$v_i' = x_i'W^V = (x_i + t)W^V = x_iW^V + tW^V = v_i + tW^V$$
IV. O score entre as representa√ß√µes transladadas √©:
    $$score'(x_i', x_j') = q_i' \cdot k_j' = (q_i + tW^Q) \cdot (k_j + tW^K) = q_i \cdot k_j + q_i \cdot tW^K + tW^Q \cdot k_j + tW^Q \cdot tW^K$$
V. Note que, se a matriz de peso $W^Q$ e $W^K$ n√£o fosse aplicada ap√≥s a transla√ß√£o, ter√≠amos $q_i' = q_i + t$ e $k_i' = k_i + t$, levando a:
    $$score''(x_i', x_j') = (q_i + t) \cdot (k_j + t) = q_i \cdot k_j + q_i \cdot t + t \cdot k_j + t \cdot t$$
VI. O que claramente n√£o √© invariante por transla√ß√£o em rela√ß√£o ao score original.
VII. O *softmax* √© invariante por transla√ß√£o, e a combina√ß√£o linear dos valores √© invariante por transla√ß√£o se as matrizes $W^V$ forem aplicadas ap√≥s a transla√ß√£o. Portanto, as representa√ß√µes contextuais s√£o invariantes por transla√ß√£o no espa√ßo de embeddings, desde que as matrizes de peso sejam aplicadas corretamente ap√≥s a transla√ß√£o. ‚ñ†

**Teorema 1.** *O mecanismo de autoaten√ß√£o com m√°scaras implementa um tipo espec√≠fico de aten√ß√£o causal, onde a representa√ß√£o de uma palavra s√≥ pode depender das palavras precedentes e da pr√≥pria palavra.*

*Prova.*
I. O c√°lculo dos *scores* no mecanismo de autoaten√ß√£o considera apenas as palavras precedentes e a pr√≥pria palavra, conforme descrito na etapa 2 da formaliza√ß√£o.
II. A aplica√ß√£o do *softmax* na etapa 3 normaliza esses *scores*, gerando pesos que refletem a relev√¢ncia de cada palavra precedente para a palavra de foco.
III. A combina√ß√£o dos vetores *value* na etapa 4 utiliza apenas esses pesos e os vetores *value* correspondentes √†s palavras precedentes.
IV. Portanto, a sa√≠da $a_i$ para a palavra $x_i$ depende exclusivamente das representa√ß√µes de $x_j$ para $j \le i$.
V. A m√°scara, formalmente, garante que a fun√ß√£o *softmax* ignore as posi√ß√µes com $j>i$, assim a sa√≠da $a_i$ n√£o depende de $x_j$ com $j>i$. Isso garante que o mecanismo de autoaten√ß√£o seja causal, onde o futuro n√£o pode afetar o presente ou passado. ‚ñ†

**Proposi√ß√£o 1.** *O mecanismo de autoaten√ß√£o pode ser visto como um caso espec√≠fico de um modelo de aten√ß√£o mais geral, onde a aten√ß√£o √© restrita √†s palavras precedentes.*

*Prova.*
I. Em um modelo de aten√ß√£o geral, a aten√ß√£o poderia ser calculada sobre qualquer conjunto de *tokens*.
II. No entanto, o mecanismo de autoaten√ß√£o com m√°scara restringe a aten√ß√£o apenas aos *tokens* que precedem o *token* atual, e inclui a si mesmo.
III. Este √© um caso espec√≠fico de aten√ß√£o, que pode ser vista como uma aplica√ß√£o de um modelo de aten√ß√£o mais geral com uma restri√ß√£o de contexto que for√ßa a causalidade. ‚ñ†

**Corol√°rio 1.1.** *A remo√ß√£o da m√°scara no mecanismo de autoaten√ß√£o permite que o modelo compute representa√ß√µes contextuais que levam em conta todas as palavras da sequ√™ncia, e n√£o apenas as precedentes.*

*Prova.*
I. Removendo a m√°scara, a etapa 3 da formaliza√ß√£o passa a considerar todos os *scores* na normaliza√ß√£o.
II. Portanto, a etapa 4 da formaliza√ß√£o passar√° a calcular a representa√ß√£o contextualizada $a_i$ considerando todos os $v_j$, para todos os $j$.
III. Isso significa que o resultado $a_i$ n√£o depender√° somente de *tokens* precedentes, mas tamb√©m de *tokens* posteriores. ‚ñ†

> üí° **Compara√ß√£o com Modelos Recorrentes**:
>
> √â importante destacar que a capacidade de processar a sequ√™ncia de entrada em paralelo √© uma das principais vantagens do mecanismo de autoaten√ß√£o em rela√ß√£o aos modelos recorrentes (RNNs). Em RNNs, as representa√ß√µes s√£o computadas sequencialmente, o que impede o processamento paralelo e torna o treinamento mais lento. A autoaten√ß√£o, por outro lado, permite que todas as posi√ß√µes da sequ√™ncia sejam processadas simultaneamente, o que acelera significativamente o treinamento, especialmente para sequ√™ncias longas.

### Conclus√£o

Este cap√≠tulo detalhou o mecanismo de autoaten√ß√£o, componente essencial dos Transformers, demonstrando como ele computa representa√ß√µes contextuais atrav√©s da combina√ß√£o de vetores *query*, *key* e *value* derivados das *embeddings* de entrada. Atrav√©s de opera√ß√µes de produto escalar e normaliza√ß√£o com *softmax*, o modelo aprende a ponderar a relev√¢ncia de diferentes palavras no contexto, produzindo representa√ß√µes contextuais ricas que s√£o usadas nas camadas subsequentes do Transformer. As matrizes de pesos $W^Q$, $W^K$ e $W^V$ s√£o par√¢metros que o modelo aprende durante o treinamento, permitindo que ele projete as *embeddings* de entrada em espa√ßos de representa√ß√£o que capturam as rela√ß√µes contextuais mais relevantes. Este mecanismo de autoaten√ß√£o, ao ser computado em paralelo e combinado com o mecanismo de mascaramento, √© o que torna os Transformers t√£o eficazes para tarefas de processamento de linguagem natural.

### Refer√™ncias

[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^3]: 10.1.1 Transformers: the intuition
[^4]: 4 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^5]: 10.1.3 Self-attention more formally
[^6]: 6 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^8]: 10.1.4 Parallelizing self-attention using a single matrix X
<!-- END -->
