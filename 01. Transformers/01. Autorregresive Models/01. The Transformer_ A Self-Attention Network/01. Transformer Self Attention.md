## O Transformer: Uma Rede de Autoaten√ß√£o
### Introdu√ß√£o
Este cap√≠tulo explora o **Transformer**, uma arquitetura fundamental para modelos de linguagem grandes, com foco em seus mecanismos de **autoaten√ß√£o** [^1]. Discutiremos como essa arquitetura inovadora permite construir representa√ß√µes contextuais ricas de palavras, integrando informa√ß√µes de palavras vizinhas em extensos trechos de texto [^1]. Em vez de usar conex√µes recorrentes como as LSTM, o Transformer utiliza mecanismos de autoaten√ß√£o que podem ser computados em paralelo, proporcionando uma implementa√ß√£o mais eficiente em larga escala [^3]. Este cap√≠tulo ir√° detalhar a intui√ß√£o por tr√°s do Transformer, o funcionamento interno da autoaten√ß√£o, e como esses mecanismos juntos formam um modelo de linguagem poderoso.

### Conceitos Fundamentais
O Transformer √© constru√≠do sobre pilhas de **blocos de transformer**, cada um mapeando sequ√™ncias de vetores de entrada $(x_1, ..., x_n)$ para sequ√™ncias de vetores de sa√≠da $(z_1, ..., z_n)$ do mesmo comprimento [^3]. Esses blocos combinam camadas lineares simples, redes *feedforward* e camadas de **autoaten√ß√£o**, que s√£o a chave da inova√ß√£o dos Transformers [^3]. A **autoaten√ß√£o** possibilita que a rede extraia e utilize informa√ß√µes de contextos arbitrariamente grandes, integrando informa√ß√µes de palavras vizinhas, ajudando o modelo a entender como as palavras se relacionam entre si [^2].

A intui√ß√£o do Transformer reside na constru√ß√£o de representa√ß√µes contextuais progressivamente mais ricas para os significados das palavras de entrada ou *tokens* ao longo de uma s√©rie de camadas [^3]. Em cada camada, para calcular a representa√ß√£o da palavra *i*, o Transformer combina a informa√ß√£o da representa√ß√£o de *i* na camada anterior com informa√ß√µes das representa√ß√µes de palavras vizinhas [^3]. O objetivo √© produzir uma representa√ß√£o contextualizada para cada palavra em cada posi√ß√£o, algo que represente o que essa palavra significa no contexto espec√≠fico em que aparece [^3].

Um aspecto crucial do mecanismo de autoaten√ß√£o √© como ele pesa e combina representa√ß√µes de diferentes palavras no contexto [^3]. Este mecanismo deve ser capaz de analisar todo o contexto, uma vez que as palavras t√™m rela√ß√µes lingu√≠sticas complexas com outras palavras, mesmo aquelas distantes no texto [^3]. Para ilustrar, considere as seguintes frases:

>(10.1) As chaves do arm√°rio est√£o sobre a mesa.
>
>(10.2) A galinha atravessou a rua porque queria chegar ao outro lado.
>
>(10.3) Caminhei ao longo da lagoa e notei que uma das √°rvores na margem havia ca√≠do na √°gua ap√≥s a tempestade.
>
Na frase (10.1), a frase "As chaves" √© o sujeito da senten√ßa e, em portugu√™s, deve concordar em n√∫mero gramatical com o verbo "est√£o". Na frase (10.2), o pronome "ela" se refere √† "galinha" demonstrando **co-refer√™ncia**. Na frase (10.3), a palavra "margem" √© referenciada por palavras como "lagoa" e "√°gua", mostrando a import√¢ncia do **contexto** para determinar o sentido da palavra [^3].

Para capturar essas rela√ß√µes, a autoaten√ß√£o permite integrar informa√ß√µes de palavras em qualquer lugar no contexto [^4]. No entanto, a fim de calcular a representa√ß√£o para uma palavra espec√≠fica, a autoaten√ß√£o pondera as representa√ß√µes das palavras vizinhas, usando uma **distribui√ß√£o de pesos** que indica a import√¢ncia relativa de cada palavra no contexto [^4]. Por exemplo, ao calcular a representa√ß√£o contextualizada do pronome "ela" na frase (10.2), o modelo deve atribuir uma alta relev√¢ncia √† palavra "galinha" [^4].

Formalmente, a **autoaten√ß√£o** pode ser vista como um mecanismo que compara um item de interesse com outros itens, revelando a sua relev√¢ncia no contexto atual [^5]. No contexto da linguagem, as compara√ß√µes s√£o feitas com outras palavras dentro de uma sequ√™ncia [^5]. O resultado dessas compara√ß√µes √© utilizado para calcular uma sequ√™ncia de sa√≠da para a sequ√™ncia de entrada [^5].  Para quantificar a rela√ß√£o entre as palavras, usa-se um produto escalar como a m√©trica principal [^5]:
$$score(x_i, x_j) = x_i \cdot x_j$$
onde $x_i$ e $x_j$ s√£o as representa√ß√µes vetoriais das palavras. Em seguida, os scores s√£o normalizados atrav√©s de uma fun√ß√£o *softmax* para criar um vetor de pesos $\alpha_{ij}$ que indica a relev√¢ncia proporcional de cada entrada para o elemento focal da entrada $i$:
$$ \alpha_{ij} = \frac{exp(score(x_i, x_j))}{\sum_{k=1}^i exp(score(x_i, x_k))}, \forall j \leq i$$
Estes pesos s√£o utilizados para gerar um valor de sa√≠da ponderada $a_i$:
$$a_i = \sum_{j=1}^i \alpha_{ij} x_j$$
Este processo essencialmente combina representa√ß√µes de palavras vizinhas atrav√©s de uma soma ponderada por pesos que refletem a sua relev√¢ncia contextual. O valor de sa√≠da $a_i$ torna-se uma nova representa√ß√£o contextualizada para a palavra *i*, refletindo a influ√™ncia das palavras vizinhas. [^6].

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos uma frase com tr√™s palavras representadas por vetores:
>
> $x_1 = [1, 0, 0]$, $x_2 = [0, 1, 0]$, $x_3 = [1, 1, 0]$
>
> Queremos calcular a representa√ß√£o contextualizada da segunda palavra ($x_2$).
>
> **Passo 1: Calcular os scores:**
>
> $score(x_2, x_1) = x_2 \cdot x_1 = [0, 1, 0] \cdot [1, 0, 0] = 0$
>
> $score(x_2, x_2) = x_2 \cdot x_2 = [0, 1, 0] \cdot [0, 1, 0] = 1$
>
> $score(x_2, x_3) = x_2 \cdot x_3 = [0, 1, 0] \cdot [1, 1, 0] = 1$
>
> **Passo 2: Aplicar Softmax para obter pesos:**
>
> $\alpha_{21} = \frac{e^0}{e^0 + e^1 + e^1} = \frac{1}{1 + 2.718 + 2.718} \approx 0.14$
>
> $\alpha_{22} = \frac{e^1}{e^0 + e^1 + e^1} = \frac{2.718}{1 + 2.718 + 2.718} \approx 0.43$
>
> $\alpha_{23} = \frac{e^1}{e^0 + e^1 + e^1} = \frac{2.718}{1 + 2.718 + 2.718} \approx 0.43$
>
> **Passo 3: Calcular o valor de sa√≠da ponderada:**
>
> $a_2 = \alpha_{21} x_1 + \alpha_{22} x_2 + \alpha_{23} x_3 = 0.14 * [1, 0, 0] + 0.43 * [0, 1, 0] + 0.43 * [1, 1, 0] = [0.57, 0.86, 0]$
>
> Observe que $a_2$ √© uma combina√ß√£o ponderada de $x_1$, $x_2$ e $x_3$, refletindo a influ√™ncia contextual de cada palavra na representa√ß√£o de $x_2$.

Para refinar o mecanismo de autoaten√ß√£o, o Transformer introduz as matrizes de peso $W^Q$, $W^K$ e $W^V$ que projetam cada vetor de entrada $x_i$ para sua representa√ß√£o como uma *query*, uma *key* e um *value*, respectivamente [^6]:
$$q_i = x_iW^Q; \quad k_i = x_iW^K; \quad v_i = x_iW^V$$
As representa√ß√µes como *query* s√£o usadas para medir a relev√¢ncia de outras palavras (*keys*) em rela√ß√£o √† palavra atual, e as representa√ß√µes como *value* contribuem para calcular a sa√≠da ponderada [^6]. A m√©trica de score √© ent√£o definida como o produto escalar entre a *query* e a *key*:
$$score(x_i, x_j) = q_i \cdot k_j$$
A normaliza√ß√£o com *softmax* e o c√°lculo do valor de sa√≠da s√£o conduzidos como anteriormente, mas usando os vetores de *value*:
$$a_i = \sum_{j=1}^i \alpha_{ij} v_j$$
Uma vez que o produto escalar pode gerar valores arbitrariamente grandes, uma normaliza√ß√£o adicional √© introduzida dividindo o *score* pela raiz quadrada da dimensionalidade dos vetores *key* e *query*  ($\sqrt{d_k}$), o que permite que a autoaten√ß√£o funcione de forma mais est√°vel, evitando valores num√©ricos muito grandes:
$$score(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}$$
Este conjunto final de equa√ß√µes define o mecanismo de autoaten√ß√£o do transformer, onde cada vetor de entrada $x_i$ √© projetado para *query*, *key* e *value*, as *queries* e *keys* s√£o usadas para calcular os scores que s√£o normalizados e usados para ponderar os *values* [^7].

> üí° **Exemplo Num√©rico (com Q, K, V):**
>
> Vamos utilizar os mesmos vetores $x_1$, $x_2$, $x_3$ e criar matrizes de proje√ß√£o aleat√≥rias para este exemplo:
>
> $W^Q = \begin{bmatrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \\ 0.7 & 0.8 & 0.9 \end{bmatrix}$, $W^K = \begin{bmatrix} 0.9 & 0.8 & 0.7 \\ 0.6 & 0.5 & 0.4 \\ 0.3 & 0.2 & 0.1 \end{bmatrix}$, $W^V = \begin{bmatrix} 0.2 & 0.4 & 0.6 \\ 0.1 & 0.3 & 0.5 \\ 0.9 & 0.7 & 0.2 \end{bmatrix}$
>
> **Passo 1: Calcular Q, K, V:**
>
> $q_i = x_i W^Q$;  $k_i = x_i W^K$;  $v_i = x_i W^V$
>
> $q_1 = [1, 0, 0] W^Q = [0.1, 0.2, 0.3]$, $k_1 = [1, 0, 0] W^K = [0.9, 0.8, 0.7]$,  $v_1 = [1, 0, 0] W^V = [0.2, 0.4, 0.6]$
>
> $q_2 = [0, 1, 0] W^Q = [0.4, 0.5, 0.6]$, $k_2 = [0, 1, 0] W^K = [0.6, 0.5, 0.4]$,  $v_2 = [0, 1, 0] W^V = [0.1, 0.3, 0.5]$
>
> $q_3 = [1, 1, 0] W^Q = [0.5, 0.7, 0.9]$, $k_3 = [1, 1, 0] W^K = [1.5, 1.3, 1.1]$, $v_3 = [1, 1, 0] W^V = [0.3, 0.7, 1.1]$
>
> **Passo 2: Calcular scores (com normaliza√ß√£o $\sqrt{d_k}$):**
>
> Aqui, $d_k = 3$.
>
> $score(x_2, x_1) = \frac{q_2 \cdot k_1}{\sqrt{3}} = \frac{[0.4, 0.5, 0.6] \cdot [0.9, 0.8, 0.7]}{\sqrt{3}} = \frac{0.36 + 0.40 + 0.42}{\sqrt{3}} = \frac{1.18}{1.732} \approx 0.68$
>
> $score(x_2, x_2) = \frac{q_2 \cdot k_2}{\sqrt{3}} = \frac{[0.4, 0.5, 0.6] \cdot [0.6, 0.5, 0.4]}{\sqrt{3}} = \frac{0.24 + 0.25 + 0.24}{\sqrt{3}} = \frac{0.73}{1.732} \approx 0.42$
>
> $score(x_2, x_3) = \frac{q_2 \cdot k_3}{\sqrt{3}} = \frac{[0.4, 0.5, 0.6] \cdot [1.5, 1.3, 1.1]}{\sqrt{3}} = \frac{0.60 + 0.65 + 0.66}{\sqrt{3}} = \frac{1.91}{1.732} \approx 1.10$
>
> **Passo 3: Aplicar Softmax:**
>
> $\alpha_{21} = \frac{e^{0.68}}{e^{0.68} + e^{0.42} + e^{1.10}} = \frac{1.97}{1.97 + 1.52 + 3.00} \approx 0.30$
>
> $\alpha_{22} = \frac{e^{0.42}}{e^{0.68} + e^{0.42} + e^{1.10}} = \frac{1.52}{1.97 + 1.52 + 3.00} \approx 0.23$
>
> $\alpha_{23} = \frac{e^{1.10}}{e^{0.68} + e^{0.42} + e^{1.10}} = \frac{3.00}{1.97 + 1.52 + 3.00} \approx 0.47$
>
> **Passo 4: Calcular a sa√≠da ponderada:**
>
> $a_2 = \alpha_{21} v_1 + \alpha_{22} v_2 + \alpha_{23} v_3 = 0.30 * [0.2, 0.4, 0.6] + 0.23 * [0.1, 0.3, 0.5] + 0.47 * [0.3, 0.7, 1.1] = [0.22, 0.48, 0.77]$
>
> A representa√ß√£o $a_2$ √© um vetor resultante da combina√ß√£o dos valores $v_i$ ponderados pelos pesos $\alpha_{2i}$. Note que as matrizes $W^Q, W^K,$ e $W^V$ foram inicializadas de forma aleat√≥ria, em um modelo real, elas seriam treinadas para aprender as melhores representa√ß√µes para cada *token*.

**Observa√ß√£o 1** O uso de $W^Q$, $W^K$, e $W^V$ como matrizes de proje√ß√£o permite que o modelo aprenda diferentes representa√ß√µes para cada palavra dependendo do seu papel no mecanismo de autoaten√ß√£o. A escolha dessas matrizes √© fundamental para o desempenho do modelo e √© aprendida durante o treinamento. Al√©m disso, √© importante notar que $d_k$ √© a dimens√£o dos vetores *query* e *key*. Em geral, $d_k$ √© menor ou igual √† dimens√£o do vetor de entrada $x_i$.

√â importante notar que todo esse processo pode ser paralelizado usando multiplica√ß√£o de matrizes, agrupando os vetores de entrada em uma √∫nica matriz $X$ e calculando os *scores* e *values* para todas as posi√ß√µes ao mesmo tempo [^8]:
$$ Q = XW^Q; \quad K = XW^K; \quad V = XW^V$$
$$A = SelfAttention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$
Finalmente, para evitar que o modelo "veja" palavras futuras durante o treinamento, a matriz de *scores* √© mascarada, for√ßando as intera√ß√µes com as palavras futuras a serem zero [^8].

**Lema 1** A opera√ß√£o de mascaramento durante o treinamento garante que o modelo auto-regressivo s√≥ considere o contexto esquerdo ao gerar a pr√≥xima palavra. Isso evita que o modelo "cole" o texto de treinamento, garantindo que ele aprenda a modelar a depend√™ncia sequencial entre as palavras, e n√£o simplesmente imitar o texto.

*Prova:*
I. O mascaramento, como descrito no texto, garante que, para um dado token na posi√ß√£o *i*, os scores calculados com todos os tokens nas posi√ß√µes *j > i* s√£o zerados (ou seja, atribu√≠do um valor de $-\infty$ antes de aplicar o softmax).
II. Ao aplicar o softmax nos scores mascarados, os pesos $\alpha_{ij}$ correspondentes aos tokens *j > i* ser√£o 0, pois $e^{-\infty} = 0$.
III. A sa√≠da ponderada $a_i$ √© uma combina√ß√£o linear dos valores $v_j$ ponderados por $\alpha_{ij}$. Como todos $\alpha_{ij}$ correspondentes a *j > i* s√£o 0, a sa√≠da $a_i$ apenas considerar√° valores $v_j$ com $j \leq i$.
IV. Isso garante que o modelo ao gerar o token na posi√ß√£o *i* utilize apenas o contexto esquerdo (tokens nas posi√ß√µes $j \leq i$), evitando que ele "veja" o futuro.
V. Portanto, o mascaramento durante o treinamento garante que o modelo auto-regressivo s√≥ considere o contexto esquerdo ao gerar a pr√≥xima palavra. $\blacksquare$

**Teorema 1** (Multi-Head Attention) Para aumentar ainda mais a capacidade do modelo de capturar diferentes tipos de rela√ß√µes entre as palavras, os Transformers utilizam o conceito de *Multi-Head Attention*. Em vez de realizar uma √∫nica autoaten√ß√£o, o processo √© repetido *h* vezes com diferentes matrizes de proje√ß√£o $W_i^Q$, $W_i^K$ e $W_i^V$ para cada head $i$. Cada head calcula sua pr√≥pria sa√≠da $a_i$, que √© ent√£o concatenada e projetada por uma matriz $W^O$:

$$
Head_i = SelfAttention(XW_i^Q, XW_i^K, XW_i^V)
$$
$$
MultiHead(Q, K, V) = Concat(Head_1, \ldots, Head_h)W^O
$$
onde $Concat$ representa a concatena√ß√£o das sa√≠das de cada head. Essa arquitetura permite que o modelo capture diferentes tipos de rela√ß√µes entre as palavras (por exemplo, rela√ß√µes sint√°ticas, sem√¢nticas, co-refer√™ncias).

> üí° **Exemplo Num√©rico (Multi-Head Attention):**
>
> Vamos considerar que temos 2 heads (h=2), e que cada head utiliza as proje√ß√µes que definimos no exemplo anterior, com a √∫nica diferen√ßa de que cada head ter√° proje√ß√µes diferentes. Para simplificar, consideremos que os valores de sa√≠da de cada head j√° foram calculados como no exemplo anterior:
>
> $Head_1(x_2) = a_2^{head1} = [0.22, 0.48, 0.77]$ (o mesmo $a_2$ calculado anteriormente)
>
> $Head_2(x_2) = a_2^{head2} = [0.5, 0.2, 0.9]$ (um exemplo de sa√≠da de um segundo head)
>
> **Passo 1: Concatenar as sa√≠das dos heads:**
>
> $Concat(Head_1, Head_2) = [0.22, 0.48, 0.77, 0.5, 0.2, 0.9]$
>
> **Passo 2: Projetar a sa√≠da concatenada com $W^O$:**
>
> Suponha que $W^O$ seja uma matriz de 6x3 (para projetar a concatena√ß√£o de volta a um espa√ßo de 3 dimens√µes):
>
> $W^O = \begin{bmatrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \\ 0.7 & 0.8 & 0.9 \\ 0.9 & 0.8 & 0.7 \\ 0.6 & 0.5 & 0.4 \\ 0.3 & 0.2 & 0.1 \end{bmatrix}$
>
> $MultiHead(x_2) = [0.22, 0.48, 0.77, 0.5, 0.2, 0.9] \cdot W^O = [0.914, 0.79, 0.761]$
>
> O resultado final √© um vetor que combina a informa√ß√£o de todos os heads, enriquecendo a representa√ß√£o contextual da palavra.

*Prova:*
I. Para cada head *i*, as matrizes de proje√ß√£o $W_i^Q$, $W_i^K$, e $W_i^V$ s√£o diferentes, o que significa que cada head ir√° projetar os vetores de entrada $X$ em diferentes espa√ßos de representa√ß√£o.
II.  Cada head *i* calcula sua sa√≠da $Head_i$ aplicando a fun√ß√£o de autoaten√ß√£o nos vetores projetados: $Head_i = SelfAttention(XW_i^Q, XW_i^K, XW_i^V)$.
III. A fun√ß√£o $SelfAttention$, conforme descrito anteriormente, permite que cada head capture diferentes tipos de rela√ß√µes entre as palavras com base nas suas proje√ß√µes.
IV. As sa√≠das de todos os heads s√£o concatenadas para formar uma √∫nica representa√ß√£o: $Concat(Head_1, \ldots, Head_h)$.
V.  Finalmente, essa representa√ß√£o concatenada √© projetada por uma matriz $W^O$ para produzir a sa√≠da final do *Multi-Head Attention*: $MultiHead(Q, K, V) = Concat(Head_1, \ldots, Head_h)W^O$.
VI. Essa arquitetura permite que o modelo capture diferentes tipos de rela√ß√µes entre as palavras, pois cada head foca em um subespa√ßo de representa√ß√£o diferente. A concatena√ß√£o garante que o modelo considere todas essas rela√ß√µes.
VII. Portanto, o Multi-Head Attention aumenta a capacidade do modelo de capturar diferentes tipos de rela√ß√µes entre as palavras ao realizar a autoaten√ß√£o em m√∫ltiplos subespa√ßos de representa√ß√£o. $\blacksquare$

### Conclus√£o
Em resumo, o mecanismo de autoaten√ß√£o permite aos Transformers construir representa√ß√µes contextuais avan√ßadas atrav√©s da compara√ß√£o de cada palavra com todas as outras palavras no contexto, pesando a relev√¢ncia de cada palavra e combinando as suas informa√ß√µes. Essa estrutura baseada na autoaten√ß√£o √© o que permite que os Transformers processem grandes quantidades de texto de forma eficiente, tornando-os a base para os modelos de linguagem grandes. Atrav√©s da combina√ß√£o de camadas lineares, *feedforward* e mecanismos de autoaten√ß√£o, o Transformer pode construir representa√ß√µes complexas para cada palavra, permitindo a compreens√£o de rela√ß√µes intrincadas em texto e, consequentemente, desempenhar tarefas de processamento de linguagem natural [^2].

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^2]: 2 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^3]: 10.1.1 Transformers: the intuition
[^4]: 4 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^5]: 10.1.3 Self-attention more formally
[^6]: 6 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^7]: 10.1 ‚Ä¢ THE TRANSFORMER: A SELF-ATTENTION NETWORK 7
[^8]: 10.1.4 Parallelizing self-attention using a single matrix X
<!-- END -->
