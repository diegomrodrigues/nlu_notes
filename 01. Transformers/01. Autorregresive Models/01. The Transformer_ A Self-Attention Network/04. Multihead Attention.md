## 10.  Multi-Head Attention: Capturando Diversidade nas Rela√ß√µes entre Palavras
### Introdu√ß√£o
Este cap√≠tulo explora o mecanismo de **multi-head attention** (aten√ß√£o multi-cabe√ßa), uma extens√£o do mecanismo de **autoaten√ß√£o** que enriquece a capacidade do Transformer de capturar uma gama diversificada de rela√ß√µes entre palavras em uma sequ√™ncia de texto [^1]. Conforme abordado nos cap√≠tulos anteriores, o mecanismo de autoaten√ß√£o permite que o modelo pondere a import√¢ncia de diferentes palavras em rela√ß√£o √† palavra de foco, mas o **multi-head attention** expande essa capacidade, utilizando m√∫ltiplas camadas de aten√ß√£o em paralelo para capturar rela√ß√µes diversas e complexas, em vez de depender de uma √∫nica camada de autoaten√ß√£o [^9]. Vamos analisar a fundo como essa arquitetura possibilita que o modelo aprenda diferentes tipos de rela√ß√µes e como ela se integra no framework do Transformer.

### Conceitos Fundamentais
O mecanismo de **multi-head attention** foi desenvolvido para lidar com a complexidade das rela√ß√µes entre palavras em uma sequ√™ncia de texto [^9]. Uma √∫nica camada de autoaten√ß√£o pode ter dificuldade em capturar simultaneamente diferentes tipos de rela√ß√µes, como as sint√°ticas, sem√¢nticas e de discurso [^9]. A **multi-head attention** aborda essa limita√ß√£o utilizando v√°rias camadas de autoaten√ß√£o paralelas, chamadas "heads" (cabe√ßas), cada uma com seu pr√≥prio conjunto de par√¢metros [^9].

O processo de **multi-head attention** come√ßa projetando as *embeddings* de entrada em diferentes espa√ßos de representa√ß√£o usando matrizes de peso espec√≠ficas para cada head [^9]. Cada head $i$ possui suas pr√≥prias matrizes de peso para *query*, *key* e *value*, denotadas como $W_i^Q$, $W_i^K$ e $W_i^V$. Assim, cada head $i$ projeta a *embedding* de entrada $X$ para $Q_i$, $K_i$ e $V_i$:
$$Q_i = XW_i^Q; \quad K_i = XW_i^K; \quad V_i = XW_i^V$$
onde $W_i^Q$, $W_i^K$ e $W_i^V$ s√£o as matrizes de pesos exclusivas para o head $i$, e $X$ √© a matriz de entrada. A representa√ß√£o $Q_i$, $K_i$ e $V_i$ s√£o usadas para computar a autoaten√ß√£o de forma independente em cada head [^9].

O output de cada head $i$ √© calculado usando o mecanismo de *self-attention* padr√£o, conforme detalhado no cap√≠tulo anterior:
$$Head_i = SelfAttention(Q_i, K_i, V_i)$$
Note que cada head calcula uma autoaten√ß√£o com suas pr√≥prias proje√ß√µes $Q_i$, $K_i$ e $V_i$ .

> üí° **Exemplo Num√©rico: Multi-Head Attention com Dois Heads**
>
>  Vamos considerar um exemplo com dois heads (h=2) e entradas de dimens√£o 4, com pesos e vetores definidos aleatoriamente:
>
>   *Input Embeddings*:
>
>  $X = \begin{bmatrix} 1 & 0 & 2 & 1 \\ 0 & 2 & 1 & 1 \\ 1 & 1 & 0 & 2 \end{bmatrix}$
>
>   *Matrizes de proje√ß√£o para o Head 1*:
>
>  $W_1^Q = \begin{bmatrix} 0.1 & 0.2 & 0.1 & 0.3 \\ 0.2 & 0.1 & 0.1 & 0.2 \\ 0.1 & 0.3 & 0.2 & 0.1 \\ 0.3 & 0.2 & 0.1 & 0.2 \end{bmatrix}$,  $W_1^K = \begin{bmatrix} 0.2 & 0.1 & 0.3 & 0.1 \\ 0.1 & 0.2 & 0.2 & 0.3 \\ 0.3 & 0.1 & 0.1 & 0.1 \\ 0.1 & 0.3 & 0.2 & 0.2 \end{bmatrix}$, $W_1^V = \begin{bmatrix} 0.1 & 0.3 & 0.2 & 0.1 \\ 0.2 & 0.2 & 0.1 & 0.3 \\ 0.1 & 0.1 & 0.3 & 0.2 \\ 0.3 & 0.2 & 0.1 & 0.1 \end{bmatrix}$
>
>   *Matrizes de proje√ß√£o para o Head 2*:
>
>   $W_2^Q = \begin{bmatrix} 0.3 & 0.1 & 0.2 & 0.2 \\ 0.2 & 0.3 & 0.1 & 0.1 \\ 0.1 & 0.2 & 0.3 & 0.1 \\ 0.1 & 0.1 & 0.2 & 0.3 \end{bmatrix}$,  $W_2^K = \begin{bmatrix} 0.1 & 0.3 & 0.1 & 0.2 \\ 0.3 & 0.1 & 0.2 & 0.1 \\ 0.1 & 0.2 & 0.3 & 0.1 \\ 0.2 & 0.1 & 0.1 & 0.3 \end{bmatrix}$, $W_2^V = \begin{bmatrix} 0.2 & 0.1 & 0.3 & 0.1 \\ 0.1 & 0.2 & 0.1 & 0.3 \\ 0.3 & 0.1 & 0.2 & 0.2 \\ 0.1 & 0.3 & 0.1 & 0.2 \end{bmatrix}$
>
>   *C√°lculo dos Q, K e V para cada head*:
>
> Para o *Head 1*:
>
>    $Q_1 = XW_1^Q = \begin{bmatrix} 0.7 & 0.9 & 0.9 & 0.7 \\ 0.9 & 0.9 & 0.7 & 0.9 \\ 0.5 & 0.5 & 0.7 & 0.6 \end{bmatrix}$
>
>   $K_1 = XW_1^K =  \begin{bmatrix} 0.7 & 0.8 & 0.7 & 0.7 \\ 0.7 & 1.0 & 0.8 & 0.6 \\ 0.5 & 0.7 & 0.5 & 0.7  \end{bmatrix}$
>
>    $V_1 = XW_1^V =  \begin{bmatrix} 0.8 & 0.7 & 0.9 & 0.4 \\ 0.7 & 0.9 & 0.5 & 0.6 \\ 0.6 & 0.8 & 0.7 & 0.5 \end{bmatrix}$
>
> Para o *Head 2*:
>
>  $Q_2 = XW_2^Q = \begin{bmatrix} 0.6 & 0.6 & 0.8 & 0.8 \\ 0.9 & 0.7 & 0.7 & 0.6 \\ 0.4 & 0.6 & 0.6 & 0.8 \end{bmatrix}$
>
>   $K_2 = XW_2^K =  \begin{bmatrix} 0.8 & 0.7 & 0.7 & 0.7 \\ 0.8 & 0.9 & 0.6 & 0.7 \\ 0.6 & 0.6 & 0.7 & 0.8  \end{bmatrix}$
>
>    $V_2 = XW_2^V =  \begin{bmatrix} 0.8 & 0.6 & 0.7 & 0.4 \\ 0.5 & 0.8 & 0.7 & 0.7 \\ 0.6 & 0.6 & 0.8 & 0.5 \end{bmatrix}$
>
> Note que cada head possui seus pr√≥prios Q, K e V, obtidos a partir da multiplica√ß√£o da mesma matriz de entrada $X$ com diferentes proje√ß√µes.
>
>  *C√°lculo da Autoaten√ß√£o para cada head*:
>
> Em seguida, para cada head, aplicamos o c√°lculo de autoaten√ß√£o. Para simplificar o exemplo, n√£o vamos detalhar essa parte e assumir que temos como resultado:
>
>  $Head_1 = \begin{bmatrix} 0.7 & 0.8 & 0.7 & 0.6 \\ 0.8 & 0.7 & 0.6 & 0.7 \\ 0.4 & 0.7 & 0.6 & 0.7 \end{bmatrix}$
>
>  $Head_2 = \begin{bmatrix} 0.6 & 0.7 & 0.8 & 0.5 \\ 0.7 & 0.7 & 0.6 & 0.6 \\ 0.5 & 0.7 & 0.7 & 0.6 \end{bmatrix}$
>
>  Note que $Head_1$ e $Head_2$ possuem as mesmas dimens√µes, mas seus valores s√£o diferentes, uma vez que os c√°lculos foram feitos usando proje√ß√µes distintas.
>
> O pr√≥ximo passo √© combinar a sa√≠da dos diferentes heads.

Ap√≥s computar a sa√≠da de cada head, as sa√≠das s√£o concatenadas em uma √∫nica matriz:
$$ConcatHeads = Concat(Head_1, Head_2, \ldots, Head_h)$$
Essa matriz concatenada tem uma dimens√£o maior e, para projet√°-la de volta ao espa√ßo de representa√ß√£o original, ela √© multiplicada por uma matriz de peso $W^O$:
$$MultiHeadAttention(X) = ConcatHeads W^O$$
onde $W^O$ √© uma matriz de peso que √© aprendida durante o treinamento. A fun√ß√£o $MultiHeadAttention(X)$ retorna a sa√≠da do layer, que √© usada no layer seguinte do Transformer.

> üí° **Exemplo Num√©rico (Continua√ß√£o): Combina√ß√£o das sa√≠das e Proje√ß√£o Final**
>
>  Continuando o exemplo anterior, e com as sa√≠das de cada head definidas, podemos combin√°-las concatenando-as:
>
>    $ConcatHeads = Concat(Head_1, Head_2) = \begin{bmatrix} 0.7 & 0.8 & 0.7 & 0.6 & 0.6 & 0.7 & 0.8 & 0.5 \\ 0.8 & 0.7 & 0.6 & 0.7 & 0.7 & 0.7 & 0.6 & 0.6 \\ 0.4 & 0.7 & 0.6 & 0.7 & 0.5 & 0.7 & 0.7 & 0.6 \end{bmatrix}$
>
> A matriz $ConcatHeads$ tem dimens√£o $3 x 8$. Para projetar essa matriz de volta para um espa√ßo de dimens√£o 4, podemos utilizar a seguinte matriz $W^O$ de dimens√£o $8x4$:
>
>   $W^O = \begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\ 0.4 & 0.3 & 0.2 & 0.1 \\ 0.2 & 0.1 & 0.4 & 0.3 \\ 0.3 & 0.4 & 0.1 & 0.2 \\ 0.4 & 0.1 & 0.2 & 0.3 \\ 0.3 & 0.2 & 0.1 & 0.4 \\ 0.1 & 0.4 & 0.3 & 0.2 \\ 0.2 & 0.3 & 0.4 & 0.1 \end{bmatrix}$
>
> Aplicando a proje√ß√£o final:
>
>    $MultiHeadAttention(X) = ConcatHeads W^O = \begin{bmatrix} 1.55 & 1.56 & 1.57 & 1.58 \\ 1.58 & 1.59 & 1.60 & 1.59 \\ 1.23 & 1.24 & 1.25 & 1.25 \end{bmatrix}$
>
> A matriz resultante $MultiHeadAttention(X)$ possui a mesma dimens√£o das entradas $X$ (3x4), mas agora cont√©m a combina√ß√£o de informa√ß√µes capturadas por cada head.

O uso de m√∫ltiplos heads permite que o modelo aprenda diferentes rela√ß√µes entre as palavras em paralelo [^9]. Cada head se concentra em diferentes aspectos das rela√ß√µes contextuais, e a combina√ß√£o dessas informa√ß√µes atrav√©s da concatena√ß√£o e proje√ß√£o final resulta em uma representa√ß√£o mais rica e abrangente da sequ√™ncia de entrada [^9]. Al√©m disso, cada head possui suas pr√≥prias matrizes de proje√ß√£o e, portanto, um espa√ßo de representa√ß√£o espec√≠fico, o que permite que o modelo capture diferentes padr√µes em paralelo.

**Lema 1.1** (Independ√™ncia das Camadas de Autoaten√ß√£o): As camadas de autoaten√ß√£o em um multi-head attention podem ser processadas em paralelo, pois cada uma opera sobre suas pr√≥prias proje√ß√µes de *query*, *key* e *value*, sem depender das sa√≠das de outros heads.

*Prova:*
I. Cada head $i$ recebe como entrada as proje√ß√µes $Q_i$, $K_i$, e $V_i$, que s√£o calculadas independentemente das proje√ß√µes de outros heads.
II. A autoaten√ß√£o $Head_i = SelfAttention(Q_i, K_i, V_i)$ √© computada dentro de cada head, sem qualquer depend√™ncia das sa√≠das de outros heads.
III. A concatena√ß√£o $ConcatHeads = Concat(Head_1, Head_2, \ldots, Head_h)$ ocorre somente ap√≥s a computa√ß√£o de todos os heads.
IV. Portanto, a computa√ß√£o de cada head √© independente, o que permite o processamento paralelo, aproveitando a capacidade de paraleliza√ß√£o das GPUs e melhorando a efici√™ncia do treinamento e infer√™ncia. ‚ñ†

**Teorema 1.1** (Aumento da Capacidade de Representa√ß√£o): O mecanismo de *multi-head attention* aumenta a capacidade do modelo de capturar diferentes rela√ß√µes entre as palavras porque cada head pode focar em diferentes aspectos das informa√ß√µes contextuais. Cada head aprende proje√ß√µes espec√≠ficas que direcionam o modelo para identificar tipos distintos de rela√ß√µes entre os *tokens* na sequ√™ncia de entrada. A combina√ß√£o das sa√≠das de todos os heads resulta em uma representa√ß√£o contextual mais rica e completa.

*Prova:*
I.  Cada head $i$ utiliza matrizes de proje√ß√£o distintas ($W_i^Q$, $W_i^K$, $W_i^V$) para gerar suas pr√≥prias representa√ß√µes de *query*, *key* e *value*.
II.  Essas proje√ß√µes distintas permitem que cada head aprenda a identificar padr√µes e rela√ß√µes diferentes nos dados de entrada.
III.  A fun√ß√£o de autoaten√ß√£o dentro de cada head √© aplicada separadamente, permitindo que cada head capture informa√ß√µes espec√≠ficas.
IV.  A concatena√ß√£o $ConcatHeads = Concat(Head_1, Head_2, \ldots, Head_h)$ combina as representa√ß√µes de todos os heads, garantindo que o modelo tenha acesso a todos os tipos de rela√ß√µes aprendidas.
V.  A matriz de proje√ß√£o $W^O$ ajusta a representa√ß√£o concatenada ao espa√ßo de representa√ß√£o desejado, combinando os padr√µes identificados pelos diferentes heads.
VI.  Portanto, o multi-head attention aumenta a capacidade de representa√ß√£o do modelo, permitindo que ele capture uma gama mais ampla de rela√ß√µes entre as palavras, e resultando em uma representa√ß√£o contextual mais rica e completa. ‚ñ†

**Proposi√ß√£o 1.1** (Rela√ß√£o com Autoaten√ß√£o Simples): A autoaten√ß√£o simples √© um caso especial de multi-head attention onde existe apenas uma cabe√ßa (h=1), demonstrando que multi-head attention √© uma generaliza√ß√£o do mecanismo de autoaten√ß√£o.

*Prova:*
I.  Em multi-head attention com h=1, existe apenas um head e portanto apenas uma matriz $W^Q$, $W^K$ e $W^V$.
II.  A matriz $ConcatHeads$ ser√°, nesse caso, igual a sa√≠da da √∫nica cabe√ßa, ou seja, $Head_1$.
III.  A opera√ß√£o $MultiHeadAttention(X) = ConcatHeads W^O$ corresponde √† multiplica√ß√£o da matriz de proje√ß√£o $W^O$ na matriz $Head_1$. Como tanto $Head_1$ como $W^O$ s√£o projetados para a mesma dimens√£o que a entrada, o multi-head attention com h=1 √© equivalente √† opera√ß√£o de autoaten√ß√£o simples, provando que multi-head attention √© uma generaliza√ß√£o do mecanismo de autoaten√ß√£o.‚ñ†

**Lema 1.2** (Dimensionalidade das Proje√ß√µes): As dimens√µes das proje√ß√µes $Q_i$, $K_i$, e $V_i$ em cada head s√£o tipicamente menores que a dimens√£o da entrada $X$. Isso reduz a complexidade computacional e permite que cada head capture informa√ß√µes mais espec√≠ficas. Se a dimens√£o da entrada $X$ √© $d_{model}$ e o n√∫mero de heads √© $h$, as proje√ß√µes $Q_i$, $K_i$, e $V_i$ ter√£o dimens√£o $d_{model}/h$, ou $d_k$, onde $d_k$ √© a dimens√£o de cada head.

*Prova:*
I. A matriz de proje√ß√£o $W_i^Q$ tem dimens√µes $d_{model} \times d_k$, onde $d_k = d_{model}/h$.
II. Ao multiplicar $X$ por $W_i^Q$, obtemos $Q_i$ com dimens√µes $n \times d_k$, onde $n$ √© o tamanho da sequ√™ncia de entrada e $d_k$ √© a dimens√£o das proje√ß√µes.
III. Da mesma forma, $K_i$ e $V_i$ tamb√©m ter√£o dimens√µes $n \times d_k$.
IV. Portanto, ao reduzir a dimensionalidade das proje√ß√µes para $d_k = d_{model}/h$, a complexidade computacional de cada head √© reduzida, permitindo um processamento mais eficiente e a especializa√ß√£o em diferentes aspectos das rela√ß√µes contextuais.‚ñ†

**Teorema 1.2** (Especializa√ß√£o dos Heads): Cada head no multi-head attention pode se especializar em diferentes tipos de rela√ß√µes contextuais, o que permite que o modelo aprenda representa√ß√µes mais ricas e diversas das palavras. Isso ocorre devido √† inicializa√ß√£o aleat√≥ria e aprendizado independente das matrizes de proje√ß√£o ($W_i^Q$, $W_i^K$, $W_i^V$) para cada head.

*Prova:*
I. As matrizes de proje√ß√£o $W_i^Q$, $W_i^K$ e $W_i^V$ s√£o inicializadas aleatoriamente para cada head $i$.
II. Durante o treinamento, cada head ajusta suas matrizes de proje√ß√£o para otimizar a captura de diferentes rela√ß√µes contextuais, resultando em especializa√ß√£o.
III.  A fun√ß√£o de autoaten√ß√£o dentro de cada head √© aplicada separadamente, permitindo que cada head capture informa√ß√µes espec√≠ficas sobre um aspecto particular das rela√ß√µes entre os tokens.
IV.  A concatena√ß√£o das sa√≠das de todos os heads permite que o modelo combine diversas rela√ß√µes aprendidas, resultando em uma representa√ß√£o contextual mais rica.
V.  Portanto, a inicializa√ß√£o aleat√≥ria e o aprendizado independente das matrizes de proje√ß√£o permitem que cada head se especialize, contribuindo para a capacidade do multi-head attention de capturar uma variedade de rela√ß√µes entre palavras.‚ñ†

### Conclus√£o
Este cap√≠tulo explorou o mecanismo de **multi-head attention**, demonstrando como ele estende o mecanismo de autoaten√ß√£o para capturar uma gama diversificada de rela√ß√µes entre as palavras em uma sequ√™ncia de texto. Ao utilizar m√∫ltiplas camadas de autoaten√ß√£o em paralelo, cada uma com suas pr√≥prias matrizes de proje√ß√£o, o modelo pode aprender diferentes tipos de rela√ß√µes (por exemplo, sint√°ticas, sem√¢nticas, co-refer√™ncias) e combin√°-las para gerar uma representa√ß√£o contextual mais rica e completa. A capacidade de processar as camadas de autoaten√ß√£o em paralelo, combinada com a riqueza da representa√ß√£o resultante, torna o **multi-head attention** um componente crucial da arquitetura dos Transformers. Essa capacidade do modelo de capturar diversas rela√ß√µes √© uma das raz√µes para o sucesso dos Transformers em tarefas de processamento de linguagem natural [^9].

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^9]: 10.2 Multihead Attention
## 10.2 Multihead Attention: Um Olhar Mais Detalhado

### Introdu√ß√£o
Em continuidade ao conceito de **self-attention** introduzido anteriormente, esta se√ß√£o aprofunda a ideia de **multihead attention**, um mecanismo fundamental para a arquitetura dos transformers [^1]. Vimos que o self-attention permite que um modelo integre informa√ß√µes de diferentes partes de uma sequ√™ncia para gerar representa√ß√µes contextuais das palavras. No entanto, as rela√ß√µes entre as palavras em uma frase s√£o complexas e multifacetadas, envolvendo aspectos sint√°ticos, sem√¢nticos e discursivos. O multihead attention surge como uma solu√ß√£o para capturar essa complexidade, permitindo que o modelo aprenda diferentes tipos de rela√ß√µes simultaneamente [^9].

### Conceitos Fundamentais

#### Multihead Self-Attention
O **multihead self-attention** √© uma extens√£o do self-attention que consiste em m√∫ltiplos *heads* de aten√ß√£o, cada um com seus pr√≥prios conjuntos de par√¢metros. Esses *heads* operam em paralelo e s√£o projetados para aprender diferentes aspectos das rela√ß√µes entre os inputs. Cada *head* possui suas pr√≥prias matrizes de pesos para projetar os inputs em *queries*, *keys* e *values*. A ideia √© que cada *head* se especializar√° em capturar diferentes tipos de depend√™ncias entre as palavras, permitindo ao modelo aprender representa√ß√µes mais ricas e completas [^1].

##### Implementa√ß√£o do Multihead Attention
Para implementar o multihead attention, seguimos os seguintes passos [^1]:
1. **Proje√ß√£o Linear:** Para cada *head* i, projetamos o input X em *queries* (Qi), *keys* (Ki) e *values* (Vi) usando matrizes de peso distintas: $$Q_i = XW_i^Q$$, $$K_i = XW_i^K$$, $$V_i = XW_i^V$$. Essas proje√ß√µes t√™m dimens√µes menores do que a dimens√£o original do input, o que permite que o modelo capture m√∫ltiplas rela√ß√µes sem aumentar significativamente a complexidade computacional.
2. **Self-Attention:** Aplicamos o self-attention a cada *head* usando as proje√ß√µes Qi, Ki, e Vi, gerando um output individual para cada *head* (head_i).
3. **Concatena√ß√£o:** Concatenamos os outputs de todos os *heads* para formar uma √∫nica matriz com dimens√£o aumentada.
4. **Proje√ß√£o Linear Final:** Aplicamos uma proje√ß√£o linear adicional para mapear a matriz concatenada de volta para a dimens√£o original do input.

Essa estrutura permite ao modelo aprender diferentes tipos de rela√ß√µes e combin√°-las para formar uma representa√ß√£o rica do texto. As equa√ß√µes que descrevem o processo s√£o:

$$ head_i = SelfAttention(Q_i, K_i, V_i) $$
$$ MultiHeadAttention(X) = Concat(head_1, head_2, \ldots, head_h) W^O $$

Onde $W^O$ √© a matriz de proje√ß√£o final [^1].

> üí° **Exemplo Num√©rico: Dimens√£o das Proje√ß√µes e Complexidade Computacional**
>
> Vamos assumir que temos uma sequ√™ncia de entrada com 10 palavras ($n=10$) e que a dimens√£o do embedding √© 512 ($d_{model}=512$).
>
>  Se usarmos um multi-head attention com 8 heads ($h=8$), ent√£o a dimens√£o das proje√ß√µes $Q_i$, $K_i$, e $V_i$ para cada head ser√° $d_k = d_{model}/h = 512/8 = 64$.
>
> A complexidade computacional para o multi-head attention ser√° de $O(n^2d + nd^2) = O(10^2 * 512 + 10 * 512^2) = O(51200 + 2621440) = O(2672640)$.
>
>  Se utiliz√°ssemos um mecanismo de autoaten√ß√£o simples, a complexidade seria de $O(n^2d) = O(10^2 * 512) = O(51200)$.
>
>  Embora o multi-head attention tenha um custo maior devido √†s proje√ß√µes e √† concatena√ß√£o, a complexidade assint√≥tica √© semelhante a da autoaten√ß√£o simples em rela√ß√£o ao tamanho da sequ√™ncia e dimens√£o do embedding.
>
> A redu√ß√£o da dimensionalidade das proje√ß√µes, de 512 para 64, ajuda a reduzir a complexidade computacional de cada head, permitindo o processamento paralelo e a captura de diferentes rela√ß√µes.

**Observa√ß√£o 2.1** (Normaliza√ß√£o das Proje√ß√µes): Em algumas implementa√ß√µes do multihead attention, as proje√ß√µes $Q_i$, $K_i$, e $V_i$ podem ser normalizadas para melhorar a estabilidade do treinamento e evitar que valores muito grandes influenciem o c√°lculo da autoaten√ß√£o. Essa normaliza√ß√£o pode ser feita atrav√©s da divis√£o pela raiz quadrada da dimens√£o das chaves $d_k$, ou atrav√©s de outras t√©cnicas de normaliza√ß√£o.

*Coment√°rio*: A normaliza√ß√£o adiciona uma etapa extra ao c√°lculo do multi-head attention, garantindo que nenhuma proje√ß√£o individual domine o processo. Isso pode levar a um treinamento mais est√°vel e uma melhor performance do modelo.

**Proposi√ß√£o 2.1** (Complexidade Computacional): A complexidade computacional do multi-head attention √© $O(n^2d + nd^2)$ onde $n$ √© o tamanho da sequ√™ncia e $d$ √© a dimens√£o do embedding, assumindo que o n√∫mero de heads √© constante em rela√ß√£o a $n$ e $d$. A complexidade de autoaten√ß√£o simples √© $O(n^2d)$. Embora o multi-head attention introduza mais opera√ß√µes em cada head, o custo n√£o aumenta em rela√ß√£o a $n$ e $d$.

*Prova:*
I. A proje√ß√£o linear de $X$ para $Q_i$, $K_i$, e $V_i$ tem complexidade $O(nd^2)$. Como existem $h$ heads, a complexidade total dessas proje√ß√µes √© $O(hnd^2)$.
II. A autoaten√ß√£o em cada head tem complexidade $O(n^2d/h)$, onde $d/h$ √© a dimens√£o das proje√ß√µes. Como existem $h$ heads, a complexidade total das autoaten√ß√µes √© $O(n^2d)$.
III. A concatena√ß√£o dos heads tem complexidade $O(nhd/h) = O(nd)$, que √© uma complexidade linear.
IV. A proje√ß√£o final para a dimens√£o original tem complexidade $O(nd^2)$.
V. Combinando todas as etapas, a complexidade computacional total do multihead attention √© $O(hnd^2 + n^2d + nd^2)$. Assumindo que o n√∫mero de heads $h$ √© uma constante, a complexidade √© simplificada para $O(n^2d + nd^2)$. Portanto, a complexidade do multihead attention, em rela√ß√£o a $n$ e $d$, n√£o cresce assintoticamente em rela√ß√£o a complexidade de autoaten√ß√£o simples, sendo ambas da ordem de $O(n^2d)$. ‚ñ†

### Conclus√£o
O multihead attention √© um componente crucial dos transformers, permitindo que o modelo aprenda as complexas rela√ß√µes entre as palavras de um texto. Ao permitir que o modelo atenda a diferentes aspectos das rela√ß√µes entre as palavras, o multihead attention possibilita a constru√ß√£o de representa√ß√µes mais completas e contextuais. Essa capacidade √© fundamental para o desempenho dos transformers em diversas tarefas de NLP [^1].

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^9]: 10.2 Multihead Attention
<!-- END -->
