## 10.  A Sa√≠da da Autoaten√ß√£o: Pondera√ß√£o de Valores e Modelagem Causal

### Introdu√ß√£o
Em continuidade √† nossa explora√ß√£o do mecanismo de **autoaten√ß√£o**, este cap√≠tulo se aprofunda em como a sa√≠da desse mecanismo √© constru√≠da, com foco na pondera√ß√£o dos vetores de *value* e na import√¢ncia do mascaramento para modelagem causal de linguagem [^1]. Examinaremos como a sa√≠da da autoaten√ß√£o √© uma soma ponderada dos vetores de *value*, capturando informa√ß√µes de diferentes palavras, e como o mascaramento impede o acesso a palavras futuras em modelos de linguagem causais, que s√£o essenciais para a modelagem de linguagem auto-regressiva [^4]. Exploraremos o conceito de "aten√ß√£o causal" (*causal attention* ou *backward-looking*), que considera apenas as palavras anteriores para prever a pr√≥xima palavra, e como o mecanismo de autoaten√ß√£o implementa esse conceito atrav√©s do mascaramento [^2].

### Conceitos Fundamentais

#### A Sa√≠da da Autoaten√ß√£o: Uma Soma Ponderada de *Values*
Como vimos nos cap√≠tulos anteriores, o mecanismo de autoaten√ß√£o calcula *scores* de relev√¢ncia entre as palavras em uma sequ√™ncia, utilizando produtos escalares entre as representa√ß√µes de *query* e *key* [^5]. Esses *scores* s√£o ent√£o normalizados atrav√©s de uma fun√ß√£o *softmax* para gerar uma distribui√ß√£o de probabilidade, que indica o quanto cada palavra vizinha √© relevante para a palavra de foco [^5]. A sa√≠da do mecanismo de autoaten√ß√£o, representada por $a_i$ para a palavra $x_i$, √© ent√£o obtida atrav√©s de uma soma ponderada dos vetores de *value* ($v_j$) de todas as palavras precedentes, utilizando os pesos derivados da fun√ß√£o *softmax*:
$$a_i = \sum_{j=1}^i \alpha_{ij} v_j$$
onde $\alpha_{ij}$ representa a relev√¢ncia da palavra $x_j$ para a palavra $x_i$, e $v_j$ √© o vetor de *value* da palavra $x_j$ [^6].

Essa soma ponderada permite que a representa√ß√£o contextualizada $a_i$ capture informa√ß√µes de diferentes palavras na sequ√™ncia, dando mais peso √†quelas que s√£o consideradas mais relevantes pelo modelo, ou seja, aquelas com maior valor de $\alpha_{ij}$ [^5]. O vetor de *value* ($v_j$) cont√©m as informa√ß√µes que ser√£o ponderadas para gerar o output, e o mecanismo de autoaten√ß√£o determina como estas informa√ß√µes devem ser combinadas para gerar representa√ß√µes contextuais de alta qualidade [^6].

> üí° **Exemplo Num√©rico: Pondera√ß√£o de Valores**
>
> Para ilustrar como a sa√≠da do self-attention √© uma soma ponderada dos vetores *value*, vamos continuar com o mesmo exemplo num√©rico da se√ß√£o anterior, e assumir que j√° temos os valores dos pesos $\alpha_{ij}$:
>
> Vamos supor que temos uma sequ√™ncia de entrada de tr√™s tokens, e os pesos calculados para o token 2 sejam:
>
> $\alpha_{21} = 0.3$
>
> $\alpha_{22} = 0.7$
>
> E os seguintes vetores de *value*:
>
> $v_1 = \begin{bmatrix} 0.2 \\ 0.4 \\ 0.6 \end{bmatrix}$
>
> $v_2 = \begin{bmatrix} 0.1 \\ 0.3 \\ 0.5 \end{bmatrix}$
>
> A sa√≠da ponderada $a_2$ ser√° calculada como:
>
> $a_2 = \alpha_{21} v_1 + \alpha_{22} v_2 = 0.3 * \begin{bmatrix} 0.2 \\ 0.4 \\ 0.6 \end{bmatrix} + 0.7 * \begin{bmatrix} 0.1 \\ 0.3 \\ 0.5 \end{bmatrix} = \begin{bmatrix} 0.06 \\ 0.12 \\ 0.18 \end{bmatrix} + \begin{bmatrix} 0.07 \\ 0.21 \\ 0.35 \end{bmatrix} = \begin{bmatrix} 0.13 \\ 0.33 \\ 0.53 \end{bmatrix}$
>
> Observe que $a_2$ √© uma combina√ß√£o ponderada de $v_1$ e $v_2$, refletindo a relev√¢ncia de cada palavra vizinha no contexto da palavra de foco. O primeiro elemento de $a_2$ (0.13) √© composto por 30% do primeiro elemento de $v_1$ e 70% do primeiro elemento de $v_2$. Similarmente para os outros elementos. Isso mostra que a sa√≠da $a_i$ √© uma representa√ß√£o contextualizada, combinando as informa√ß√µes dos valores de forma ponderada pelos seus respectivos pesos de aten√ß√£o.

**Observa√ß√£o 3.1** (Diversidade na Pondera√ß√£o): √â crucial notar que a pondera√ß√£o dos vetores de *value* n√£o √© um simples c√°lculo de m√©dia. O mecanismo de autoaten√ß√£o, atrav√©s das proje√ß√µes para gerar os vetores *query* e *key*, e atrav√©s da normaliza√ß√£o via *softmax*, determina de forma din√¢mica quais vetores de *value* s√£o mais relevantes para o contexto atual [^6].

**Observa√ß√£o 3.2** (Interpreta√ß√£o da Sa√≠da): A sa√≠da $a_i$ n√£o √© apenas uma representa√ß√£o contextualizada, mas tamb√©m uma forma de agrega√ß√£o de informa√ß√£o. Cada elemento do vetor $a_i$ representa uma caracter√≠stica agregada das palavras anteriores, ponderada pela relev√¢ncia dessas palavras para a palavra $x_i$.

#### Modelagem Causal e o Mascaramento
Em modelos de linguagem causais ou auto-regressivos, a predi√ß√£o da pr√≥xima palavra √© feita com base nas palavras anteriores [^2]. Isso significa que, durante o treinamento, o modelo n√£o pode ter acesso a informa√ß√µes sobre as palavras que ainda n√£o foram geradas, ou seja, o "futuro" [^8]. Para implementar essa causalidade, √© necess√°rio um mecanismo que impe√ßa o modelo de "ver" as palavras seguintes durante o c√°lculo da autoaten√ß√£o. Esse mecanismo √© o mascaramento (*masking*) [^4].

O mascaramento consiste em zerar os scores correspondentes √†s palavras futuras na matriz de *scores* antes da aplica√ß√£o da fun√ß√£o *softmax*. Ao zerar esses *scores*, a fun√ß√£o *softmax* ir√° gerar pesos de aten√ß√£o $\alpha_{ij}$ iguais a zero para todas as palavras futuras, garantindo que a sa√≠da $a_i$ dependa apenas das palavras precedentes [^8]. Dessa forma, a autoaten√ß√£o se torna *causal* ou *backward-looking*, pois a representa√ß√£o de uma palavra s√≥ depende das informa√ß√µes das palavras anteriores a ela na sequ√™ncia [^4].

> üí° **Exemplo Num√©rico: Mascaramento em Autoaten√ß√£o Causal**
>
> Para ilustrar, vamos considerar a seguinte matriz de scores S, com 3 tokens, sem mascaramento:
>
> $S = \begin{bmatrix}
>   1.0 & 0.5 & 0.2 \\
>   0.8 & 1.2 & 0.6 \\
>   0.4 & 0.7 & 1.1
>  \end{bmatrix}$
>
> Para autoaten√ß√£o causal, devemos aplicar uma m√°scara triangular inferior, que nesse caso √© dada por:
>
> $M = \begin{bmatrix}
>   0 & -\infty & -\infty \\
>   0 & 0 & -\infty \\
>   0 & 0 & 0
>  \end{bmatrix}$
>
> A matriz de scores mascarada $S'$ √© obtida como a soma das duas matrizes $S$ e $M$:
>
>  $S' = S + M = \begin{bmatrix}
>   1.0 & -\infty & -\infty \\
>   0.8 & 1.2 & -\infty \\
>   0.4 & 0.7 & 1.1
>  \end{bmatrix}$
>
> Aplicando a fun√ß√£o *softmax* em cada linha da matriz mascarada $S'$, obtemos a matriz de pesos $\alpha$:
>
> $\alpha =  \begin{bmatrix}
>   1.0 & 0 & 0 \\
>   0.35 & 0.65 & 0 \\
>   0.15 & 0.27 & 0.58
>  \end{bmatrix}$
>
> Observe que a matriz de pesos mascarada tem 0 para as posi√ß√µes correspondentes a palavras futuras, garantindo o comportamento causal da autoaten√ß√£o. A primeira palavra ($x_1$) s√≥ tem aten√ß√£o para si mesma (peso 1.0), a segunda palavra ($x_2$) tem aten√ß√£o para a primeira (peso 0.35) e para si mesma (peso 0.65), e a terceira palavra ($x_3$) tem aten√ß√£o para a primeira (peso 0.15), segunda (peso 0.27) e para si mesma (peso 0.58). Isso impede que o modelo olhe para o futuro ao calcular as representa√ß√µes contextuais.

**Lema 3.2** (Efeito da M√°scara): A m√°scara $M$ aplicada sobre a matriz de scores $S$ garante que a matriz resultante $S'$ tenha valores $-\infty$ (ou um valor muito pequeno) nas posi√ß√µes correspondentes a palavras futuras. Isso impede que essas palavras tenham influ√™ncia na sa√≠da ap√≥s a aplica√ß√£o do *softmax*.

*Prova:*
I. A m√°scara $M$ √© uma matriz triangular inferior com zeros na diagonal principal e abaixo dela, e $-\infty$ nas posi√ß√µes acima da diagonal principal.
II. Ao somar $S$ e $M$, os elementos de $S$ correspondentes a palavras anteriores (e a pr√≥pria palavra) permanecem inalterados, pois s√£o somados a 0.
III. Os elementos de $S$ correspondentes a palavras futuras (acima da diagonal principal) s√£o somados a $-\infty$, resultando em $-\infty$ na matriz $S'$.
IV. O *softmax* mapeia valores muito pequenos (pr√≥ximos de $-\infty$) para valores pr√≥ximos de 0.
V. Portanto, a m√°scara $M$ garante que os pesos de aten√ß√£o para as palavras futuras sejam aproximadamente zero, implementando a causalidade. $\blacksquare$

#### Aten√ß√£o Causal
O conceito de "aten√ß√£o causal" (tamb√©m conhecida como *backward-looking attention*) √© essencial para modelos de linguagem auto-regressivos [^4]. Em modelos de linguagem, √© fundamental que a predi√ß√£o de uma palavra em particular dependa exclusivamente das palavras que a precedem na sequ√™ncia. Essa propriedade √© o que permite que o modelo gere texto sequencialmente e de maneira coerente.

A aten√ß√£o causal √© implementada atrav√©s do mascaramento, o que garante que a matriz de pesos resultante do *softmax* tenha 0 para a contribui√ß√£o das palavras futuras. √â essa restri√ß√£o de causalidade que torna o *Transformer* adequado para a tarefa de modelagem de linguagem auto-regressiva.

> üí° **Diferen√ßa entre Aten√ß√£o Causal e Bidirecional**
>
> Enquanto a aten√ß√£o causal (backward-looking) s√≥ considera as palavras precedentes para calcular as representa√ß√µes contextuais, a aten√ß√£o bidirecional (que ser√° explorada em cap√≠tulos posteriores) considera tanto as palavras precedentes quanto as palavras subsequentes. A aten√ß√£o bidirecional √© √∫til para tarefas onde o contexto completo √© relevante para a compreens√£o, como em tarefas de classifica√ß√£o de textos, enquanto a aten√ß√£o causal √© fundamental para tarefas de gera√ß√£o de texto, onde a predi√ß√£o de cada palavra deve depender das palavras anteriores, em uma sequ√™ncia da esquerda para a direita. Por exemplo, em uma tarefa de tradu√ß√£o, um modelo com aten√ß√£o bidirecional pode olhar para a senten√ßa completa para gerar a tradu√ß√£o de cada palavra, enquanto em um modelo de gera√ß√£o de texto, o modelo precisa gerar as pr√≥ximas palavras com base apenas no texto j√° gerado, de forma causal.

**Lema 3.1** (Causalidade e Mascaramento): O mascaramento da matriz de *scores* no mecanismo de autoaten√ß√£o √© uma condi√ß√£o necess√°ria e suficiente para que o mecanismo de autoaten√ß√£o se comporte de forma causal (ou *backward-looking*), ou seja, que o output de cada palavra dependa apenas das palavras precedentes e da pr√≥pria palavra.

*Prova:*
I.  Se a matriz de scores n√£o fosse mascarada, o *softmax* seria aplicado a todos os scores, e o output para uma dada palavra $x_i$ dependeria das palavras posteriores $x_j$ com $j > i$, invalidando a causalidade.
II.  Ao mascarar a matriz de scores, os scores correspondentes √†s palavras futuras s√£o zerados antes de entrar no *softmax*, o que implica que os pesos de aten√ß√£o para essas palavras ser√£o iguais a zero.
III.  Como a sa√≠da do mecanismo de autoaten√ß√£o √© uma combina√ß√£o linear dos vetores *value* ponderados pelos pesos de aten√ß√£o, e os pesos para as palavras futuras s√£o zero, o output depender√° exclusivamente das palavras precedentes. Portanto, o mascaramento √© uma condi√ß√£o suficiente para causalidade.
IV.  Se o modelo for causal, isso significa que a matriz de sa√≠da s√≥ pode depender das entradas precedentes. Isso imp√µe restri√ß√µes na aten√ß√£o que podem ser expressas por uma matriz de m√°scara. Portanto, o mascaramento √© uma condi√ß√£o necess√°ria para causalidade.
V. Portanto, a opera√ß√£o de mascaramento √© tanto necess√°ria quanto suficiente para garantir que a autoaten√ß√£o se comporte de maneira causal, em que o output da palavra $x_i$ s√≥ depende de $x_j$, onde $j \le i$. $\blacksquare$

**Teorema 3.1** (Aten√ß√£o Causal e Modelagem Auto-Regressiva): A combina√ß√£o da autoaten√ß√£o com o mascaramento permite criar modelos de linguagem auto-regressivos que geram texto palavra por palavra, onde a predi√ß√£o de cada palavra depende apenas das palavras geradas anteriormente.

*Prova:*
I.   O modelo de linguagem auto-regressivo gera texto palavra por palavra, onde a probabilidade da pr√≥xima palavra √© condicionada a todas as palavras anteriores.
II.  O mecanismo de autoaten√ß√£o com mascaramento garante que o output para cada palavra s√≥ dependa das informa√ß√µes das palavras precedentes.
III. O output do multihead attention, que √© uma combina√ß√£o ponderada dos vetores *value* de todas as palavras precedentes, √© o input para a pr√≥xima camada do Transformer.
IV. O modelo √© treinado atrav√©s da maximiza√ß√£o da probabilidade das palavras reais dadas as palavras precedentes. A maximiza√ß√£o dessa probabilidade resulta em vetores de *query*, *key*, *value* e matrizes de proje√ß√£o que permitem modelar rela√ß√µes complexas entre as palavras.
V.  Essa combina√ß√£o entre autoaten√ß√£o, mascaramento e treinamento resulta em um modelo auto-regressivo que gera texto sequencialmente, onde cada palavra depende apenas das palavras anteriores, demonstrando que a aten√ß√£o causal (e o seu mascaramento) √© fundamental para criar modelos auto-regressivos.$\blacksquare$

**Proposi√ß√£o 3.1** (Generaliza√ß√£o para M√∫ltiplas Camadas): O mecanismo de autoaten√ß√£o causal, com mascaramento, pode ser aplicado em m√∫ltiplas camadas, mantendo a propriedade de auto-regressividade do modelo.
*Prova:*
I.  Em cada camada de autoaten√ß√£o, o mascaramento garante que a sa√≠da de cada palavra dependa apenas das representa√ß√µes das palavras anteriores na camada anterior.
II.  Como a sa√≠da de cada camada √© alimentada na camada seguinte, e a sa√≠da da primeira camada depende apenas da entrada original (palavras precedentes), a causalidade √© mantida ao longo das camadas.
III. Portanto, empilhar camadas de autoaten√ß√£o causal n√£o compromete a propriedade auto-regressiva do modelo, permitindo a modelagem de rela√ß√µes hier√°rquicas entre as palavras. $\blacksquare$

### Conclus√£o
Este cap√≠tulo aprofundou a nossa compreens√£o de como o mecanismo de **autoaten√ß√£o** produz um *output* ponderado e como o mascaramento permite que este mecanismo seja utilizado para a modelagem causal de linguagem. A pondera√ß√£o dos vetores de *value* permite que o modelo capture informa√ß√µes contextuais de diferentes partes da sequ√™ncia, enquanto o mascaramento garante que a modelagem seja feita de forma auto-regressiva. A combina√ß√£o destes elementos √© crucial para o funcionamento dos **Transformers** em tarefas de modelagem de linguagem e outras aplica√ß√µes relacionadas a gera√ß√£o de texto. Atrav√©s da compreens√£o do mecanismo de autoaten√ß√£o, da pondera√ß√£o dos *values*, e do uso do mascaramento para impor causalidade, podemos ver como os Transformers conseguem gerar texto coerente e contextualmente relevante, demonstrando o poder da autoaten√ß√£o e sua arquitetura.

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^2]:  2 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^4]: 4 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^5]: 10.1.3 Self-attention more formally
[^6]: 6 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^8]: 10.1.4 Parallelizing self-attention using a single matrix X
<!-- END -->
