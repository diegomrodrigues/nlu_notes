## Processo de AtenÃ§Ã£o: ProjeÃ§Ã£o, PontuaÃ§Ã£o e PonderaÃ§Ã£o em AutoatenÃ§Ã£o

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda ainda mais o processo de **atenÃ§Ã£o** dentro do mecanismo de **autoatenÃ§Ã£o** do Transformer, explorando as operaÃ§Ãµes de projeÃ§Ã£o, pontuaÃ§Ã£o e ponderaÃ§Ã£o que sÃ£o cruciais para gerar representaÃ§Ãµes contextuais ricas [^1]. Conforme estabelecido nos capÃ­tulos anteriores, o processo de autoatenÃ§Ã£o envolve a projeÃ§Ã£o de cada vetor de entrada em espaÃ§os de *query*, *key* e *value* atravÃ©s da multiplicaÃ§Ã£o por matrizes de peso ($W^Q$, $W^K$ e $W^V$), respectivamente [^6]. Em seguida, o *score* entre um foco atual e seu contexto Ã© computado como um produto escalar entre as projeÃ§Ãµes de *query* e *key* [^6]. AlÃ©m disso, o produto escalar entre *queries* e *keys* Ã© usado para calcular a similaridade ou relevÃ¢ncia entre as palavras, e esses *scores* de autoatenÃ§Ã£o sÃ£o normalizados com *softmax* para gerar um vetor de pesos [^5]. Finalmente, a funÃ§Ã£o *softmax* transforma os *scores* em pesos normalizados, que sÃ£o usados para ponderar os vetores *value*, gerando representaÃ§Ãµes contextuais e produzindo uma distribuiÃ§Ã£o de probabilidade que representa a relevÃ¢ncia de cada palavra de entrada [^5].

### Conceitos Fundamentais
O processo de atenÃ§Ã£o no Transformer Ã© projetado para capturar as relaÃ§Ãµes complexas entre as palavras em uma sequÃªncia, ponderando a importÃ¢ncia de cada palavra vizinha no contexto da palavra de foco [^3]. Este processo Ã© alcanÃ§ado atravÃ©s da projeÃ§Ã£o das *embeddings* de entrada em trÃªs espaÃ§os distintos: *query*, *key* e *value* [^6].

As representaÃ§Ãµes *query* ($q_i$) sÃ£o usadas para consultar o contexto, buscando por outras palavras que sÃ£o relevantes para a palavra de foco. As representaÃ§Ãµes *key* ($k_j$) sÃ£o usadas como chaves para identificar palavras relevantes no contexto. As representaÃ§Ãµes *value* ($v_j$) contÃ©m as informaÃ§Ãµes que serÃ£o utilizadas para construir a representaÃ§Ã£o contextualizada da palavra de foco [^6]. Cada palavra de entrada $x_i$ Ã© projetada nesses trÃªs espaÃ§os atravÃ©s de matrizes de peso ($W^Q$, $W^K$ e $W^V$), permitindo que o modelo aprenda diferentes representaÃ§Ãµes para a mesma palavra, dependendo de seu papel no contexto:
$$q_i = x_iW^Q; \quad k_i = x_iW^K; \quad v_i = x_iW^V$$
Essas matrizes de peso sÃ£o os parÃ¢metros que o modelo aprende durante o treinamento, permitindo que ele ajuste a forma como as palavras sÃ£o representadas nos espaÃ§os *query*, *key* e *value* para maximizar o desempenho em tarefas de processamento de linguagem natural [^6].

ApÃ³s a projeÃ§Ã£o, o processo de atenÃ§Ã£o calcula a similaridade ou relevÃ¢ncia entre cada palavra de foco e todas as outras palavras precedentes, usando o produto escalar entre as representaÃ§Ãµes *query* ($q_i$) da palavra de foco e as representaÃ§Ãµes *key* ($k_j$) das outras palavras:
$$score(x_i, x_j) = q_i \cdot k_j$$
Este produto escalar quantifica o quÃ£o bem uma palavra vizinha corresponde Ã  consulta da palavra de foco. Palavras que sÃ£o semanticamente similares ou que tÃªm relaÃ§Ãµes sintÃ¡ticas relevantes tendem a ter um score mais alto [^5].

> ðŸ’¡ **Exemplo NumÃ©rico: ProjeÃ§Ã£o, PontuaÃ§Ã£o e Similaridade**
>
> Para ilustrar, consideremos uma sequÃªncia de trÃªs palavras com *embeddings* de dimensÃ£o 3:
>
> $x_1 = \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}$, $x_2 = \begin{bmatrix} 0 \\ 2 \\ 1 \end{bmatrix}$, $x_3 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$
>
> E matrizes de projeÃ§Ã£o inicializadas aleatoriamente (3x3):
>
>  $W^Q = \begin{bmatrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \\ 0.7 & 0.8 & 0.9 \end{bmatrix}$, $W^K = \begin{bmatrix} 0.9 & 0.8 & 0.7 \\ 0.6 & 0.5 & 0.4 \\ 0.3 & 0.2 & 0.1 \end{bmatrix}$, $W^V = \begin{bmatrix} 0.2 & 0.4 & 0.6 \\ 0.1 & 0.3 & 0.5 \\ 0.9 & 0.7 & 0.2 \end{bmatrix}$
>
> **1. ProjeÃ§Ã£o em Query, Key e Value:**
>
>   $q_1 = x_1W^Q = \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}\begin{bmatrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \\ 0.7 & 0.8 & 0.9 \end{bmatrix} = \begin{bmatrix} 1.5 & 1.8 & 2.1 \end{bmatrix}$
>
>   $k_1 = x_1W^K = \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}\begin{bmatrix} 0.9 & 0.8 & 0.7 \\ 0.6 & 0.5 & 0.4 \\ 0.3 & 0.2 & 0.1 \end{bmatrix} = \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix}$
>
>  $v_1 = x_1W^V =  \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}\begin{bmatrix} 0.2 & 0.4 & 0.6 \\ 0.1 & 0.3 & 0.5 \\ 0.9 & 0.7 & 0.2 \end{bmatrix} = \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix}$
>
>  Analogamente, para $x_2$ e $x_3$:
>
>   $q_2 = x_2W^Q =  \begin{bmatrix} 0.4 & 1.3 & 1.2 \end{bmatrix}$,  $k_2 = x_2W^K =  \begin{bmatrix} 1.5 & 1.3 & 1.1 \end{bmatrix}$,   $v_2 = x_2W^V =  \begin{bmatrix} 1.9 & 1.7 & 0.8 \end{bmatrix}$
>
>   $q_3 = x_3W^Q =  \begin{bmatrix} 0.5 & 0.7 & 0.9 \end{bmatrix}$,  $k_3 = x_3W^K =  \begin{bmatrix} 1.5 & 1.3 & 1.1 \end{bmatrix}$,  $v_3 = x_3W^V =  \begin{bmatrix} 1.1 & 1.0 & 0.9 \end{bmatrix}$
>
> **2. CÃ¡lculo dos Scores:**
>
>   Para $x_1$ (scores com relaÃ§Ã£o a si prÃ³prio):
>
>    $score(x_1, x_1) = q_1 \cdot k_1 = \begin{bmatrix} 1.5 & 1.8 & 2.1 \end{bmatrix} \cdot \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix} = 1.5 * 1.5 + 1.8 * 1.2 + 2.1 * 0.9 = 5.55$
>
> Para $x_2$:
>
>    $score(x_2, x_1) = q_2 \cdot k_1 = \begin{bmatrix} 0.4 & 1.3 & 1.2 \end{bmatrix} \cdot \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix} = 0.6 + 1.56 + 1.08 = 3.24$
>
>   $score(x_2, x_2) = q_2 \cdot k_2 = \begin{bmatrix} 0.4 & 1.3 & 1.2 \end{bmatrix} \cdot \begin{bmatrix} 0.6 & 0.5 & 0.4 \end{bmatrix} = 0.24 + 0.65 + 0.48 = 1.37$
>
>  Para $x_3$:
>
>   $score(x_3, x_1) = q_3 \cdot k_1 = \begin{bmatrix} 0.5 & 0.7 & 0.9 \end{bmatrix} \cdot \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix} = 0.75 + 0.84 + 0.81 = 2.4$
>
>  $score(x_3, x_2) = q_3 \cdot k_2 = \begin{bmatrix} 0.5 & 0.7 & 0.9 \end{bmatrix} \cdot \begin{bmatrix} 0.6 & 0.5 & 0.4 \end{bmatrix} = 0.3 + 0.35 + 0.36 = 1.01$
>
>  $score(x_3, x_3) = q_3 \cdot k_3 = \begin{bmatrix} 0.5 & 0.7 & 0.9 \end{bmatrix} \cdot \begin{bmatrix} 1.5 & 1.3 & 1.1 \end{bmatrix} = 0.75 + 0.91 + 0.99 = 2.65$
>
> Note como os *scores* entre $x_1$, $x_2$ e $x_3$ refletem a relaÃ§Ã£o entre cada palavra, com um *score* maior indicando mais similaridade ou relevÃ¢ncia.

Para transformar esses *scores* em pesos normalizados, a funÃ§Ã£o *softmax* Ã© aplicada:
$$\alpha_{ij} = \frac{exp(score(x_i, x_j))}{\sum_{k=1}^i exp(score(x_i, x_k))}, \forall j \leq i$$
A funÃ§Ã£o *softmax* transforma os *scores* em uma distribuiÃ§Ã£o de probabilidade, onde cada peso $\alpha_{ij}$ representa a relevÃ¢ncia da palavra $x_j$ para a palavra $x_i$. AlÃ©m disso, a funÃ§Ã£o *softmax* garante que os pesos $\alpha_{ij}$ somem 1, permitindo que a combinaÃ§Ã£o dos vetores *value* seja uma mÃ©dia ponderada.

A divisÃ£o pela raiz quadrada da dimensionalidade dos vetores *key* e *query* ($d_k$) antes de aplicar o *softmax* ($score(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}$) Ã© importante para estabilizar o treinamento do modelo [^7]. Sem essa divisÃ£o, o produto escalar pode gerar valores muito grandes, o que pode levar Ã  saturaÃ§Ã£o da funÃ§Ã£o *softmax*, dificultando o aprendizado. A divisÃ£o pela raiz quadrada de $d_k$ garante que os *scores* tenham uma escala apropriada, o que ajuda o modelo a aprender de forma mais eficiente.

> ðŸ’¡ **Exemplo NumÃ©rico: Impacto da DimensÃ£o e NormalizaÃ§Ã£o do Score**
>
>  Para demonstrar a importÃ¢ncia da normalizaÃ§Ã£o por $\sqrt{d_k}$, consideremos que, no exemplo anterior, os vetores tivessem uma dimensÃ£o de 300 (um valor comum em *embeddings* de palavras), ao invÃ©s de 3. Isso implicaria em scores muito maiores e uma instabilidade no treinamento. Vamos analisar o score de $x_1$ com relaÃ§Ã£o a ele prÃ³prio. Relembrando que:
>
>  $q_1 = x_1W^Q = \begin{bmatrix} 1.5 & 1.8 & 2.1 \end{bmatrix}$
>  $k_1 = x_1W^K = \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix}$
>  $score(x_1, x_1) = q_1 \cdot k_1 = 5.55$
>
>  Agora, vamos supor que os vetores $q_1$ e $k_1$ sejam vetores de dimensÃ£o 300, com cada um de seus elementos gerados aleatoriamente a partir de uma distribuiÃ§Ã£o normal com mÃ©dia 0 e desvio padrÃ£o 1. O produto escalar entre esses vetores tem uma distribuiÃ§Ã£o aproximadamente normal com mÃ©dia 0 e variÃ¢ncia igual Ã  dimensÃ£o, ou seja, 300.  Portanto, o *score* sem normalizaÃ§Ã£o pode ter magnitudes da ordem de centenas, gerando valores muito grandes na funÃ§Ã£o exponencial do *softmax*.
>
>  Agora vamos normalizar. Dividindo o score por $\sqrt{d_k} = \sqrt{300} \approx 17.32$.
>
>  $score_{norm}(x_1, x_1) = \frac{5.55}{\sqrt{300}} = 0.32$
>
>  A normalizaÃ§Ã£o garante que os scores nÃ£o tenham magnitudes muito altas ou baixas, permitindo que o *softmax* opere dentro de uma faixa adequada, evitando que os gradientes se tornem muito pequenos durante o treinamento do modelo.

Finalmente, os pesos normalizados $\alpha_{ij}$ sÃ£o usados para ponderar os vetores *value* ($v_j$), resultando em uma representaÃ§Ã£o contextualizada para a palavra de foco:
$$a_i = \sum_{j=1}^i \alpha_{ij} v_j$$
Esta representaÃ§Ã£o $a_i$ combina as informaÃ§Ãµes das palavras vizinhas, ponderando-as de acordo com sua relevÃ¢ncia para a palavra de foco. Palavras que tÃªm um score mais alto (e, portanto, um peso maior) tÃªm uma influÃªncia maior na representaÃ§Ã£o contextualizada da palavra de foco [^6].

> ðŸ’¡ **Exemplo NumÃ©rico: NormalizaÃ§Ã£o com Softmax e PonderaÃ§Ã£o**
>
>  Continuando com o exemplo anterior, vamos calcular os pesos com Softmax e a combinaÃ§Ã£o dos *values*. Assumiremos para simplificaÃ§Ã£o que $d_k = 1$:
>
>   **1. NormalizaÃ§Ã£o com Softmax:**
>  Para $x_1$ (apenas com relaÃ§Ã£o a si prÃ³prio):
>
>      $\alpha_{11} = \frac{e^{5.55}}{e^{5.55}} = 1$
>
>  Para $x_2$:
>
>      $\alpha_{21} = \frac{e^{3.24}}{e^{3.24} + e^{1.37}} = \frac{25.53}{25.53 + 3.93} = 0.87 $
>
>      $\alpha_{22} = \frac{e^{1.37}}{e^{3.24} + e^{1.37}} = \frac{3.93}{25.53 + 3.93} = 0.13$
>
>  Para $x_3$:
>
>   $\alpha_{31} = \frac{e^{2.4}}{e^{2.4} + e^{1.01} + e^{2.65}} = \frac{11.02}{11.02 + 2.74 + 14.15} = 0.39$
>
>    $\alpha_{32} = \frac{e^{1.01}}{e^{2.4} + e^{1.01} + e^{2.65}} = \frac{2.74}{11.02 + 2.74 + 14.15} = 0.10$
>
>   $\alpha_{33} = \frac{e^{2.65}}{e^{2.4} + e^{1.01} + e^{2.65}} = \frac{14.15}{11.02 + 2.74 + 14.15} = 0.51$
>
> **2. PonderaÃ§Ã£o dos Values:**
>
>   $a_1 = \alpha_{11} * v_1 = 1 * \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix}$
>
>   $a_2 = \alpha_{21}*v_1 + \alpha_{22}*v_2 = 0.87 *  \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix} + 0.13 * \begin{bmatrix} 1.9 & 1.7 & 0.8 \end{bmatrix} = \begin{bmatrix} 1.98 & 1.79 & 0.97 \end{bmatrix}$
>
>   $a_3 = \alpha_{31}*v_1 + \alpha_{32}*v_2 + \alpha_{33}*v_3 = 0.39 *  \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix} + 0.10 * \begin{bmatrix} 1.9 & 1.7 & 0.8 \end{bmatrix} + 0.51 *  \begin{bmatrix} 1.1 & 1.0 & 0.9 \end{bmatrix} = \begin{bmatrix} 1.33 & 1.38 & 0.95 \end{bmatrix}$
>
>  Note que $a_1$ Ã© igual a $v_1$, uma vez que sÃ³ hÃ¡ atenÃ§Ã£o Ã  prÃ³pria palavra. $a_2$ Ã© uma combinaÃ§Ã£o ponderada de $v_1$ e $v_2$, e $a_3$ uma combinaÃ§Ã£o ponderada de $v_1$, $v_2$ e $v_3$. Os pesos, determinados pelo *softmax*, indicam a relevÃ¢ncia de cada palavra vizinha para a construÃ§Ã£o da representaÃ§Ã£o contextualizada.

> ðŸ’¡ **FormalizaÃ§Ã£o do Processo de AtenÃ§Ã£o**
>
> Vamos agora formalizar todo o processo de atenÃ§Ã£o no mecanismo de autoatenÃ§Ã£o do Transformer:
>
> 1.  **ProjeÃ§Ã£o:** Cada vetor de entrada $x_i$ Ã© projetado em trÃªs espaÃ§os distintos, *query*, *key* e *value*, usando matrizes de peso $W^Q$, $W^K$ e $W^V$:
>
>     $$q_i = x_iW^Q$$
>
>     $$k_i = x_iW^K$$
>
>     $$v_i = x_iW^V$$
>
> 2.  **PontuaÃ§Ã£o:** O score entre a palavra de foco $x_i$ e todas as palavras precedentes $x_j$ (incluindo ela prÃ³pria) Ã© calculado atravÃ©s do produto escalar entre as representaÃ§Ãµes *query* e *key*, normalizado pela raiz quadrada da dimensÃ£o dos vetores:
>
>    $$score(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}$$
>
>     onde $d_k$ Ã© a dimensÃ£o dos vetores *key* e *query*.
>
> 3.  **NormalizaÃ§Ã£o:** Os scores sÃ£o transformados em pesos normalizados $\alpha_{ij}$ utilizando a funÃ§Ã£o *softmax*:
>
>     $$\alpha_{ij} = \frac{exp(score(x_i, x_j))}{\sum_{k=1}^i exp(score(x_i, x_k))}, \forall j \leq i$$
>
> 4. **PonderaÃ§Ã£o:** Os pesos normalizados sÃ£o usados para ponderar os vetores *value* e produzir uma representaÃ§Ã£o contextualizada $a_i$ para a palavra de foco:
>
>     $$a_i = \sum_{j=1}^i \alpha_{ij} v_j$$
>
> Esses quatro passos combinam operaÃ§Ãµes lineares (projeÃ§Ãµes), nÃ£o lineares (*softmax*) e produtos escalares para criar representaÃ§Ãµes contextuais ricas e relevantes para cada palavra em uma sequÃªncia.

**ProposiÃ§Ã£o 1** (EquivalÃªncia Matricial da AutoatenÃ§Ã£o): O processo de autoatenÃ§Ã£o pode ser expresso de forma concisa utilizando notaÃ§Ã£o matricial, o que Ã© fundamental para sua implementaÃ§Ã£o eficiente em GPUs.
  *   Seja $X$ a matriz de entrada onde cada linha corresponde a um vetor de entrada $x_i$, as matrizes $Q$, $K$ e $V$ sÃ£o obtidas por:
  $$Q = XW^Q$$
  $$K = XW^K$$
  $$V = XW^V$$

  *   A matriz de *scores* $S$ Ã© dada por:
  $$S = \frac{QK^T}{\sqrt{d_k}}$$

  *   A matriz de pesos normalizados $A$ Ã© obtida aplicando *softmax* nas linhas de $S$:
    $$A_{ij} = \frac{exp(S_{ij})}{\sum_{k=1}^i exp(S_{ik})}, \forall j \leq i$$
    Note que na autoatenÃ§Ã£o padrÃ£o, essa soma considera todos os elementos de $K$. No caso de *masked self-attention* a soma se restringe aos elementos atÃ© a posiÃ§Ã£o $i$.
  *  Finalmente, a matriz de representaÃ§Ãµes contextuais $Z$ Ã© calculada por:
   $$Z = AV$$
  *   Esta representaÃ§Ã£o matricial condensa todas as operaÃ§Ãµes de projeÃ§Ã£o, pontuaÃ§Ã£o, normalizaÃ§Ã£o e ponderaÃ§Ã£o, oferecendo uma visÃ£o mais concisa do processo de autoatenÃ§Ã£o. AlÃ©m disso, a formulaÃ§Ã£o matricial permite o processamento paralelo de todos os vetores de entrada simultaneamente, o que Ã© essencial para o desempenho dos Transformers em hardware moderno.

### ConclusÃ£o
Em resumo, o processo de atenÃ§Ã£o no Transformer envolve a projeÃ§Ã£o de vetores de entrada em espaÃ§os de *query*, *key* e *value*, o cÃ¡lculo de *scores* de similaridade entre palavras, a normalizaÃ§Ã£o desses *scores* usando *softmax* e a ponderaÃ§Ã£o dos vetores *value* com base nos pesos gerados. Esse processo permite que o modelo capture as relaÃ§Ãµes complexas entre as palavras em uma sequÃªncia, produzindo representaÃ§Ãµes contextuais que sÃ£o cruciais para o desempenho dos Transformers em tarefas de processamento de linguagem natural. A projeÃ§Ã£o por meio das matrizes $W^Q, W^K,$ e $W^V$ permite que o modelo aprenda diferentes representaÃ§Ãµes para cada palavra de acordo com seu papel no contexto [^6]. A normalizaÃ§Ã£o dos scores por $\sqrt{d_k}$ estabiliza o treinamento, enquanto o softmax transforma os scores em pesos normalizados [^7]. A combinaÃ§Ã£o dos *values* com esses pesos gera representaÃ§Ãµes contextuais ricas, demonstrando o poder do mecanismo de autoatenÃ§Ã£o [^6]. A sequÃªncia de projeÃ§Ã£o, pontuaÃ§Ã£o, normalizaÃ§Ã£o e ponderaÃ§Ã£o, que opera em paralelo para todas as posiÃ§Ãµes, Ã© o que torna os Transformers tÃ£o eficientes e eficazes para o processamento de linguagem natural [^8].

### ReferÃªncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright Â© 2023. All rights reserved. Draft of February 3, 2024.
[^3]: 10.1.1 Transformers: the intuition
[^5]: 10.1.3 Self-attention more formally
[^6]: 6 CHAPTER 10 â€¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^7]: 10.1 â€¢ THE TRANSFORMER: A SELF-ATTENTION NETWORK 7
[^8]: 10.1.4 Parallelizing self-attention using a single matrix X
## 10.  <SUBTOPIC_PLACEHOLDER>: Causal Language Modeling with Transformers
### IntroduÃ§Ã£o
Em continuidade ao estudo dos **Transformers** e sua arquitetura fundamental, exploraremos neste capÃ­tulo como esses modelos sÃ£o aplicados especificamente para a tarefa de modelagem de linguagem causal, tambÃ©m conhecida como *autoregressive language modeling*. Este tipo de modelagem, onde o modelo prediz sequencialmente cada palavra com base nas palavras precedentes, Ã© essencial para tarefas como geraÃ§Ã£o de texto e outras aplicaÃ§Ãµes de processamento de linguagem natural (PLN) [^1]. Expandindo o conceito apresentado em seÃ§Ãµes anteriores, vamos nos aprofundar na aplicaÃ§Ã£o prÃ¡tica dos **Transformers** nesse contexto especÃ­fico.

### Conceitos Fundamentais
O cerne da modelagem de linguagem causal com **Transformers** reside na capacidade de prever a prÃ³xima palavra em uma sequÃªncia de texto, dado o contexto das palavras anteriores [^2]. Essa prediÃ§Ã£o Ã© feita iterativamente, onde a saÃ­da do modelo em um passo Ã© utilizada como entrada para o prÃ³ximo passo, caracterizando o aspecto *autoregressivo*.

O processo inicia com a entrada de uma sequÃªncia de *tokens* (que podem ser palavras, subpalavras ou caracteres) que sÃ£o convertidos em *embeddings* de entrada [^1]. Esses *embeddings* sÃ£o entÃ£o alimentados em uma sÃ©rie de blocos de *Transformer*, cada um contendo camadas de *self-attention*, *feedforward*, conexÃµes residuais e normalizaÃ§Ã£o de camada [^7].

Na camada de *self-attention*, o mecanismo de atenÃ§Ã£o *causal* garante que cada palavra atenda apenas Ã s palavras que a precedem na sequÃªncia, impedindo que o modelo "veja" o futuro, o que seria contraproducente para o processo de modelagem de linguagem. Essa Ã© uma distinÃ§Ã£o crucial em relaÃ§Ã£o a outras aplicaÃ§Ãµes dos **Transformers**, onde informaÃ§Ãµes de todo o contexto podem ser utilizadas [^4]. Conforme previamente discutido, essa causalidade Ã© implementada mascarando a matriz $QK^T$ [^8].

> ðŸ’¡ **Exemplo NumÃ©rico: Mascaramento em AutoatenÃ§Ã£o Causal**
>
>  Vamos considerar uma sequÃªncia de 4 tokens, e que a matriz de scores *S* calculada sem mascaramento seja:
>
>   $S = \begin{bmatrix}
>    1.0 & 0.5 & 0.2 & 0.1 \\
>    0.8 & 1.2 & 0.6 & 0.3 \\
>    0.4 & 0.7 & 1.1 & 0.9 \\
>    0.2 & 0.5 & 0.8 & 1.3
>  \end{bmatrix}$
>
> Para a autoatenÃ§Ã£o causal, precisamos aplicar uma mÃ¡scara triangular inferior. A matriz de mÃ¡scara *M* Ã© dada por:
>
> $M = \begin{bmatrix}
>   0 & -\infty & -\infty & -\infty \\
>   0 & 0 & -\infty & -\infty \\
>   0 & 0 & 0 & -\infty \\
>   0 & 0 & 0 & 0
>  \end{bmatrix}$
>
> A matriz de scores mascarada $S'$ Ã© dada por $S' = S + M$:
>
>  $S' = \begin{bmatrix}
>    1.0 & -\infty & -\infty & -\infty \\
>    0.8 & 1.2 & -\infty & -\infty \\
>    0.4 & 0.7 & 1.1 & -\infty \\
>    0.2 & 0.5 & 0.8 & 1.3
>  \end{bmatrix}$
>
>  Agora, aplicando o softmax nas linhas de $S'$, obtemos a matriz de pesos $A$. Os valores $-\infty$ na matriz de scores resultam em pesos de atenÃ§Ã£o iguais a 0.
>
>  $A = \begin{bmatrix}
>  1.0 & 0 & 0 & 0 \\
>   0.35 & 0.65 & 0 & 0 \\
>   0.15 & 0.27 & 0.58 & 0 \\
>  0.08 & 0.18 & 0.29 & 0.45
>  \end{bmatrix}$
>
> Note como, por exemplo, o token 3 (terceira linha) sÃ³ tem atenÃ§Ã£o com relaÃ§Ã£o aos tokens 1, 2 e 3, mas nÃ£o com relaÃ§Ã£o ao token 4. Isto garante a propriedade de modelagem causal.

A saÃ­da do Ãºltimo bloco de *Transformer* Ã© entÃ£o passada atravÃ©s de uma camada linear e uma funÃ§Ã£o *softmax* para produzir uma distribuiÃ§Ã£o de probabilidade sobre o vocabulÃ¡rio. Essa distribuiÃ§Ã£o representa a previsÃ£o do modelo para a prÃ³xima palavra na sequÃªncia.

O treinamento do modelo Ã© realizado atravÃ©s de um processo de auto-supervisÃ£o, onde o modelo aprende a predizer a prÃ³xima palavra em uma sequÃªncia de texto usando a sequÃªncia real como supervisÃ£o [^2]. A funÃ§Ã£o de perda utilizada Ã© a *cross-entropy*, que mede a diferenÃ§a entre a distribuiÃ§Ã£o de probabilidade prevista pelo modelo e a distribuiÃ§Ã£o real da prÃ³xima palavra.

Para geraÃ§Ã£o de texto, uma vez treinado o modelo, utiliza-se uma estratÃ©gia de *decoding*, que pode variar entre *greedy decoding*, *beam search*, *top-k sampling* ou *temperature sampling* [^1]. Essas estratÃ©gias permitem gerar texto de forma sequencial, palavra por palavra, com base nas previsÃµes do modelo.  No greedy decoding, por exemplo, escolhe-se sempre a palavra de maior probabilidade no passo corrente, enquanto os mÃ©todos de *sampling* introduzem aleatoriedade para diversificar o texto gerado [^2].

> ðŸ’¡ **Exemplo NumÃ©rico: GeraÃ§Ã£o de Texto com Decoding Greedy**
>
>  Suponha que, apÃ³s o treinamento, o modelo receba a sequÃªncia "O gato estÃ¡" e, apÃ³s a passagem pela rede, a distribuiÃ§Ã£o de probabilidade de saÃ­da para a prÃ³xima palavra seja:
>
>   $P(\text{dormindo}) = 0.6$
>   $P(\text{pulando}) = 0.2$
>    $P(\text{correndo}) = 0.15$
>    $P(\text{comendo}) = 0.05$
>
> No *greedy decoding*, a palavra com maior probabilidade seria selecionada, neste caso, "dormindo". A prÃ³xima entrada para o modelo seria entÃ£o "O gato estÃ¡ dormindo", e o processo se repetiria atÃ© gerar uma sentenÃ§a completa.
>
> MÃ©todos de *sampling* introduzem alguma aleatoriedade, por exemplo, usando *temperature sampling* o modelo poderia escolher a palavra "pulando" com uma probabilidade de 0.2.

A capacidade de processar sequÃªncias longas com o uso de *self-attention* e a eficiÃªncia da computaÃ§Ã£o paralela sÃ£o caracterÃ­sticas cruciais que permitem aos **Transformers** alcanÃ§ar desempenhos superiores na modelagem de linguagem em comparaÃ§Ã£o a modelos de arquiteturas como *RNNs*. Essa capacidade de modelar dependÃªncias de longo alcance permite que os **Transformers** capturem o contexto de forma mais eficaz [^7], permitindo a geraÃ§Ã£o de textos mais coerentes e contextualmente relevantes.

**Lema 1.1** (ConexÃ£o entre Masked Self-Attention e Modelagem Causal): A operaÃ§Ã£o de *masked self-attention* Ã© essencial para modelagem de linguagem causal porque garante que, durante a fase de treinamento, a prediÃ§Ã£o de uma palavra em particular ($x_i$) dependa apenas das palavras precedentes ($x_1, x_2, ..., x_{i-1}$). Isso Ã© obtido atravÃ©s da aplicaÃ§Ã£o de uma mÃ¡scara triangular inferior sobre a matriz de scores ($S$), que impede que o cÃ¡lculo dos pesos de atenÃ§Ã£o incorpore informaÃ§Ãµes do "futuro" (palavras apÃ³s $x_i$). Formalmente, a mÃ¡scara garante que $\alpha_{ij} = 0$ se $j > i$ no cÃ¡lculo de autoatenÃ§Ã£o.

*   **Prova:**
I.  A matriz de *scores* $S$ Ã© calculada como $S = \frac{QK^T}{\sqrt{d_k}}$, onde $Q$ e $K$ sÃ£o as matrizes de *queries* e *keys*, respectivamente.
II. Na autoatenÃ§Ã£o causal, uma mÃ¡scara $M$ Ã© aplicada em $S$, tal que $M_{ij} = -\infty$ se $j > i$, e $M_{ij} = 0$ caso contrÃ¡rio.
III. Assim, a matriz de scores *mascarada* $S'$ Ã© definida por:
     $$S' = S + M $$
IV. Quando o *softmax* Ã© aplicado em $S'$, o resultado Ã© que, para $j > i$, temos $\alpha_{ij} = \frac{exp(-\infty)}{\sum_k exp(S'_{ik})} = 0$.
    Essa operaÃ§Ã£o impede a dependÃªncia de palavras futuras no cÃ¡lculo dos pesos de atenÃ§Ã£o para a palavra $x_i$.

    Essa mÃ¡scara garante que cada posiÃ§Ã£o tenha acesso somente as informaÃ§Ãµes das posiÃ§Ãµes anteriores. Esse Ã© um requerimento fundamental para construir um modelo de linguagem causal. â– 

### ConclusÃ£o
Nesta seÃ§Ã£o, exploramos como os **Transformers** sÃ£o adaptados para a modelagem de linguagem causal, um componente essencial em muitas tarefas de PLN. Vimos como a arquitetura, combinada com mecanismos como a *self-attention* causal e estratÃ©gias de *decoding*, permite a geraÃ§Ã£o de texto de alta qualidade. Em seÃ§Ãµes subsequentes, iremos explorar como esses modelos podem ser ainda mais adaptados para tarefas de *fine-tuning*, *prompting* e aplicaÃ§Ãµes em cenÃ¡rios de encoder-decoder. A discussÃ£o aqui fornece uma base sÃ³lida para a compreensÃ£o de como a arquitetura do **Transformer**, explorada em capÃ­tulos anteriores, pode ser utilizada para construir modelos de linguagem poderosos e versÃ¡teis.

### ReferÃªncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright Â© 2023. All rights reserved. Draft of February 3, 2024.
[^2]:  2 CHAPTER 10 â€¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^4]: 4 CHAPTER 10 â€¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^6]: 6 CHAPTER 10 â€¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^7]: 10.1 â€¢ THE TRANSFORMER: A SELF-ATTENTION NETWORK 7
[^8]: 10.1.4 Parallelizing self-attention using a single matrix X
<!-- END -->
