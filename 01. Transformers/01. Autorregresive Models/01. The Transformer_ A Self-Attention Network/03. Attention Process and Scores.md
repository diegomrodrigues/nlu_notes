## Processo de Aten√ß√£o: Proje√ß√£o, Pontua√ß√£o e Pondera√ß√£o em Autoaten√ß√£o

### Introdu√ß√£o
Este cap√≠tulo aprofunda ainda mais o processo de **aten√ß√£o** dentro do mecanismo de **autoaten√ß√£o** do Transformer, explorando as opera√ß√µes de proje√ß√£o, pontua√ß√£o e pondera√ß√£o que s√£o cruciais para gerar representa√ß√µes contextuais ricas [^1]. Conforme estabelecido nos cap√≠tulos anteriores, o processo de autoaten√ß√£o envolve a proje√ß√£o de cada vetor de entrada em espa√ßos de *query*, *key* e *value* atrav√©s da multiplica√ß√£o por matrizes de peso ($W^Q$, $W^K$ e $W^V$), respectivamente [^6]. Em seguida, o *score* entre um foco atual e seu contexto √© computado como um produto escalar entre as proje√ß√µes de *query* e *key* [^6]. Al√©m disso, o produto escalar entre *queries* e *keys* √© usado para calcular a similaridade ou relev√¢ncia entre as palavras, e esses *scores* de autoaten√ß√£o s√£o normalizados com *softmax* para gerar um vetor de pesos [^5]. Finalmente, a fun√ß√£o *softmax* transforma os *scores* em pesos normalizados, que s√£o usados para ponderar os vetores *value*, gerando representa√ß√µes contextuais e produzindo uma distribui√ß√£o de probabilidade que representa a relev√¢ncia de cada palavra de entrada [^5].

### Conceitos Fundamentais
O processo de aten√ß√£o no Transformer √© projetado para capturar as rela√ß√µes complexas entre as palavras em uma sequ√™ncia, ponderando a import√¢ncia de cada palavra vizinha no contexto da palavra de foco [^3]. Este processo √© alcan√ßado atrav√©s da proje√ß√£o das *embeddings* de entrada em tr√™s espa√ßos distintos: *query*, *key* e *value* [^6].

As representa√ß√µes *query* ($q_i$) s√£o usadas para consultar o contexto, buscando por outras palavras que s√£o relevantes para a palavra de foco. As representa√ß√µes *key* ($k_j$) s√£o usadas como chaves para identificar palavras relevantes no contexto. As representa√ß√µes *value* ($v_j$) cont√©m as informa√ß√µes que ser√£o utilizadas para construir a representa√ß√£o contextualizada da palavra de foco [^6]. Cada palavra de entrada $x_i$ √© projetada nesses tr√™s espa√ßos atrav√©s de matrizes de peso ($W^Q$, $W^K$ e $W^V$), permitindo que o modelo aprenda diferentes representa√ß√µes para a mesma palavra, dependendo de seu papel no contexto:
$$q_i = x_iW^Q; \quad k_i = x_iW^K; \quad v_i = x_iW^V$$
Essas matrizes de peso s√£o os par√¢metros que o modelo aprende durante o treinamento, permitindo que ele ajuste a forma como as palavras s√£o representadas nos espa√ßos *query*, *key* e *value* para maximizar o desempenho em tarefas de processamento de linguagem natural [^6].

Ap√≥s a proje√ß√£o, o processo de aten√ß√£o calcula a similaridade ou relev√¢ncia entre cada palavra de foco e todas as outras palavras precedentes, usando o produto escalar entre as representa√ß√µes *query* ($q_i$) da palavra de foco e as representa√ß√µes *key* ($k_j$) das outras palavras:
$$score(x_i, x_j) = q_i \cdot k_j$$
Este produto escalar quantifica o qu√£o bem uma palavra vizinha corresponde √† consulta da palavra de foco. Palavras que s√£o semanticamente similares ou que t√™m rela√ß√µes sint√°ticas relevantes tendem a ter um score mais alto [^5].

> üí° **Exemplo Num√©rico: Proje√ß√£o, Pontua√ß√£o e Similaridade**
>
> Para ilustrar, consideremos uma sequ√™ncia de tr√™s palavras com *embeddings* de dimens√£o 3:
>
> $x_1 = \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}$, $x_2 = \begin{bmatrix} 0 \\ 2 \\ 1 \end{bmatrix}$, $x_3 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$
>
> E matrizes de proje√ß√£o inicializadas aleatoriamente (3x3):
>
>  $W^Q = \begin{bmatrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \\ 0.7 & 0.8 & 0.9 \end{bmatrix}$, $W^K = \begin{bmatrix} 0.9 & 0.8 & 0.7 \\ 0.6 & 0.5 & 0.4 \\ 0.3 & 0.2 & 0.1 \end{bmatrix}$, $W^V = \begin{bmatrix} 0.2 & 0.4 & 0.6 \\ 0.1 & 0.3 & 0.5 \\ 0.9 & 0.7 & 0.2 \end{bmatrix}$
>
> **1. Proje√ß√£o em Query, Key e Value:**
>
>   $q_1 = x_1W^Q = \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}\begin{bmatrix} 0.1 & 0.2 & 0.3 \\ 0.4 & 0.5 & 0.6 \\ 0.7 & 0.8 & 0.9 \end{bmatrix} = \begin{bmatrix} 1.5 & 1.8 & 2.1 \end{bmatrix}$
>
>   $k_1 = x_1W^K = \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}\begin{bmatrix} 0.9 & 0.8 & 0.7 \\ 0.6 & 0.5 & 0.4 \\ 0.3 & 0.2 & 0.1 \end{bmatrix} = \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix}$
>
>  $v_1 = x_1W^V =  \begin{bmatrix} 1 \\ 0 \\ 2 \end{bmatrix}\begin{bmatrix} 0.2 & 0.4 & 0.6 \\ 0.1 & 0.3 & 0.5 \\ 0.9 & 0.7 & 0.2 \end{bmatrix} = \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix}$
>
>  Analogamente, para $x_2$ e $x_3$:
>
>   $q_2 = x_2W^Q =  \begin{bmatrix} 0.4 & 1.3 & 1.2 \end{bmatrix}$,  $k_2 = x_2W^K =  \begin{bmatrix} 1.5 & 1.3 & 1.1 \end{bmatrix}$,   $v_2 = x_2W^V =  \begin{bmatrix} 1.9 & 1.7 & 0.8 \end{bmatrix}$
>
>   $q_3 = x_3W^Q =  \begin{bmatrix} 0.5 & 0.7 & 0.9 \end{bmatrix}$,  $k_3 = x_3W^K =  \begin{bmatrix} 1.5 & 1.3 & 1.1 \end{bmatrix}$,  $v_3 = x_3W^V =  \begin{bmatrix} 1.1 & 1.0 & 0.9 \end{bmatrix}$
>
> **2. C√°lculo dos Scores:**
>
>   Para $x_1$ (scores com rela√ß√£o a si pr√≥prio):
>
>    $score(x_1, x_1) = q_1 \cdot k_1 = \begin{bmatrix} 1.5 & 1.8 & 2.1 \end{bmatrix} \cdot \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix} = 1.5 * 1.5 + 1.8 * 1.2 + 2.1 * 0.9 = 5.55$
>
> Para $x_2$:
>
>    $score(x_2, x_1) = q_2 \cdot k_1 = \begin{bmatrix} 0.4 & 1.3 & 1.2 \end{bmatrix} \cdot \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix} = 0.6 + 1.56 + 1.08 = 3.24$
>
>   $score(x_2, x_2) = q_2 \cdot k_2 = \begin{bmatrix} 0.4 & 1.3 & 1.2 \end{bmatrix} \cdot \begin{bmatrix} 0.6 & 0.5 & 0.4 \end{bmatrix} = 0.24 + 0.65 + 0.48 = 1.37$
>
>  Para $x_3$:
>
>   $score(x_3, x_1) = q_3 \cdot k_1 = \begin{bmatrix} 0.5 & 0.7 & 0.9 \end{bmatrix} \cdot \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix} = 0.75 + 0.84 + 0.81 = 2.4$
>
>  $score(x_3, x_2) = q_3 \cdot k_2 = \begin{bmatrix} 0.5 & 0.7 & 0.9 \end{bmatrix} \cdot \begin{bmatrix} 0.6 & 0.5 & 0.4 \end{bmatrix} = 0.3 + 0.35 + 0.36 = 1.01$
>
>  $score(x_3, x_3) = q_3 \cdot k_3 = \begin{bmatrix} 0.5 & 0.7 & 0.9 \end{bmatrix} \cdot \begin{bmatrix} 1.5 & 1.3 & 1.1 \end{bmatrix} = 0.75 + 0.91 + 0.99 = 2.65$
>
> Note como os *scores* entre $x_1$, $x_2$ e $x_3$ refletem a rela√ß√£o entre cada palavra, com um *score* maior indicando mais similaridade ou relev√¢ncia.

Para transformar esses *scores* em pesos normalizados, a fun√ß√£o *softmax* √© aplicada:
$$\alpha_{ij} = \frac{exp(score(x_i, x_j))}{\sum_{k=1}^i exp(score(x_i, x_k))}, \forall j \leq i$$
A fun√ß√£o *softmax* transforma os *scores* em uma distribui√ß√£o de probabilidade, onde cada peso $\alpha_{ij}$ representa a relev√¢ncia da palavra $x_j$ para a palavra $x_i$. Al√©m disso, a fun√ß√£o *softmax* garante que os pesos $\alpha_{ij}$ somem 1, permitindo que a combina√ß√£o dos vetores *value* seja uma m√©dia ponderada.

A divis√£o pela raiz quadrada da dimensionalidade dos vetores *key* e *query* ($d_k$) antes de aplicar o *softmax* ($score(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}$) √© importante para estabilizar o treinamento do modelo [^7]. Sem essa divis√£o, o produto escalar pode gerar valores muito grandes, o que pode levar √† satura√ß√£o da fun√ß√£o *softmax*, dificultando o aprendizado. A divis√£o pela raiz quadrada de $d_k$ garante que os *scores* tenham uma escala apropriada, o que ajuda o modelo a aprender de forma mais eficiente.

> üí° **Exemplo Num√©rico: Impacto da Dimens√£o e Normaliza√ß√£o do Score**
>
>  Para demonstrar a import√¢ncia da normaliza√ß√£o por $\sqrt{d_k}$, consideremos que, no exemplo anterior, os vetores tivessem uma dimens√£o de 300 (um valor comum em *embeddings* de palavras), ao inv√©s de 3. Isso implicaria em scores muito maiores e uma instabilidade no treinamento. Vamos analisar o score de $x_1$ com rela√ß√£o a ele pr√≥prio. Relembrando que:
>
>  $q_1 = x_1W^Q = \begin{bmatrix} 1.5 & 1.8 & 2.1 \end{bmatrix}$
>  $k_1 = x_1W^K = \begin{bmatrix} 1.5 & 1.2 & 0.9 \end{bmatrix}$
>  $score(x_1, x_1) = q_1 \cdot k_1 = 5.55$
>
>  Agora, vamos supor que os vetores $q_1$ e $k_1$ sejam vetores de dimens√£o 300, com cada um de seus elementos gerados aleatoriamente a partir de uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1. O produto escalar entre esses vetores tem uma distribui√ß√£o aproximadamente normal com m√©dia 0 e vari√¢ncia igual √† dimens√£o, ou seja, 300.  Portanto, o *score* sem normaliza√ß√£o pode ter magnitudes da ordem de centenas, gerando valores muito grandes na fun√ß√£o exponencial do *softmax*.
>
>  Agora vamos normalizar. Dividindo o score por $\sqrt{d_k} = \sqrt{300} \approx 17.32$.
>
>  $score_{norm}(x_1, x_1) = \frac{5.55}{\sqrt{300}} = 0.32$
>
>  A normaliza√ß√£o garante que os scores n√£o tenham magnitudes muito altas ou baixas, permitindo que o *softmax* opere dentro de uma faixa adequada, evitando que os gradientes se tornem muito pequenos durante o treinamento do modelo.

Finalmente, os pesos normalizados $\alpha_{ij}$ s√£o usados para ponderar os vetores *value* ($v_j$), resultando em uma representa√ß√£o contextualizada para a palavra de foco:
$$a_i = \sum_{j=1}^i \alpha_{ij} v_j$$
Esta representa√ß√£o $a_i$ combina as informa√ß√µes das palavras vizinhas, ponderando-as de acordo com sua relev√¢ncia para a palavra de foco. Palavras que t√™m um score mais alto (e, portanto, um peso maior) t√™m uma influ√™ncia maior na representa√ß√£o contextualizada da palavra de foco [^6].

> üí° **Exemplo Num√©rico: Normaliza√ß√£o com Softmax e Pondera√ß√£o**
>
>  Continuando com o exemplo anterior, vamos calcular os pesos com Softmax e a combina√ß√£o dos *values*. Assumiremos para simplifica√ß√£o que $d_k = 1$:
>
>   **1. Normaliza√ß√£o com Softmax:**
>  Para $x_1$ (apenas com rela√ß√£o a si pr√≥prio):
>
>      $\alpha_{11} = \frac{e^{5.55}}{e^{5.55}} = 1$
>
>  Para $x_2$:
>
>      $\alpha_{21} = \frac{e^{3.24}}{e^{3.24} + e^{1.37}} = \frac{25.53}{25.53 + 3.93} = 0.87 $
>
>      $\alpha_{22} = \frac{e^{1.37}}{e^{3.24} + e^{1.37}} = \frac{3.93}{25.53 + 3.93} = 0.13$
>
>  Para $x_3$:
>
>   $\alpha_{31} = \frac{e^{2.4}}{e^{2.4} + e^{1.01} + e^{2.65}} = \frac{11.02}{11.02 + 2.74 + 14.15} = 0.39$
>
>    $\alpha_{32} = \frac{e^{1.01}}{e^{2.4} + e^{1.01} + e^{2.65}} = \frac{2.74}{11.02 + 2.74 + 14.15} = 0.10$
>
>   $\alpha_{33} = \frac{e^{2.65}}{e^{2.4} + e^{1.01} + e^{2.65}} = \frac{14.15}{11.02 + 2.74 + 14.15} = 0.51$
>
> **2. Pondera√ß√£o dos Values:**
>
>   $a_1 = \alpha_{11} * v_1 = 1 * \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix}$
>
>   $a_2 = \alpha_{21}*v_1 + \alpha_{22}*v_2 = 0.87 *  \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix} + 0.13 * \begin{bmatrix} 1.9 & 1.7 & 0.8 \end{bmatrix} = \begin{bmatrix} 1.98 & 1.79 & 0.97 \end{bmatrix}$
>
>   $a_3 = \alpha_{31}*v_1 + \alpha_{32}*v_2 + \alpha_{33}*v_3 = 0.39 *  \begin{bmatrix} 2 & 1.8 & 1 \end{bmatrix} + 0.10 * \begin{bmatrix} 1.9 & 1.7 & 0.8 \end{bmatrix} + 0.51 *  \begin{bmatrix} 1.1 & 1.0 & 0.9 \end{bmatrix} = \begin{bmatrix} 1.33 & 1.38 & 0.95 \end{bmatrix}$
>
>  Note que $a_1$ √© igual a $v_1$, uma vez que s√≥ h√° aten√ß√£o √† pr√≥pria palavra. $a_2$ √© uma combina√ß√£o ponderada de $v_1$ e $v_2$, e $a_3$ uma combina√ß√£o ponderada de $v_1$, $v_2$ e $v_3$. Os pesos, determinados pelo *softmax*, indicam a relev√¢ncia de cada palavra vizinha para a constru√ß√£o da representa√ß√£o contextualizada.

> üí° **Formaliza√ß√£o do Processo de Aten√ß√£o**
>
> Vamos agora formalizar todo o processo de aten√ß√£o no mecanismo de autoaten√ß√£o do Transformer:
>
> 1.  **Proje√ß√£o:** Cada vetor de entrada $x_i$ √© projetado em tr√™s espa√ßos distintos, *query*, *key* e *value*, usando matrizes de peso $W^Q$, $W^K$ e $W^V$:
>
>     $$q_i = x_iW^Q$$
>
>     $$k_i = x_iW^K$$
>
>     $$v_i = x_iW^V$$
>
> 2.  **Pontua√ß√£o:** O score entre a palavra de foco $x_i$ e todas as palavras precedentes $x_j$ (incluindo ela pr√≥pria) √© calculado atrav√©s do produto escalar entre as representa√ß√µes *query* e *key*, normalizado pela raiz quadrada da dimens√£o dos vetores:
>
>    $$score(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}$$
>
>     onde $d_k$ √© a dimens√£o dos vetores *key* e *query*.
>
> 3.  **Normaliza√ß√£o:** Os scores s√£o transformados em pesos normalizados $\alpha_{ij}$ utilizando a fun√ß√£o *softmax*:
>
>     $$\alpha_{ij} = \frac{exp(score(x_i, x_j))}{\sum_{k=1}^i exp(score(x_i, x_k))}, \forall j \leq i$$
>
> 4. **Pondera√ß√£o:** Os pesos normalizados s√£o usados para ponderar os vetores *value* e produzir uma representa√ß√£o contextualizada $a_i$ para a palavra de foco:
>
>     $$a_i = \sum_{j=1}^i \alpha_{ij} v_j$$
>
> Esses quatro passos combinam opera√ß√µes lineares (proje√ß√µes), n√£o lineares (*softmax*) e produtos escalares para criar representa√ß√µes contextuais ricas e relevantes para cada palavra em uma sequ√™ncia.

**Proposi√ß√£o 1** (Equival√™ncia Matricial da Autoaten√ß√£o): O processo de autoaten√ß√£o pode ser expresso de forma concisa utilizando nota√ß√£o matricial, o que √© fundamental para sua implementa√ß√£o eficiente em GPUs.
  *   Seja $X$ a matriz de entrada onde cada linha corresponde a um vetor de entrada $x_i$, as matrizes $Q$, $K$ e $V$ s√£o obtidas por:
  $$Q = XW^Q$$
  $$K = XW^K$$
  $$V = XW^V$$

  *   A matriz de *scores* $S$ √© dada por:
  $$S = \frac{QK^T}{\sqrt{d_k}}$$

  *   A matriz de pesos normalizados $A$ √© obtida aplicando *softmax* nas linhas de $S$:
    $$A_{ij} = \frac{exp(S_{ij})}{\sum_{k=1}^i exp(S_{ik})}, \forall j \leq i$$
    Note que na autoaten√ß√£o padr√£o, essa soma considera todos os elementos de $K$. No caso de *masked self-attention* a soma se restringe aos elementos at√© a posi√ß√£o $i$.
  *  Finalmente, a matriz de representa√ß√µes contextuais $Z$ √© calculada por:
   $$Z = AV$$
  *   Esta representa√ß√£o matricial condensa todas as opera√ß√µes de proje√ß√£o, pontua√ß√£o, normaliza√ß√£o e pondera√ß√£o, oferecendo uma vis√£o mais concisa do processo de autoaten√ß√£o. Al√©m disso, a formula√ß√£o matricial permite o processamento paralelo de todos os vetores de entrada simultaneamente, o que √© essencial para o desempenho dos Transformers em hardware moderno.

### Conclus√£o
Em resumo, o processo de aten√ß√£o no Transformer envolve a proje√ß√£o de vetores de entrada em espa√ßos de *query*, *key* e *value*, o c√°lculo de *scores* de similaridade entre palavras, a normaliza√ß√£o desses *scores* usando *softmax* e a pondera√ß√£o dos vetores *value* com base nos pesos gerados. Esse processo permite que o modelo capture as rela√ß√µes complexas entre as palavras em uma sequ√™ncia, produzindo representa√ß√µes contextuais que s√£o cruciais para o desempenho dos Transformers em tarefas de processamento de linguagem natural. A proje√ß√£o por meio das matrizes $W^Q, W^K,$ e $W^V$ permite que o modelo aprenda diferentes representa√ß√µes para cada palavra de acordo com seu papel no contexto [^6]. A normaliza√ß√£o dos scores por $\sqrt{d_k}$ estabiliza o treinamento, enquanto o softmax transforma os scores em pesos normalizados [^7]. A combina√ß√£o dos *values* com esses pesos gera representa√ß√µes contextuais ricas, demonstrando o poder do mecanismo de autoaten√ß√£o [^6]. A sequ√™ncia de proje√ß√£o, pontua√ß√£o, normaliza√ß√£o e pondera√ß√£o, que opera em paralelo para todas as posi√ß√µes, √© o que torna os Transformers t√£o eficientes e eficazes para o processamento de linguagem natural [^8].

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^3]: 10.1.1 Transformers: the intuition
[^5]: 10.1.3 Self-attention more formally
[^6]: 6 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^7]: 10.1 ‚Ä¢ THE TRANSFORMER: A SELF-ATTENTION NETWORK 7
[^8]: 10.1.4 Parallelizing self-attention using a single matrix X
## 10.  <SUBTOPIC_PLACEHOLDER>: Causal Language Modeling with Transformers
### Introdu√ß√£o
Em continuidade ao estudo dos **Transformers** e sua arquitetura fundamental, exploraremos neste cap√≠tulo como esses modelos s√£o aplicados especificamente para a tarefa de modelagem de linguagem causal, tamb√©m conhecida como *autoregressive language modeling*. Este tipo de modelagem, onde o modelo prediz sequencialmente cada palavra com base nas palavras precedentes, √© essencial para tarefas como gera√ß√£o de texto e outras aplica√ß√µes de processamento de linguagem natural (PLN) [^1]. Expandindo o conceito apresentado em se√ß√µes anteriores, vamos nos aprofundar na aplica√ß√£o pr√°tica dos **Transformers** nesse contexto espec√≠fico.

### Conceitos Fundamentais
O cerne da modelagem de linguagem causal com **Transformers** reside na capacidade de prever a pr√≥xima palavra em uma sequ√™ncia de texto, dado o contexto das palavras anteriores [^2]. Essa predi√ß√£o √© feita iterativamente, onde a sa√≠da do modelo em um passo √© utilizada como entrada para o pr√≥ximo passo, caracterizando o aspecto *autoregressivo*.

O processo inicia com a entrada de uma sequ√™ncia de *tokens* (que podem ser palavras, subpalavras ou caracteres) que s√£o convertidos em *embeddings* de entrada [^1]. Esses *embeddings* s√£o ent√£o alimentados em uma s√©rie de blocos de *Transformer*, cada um contendo camadas de *self-attention*, *feedforward*, conex√µes residuais e normaliza√ß√£o de camada [^7].

Na camada de *self-attention*, o mecanismo de aten√ß√£o *causal* garante que cada palavra atenda apenas √†s palavras que a precedem na sequ√™ncia, impedindo que o modelo "veja" o futuro, o que seria contraproducente para o processo de modelagem de linguagem. Essa √© uma distin√ß√£o crucial em rela√ß√£o a outras aplica√ß√µes dos **Transformers**, onde informa√ß√µes de todo o contexto podem ser utilizadas [^4]. Conforme previamente discutido, essa causalidade √© implementada mascarando a matriz $QK^T$ [^8].

> üí° **Exemplo Num√©rico: Mascaramento em Autoaten√ß√£o Causal**
>
>  Vamos considerar uma sequ√™ncia de 4 tokens, e que a matriz de scores *S* calculada sem mascaramento seja:
>
>   $S = \begin{bmatrix}
>    1.0 & 0.5 & 0.2 & 0.1 \\
>    0.8 & 1.2 & 0.6 & 0.3 \\
>    0.4 & 0.7 & 1.1 & 0.9 \\
>    0.2 & 0.5 & 0.8 & 1.3
>  \end{bmatrix}$
>
> Para a autoaten√ß√£o causal, precisamos aplicar uma m√°scara triangular inferior. A matriz de m√°scara *M* √© dada por:
>
> $M = \begin{bmatrix}
>   0 & -\infty & -\infty & -\infty \\
>   0 & 0 & -\infty & -\infty \\
>   0 & 0 & 0 & -\infty \\
>   0 & 0 & 0 & 0
>  \end{bmatrix}$
>
> A matriz de scores mascarada $S'$ √© dada por $S' = S + M$:
>
>  $S' = \begin{bmatrix}
>    1.0 & -\infty & -\infty & -\infty \\
>    0.8 & 1.2 & -\infty & -\infty \\
>    0.4 & 0.7 & 1.1 & -\infty \\
>    0.2 & 0.5 & 0.8 & 1.3
>  \end{bmatrix}$
>
>  Agora, aplicando o softmax nas linhas de $S'$, obtemos a matriz de pesos $A$. Os valores $-\infty$ na matriz de scores resultam em pesos de aten√ß√£o iguais a 0.
>
>  $A = \begin{bmatrix}
>  1.0 & 0 & 0 & 0 \\
>   0.35 & 0.65 & 0 & 0 \\
>   0.15 & 0.27 & 0.58 & 0 \\
>  0.08 & 0.18 & 0.29 & 0.45
>  \end{bmatrix}$
>
> Note como, por exemplo, o token 3 (terceira linha) s√≥ tem aten√ß√£o com rela√ß√£o aos tokens 1, 2 e 3, mas n√£o com rela√ß√£o ao token 4. Isto garante a propriedade de modelagem causal.

A sa√≠da do √∫ltimo bloco de *Transformer* √© ent√£o passada atrav√©s de uma camada linear e uma fun√ß√£o *softmax* para produzir uma distribui√ß√£o de probabilidade sobre o vocabul√°rio. Essa distribui√ß√£o representa a previs√£o do modelo para a pr√≥xima palavra na sequ√™ncia.

O treinamento do modelo √© realizado atrav√©s de um processo de auto-supervis√£o, onde o modelo aprende a predizer a pr√≥xima palavra em uma sequ√™ncia de texto usando a sequ√™ncia real como supervis√£o [^2]. A fun√ß√£o de perda utilizada √© a *cross-entropy*, que mede a diferen√ßa entre a distribui√ß√£o de probabilidade prevista pelo modelo e a distribui√ß√£o real da pr√≥xima palavra.

Para gera√ß√£o de texto, uma vez treinado o modelo, utiliza-se uma estrat√©gia de *decoding*, que pode variar entre *greedy decoding*, *beam search*, *top-k sampling* ou *temperature sampling* [^1]. Essas estrat√©gias permitem gerar texto de forma sequencial, palavra por palavra, com base nas previs√µes do modelo.  No greedy decoding, por exemplo, escolhe-se sempre a palavra de maior probabilidade no passo corrente, enquanto os m√©todos de *sampling* introduzem aleatoriedade para diversificar o texto gerado [^2].

> üí° **Exemplo Num√©rico: Gera√ß√£o de Texto com Decoding Greedy**
>
>  Suponha que, ap√≥s o treinamento, o modelo receba a sequ√™ncia "O gato est√°" e, ap√≥s a passagem pela rede, a distribui√ß√£o de probabilidade de sa√≠da para a pr√≥xima palavra seja:
>
>   $P(\text{dormindo}) = 0.6$
>   $P(\text{pulando}) = 0.2$
>    $P(\text{correndo}) = 0.15$
>    $P(\text{comendo}) = 0.05$
>
> No *greedy decoding*, a palavra com maior probabilidade seria selecionada, neste caso, "dormindo". A pr√≥xima entrada para o modelo seria ent√£o "O gato est√° dormindo", e o processo se repetiria at√© gerar uma senten√ßa completa.
>
> M√©todos de *sampling* introduzem alguma aleatoriedade, por exemplo, usando *temperature sampling* o modelo poderia escolher a palavra "pulando" com uma probabilidade de 0.2.

A capacidade de processar sequ√™ncias longas com o uso de *self-attention* e a efici√™ncia da computa√ß√£o paralela s√£o caracter√≠sticas cruciais que permitem aos **Transformers** alcan√ßar desempenhos superiores na modelagem de linguagem em compara√ß√£o a modelos de arquiteturas como *RNNs*. Essa capacidade de modelar depend√™ncias de longo alcance permite que os **Transformers** capturem o contexto de forma mais eficaz [^7], permitindo a gera√ß√£o de textos mais coerentes e contextualmente relevantes.

**Lema 1.1** (Conex√£o entre Masked Self-Attention e Modelagem Causal): A opera√ß√£o de *masked self-attention* √© essencial para modelagem de linguagem causal porque garante que, durante a fase de treinamento, a predi√ß√£o de uma palavra em particular ($x_i$) dependa apenas das palavras precedentes ($x_1, x_2, ..., x_{i-1}$). Isso √© obtido atrav√©s da aplica√ß√£o de uma m√°scara triangular inferior sobre a matriz de scores ($S$), que impede que o c√°lculo dos pesos de aten√ß√£o incorpore informa√ß√µes do "futuro" (palavras ap√≥s $x_i$). Formalmente, a m√°scara garante que $\alpha_{ij} = 0$ se $j > i$ no c√°lculo de autoaten√ß√£o.

*   **Prova:**
I.  A matriz de *scores* $S$ √© calculada como $S = \frac{QK^T}{\sqrt{d_k}}$, onde $Q$ e $K$ s√£o as matrizes de *queries* e *keys*, respectivamente.
II. Na autoaten√ß√£o causal, uma m√°scara $M$ √© aplicada em $S$, tal que $M_{ij} = -\infty$ se $j > i$, e $M_{ij} = 0$ caso contr√°rio.
III. Assim, a matriz de scores *mascarada* $S'$ √© definida por:
     $$S' = S + M $$
IV. Quando o *softmax* √© aplicado em $S'$, o resultado √© que, para $j > i$, temos $\alpha_{ij} = \frac{exp(-\infty)}{\sum_k exp(S'_{ik})} = 0$.
    Essa opera√ß√£o impede a depend√™ncia de palavras futuras no c√°lculo dos pesos de aten√ß√£o para a palavra $x_i$.

    Essa m√°scara garante que cada posi√ß√£o tenha acesso somente as informa√ß√µes das posi√ß√µes anteriores. Esse √© um requerimento fundamental para construir um modelo de linguagem causal. ‚ñ†

### Conclus√£o
Nesta se√ß√£o, exploramos como os **Transformers** s√£o adaptados para a modelagem de linguagem causal, um componente essencial em muitas tarefas de PLN. Vimos como a arquitetura, combinada com mecanismos como a *self-attention* causal e estrat√©gias de *decoding*, permite a gera√ß√£o de texto de alta qualidade. Em se√ß√µes subsequentes, iremos explorar como esses modelos podem ser ainda mais adaptados para tarefas de *fine-tuning*, *prompting* e aplica√ß√µes em cen√°rios de encoder-decoder. A discuss√£o aqui fornece uma base s√≥lida para a compreens√£o de como a arquitetura do **Transformer**, explorada em cap√≠tulos anteriores, pode ser utilizada para construir modelos de linguagem poderosos e vers√°teis.

### Refer√™ncias
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright ¬© 2023. All rights reserved. Draft of February 3, 2024.
[^2]:  2 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^4]: 4 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^6]: 6 CHAPTER 10 ‚Ä¢ TRANSFORMERS AND LARGE LANGUAGE MODELS
[^7]: 10.1 ‚Ä¢ THE TRANSFORMER: A SELF-ATTENTION NETWORK 7
[^8]: 10.1.4 Parallelizing self-attention using a single matrix X
<!-- END -->
