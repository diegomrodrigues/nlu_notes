{
  "topics": [
    {
      "topic": "Masked Language Models",
      "sub_topics": [
        "Masked Language Models (MLMs) são modelos de linguagem pré-treinados que utilizam um paradigma de modelagem de linguagem mascarada. Neste paradigma, o modelo tenta prever palavras 'mascaradas' dentro de uma sequência, em contraste com modelos causais que preveem a próxima palavra. MLMs empregam um codificador transformer bidirecional para entender o contexto em ambas as direções, diferentemente dos modelos causais que usam apenas o contexto da esquerda para a direita. Essa bidirecionalidade permite capturar melhor as relações contextuais, tornando os MLMs adequados para tarefas de compreensão e inferência, mas menos para geração de texto.",
        "A função de atenção em MLMs difere dos modelos causais ao permitir que o mecanismo de atenção opere sobre toda a entrada, utilizando informações tanto do contexto anterior quanto do posterior de cada token. Modelos bidirecionais como o BERT utilizam um encoder transformer que habilita a atenção por toda a sequência de entrada, removendo a máscara de atenção presente em modelos causais. Matematicamente, o processo de atenção em transformadores bidirecionais usa uma função softmax para calcular pesos de atenção, ponderando os valores de entrada e permitindo que o modelo aprenda a importância relativa de cada parte do contexto para prever o token mascarado."
        ,
        "O treinamento de MLMs envolve a corrupção de textos de entrada através do mascaramento aleatório de tokens, substituição por tokens aleatórios ou deixando-os inalterados. O objetivo é treinar o modelo para prever o token original com base no contexto disponível. No BERT, por exemplo, 15% dos tokens são amostrados para treinamento: 80% são mascarados com um token especial '[MASK]', 10% são substituídos por tokens aleatórios e 10% permanecem inalterados. Essa estratégia de mascaramento probabilístico introduz variação e robustez ao aprendizado do modelo, tornando-o eficaz para tarefas de classificação e compreensão.",
        "A função de perda para o treinamento de MLMs é baseada na entropia cruzada, que quantifica a diferença entre as distribuições de probabilidade previstas para os tokens mascarados e as distribuições reais. O objetivo do treinamento é minimizar essa diferença, otimizando os parâmetros do modelo para prever com precisão as palavras mascaradas. O processo de treinamento envolve o cálculo da perda de entropia cruzada entre as predições e o texto original, ajustando os parâmetros do modelo por retropropagação para corresponder as predições às palavras reais corrompidas. O modelo pre-treinado fornece uma distribuição de probabilidade sobre os tokens, que é usada para calcular a perda de entropia cruzada durante o treinamento, servindo como base para a aprendizagem dos embeddings contextuais.",
        "Após o pré-treinamento, um MLM pode ser adaptado para tarefas específicas através de 'fine-tuning'. Este processo envolve adicionar camadas de classificação ou regressão sobre a camada superior do modelo e treinar essas camadas adicionais usando dados de treinamento específicos para a tarefa desejada."
        ,
        "Contextual embeddings, gerados por MLMs, são representações vetoriais de palavras que capturam o significado da palavra em contexto. Diferente de static embeddings, onde cada palavra tem uma representação única, contextual embeddings variam dependendo do contexto em que a palavra é usada. Modelos causais são generativos, enquanto MLMs são encoders, produzindo representações contextuais dos tokens de entrada, adequadas para tarefas interpretativas."
      ]
    },
    {
      "topic": "Bidirectional Transformer Encoders",
      "sub_topics": [
        "Bidirectional transformer encoders são arquiteturas de redes neurais que processam sequências de texto em ambas as direções, permitindo capturar dependências contextuais ricas entre as palavras. A operação central é a autoatenção, que calcula a importância de cada palavra no contexto das outras. Codificadores bidirecionais utilizam autoatenção para contextualizar as representações de tokens de entrada, considerando toda a sequência de entrada. Ao contrário dos modelos causais, os codificadores bidirecionais não são generativos e produzem uma codificação de cada token, sendo mais adequados para tarefas interpretativas.",
        "A arquitetura de encoders transformadores bidirecionais remove o mascaramento da matriz QKT no cálculo da atenção, permitindo que a atenção abranja toda a sequência de entrada. Essa arquitetura difere dos transformers causais na função de atenção, que não é causal, habilitando cada token a atender tanto tokens precedentes quanto seguintes. A implementação envolve remover a máscara da matriz QKT, mantendo a computação de atenção e a arquitetura de bloco transformer idênticas aos modelos causais, incluindo camadas feedforward e normalização de camadas. A arquitetura do encoder transformer bidirecional usa atenção auto direcionada (self-attention) para obter representações contextuais de cada token na sequência de entrada. ",
        "Matematicamente, a função de atenção em transformadores bidirecionais pode ser expressa como uma combinação de consultas (Q), chaves (K) e valores (V), onde os pesos de atenção são calculados por meio de um softmax sobre os produtos escalares entre consultas e chaves. O mecanismo de autoatenção utiliza três matrizes de projeção: Query (Q), Key (K) e Value (V). A matriz de atenção é calculada como softmax(QKT / raiz(dk)), onde dk é a dimensão das matrizes Q e K. Essa matriz determina como os vetores de cada palavra são combinados para formar um vetor de contexto.",
        "A entrada para o encoder é uma série de tokens de subpalavras computados via WordPiece ou SentencePiece, o que significa que o pré-processamento da sequência de entrada deve ser feito por meio da tokenização de subpalavras antes de ser enviada para o modelo. Modelos transformer bidirecionais geralmente usam subword tokenization (WordPiece ou SentencePiece), que tokeniza as sentenças de entrada em unidades menores que palavras, e todos os processamentos posteriores ocorrem nessas unidades. Para algumas tarefas de processamento de linguagem natural que requerem noções de palavras completas (ex: parsing) é necessário um mapeamento de tokens de subpalavras de volta para palavras.",
        "O modelo BERT original é uma arquitetura relativamente simples composta por um vocabulário subpalavra de 30.000 tokens, camadas ocultas de dimensionalidade 768 e 12 camadas de blocos transformer com 12 camadas de autoatenção multi-cabeça. Versões maiores como o XLM-ROBERTa são treinadas com vocabulários maiores (250.000) para acomodar vários idiomas, e também têm mais camadas (24) e mais cabeças de atenção multi-cabeça (16), mas com o mesmo método de treinamento."
      ]
    },
    {
      "topic": "Training Bidirectional Encoders",
      "sub_topics": [
        "A abordagem principal para treinar modelos bidirecionais é o Masked Language Modeling (MLM). O MLM utiliza texto não anotado e aleatoriamente mascara, substitui ou mantém inalterados alguns tokens de cada sequência de treinamento. A tarefa de aprendizado é prever os tokens originais mascarados. Este método de treinamento é uma forma de 'denoising', onde o modelo é treinado para recuperar a entrada original após a introdução de ruído.",
        "Durante o treinamento com MLM, tokens de entrada são selecionados aleatoriamente e substituídos por [MASK], tokens aleatórios do vocabulário, ou deixados inalterados. O modelo é treinado para predizer o token original com base nos tokens restantes. Em BERT, 15% dos tokens de entrada são amostrados para aprendizado. Desses, 80% são substituídos por [MASK], 10% são substituídos por tokens aleatórios, e os 10% restantes são deixados inalterados. A função de perda no treinamento de modelos de linguagem masked é calculada usando a entropia cruzada entre a distribuição de probabilidade prevista sobre o vocabulário e os tokens mascarados reais, com o objetivo de minimizar a perda.",
        "O modelo BERT original também utilizava a tarefa de predição da próxima sentença (NSP), que visava classificar se dois segmentos de texto eram sequências adjacentes reais do corpus de treinamento. O modelo era apresentado com pares de sentenças e treinado para prever se eles consistiam em um par real de sentenças adjacentes ou um par não relacionado de sentenças. A perda de NSP era usada juntamente com a perda de MLM para a fase de treinamento. No entanto, modelos mais recentes como RoBERTa descontinuaram o uso de NSP, utilizando uma técnica de treinamento simplificada com sequências contíguas de sentenças para maior eficiência."
        ,
        "Para o treinamento de modelos multilinguais, é crucial balancear a representação de diferentes idiomas nos dados para evitar enviesar o vocabulário para um idioma mais frequente. Uma abordagem comum é dividir os dados de treinamento em subcorpora por idioma e reajustar as probabilidades de amostragem para beneficiar idiomas menos representados."
      ]
    },
    {
      "topic": "Contextual Embeddings",
      "sub_topics": [
        "Contextual embeddings são representações vetoriais de tokens que incorporam o contexto em que aparecem. Ao contrário de embeddings estáticos, que atribuem um vetor fixo a cada palavra, embeddings contextuais são dinâmicos e variam de acordo com o contexto da sentença, capturando nuances de significado em diferentes situações. Modelos de linguagem pré-treinados geram embeddings contextuais utilizando as camadas do transformer, onde as saídas dos transformers para cada token da entrada formam esses embeddings. Esses embeddings capturam a complexidade semântica e sintática da linguagem.",
        "Contextual embeddings podem ser utilizados em diversas tarefas, incluindo medir a similaridade entre palavras em diferentes contextos e em tarefas de processamento de linguagem natural que exigem conhecimento de significados e relações contextuais, como word sense disambiguation (WSD). Para medir a similaridade semântica, a função cosseno pode ser utilizada para comparar embeddings contextuais e quantificar a proximidade semântica entre instâncias de palavras ou frases, considerando o contexto em que aparecem.",
        "As contextual embeddings são produzidas pelas camadas superiores dos modelos de linguagem, que capturam o significado das palavras e suas relações com outros termos na sequência. As representações contextuais podem ser derivadas usando o vetor da última camada do modelo ou através da média dos vetores das últimas camadas, resultando em vetores que representam os significados contextuais. Para obter embeddings contextuais mais robustas, pode-se calcular a média dos vetores de saída de várias camadas inferiores do modelo.",
        "A similaridade semântica entre duas palavras em um contexto específico pode ser calculada usando a similaridade do cosseno entre seus embeddings contextuais. No entanto, embeddings contextuais podem apresentar anisotropia, a tendência de vetores apontarem para a mesma direção. Para mitigar esse problema, os embeddings podem ser normalizados (z-scoring) antes do cálculo da similaridade do cosseno, embora o cosseno possa subestimar a similaridade de palavras muito frequentes mesmo após a padronização."
      ]
    },
    {
      "topic": "Fine-Tuning for Classification",
      "sub_topics": [
        "Fine-tuning é um processo para adaptar modelos de linguagem pré-treinados a tarefas específicas. Isso é feito adicionando camadas específicas da aplicação, conhecidas como 'heads', sobre o modelo pré-treinado e treinando os parâmetros dessas camadas com dados rotulados para a tarefa alvo. Em geral, os parâmetros do modelo base pré-treinado são mantidos congelados ou ajustados minimamente durante o fine-tuning, focando o aprendizado nos parâmetros adicionados para a tarefa específica. Este processo permite aplicar o conhecimento generalizado aprendido durante o pré-treinamento em aplicações práticas.",
        "O processo de fine-tuning envolve o uso de dados rotulados da tarefa alvo para treinar os pesos da camada classificadora adicional. A função de perda é calculada entre a saída do modelo e os rótulos verdadeiros, e a retropropagação ajusta os pesos da camada e, em alguns casos, os pesos do modelo pré-treinado. As saídas dos transformadores bidirecionais são utilizadas como entradas para os classificadores.",
        "As tarefas de classificação para fine-tuning podem incluir classificação de sequências (classificar um texto inteiro), classificação de pares de sequências (classificar a relação entre dois textos) e classificação de rótulos de sequência (atribuir um rótulo a cada token em um texto). Cada tipo de tarefa exige uma abordagem de fine-tuning específica, mas a lógica geral é similar.",
        "Na classificação de sequência, o objetivo é prever um rótulo para toda a sequência de entrada. Isso é alcançado adicionando uma camada de classificação sobre a saída do transformador para o token [CLS], que representa toda a sequência. O treinamento utiliza dados supervisionados para ajustar os parâmetros do classificador e, opcionalmente, ajustar minimamente o modelo base por retropropagação e otimização da perda de entropia cruzada.",
        "A classificação de pares de sentenças utiliza um processo de fine-tuning similar ao pré-treinamento com o objetivo de Next Sentence Prediction. O modelo é apresentado com dois textos e deve classificar sua relação, utilizando o vetor de saída do token [CLS] como entrada para um classificador. Para tarefas de classificação de pares de sequência, a entrada consiste em dois segmentos separados por um token especial [SEP], e o processo de classificação ocorre de maneira similar ao da classificação de sequência.",
        "Em sequence labeling, o modelo atribui um rótulo para cada token em uma sequência. O modelo usa o vetor de saída para cada token na sequência, passando para uma camada de classificação, ou para uma camada de conditional random field (CRF)."
        ,
        "Para tarefas de classificação, a camada de saída do modelo de linguagem é substituída por um classificador que calcula a distribuição de probabilidade sobre as classes alvo. A função de perda utilizada durante o ajuste fino é tipicamente a entropia cruzada."
      ]
    },
    {
      "topic": "Fine-Tuning for Sequence Labeling",
      "sub_topics": [
        "O ajuste fino para sequence labeling envolve otimizar os parâmetros do modelo e do classificador com base em uma função de perda, tipicamente entropia cruzada, para que o modelo preveja os rótulos corretos para cada token na sequência.",
        "O problema de sequence labeling, que atribui rótulos a cada token na sequência, utiliza uma arquitetura similar ao fine-tuning, onde as saídas do transformador são levadas para um classificador, ou um CRF, para prever a sequência de rótulos.",
        "Sequence labeling é uma tarefa que envolve a atribuição de labels a cada token em uma sequência de entrada, como em Named Entity Recognition (NER).",
        "BIO tagging é um método utilizado para modelar o NER como uma tarefa de sequence labeling, definindo o início, o interior ou a parte de fora de uma entidade nomeada.",
        "O etiquetamento BIO é usado para rotular os limites e tipos de entidades nomeadas na sequência de entrada. As tags BIO (B-início, I-interior, O-fora) são combinadas com os tipos de entidades para representar as entidades e a informação sobre a extensão.",
        "O reconhecimento de entidade nomeada (NER) utiliza modelos de aprendizado de máquina treinados para identificar entidades nomeadas em texto, tais como pessoas, organizações ou locais. O ajuste fino é um método eficaz para adaptar modelos de linguagem pré-treinados a este problema."
      ]
    },
    {
      "topic": "Fine-Tuning for Sequence Labelling: Named Entity Recognition",
      "sub_topics": [
        "Named entity recognition (NER) é uma tarefa de sequence labeling onde o modelo identifica e classifica as named entities (como nomes de pessoas, lugares ou organizações) em um texto. Na rotulagem de sequência, o objetivo é atribuir rótulos a cada token de uma sequência, com a tarefa de reconhecimento de entidade nomeada sendo um exemplo comum onde entidades como pessoas, organizações e locais são identificados em textos. O NER busca por textos que correspondam a nomes próprios (pessoas, locais, organizações) e as classifica em categorias.",
        "O reconhecimento de entidades nomeadas (NER) utiliza o ajuste fino para identificar e classificar entidades em textos. A implementação típica de um modelo NER envolve a aplicação de técnicas de marcação BIO/BIOES para converter o problema em uma rotulação de sequência, e então usa um modelo de linguagem para obter os embeddings contextuais e o classificador para gerar uma tag para cada token.",
        "A abordagem de rotulagem BIO é usada no NER para rotular tokens começando uma entidade com um rótulo 'B', tokens dentro de uma entidade com um rótulo 'I' e tokens fora de qualquer entidade com um rótulo 'O', para cada classe de entidade. O formato BIO tagging é usado para representar limites e tipos de entidades nomeadas, com os rótulos B, I e O marcando o início, o interior e o exterior de uma entidade, respectivamente. O modelo de rotulagem de sequência usa um classificador para cada token de entrada para prever um conjunto de rótulos usando o modelo transformer bidirecional. O treinamento para NER utiliza o conceito de BIO tagging, em que os tokens no início de um intervalo de uma entidade nomeada são marcados com 'B-', os tokens no interior com 'I-', e os tokens que não fazem parte de entidades nomeadas com 'O'. O modelo é treinado para prever as classes de BIO a cada token da sequência.",
        "O processo de NER envolve o uso de uma camada de classificação que atribui uma label a cada token no texto. Um esquema de tagging como o BIO ou o BIOES são utilizados para codificar a informação sobre os limites e o tipo de cada named entity. Depois do treinamento, as layers de classificação atribuem um rótulo a cada token no texto, indicando se o token faz parte de um named entity ou não e de que tipo. O modelo é treinado com dados que contêm o texto original e as marcações BIO. Os modelos de linguagem com encoders transformers bidirecionais são altamente eficazes para esta tarefa. Para sequence labeling, a saída de cada token é passada para um classificador, gerando uma distribuição de softmax sobre o conjunto de labels. Modelos de rotulagem de sequência utilizam classificadores que produzem distribuições de softmax sobre o conjunto de rótulos para cada token, com a função de perda de entropia cruzada entre as distribuições e os rótulos. A implementação da tarefa de rotulagem de sequência envolve o uso de um classificador sobre os vetores de saída do modelo para cada token e a aplicação da loss de cross-entropia entre as predições e os rótulos de cada token. O vetor de saída final correspondente a cada token é passado para um classificador que produz uma distribuição softmax sobre o conjunto possível de rótulos. Uma abordagem de ganância é usada para obter os rótulos finais para toda a sequência. Durante o treinamento do NER, deve-se usar um conjunto de treinamento supervisionado em formato BIO, onde os intervalos de texto nomeados são rotulados por tipo.",
        "A avaliação do NER é feita usando métricas como recall, precision e F1-score, que avaliam a capacidade do modelo de identificar corretamente os intervalos de entidades nomeadas e seus tipos. As métricas de avaliação usadas no NER incluem precisão, revocação e F1-score, avaliando o desempenho do modelo, e para avaliar a significância da diferença entre modelos, os testes bootstrap pareados são utilizados. No processo de avaliação, as previsões do modelo são comparadas com os rótulos reais usando as métricas recall, precision e F1-score. Estas métricas ajudam a avaliar o quão bem o modelo é capaz de prever corretamente o named entity.",
        "As implementações práticas do NER envolvem a necessidade de um alinhamento entre os rótulos BIO em nível de palavras com os rótulos em nível de subword tokens durante o treinamento e a decodificação dos rótulos de volta para o nível de palavra após a classificação. No caso de o vocabulário do modelo ser o de subtokens, e a anotação se dar em nível de palavra, a anotação é distribuída para os subtokens e, na decodificação, os rótulos do subtokens são combinados para formar um rótulo no nível da palavra. O texto é geralmente segmentado no nível da palavra para dados de treinamento de NER, que normalmente contém marcações BIO. O modelo de linguagem usa WordPiece tokenization que pode ter um problema de falta de alinhamento com dados de treinamento NER. Ao treinar e decodificar, o modelo precisa ter um mecanismo para atribuir rótulos BIO aos tokens de subpalavra. Uma particularidade do NER é o desalinhamento entre as marcações BIO/BIOES e as subpalavras geradas pelo tokenizer, o que necessita de um tratamento especial. Uma abordagem simples para lidar com o desalinhamento é aplicar a tag da palavra à qual a subpalavra pertence. Mais complexas abordagens combinam a distribuição de probabilidade das tags das subpalavras para encontrar uma tag de nível de palavra. Os modelos de NER são avaliados usando recall, precisão e medidas F1. Uma abordagem para lidar com os desalinhamentos de tokenização é atribuir o rótulo gold-standard associado a cada palavra a todos os seus subword tokens. Na decodificação, a saída do rótulo para o primeiro token de cada palavra é usada como rótulo predito da palavra. Uma abordagem mais complexa combina as distribuições de probabilidade dos rótulos nos subwords para prever rótulos no nível da palavra.",
        "O treinamento de NER usa um dataset de textos com as marcações BIO/BIOES pré-atribuídas. Durante o treinamento, as probabilidades de cada token ser um label BIO são ajustadas. Na fase de avaliação, uma etiqueta BIO é atribuída a cada token.  O label de cada token da saída é definido usando o argmax da distribuição de probabilidades (abordagem greedy)."
      ]
    },
    {
      "topic": "Sequence Labeling e Named Entity Recognition",
      "sub_topics": [
        "Sequence labeling (rotulação de sequência) é a tarefa de atribuir um rótulo a cada token em uma sequência. Named entity recognition (NER) é uma aplicação comum de sequence labeling, focada em identificar e classificar nomes de entidades em texto.",
        "O treinamento para sequence labeling utiliza dados rotulados para cada token. O modelo gera uma distribuição de probabilidade sobre o conjunto de rótulos, e a função de perda é calculada comparando essa distribuição com os rótulos reais, permitindo que o modelo aprenda a predizer os rótulos corretos para cada token.",
        "BIO tagging é uma abordagem comum em NER. Utiliza rótulos 'B' (início da entidade), 'I' (dentro da entidade) e 'O' (fora da entidade) para identificar entidades e seus limites na sequência."
      ]
    }
  ]
}