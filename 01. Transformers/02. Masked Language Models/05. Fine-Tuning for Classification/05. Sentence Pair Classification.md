## Fine-Tuning para ClassificaÃ§Ã£o de Pares de SequÃªncias: Detalhes e ConexÃ£o com NSP

### IntroduÃ§Ã£o

Como vimos nos capÃ­tulos anteriores, o *fine-tuning* Ã© um processo essencial para adaptar modelos de linguagem prÃ©-treinados a tarefas especÃ­ficas [^1]. Este capÃ­tulo aprofunda o tÃ³pico da **classificaÃ§Ã£o de pares de sequÃªncias**, uma tarefa crucial no Processamento de Linguagem Natural (PLN) que envolve a classificaÃ§Ã£o da relaÃ§Ã£o semÃ¢ntica entre dois segmentos de texto [^1]. Exploraremos como o *fine-tuning* Ã© aplicado especificamente a essa tarefa, com particular atenÃ§Ã£o Ã  sua conexÃ£o com a tarefa de prÃ©-treinamento de *Next Sentence Prediction* (NSP), como as representaÃ§Ãµes sÃ£o construÃ­das, e como os classificadores sÃ£o treinados para atingir resultados Ã³timos.

### ClassificaÃ§Ã£o de Pares de SequÃªncias: ConexÃ£o com Next Sentence Prediction

A tarefa de **classificaÃ§Ã£o de pares de sequÃªncias** visa classificar a relaÃ§Ã£o entre dois segmentos de texto, que podem ser frases, parÃ¡grafos ou atÃ© documentos [^1]. Tarefas comuns nessa categoria incluem detecÃ§Ã£o de parÃ¡frase, reconhecimento de inferÃªncia textual (NLI) e avaliaÃ§Ã£o de coerÃªncia discursiva. A classificaÃ§Ã£o de pares de sequÃªncias tem uma conexÃ£o intrÃ­nseca com o objetivo de prÃ©-treinamento de *Next Sentence Prediction* (NSP), como veremos a seguir.

#### Revisitando o NSP
No capÃ­tulo anterior, exploramos como o *Next Sentence Prediction* (NSP) Ã© uma tarefa de prÃ©-treinamento usada em modelos como o BERT, onde o modelo Ã© apresentado a pares de sentenÃ§as e deve classificar se o segundo segmento segue o primeiro em um texto real [^1]. Em BERT, 50% dos pares de treinamento sÃ£o pares reais adjacentes, enquanto os outros 50% sÃ£o construÃ­dos com um segundo segmento aleatÃ³rio [^1].
O NSP tem como objetivo fazer com que o modelo aprenda a capturar relaÃ§Ãµes contextuais entre segmentos de texto [^1]. O *fine-tuning* para classificaÃ§Ã£o de pares de sequÃªncias, portanto, aproveita o aprendizado prÃ©vio do modelo em NSP, como veremos a seguir.

#### Fine-Tuning e a ConexÃ£o com o NSP

O *fine-tuning* para classificaÃ§Ã£o de pares de sequÃªncias, como mencionado anteriormente, envolve a apresentaÃ§Ã£o do modelo com pares de segmentos separados por um token especial `[SEP]` e utiliza o vetor correspondente ao token `[CLS]` para classificaÃ§Ã£o [^1]. Este procedimento se assemelha ao processo de treinamento com o objetivo NSP, onde o modelo tambÃ©m Ã© apresentado com pares de segmentos de texto separados pelo token `[SEP]` e deve classificar sua relaÃ§Ã£o (i.e., se sÃ£o ou nÃ£o contÃ­guas) [^1].

A principal diferenÃ§a entre o NSP e o *fine-tuning* para classificaÃ§Ã£o de pares de sequÃªncias Ã© a tarefa de classificaÃ§Ã£o que deve ser aprendida pelo modelo. Enquanto o NSP envolve uma classificaÃ§Ã£o binÃ¡ria (contÃ­guas/nÃ£o contÃ­guas), o *fine-tuning* para classificaÃ§Ã£o de pares de sequÃªncias envolve classificar a relaÃ§Ã£o semÃ¢ntica entre os pares de texto, que pode ser multi-classe (e.g., parÃ¡frase/nÃ£o parÃ¡frase; *entailment*, *contradiction*, *neutral*).

**Lema 1:** A similaridade na arquitetura e nos tokens de entrada entre o prÃ©-treinamento com NSP e o *fine-tuning* para classificaÃ§Ã£o de pares de sequÃªncias permite que o modelo transfira conhecimento aprendido durante o prÃ©-treinamento para a tarefa de classificaÃ§Ã£o, acelerando o treinamento e melhorando o desempenho.
*Prova:*
I. Tanto o NSP quanto o *fine-tuning* para classificaÃ§Ã£o de pares de sequÃªncias utilizam o token `[SEP]` para separar os dois segmentos de entrada e o vetor $h_{CLS}$ como representaÃ§Ã£o agregada do par.
II. O modelo prÃ©-treinado com NSP jÃ¡ aprendeu a capturar relaÃ§Ãµes contextuais entre segmentos, o que Ã© Ãºtil para tarefas de classificaÃ§Ã£o de pares de sequÃªncias.
III. A transferÃªncia do conhecimento aprendido durante o prÃ©-treinamento permite que o *fine-tuning* se concentre na adaptaÃ§Ã£o do modelo Ã  tarefa de classificaÃ§Ã£o especÃ­fica, ao invÃ©s de aprender as relaÃ§Ãµes entre os segmentos do zero.
IV. Portanto, a similaridade entre as tarefas de prÃ©-treinamento e *fine-tuning* leva a melhorias no treinamento. â– 

**Lema 1.1:**  A eficÃ¡cia da transferÃªncia de conhecimento do NSP para tarefas de classificaÃ§Ã£o de pares de sequÃªncia pode variar dependendo da similaridade entre a tarefa de prÃ©-treinamento e a tarefa de *fine-tuning*, bem como da qualidade dos dados de *fine-tuning*.
*Prova:*
I.  Se a tarefa de *fine-tuning* for semanticamente distante do objetivo do NSP (i.e., identificar se duas sentenÃ§as sÃ£o adjacentes em um texto), a transferÃªncia de conhecimento pode ser menos eficaz.
II. A qualidade dos dados de *fine-tuning* tambÃ©m influencia. Se os dados de *fine-tuning* forem ruidosos ou nÃ£o representativos da tarefa, o modelo poderÃ¡ nÃ£o aprender adequadamente.
III.  Em tarefas onde as relaÃ§Ãµes semÃ¢nticas sÃ£o complexas e nÃ£o diretamente relacionadas Ã  contiguidade,  o modelo pode necessitar de mais *fine-tuning* e abordagens de modelagem mais sofisticadas para obter resultados Ã³timos.
IV. Portanto, enquanto a similaridade entre as tarefas de prÃ©-treinamento e *fine-tuning* Ã© geralmente benÃ©fica, sua eficÃ¡cia Ã© modulada por outros fatores. â– 

### Construindo RepresentaÃ§Ãµes de Pares de SequÃªncias

Como vimos no capÃ­tulo anterior, na classificaÃ§Ã£o de pares de sequÃªncias, Ã© comum concatenar os vetores de saÃ­da do token `[CLS]` correspondentes a cada sequÃªncia individual, $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$, gerando a representaÃ§Ã£o do par $h_{pair}$ [^1].
$$
h_{pair} = \text{concat}(h_{CLS}^{(1)}, h_{CLS}^{(2)})
$$
Entretanto, existem outras abordagens para construir representaÃ§Ãµes de pares de sequÃªncia.

#### ConcatenaÃ§Ã£o
A concatenaÃ§Ã£o Ã© uma abordagem simples e eficaz, que permite ao classificador acessar informaÃ§Ãµes de ambas as sequÃªncias [^1]. No entanto, a concatenaÃ§Ã£o trata ambas as sequÃªncias de forma independente e nÃ£o modela explicitamente a interaÃ§Ã£o entre elas.

#### Abordagens Alternativas
Outras abordagens incluem:
1.  **DiferenÃ§a:** Calcular a diferenÃ§a entre os vetores, $h_{diff} = h_{CLS}^{(1)} - h_{CLS}^{(2)}$, que pode capturar as relaÃ§Ãµes diferenciais entre as duas sequÃªncias.
2.  **MultiplicaÃ§Ã£o ponto a ponto:** Calcular a multiplicaÃ§Ã£o ponto a ponto, $h_{mult} = h_{CLS}^{(1)} \odot h_{CLS}^{(2)}$, que pode dar Ãªnfase a caracterÃ­sticas em comum entre as sequÃªncias.
3.  **CombinaÃ§Ã£o:** Combinar concatenaÃ§Ã£o, diferenÃ§a e multiplicaÃ§Ã£o, que pode gerar representaÃ§Ãµes mais ricas do par de sequÃªncias.
4. **AtenÃ§Ã£o:** Adicionar uma camada de atenÃ§Ã£o sobre a concatenaÃ§Ã£o, que permite que o modelo aprenda quais partes de cada sequÃªncia sÃ£o mais importantes para a classificaÃ§Ã£o.

**ProposiÃ§Ã£o 1:** A utilizaÃ§Ã£o da diferenÃ§a ou multiplicaÃ§Ã£o ponto a ponto dos vetores $h_{CLS}$ pode ser mais eficaz do que a concatenaÃ§Ã£o em certas tarefas de classificaÃ§Ã£o de pares de sequÃªncia, especialmente quando a relaÃ§Ã£o entre as sequÃªncias Ã© baseada em semelhanÃ§as ou diferenÃ§as.
*Prova:*
I. A concatenaÃ§Ã£o trata as duas sequÃªncias como duas representaÃ§Ãµes separadas que sÃ£o combinadas apenas no final, sem explicitamente modelar a relaÃ§Ã£o entre as duas representaÃ§Ãµes.
II. A diferenÃ§a permite capturar o que Ã© diferente entre as duas sequÃªncias, que pode ser relevante para identificar relaÃ§Ãµes como contradiÃ§Ã£o.
III. A multiplicaÃ§Ã£o ponto a ponto pode capturar padrÃµes em comum, que podem ser relevantes para identificar relaÃ§Ãµes como parÃ¡frase.
IV. Cada uma dessas representaÃ§Ãµes pode ser mais adequada para diferentes tarefas ou tipos de relaÃ§Ãµes entre sequÃªncias.
V. A eficÃ¡cia de uma representaÃ§Ã£o especÃ­fica depende do tipo de tarefa e dos dados, e Ã© um hiperparÃ¢metro que deve ser ajustado experimentalmente. Portanto, utilizar diferenciaÃ§Ã£o e multiplicaÃ§Ã£o pode ser mais eficaz em cenÃ¡rios especÃ­ficos. â– 

**ProposiÃ§Ã£o 1.1:** A combinaÃ§Ã£o de concatenaÃ§Ã£o, diferenÃ§a e multiplicaÃ§Ã£o ponto a ponto pode levar a uma representaÃ§Ã£o mais robusta e informativa do par de sequÃªncias, pois engloba tanto a informaÃ§Ã£o individual de cada sequÃªncia quanto suas relaÃ§Ãµes diferenciais e compartilhadas.
*Prova:*
I. A concatenaÃ§Ã£o preserva a informaÃ§Ã£o individual de cada sequÃªncia, enquanto a diferenÃ§a captura as disparidades e a multiplicaÃ§Ã£o destaca as similaridades.
II. A combinaÃ§Ã£o desses mÃ©todos fornece uma representaÃ§Ã£o que Ã© capaz de modelar diferentes tipos de relaÃ§Ãµes semÃ¢nticas entre as sequÃªncias, seja por semelhanÃ§a, diferenÃ§a ou por outros padrÃµes mais complexos.
III. A representaÃ§Ã£o combinada, que contÃ©m informaÃ§Ãµes distintas sobre as sequÃªncias e suas relaÃ§Ãµes,  pode ser utilizada como entrada para camadas subsequentes (e.g., classificaÃ§Ã£o ou atenÃ§Ã£o), permitindo uma modelagem mais rica da relaÃ§Ã£o entre as sentenÃ§as.
IV. Portanto, a representaÃ§Ã£o combinada tem potencial para melhorar o desempenho em uma variedade de tarefas de classificaÃ§Ã£o de pares de sequÃªncia, pois engloba os benefÃ­cios das trÃªs operaÃ§Ãµes isoladas. â– 

> ðŸ’¡ **Exemplo NumÃ©rico (ConstruÃ§Ã£o de RepresentaÃ§Ãµes):**
>
> Suponha que temos duas sentenÃ§as:
>
> SentenÃ§a 1: "O gato estÃ¡ no tapete."
> SentenÃ§a 2: "HÃ¡ um felino sobre o tapete."
>
> ApÃ³s passar essas sentenÃ§as por um modelo BERT, obtemos os vetores $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$, ambos com dimensÃ£o 768. Vamos usar nÃºmeros fictÃ­cios para ilustrar:
>
> $$h_{CLS}^{(1)} = [0.1, 0.2, \ldots, 0.768]$$
> $$h_{CLS}^{(2)} = [0.2, 0.1, \ldots, 0.8]$$
>
> 1. **ConcatenaÃ§Ã£o:**
>    $$ h_{pair} = \text{concat}(h_{CLS}^{(1)}, h_{CLS}^{(2)}) = [0.1, 0.2, \ldots, 0.768, 0.2, 0.1, \ldots, 0.8] $$
>     $h_{pair}$ terÃ¡ dimensÃ£o 1536.
> 2. **DiferenÃ§a:**
>    $$ h_{diff} = h_{CLS}^{(1)} - h_{CLS}^{(2)} = [0.1 - 0.2, 0.2 - 0.1, \ldots, 0.768 - 0.8] = [-0.1, 0.1, \ldots, -0.032]$$
>    $h_{diff}$ terÃ¡ dimensÃ£o 768.
> 3. **MultiplicaÃ§Ã£o ponto a ponto:**
> $$h_{mult} = h_{CLS}^{(1)} \odot h_{CLS}^{(2)} = [0.1 * 0.2, 0.2 * 0.1, \ldots, 0.768 * 0.8] = [0.02, 0.02, \ldots, 0.6144]$$
> $h_{mult}$ terÃ¡ dimensÃ£o 768.
> 4.  **CombinaÃ§Ã£o:**
>    $$h_{comb} = \text{concat}(h_{CLS}^{(1)}, h_{CLS}^{(2)}, h_{diff}, h_{mult})$$
>    $$h_{comb} = [0.1, 0.2, \ldots, 0.768, 0.2, 0.1, \ldots, 0.8, -0.1, 0.1, \ldots, -0.032, 0.02, 0.02, \ldots, 0.6144]$$
> $h_{comb}$ terÃ¡ dimensÃ£o 768 * 4 = 3072.
>
> Note que, na prÃ¡tica, os vetores $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$ nÃ£o sÃ£o tÃ£o simples e geralmente contÃ©m valores complexos gerados pelo modelo BERT. Esses exemplos ilustram como as representaÃ§Ãµes sÃ£o construÃ­das e a dimensÃ£o resultante de cada abordagem.

#### A Camada de ClassificaÃ§Ã£o

ApÃ³s a construÃ§Ã£o da representaÃ§Ã£o do par de sequÃªncias ($h_{pair}$ ou variaÃ§Ãµes), ela Ã© passada para uma camada de classificaÃ§Ã£o [^1]. Essa camada, semelhante ao caso de classificaÃ§Ã£o de sequÃªncia, pode ser uma camada linear seguida por uma funÃ§Ã£o *softmax*:
$$
\mathbf{y} = \text{softmax}(h_{pair} \mathbf{W}_P)
$$
Onde $\mathbf{W}_P$ Ã© a matriz de pesos do classificador e $\mathbf{y}$ Ã© a distribuiÃ§Ã£o de probabilidade sobre as classes de relaÃ§Ã£o entre os pares de sequÃªncias. A dimensÃ£o de $\mathbf{W}_P$ depende da dimensÃ£o de $h_{pair}$ e do nÃºmero de classes da tarefa.

A escolha da arquitetura do classificador e a funÃ§Ã£o de ativaÃ§Ã£o podem influenciar o desempenho do modelo. FunÃ§Ãµes nÃ£o lineares (como ReLU) adicionadas antes da camada softmax podem aumentar a capacidade de modelar relaÃ§Ãµes complexas entre as sequÃªncias, como vimos no capÃ­tulo anterior. AlÃ©m disso, a utilizaÃ§Ã£o de *dropout* ajuda a melhorar a generalizaÃ§Ã£o.

> ðŸ’¡ **Exemplo NumÃ©rico (Arquiteturas Alternativas):**
>
> Suponha que, em um problema de detecÃ§Ã£o de parÃ¡frase, temos dois vetores de saÃ­da de BERT, $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$, ambos com dimensÃ£o 768. Podemos:
>
> 1.  **ConcatenaÃ§Ã£o:** $h_{pair} = \text{concat}(h_{CLS}^{(1)}, h_{CLS}^{(2)})$, com dimensÃ£o 1536.
> 2.  **DiferenÃ§a:** $h_{diff} = h_{CLS}^{(1)} - h_{CLS}^{(2)}$, com dimensÃ£o 768.
> 3.  **MultiplicaÃ§Ã£o:** $h_{mult} = h_{CLS}^{(1)} \odot h_{CLS}^{(2)}$, com dimensÃ£o 768.
> 4. **CombinaÃ§Ã£o:** $h_{comb} = \text{concat}(h_{CLS}^{(1)}, h_{CLS}^{(2)}, h_{diff}, h_{mult})$, com dimensÃ£o 3840.
>
> Em cada caso, a matriz de pesos do classificador $\mathbf{W}_P$ teria dimensÃµes diferentes, correspondentes Ã  dimensÃ£o do vetor de entrada e ao nÃºmero de classes do problema. ApÃ³s a operaÃ§Ã£o de mapeamento linear, podemos ou nÃ£o utilizar funÃ§Ãµes de ativaÃ§Ã£o nÃ£o lineares, como ReLU. Por exemplo, para o caso de combinaÃ§Ã£o, podemos ter:
>
> $$
> z = h_{comb}\mathbf{W}_P
> $$
>
> $$
> a = ReLU(z)
> $$
>
> $$
> \mathbf{y} = \text{softmax}(a)
> $$
>
> Este exemplo mostra como diferentes arquiteturas, funÃ§Ãµes de ativaÃ§Ã£o e operaÃ§Ãµes podem ser utilizadas na tarefa de classificaÃ§Ã£o de pares de sequÃªncias.
>
> Suponha que tenhamos um problema de classificaÃ§Ã£o binÃ¡ria (parÃ¡frase ou nÃ£o parÃ¡frase) e que, apÃ³s combinar os vetores $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$ usando concatenaÃ§Ã£o, obtemos o vetor $h_{pair}$ com dimensÃ£o 1536. Portanto, a matriz $\mathbf{W}_P$ terÃ¡ dimensÃ£o $1536 \times 2$ (duas classes), e a saÃ­da da camada linear, $z$, serÃ¡ um vetor de dimensÃ£o 2. ApÃ³s aplicar a funÃ§Ã£o ReLU e o softmax, obtemos as probabilidades para cada classe. Por exemplo:
>
> $$
> h_{pair} = [0.1, -0.2, \ldots, 0.3] \text{  (dimensÃ£o 1536)}
> $$
> $$
> \mathbf{W}_P = \begin{bmatrix}
>  0.01 & -0.02 \\
>  0.03 &  0.04 \\
> \ldots & \ldots \\
>  -0.01 & 0.02
>\end{bmatrix} \text{ (dimensÃ£o } 1536 \times 2 \text{)}
>$$
>
> $$
> z = h_{pair} \mathbf{W}_P =  [-0.15, 0.25]
> $$
>
> $$
> a = ReLU(z) = [0, 0.25]
> $$
>
> $$
> \mathbf{y} = \text{softmax}(a) = [0.437, 0.563]
> $$
>
> No exemplo, a probabilidade de o par de sentenÃ§as ser parÃ¡frase Ã© 0.563, e a probabilidade de nÃ£o ser parÃ¡frase Ã© 0.437.

**Lema 2:** A adiÃ§Ã£o de uma camada de *attention* sobre as duas sequÃªncias, ou sobre a concatenaÃ§Ã£o, pode melhorar a capacidade do modelo de capturar relaÃ§Ãµes complexas entre as sequÃªncias.
*Prova:*
I. A camada de *attention* permite que o modelo aprenda a ponderar a importÃ¢ncia de cada token em uma sequÃªncia em relaÃ§Ã£o aos outros tokens na mesma sequÃªncia ou na outra.
II. A representaÃ§Ã£o do par de sequÃªncias, seja a concatenaÃ§Ã£o, a diferenÃ§a, a multiplicaÃ§Ã£o ou outra, pode ser utilizada como entrada da camada de *attention*.
III. A camada de *attention* gera uma representaÃ§Ã£o contextualizada das sequÃªncias, capturando suas interaÃ§Ãµes.
IV. A representaÃ§Ã£o contextualizada pelo *attention* pode, entÃ£o, ser usada para realizar a classificaÃ§Ã£o, levando a um melhor desempenho em certas tarefas.
V. Portanto, adicionar uma camada de *attention* permite melhor explorar as relaÃ§Ãµes entre as sequÃªncias. â– 

**Lema 2.1:** A escolha do tipo de *attention* (e.g., *self-attention* ou *cross-attention*) e sua arquitetura (nÃºmero de camadas, nÃºmero de cabeÃ§as de atenÃ§Ã£o) sÃ£o hiperparÃ¢metros que afetam o desempenho do modelo e devem ser definidos de forma experimental para cada tarefa.
*Prova:*
I. *Self-attention* opera sobre uma Ãºnica sequÃªncia, enquanto *cross-attention* opera entre duas sequÃªncias. A escolha entre elas depende da necessidade de modelar as relaÃ§Ãµes internas ou inter-sequÃªncias.
II. O nÃºmero de camadas de *attention* afeta a capacidade do modelo de modelar dependÃªncias complexas, e o nÃºmero de cabeÃ§as de atenÃ§Ã£o afeta a capacidade de capturar diferentes nuances da relaÃ§Ã£o entre as sequÃªncias.
III.  A escolha Ã³tima para cada hiperparÃ¢metro Ã© tarefa e contexto dependente, e uma busca sistemÃ¡tica usando validaÃ§Ã£o cruzada pode ser necessÃ¡ria para achar a melhor configuraÃ§Ã£o.
IV. Portanto, a configuraÃ§Ã£o da camada de *attention* impacta diretamente o desempenho e deve ser ajustada experimentalmente para cada tarefa especÃ­fica. â– 

> ðŸ’¡ **Exemplo NumÃ©rico (AtenÃ§Ã£o):**
>
> Em um problema de *Natural Language Inference*, apÃ³s concatenar $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$ para obter $h_{pair}$, adicionamos uma camada de *attention*, seja ela *self-attention* ou *cross-attention*, para ponderar a importÃ¢ncia de diferentes partes das sequÃªncias. Por exemplo, os pesos de atenÃ§Ã£o podem ser mais altos para as partes da sentenÃ§a 1 que sÃ£o semanticamente relevantes para a sentenÃ§a 2. A saÃ­da da camada de atenÃ§Ã£o, denotada como $h_{pair}^{att}$ pode ser utilizada para a classificaÃ§Ã£o com o classificador final.
>
> Imagine que temos as seguintes sequÃªncias e seus respectivos embeddings:
>
> SentenÃ§a 1: "O cachorro correu rapidamente."
> $h_{CLS}^{(1)}$ =  [0.1, 0.2, 0.3, 0.4, 0.5]
>
> SentenÃ§a 2: "O animal foi veloz."
> $h_{CLS}^{(2)}$ = [0.6, 0.7, 0.8, 0.9, 1.0]
>
> ApÃ³s a concatenaÃ§Ã£o, temos:
>
> $h_{pair} = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]$
>
> Suponha que a camada de *attention* (que seria mais complexa do que um simples exemplo) produza os seguintes pesos para cada elemento de $h_{pair}$:
>
> Attention Weights = [0.1, 0.1, 0.3, 0.3, 0.2, 0.2, 0.4, 0.3, 0.0, 0.1]
>
> A representaÃ§Ã£o ponderada com atenÃ§Ã£o seria:
>
> $h_{pair}^{att} =  [0.1*0.1, 0.2*0.1, 0.3*0.3, 0.4*0.3, 0.5*0.2, 0.6*0.2, 0.7*0.4, 0.8*0.3, 0.9*0.0, 1.0*0.1] = [0.01, 0.02, 0.09, 0.12, 0.10, 0.12, 0.28, 0.24, 0.00, 0.10]$
>
> Note que a camada de *attention* atribuiu pesos maiores aos elementos que representam "correu rapidamente" e "foi veloz" ($0.3, 0.3, 0.4, 0.3$), indicando que essas partes sÃ£o mais relevantes para determinar a relaÃ§Ã£o entre as sentenÃ§as (neste caso, uma relaÃ§Ã£o de *entailment*). A saÃ­da $h_{pair}^{att}$ seria entÃ£o utilizada como entrada para a camada de classificaÃ§Ã£o final. Este exemplo demonstra como o *attention* pode ponderar diferentes partes das sequÃªncias para uma melhor classificaÃ§Ã£o.

### Treinamento Supervisionado e Ajuste de ParÃ¢metros

O treinamento para a tarefa de classificaÃ§Ã£o de pares de sequÃªncias tambÃ©m envolve o uso de dados rotulados, onde cada par de sequÃªncias tem um rÃ³tulo que indica a relaÃ§Ã£o entre elas [^1]. O processo de treinamento envolve os seguintes passos:

1.  **PropagaÃ§Ã£o Direta:** Os dois segmentos de texto sÃ£o passados pelo modelo prÃ©-treinado, gerando as representaÃ§Ãµes $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$. As representaÃ§Ãµes sÃ£o combinadas (concatenadas, subtraÃ­das ou multiplicadas), opcionalmente usando uma camada de atenÃ§Ã£o. Em seguida, a representaÃ§Ã£o resultante Ã© passada para a camada de classificaÃ§Ã£o para gerar a probabilidade $\mathbf{y}$.
2.  **CÃ¡lculo da Perda:** A perda de entropia cruzada Ã© calculada com base na diferenÃ§a entre as probabilidades previstas e os rÃ³tulos verdadeiros.
3.  **RetropropagaÃ§Ã£o:** O gradiente da perda Ã© calculado em relaÃ§Ã£o aos parÃ¢metros do modelo, incluindo a matriz de pesos $\mathbf{W}_P$ e opcionalmente os pesos das camadas anteriores do transformador.
4.  **OtimizaÃ§Ã£o:** Os parÃ¢metros do modelo sÃ£o ajustados usando um otimizador, como o Adam, para minimizar a perda e otimizar o desempenho do modelo na classificaÃ§Ã£o de pares de sequÃªncias.

#### Ajuste de ParÃ¢metros do Modelo PrÃ©-Treinado
Em alguns cenÃ¡rios, o ajuste dos parÃ¢metros do modelo prÃ©-treinado (e nÃ£o apenas da camada de classificaÃ§Ã£o) pode levar a melhorias adicionais de desempenho, mas deve ser usado com cautela para nÃ£o perder a generalidade.

**CorolÃ¡rio 1.1:** O ajuste dos parÃ¢metros do modelo prÃ©-treinado deve ser feito com cautela, especialmente em cenÃ¡rios com poucos dados de *fine-tuning*, pois pode levar ao *overfitting*.
*Prova:*
I. Ajustar os parÃ¢metros do modelo prÃ©-treinado permite que o modelo se adapte mais especificamente Ã  tarefa de *fine-tuning*.
II. Em cenÃ¡rios com poucos dados, a adaptaÃ§Ã£o excessiva pode levar a que o modelo memorize padrÃµes especÃ­ficos dos dados de treinamento, resultando em *overfitting*.
III. O ajuste do modelo prÃ©-treinado deve ser feito com taxas de aprendizado menores e tÃ©cnicas de regularizaÃ§Ã£o (e.g., dropout, weight decay) para evitar o *overfitting*.
IV. Portanto, a cautela Ã© necessÃ¡ria no ajuste do modelo base, especialmente quando os dados para fine-tuning sÃ£o escassos. â– 

**CorolÃ¡rio 1.2:** O uso de tÃ©cnicas de *regularization*, como *dropout* e *weight decay*, pode mitigar o risco de *overfitting* ao ajustar os parÃ¢metros do modelo prÃ©-treinado, permitindo um ajuste mais seguro e eficaz.
*Prova:*
I.  *Dropout* desativa aleatoriamente neurÃ´nios durante o treinamento, impedindo que o modelo se torne excessivamente dependente de neurÃ´nios individuais e forÃ§ando-o a aprender representaÃ§Ãµes mais robustas.
II.  *Weight decay* penaliza pesos grandes, desencorajando o modelo de aprender padrÃµes muito especÃ­ficos dos dados de treinamento, e promovendo soluÃ§Ãµes mais gerais.
III.  Ambas as tÃ©cnicas auxiliam em generalizar melhor o modelo em dados desconhecidos e reduzir o overfitting.
IV. Portanto, ao ajustar os parÃ¢metros do modelo prÃ©-treinado, o uso de tÃ©cnicas de *regularization* permite que o modelo aprenda padrÃµes gerais em vez de memorizar os dados de treinamento, melhorando o desempenho de generalizaÃ§Ã£o. â– 

> ðŸ’¡ **Exemplo NumÃ©rico (Treinamento):**
>
> Vamos ilustrar o processo de treinamento com um exemplo simples. Suponha que temos um par de sentenÃ§as rotulado como "parÃ¡frase" (classe 1) e que a representaÃ§Ã£o combinada das sentenÃ§as $h_{pair}$  tem dimensÃ£o 1536. Usaremos o modelo com a arquitetura de concatenaÃ§Ã£o, descrito nos exemplos anteriores.  ApÃ³s a concatenaÃ§Ã£o, temos $h_{pair}$ que Ã© usada para calcular a probabilidade prevista $\mathbf{y}$ atravÃ©s de uma camada linear e softmax:
> $$
> \mathbf{y} = \text{softmax}(h_{pair} \mathbf{W}_P)
> $$
> Onde $\mathbf{W}_P$ Ã© a matriz de pesos da camada de classificaÃ§Ã£o de dimensÃ£o $1536 \times 2$. Suponha que, apÃ³s a propagaÃ§Ã£o direta, obtemos a seguinte saÃ­da:
> $$
> \mathbf{y} = [0.3, 0.7]
> $$
> Onde 0.3 Ã© a probabilidade da classe "nÃ£o parÃ¡frase" (classe 0) e 0.7 Ã© a probabilidade da classe "parÃ¡frase" (classe 1). O rÃ³tulo verdadeiro para este exemplo Ã© [0, 1] (onde 0 representa "nÃ£o parÃ¡frase" e 1 representa "parÃ¡frase"), ou seja, a segunda classe Ã© a correta. Calculamos a perda de entropia cruzada:
>
> $$
> \text{Loss} = - \sum_{i} y_{true_i} \log(y_{pred_i})
> $$
>
> $$
> \text{Loss} = - (0 * \log(0.3) + 1 * \log(0.7)) = - \log(0.7) \approx 0.356
> $$
>
> O gradiente da perda Ã© entÃ£o retropropagado, e os pesos $\mathbf{W}_P$ e os pesos das camadas anteriores (dependendo se estamos ajustando ou nÃ£o os parÃ¢metros do modelo prÃ©-treinado) sÃ£o atualizados usando um otimizador (ex: Adam). Este processo Ã© repetido para todos os pares de sentenÃ§as de treinamento.
>
> A tabela abaixo ilustra o uso de *dropout* durante o treinamento, utilizando um exemplo simplificado.
>
> | Step | Input Vector $h_{pair}$ | Dropout Mask |  $h_{pair}$ * Mask | Output Vector before Softmax z | Softmax Output y (probabilities) | True Label | Loss |
> |---|---|---|---|---|---|---|---|
> | 1 | [0.2, 0.5, 0.8, 0.1, 0.9] | [1, 0, 1, 1, 0] |  [0.2, 0, 0.8, 0.1, 0] | [0.1, 0.9] | [0.27, 0.73]  |  [0, 1] |  0.31 |
> | 2 | [0.3, 0.6, 0.7, 0.2, 0.8] | [0, 1, 1, 0, 1] | [0, 0.6, 0.7, 0, 0.8] | [0.5, 0.5] |  [0.5, 0.5]   |  [1, 0]  | 0.69 |
> | 3 | [0.1, 0.4, 0.9, 0.3, 0.7] | [1, 1, 0, 1, 0] | [0.1, 0.4, 0, 0.3, 0] | [0.8, 0.2] | [0.69, 0.31]   | [1, 0] | 0.37 |
>
> Neste exemplo, a camada de *dropout* desativa aleatoriamente algumas entradas no vetor $h_{pair}$ a cada passo. Isto faz com que o modelo aprenda a nÃ£o depender de nenhuma entrada especÃ­fica, aumentando a robustez. Observe que o modelo tenta minimizar a perda (calculada utilizando entropia cruzada), ajustando os pesos ao longo de muitas iteraÃ§Ãµes, tornando as probabilidades previstas mais prÃ³ximas dos rÃ³tulos verdadeiros.
>
> Uma representaÃ§Ã£o simplificada do cÃ¡lculo da perda:
>
> ```mermaid
> graph LR
>     A[h_pair] --> B(Camada Linear)
>     B --> C(Softmax)
>     C --> D(Probabilidade Prevista y_pred)
>     E[RÃ³tulo Verdadeiro y_true] --> F(CÃ¡lculo da Perda)
>     D --> F
>     F --> G(Perda)
> ```
> Este diagrama mostra o fluxo de dados durante uma Ãºnica iteraÃ§Ã£o de treinamento.

### ConclusÃ£o

Este capÃ­tulo detalhou o processo de *fine-tuning* para classificaÃ§Ã£o de pares de sequÃªncias, destacando sua conexÃ£o com o *Next Sentence Prediction* (NSP) [^1]. Exploramos as diferentes abordagens para construir representaÃ§Ãµes de pares de sequÃªncia, as arquiteturas de classificaÃ§Ã£o e as abordagens avanÃ§adas para melhorar o desempenho [^1]. A concatenaÃ§Ã£o Ã© a abordagem mais comum, mas vimos outras alternativas como diferenciaÃ§Ã£o, multiplicaÃ§Ã£o e *attention*. As provas, lemas, corolÃ¡rios e exemplos numÃ©ricos fornecem um entendimento sÃ³lido do processo. Ao compreender esses detalhes e abordagens, Ã© possÃ­vel ajustar modelos de linguagem prÃ©-treinados para realizar tarefas de classificaÃ§Ã£o de pares de sequÃªncia de forma eficaz, aproveitando o conhecimento generalizado aprendido durante o prÃ©-treinamento com NSP e refinando-o para tarefas especÃ­ficas [^1].

### ReferÃªncias

[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright Â© 2024. All rights reserved. Draft of August 20, 2024.
<!-- END -->
