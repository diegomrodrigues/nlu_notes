## Fine-Tuning para Rotulação de Sequências: Detalhes e o Uso de CRF

### Introdução

Nos capítulos anteriores, discutimos o processo de *fine-tuning* para classificação de sequências e pares de sequências, que envolvem a atribuição de rótulos a textos completos ou às relações entre eles [^1]. Este capítulo aborda um outro tipo de tarefa, a **rotulação de sequências**, na qual o objetivo é atribuir um rótulo a cada token em uma sequência de texto [^1]. Essa tarefa é essencial em diversas aplicações de Processamento de Linguagem Natural (PLN), como reconhecimento de entidades nomeadas (NER), marcação morfossintática (POS tagging) e extração de informações. Aprofundaremos a compreensão sobre como o *fine-tuning* é aplicado para essa tarefa, com particular foco na utilização do *Conditional Random Field* (CRF) para modelar as dependências entre os rótulos.

### Rotulação de Sequências: Conceitos e Objetivos

A **rotulação de sequências**, também conhecida como classificação *token-level*, consiste em atribuir um rótulo a cada token individual em uma sequência de texto [^1]. Essa tarefa difere da classificação de sequências, que atribui um único rótulo a toda a sequência, e da classificação de pares de sequências, que rotula a relação entre dois segmentos de texto [^1].

#### Aplicações da Rotulação de Sequências
A rotulação de sequências é utilizada em diversas aplicações de PLN, como:

1.  **Reconhecimento de Entidades Nomeadas (NER):** Identificar e classificar entidades como nomes de pessoas, organizações e locais em um texto.
2.  **Marcação Morfossintática (POS Tagging):** Atribuir rótulos gramaticais a cada palavra, como substantivo, verbo, adjetivo, etc.
3.  **Extração de Informações:** Identificar e classificar informações relevantes em um texto, como relações entre entidades e eventos.
4.  **Análise de Sentimento Fina-Granularidade:** Atribuir rótulos de sentimento a cada palavra ou frase em uma sentença.

#### Fine-Tuning para Rotulação de Sequências
No processo de *fine-tuning* para rotulação de sequências, o modelo pré-treinado recebe uma sequência de texto como entrada e gera uma representação para cada token na sequência, através dos vetores $h_i$, onde $i$ representa o índice do token na sequência [^1]. Em seguida, cada vetor $h_i$ é passado para um classificador que gera uma distribuição de probabilidade sobre o conjunto de rótulos possíveis para aquele token,  como visto na equação (11.12):
$$
\mathbf{y}_i = \text{softmax}(\mathbf{h}_i\mathbf{W}_K) \quad (11.12)
$$
Onde $\mathbf{W}_K$ é a matriz de pesos do classificador, e $\mathbf{y}_i$ é o vetor de probabilidades para cada token. Após a camada softmax, o rótulo predito é o rótulo com a maior probabilidade, usando a função argmax, como visto na equação (11.13):
$$
t_i = \text{argmax}(\mathbf{y}_i) \quad (11.13)
$$
O objetivo do treinamento é ajustar os parâmetros $\mathbf{W}_K$ (e possivelmente os parâmetros do modelo pré-treinado) para que os rótulos previstos correspondam aos rótulos verdadeiros nos dados de treinamento.

> 💡 **Exemplo Numérico (Softmax):**
>
>  Considere que temos um modelo pré-treinado que gera embeddings de dimensão $d=100$ para cada token, ou seja, cada $h_i$ é um vetor de 100 dimensões. Para uma tarefa de POS tagging, temos $K=10$ rótulos possíveis (e.g., substantivo, verbo, adjetivo, etc). Portanto, a matriz de pesos $\mathbf{W}_K$ terá dimensão $100 \times 10$.
>
>  Suponha que para um token específico, após o cálculo de $\mathbf{h}_i\mathbf{W}_K$, obtemos o vetor de scores $[2.1, 0.8, -0.5, 1.2, 0.1, -1.0, 1.5, 0.3, -0.2, 0.7]$.  Aplicamos a função softmax (equação 11.12) para obter probabilidades:
>
> ```python
> import numpy as np
>
> scores = np.array([2.1, 0.8, -0.5, 1.2, 0.1, -1.0, 1.5, 0.3, -0.2, 0.7])
> probabilities = np.exp(scores) / np.sum(np.exp(scores))
> print(probabilities)
> ```
>
>  A saída será um vetor de probabilidades, por exemplo, `[0.29, 0.12, 0.03, 0.17, 0.07, 0.02, 0.21, 0.08, 0.05, 0.06]`.  O rótulo predito (equação 11.13) seria o índice do maior valor, que neste caso é o índice 0, correspondendo ao primeiro rótulo. O processo é repetido para cada token na sequência.
>
>  Este exemplo ilustra como a camada softmax converte scores brutos em probabilidades e como a função argmax seleciona o rótulo com a maior probabilidade. O objetivo do fine-tuning é ajustar $\mathbf{W}_K$ para que os rótulos corretos tenham sempre a maior probabilidade após o softmax.

**Lema 1:** A dimensão da matriz de pesos $\mathbf{W}_K$ é $d \times K$, onde $d$ é a dimensão do vetor $h_i$ e $K$ é o número de rótulos possíveis para a tarefa de rotulação.
*Prova:*
I.  O vetor $h_i$, correspondente ao embedding do token $i$, possui dimensão $d$, resultado da saída do modelo pré-treinado.
II. O objetivo da camada linear de classificação (representada pela matriz $\mathbf{W}_K$) é mapear o vetor $h_i$ para um vetor de probabilidades, onde cada elemento do vetor represente a probabilidade de cada um dos $K$ rótulos possíveis para a tarefa de classificação.
III. A matriz $\mathbf{W}_K$ deve multiplicar o vetor $h_i$ de dimensão $d$ para gerar um vetor de dimensão $K$, o que ocorre quando $\mathbf{W}_K$ possui dimensão $d \times K$.
IV.  Portanto, para que a operação seja válida e que a saída do classificador tenha dimensão $K$, a matriz de pesos $\mathbf{W}_K$ deve ter dimensões $d \times K$. ■

**Proposição 1:** A complexidade computacional da etapa de softmax para uma sequência de $n$ tokens é $O(n \cdot d \cdot K)$, onde $d$ é a dimensão de $h_i$ e $K$ é o número de rótulos.
*Prova:*
I. Para cada um dos $n$ tokens, a operação $\mathbf{h}_i \mathbf{W}_K$ requer $d \cdot K$ multiplicações e adições.
II. O cálculo do softmax para cada token envolve operações que são $O(K)$.
III. Portanto, o custo para cada token é $O(d \cdot K)$, e para todos os $n$ tokens é $O(n \cdot d \cdot K)$. ■

### Conditional Random Fields (CRF): Modelando Dependências entre Rótulos

Embora a camada softmax, como descrito nas equações (11.12) e (11.13), permita prever o rótulo mais provável para cada token individualmente, ela ignora a relação entre os rótulos dos tokens adjacentes [^1]. Em muitas tarefas de rotulação de sequência, como NER e POS Tagging, os rótulos dos tokens adjacentes são altamente dependentes [^1]. Por exemplo, no NER, um nome de pessoa é geralmente seguido por outro nome de pessoa ou por um nome de organização. É nesse cenário que um modelo CRF é útil.

#### Funcionamento do CRF
Um *Conditional Random Field* (CRF) é um modelo probabilístico que considera a dependência entre os rótulos dos tokens adjacentes, modelando a probabilidade de uma sequência de rótulos como um todo, em vez de rótulos individuais [^1]. O CRF aprende matrizes de transição entre os rótulos, o que permite que o modelo capture as dependências entre os rótulos em uma sequência.

A probabilidade de uma sequência de rótulos $t = [t_1, t_2, \ldots, t_n]$ para uma sequência de entrada $x = [x_1, x_2, \ldots, x_n]$  é dada por:

$$ P(t|x) = \frac{exp(score(t, x))}{\sum_{t' \in T} exp(score(t',x))} $$

Onde $T$ é o conjunto de todas as sequências de rótulos possíveis e $score(t,x)$ é uma pontuação que mede a adequação da sequência de rótulos $t$ para a sequência de entrada $x$. O score pode ser definido como:

$$ score(t, x) = \sum_{i=1}^{n} \mathbf{W}_K[t_i] \cdot h_i + \sum_{i=2}^{n} \mathbf{A}[t_{i-1}, t_{i}] $$

O primeiro termo $\sum_{i=1}^{n} \mathbf{W}_K[t_i] \cdot h_i$ representa a pontuação de cada rótulo de token de forma independente (de forma similar ao que fazíamos com o softmax), onde $\mathbf{W}_K[t_i]$ corresponde à linha da matriz $\mathbf{W}_K$ correspondente ao rótulo $t_i$ e $h_i$ é o embedding daquele token [^1]. O segundo termo $\sum_{i=2}^{n} \mathbf{A}[t_{i-1}, t_{i}]$ representa a pontuação de transição entre dois rótulos consecutivos, onde $\mathbf{A}$ é a matriz de transição e $\mathbf{A}[t_{i-1}, t_i]$ corresponde à pontuação de transição do rótulo $t_{i-1}$ para o rótulo $t_i$. A matriz $\mathbf{A}$ de transição entre rótulos é um parâmetro aprendível que o modelo ajusta durante o treinamento.

Durante o treinamento, o CRF ajusta seus parâmetros (as matrizes $\mathbf{W}_K$ e $\mathbf{A}$) para maximizar a probabilidade dos rótulos verdadeiros nas sequências de treinamento. Na inferência, o CRF encontra a sequência de rótulos mais provável para cada sequência de entrada, usando algoritmos como o algoritmo de Viterbi [^1].

> 💡 **Exemplo Numérico (CRF):**
>
> Considere a frase "Elon Musk trabalha na Tesla.", para a qual temos os embeddings dos tokens, $h_i$, gerados por um modelo pré-treinado e as probabilidades para cada token para as classes *PER* (pessoa), *ORG* (organização) e *O* (outros), calculadas usando a equação (11.12). Para o token "Elon", a probabilidade usando softmax poderia ser $[0.9, 0.05, 0.05]$. Para o token "Tesla", a probabilidade seria $[0.1, 0.8, 0.1]$. Entretanto, vamos considerar agora o CRF, e para isso vamos simplificar o problema e usar apenas 2 tokens e 2 rótulos (PER e O).
>
> Suponha que as pontuações da camada softmax ($\mathbf{W}_K[t_i] \cdot h_i$) e as matrizes de transição ($\mathbf{A}[t_{i-1}, t_i]$)  produzem os seguintes scores:
>
> |             |   PER    |   O    |
> |:------------|:--------:|:------:|
> | **Elon**    |   2.1    |  -0.1  |
> | **Musk**    |  1.8    |  -0.3  |
>
>   E a matriz de transição:
>
> |    |   PER    |   O    |
> |:---|:--------:|:------:|
> | PER |   1.0    | -0.5   |
> | O | -0.2   | 0.8   |
>
> Para a sequência "Elon Musk", temos quatro possíveis sequências de rótulos:
>
> 1.  PER, PER:  score = 2.1 + 1.8 + 1.0 = 4.9
> 2.  PER, O: score = 2.1 + (-0.3) + (-0.5) = 1.3
> 3.  O, PER: score = -0.1 + 1.8 + (-0.2) = 1.5
> 4.  O, O: score = -0.1 + (-0.3) + 0.8 = 0.4
>
> Aplicamos a fórmula do CRF para normalizar as pontuações e criar probabilidades. A sequência mais provável é a sequência com maior score: PER, PER. Embora cada rótulo, individualmente, pudesse ter uma maior probabilidade de ser do tipo *O* (i.e., o softmax, sem o CRF, poderia classificar *Musk* como *O*), a probabilidade da sequência como um todo (PER, PER) é maior utilizando a matriz de transição.

**Lema 2:** A adição de uma camada de CRF após a camada softmax pode melhorar o desempenho da rotulação de sequências em tarefas onde há dependências entre os rótulos, como NER e POS tagging.
*Prova:*
I.  A camada softmax rotula cada token individualmente, ignorando as dependências contextuais entre os rótulos [^1].
II. A camada CRF aprende as matrizes de transição entre os rótulos, modelando explicitamente as dependências contextuais [^1].
III.  A probabilidade da sequência de rótulos é calculada considerando tanto as probabilidades das etiquetas individuais como as transições entre elas.
IV. Portanto, a camada CRF captura as dependências entre rótulos, levando a melhores resultados em problemas onde essas dependências são importantes. ■

**Lema 2.1:** A escolha dos hiperparâmetros do CRF, como a estrutura da matriz de transição e a força das regularizações aplicadas, é essencial para o desempenho do modelo e depende da tarefa específica.
*Prova:*
I. A complexidade da matriz de transição e a necessidade de regularização são específicas para cada tarefa, dado que diferentes tarefas têm diferentes níveis de dependência entre rótulos.
II.  Se a tarefa tiver um número elevado de rótulos com transições complexas, o modelo CRF poderá necessitar de uma matriz de transição mais complexa, ou então, com mais regularização.
III. Em tarefas com poucas dependências, o uso de uma matriz de transição simples ou sem CRF pode apresentar bons resultados.
IV. A escolha adequada de hiperparâmetros para cada tarefa melhora o desempenho do CRF.
V. Portanto, os hiperparâmetros do CRF dependem da tarefa específica. ■

**Observação 1:** A matriz de transição $\mathbf{A}$ tem dimensões $K \times K$, onde $K$ é o número de rótulos possíveis. Cada elemento $\mathbf{A}[i,j]$ representa a pontuação da transição do rótulo $i$ para o rótulo $j$.
*Justificativa:* A matriz de transição modela a transição entre todos os possíveis rótulos, necessitando portanto de uma linha e coluna para cada um deles.

**Proposição 2:** O número de parâmetros adicionais introduzidos pelo uso de um CRF é $K^2$, correspondente ao número de elementos na matriz de transição $\mathbf{A}$, onde $K$ é o número de rótulos.
*Prova:* A matriz de transição $\mathbf{A}$ tem dimensões $K \times K$, e cada elemento é um parâmetro a ser aprendido. Portanto, o número total de parâmetros é $K \times K = K^2$. ■

### Fine-Tuning com CRF

O *fine-tuning* para rotulação de sequência com CRF envolve as seguintes etapas:

1.  **Propagação Direta:** A sequência de entrada é passada pelo modelo pré-treinado, gerando as representações $h_i$ para cada token, e em seguida, as pontuações da camada softmax $\mathbf{y_i} = \text{softmax}(\mathbf{h}_i\mathbf{W}_K)$.
2. **Cálculo da Pontuação CRF:** As pontuações da camada softmax, as matrizes de transição $\mathbf{A}$ e as pontuações dos rótulos $\mathbf{W}_K[t_i]$  são usadas para calcular a pontuação da sequência de rótulos $score(t,x)$ para todas as sequências de rótulos possíveis.
3. **Cálculo da Perda CRF:**  As pontuações $score(t,x)$ são usadas para calcular a probabilidade da sequência de rótulos verdadeira $P(t|x)$. A perda é a entropia cruzada negativa $\text{Loss} = -log(P(t|x))$, que visa maximizar a probabilidade da sequência de rótulos verdadeira.
4. **Retropropagação:** O gradiente da perda é calculado em relação aos parâmetros do modelo, e os pesos da matriz de transição $\mathbf{A}$, da matriz do classificador $\mathbf{W}_K$ e, opcionalmente, as camadas do modelo pré-treinado, são ajustados com retropropagação.
5. **Otimização:** Os parâmetros são atualizados utilizando um otimizador, como Adam, com o objetivo de minimizar a perda e maximizar a probabilidade da sequência de rótulos correta.

#### Inferencia com o Algoritmo de Viterbi

Durante a inferência, o modelo deve encontrar a sequência de rótulos mais provável para uma dada sequência de entrada [^1]. Essa busca é realizada utilizando o algoritmo de Viterbi, um algoritmo de programação dinâmica que encontra a sequência de rótulos com o maior score possível dentre todas as sequências. O algoritmo de Viterbi explora o espaço de todas as sequências de rótulos possíveis e retorna a sequência ótima que maximiza a probabilidade $P(t|x)$, o que se resume a maximizar a soma dos scores de rótulo $\mathbf{W}_K[t_i] \cdot h_i$ e a pontuação de transição $\mathbf{A}[t_{i-1}, t_i]$.

> 💡 **Exemplo Numérico (Fine-Tuning com CRF):**
>
> Vamos detalhar um passo de *fine-tuning* com CRF, usando um problema de NER. Considere novamente a frase “Elon Musk trabalha na Tesla.”. Após a tokenização e a passagem pelo modelo, obtemos as representações $h_i$ para cada token.
>
> Usando a equação (11.12), calculamos as probabilidades para os rótulos PER, ORG e O para cada token:
>
> **Token "Elon":** $\mathbf{y}_{\text{Elon}} = [0.9, 0.05, 0.05]$ (PER, ORG, O)
> **Token "Musk":** $\mathbf{y}_{\text{Musk}} = [0.8, 0.1, 0.1]$
> **Token "trabalha":** $\mathbf{y}_{\text{trabalha}} = [0.1, 0.1, 0.8]$
> **Token "na":** $\mathbf{y}_{\text{na}} =  [0.05, 0.05, 0.9]$
> **Token "Tesla":** $\mathbf{y}_{\text{Tesla}} = [0.1, 0.8, 0.1]$
>
> O CRF é usado para modelar as transições entre os rótulos. O CRF aprende as pontuações de transição entre todos os rótulos (matriz $\mathbf{A}$). Por exemplo:
>
> **Transição de O para O:** +0.8
> **Transição de O para PER:** -0.2
> **Transição de O para ORG:** -0.2
> **Transição de PER para PER:** +0.9
> **Transição de PER para O:** -0.5
> **Transição de PER para ORG:** -0.5
> **Transição de ORG para PER:** -0.5
> **Transição de ORG para O:** -0.5
> **Transição de ORG para ORG:** +0.9
>
> Para cada sequência de rótulos, calculamos a pontuação usando a equação de score mencionada anteriormente, que considera as probabilidades de cada token e as transições entre os rótulos. Por exemplo, uma sequência possível seria  PER, PER, O, O, ORG:
>
> $$ score(PER, PER, O, O, ORG) = score(PER|\text{Elon}) + score(PER|\text{Musk}) + score(O|\text{trabalha}) + score(O|\text{na}) + score(ORG|\text{Tesla}) + score(PER\rightarrow PER) + score(PER \rightarrow O) + score(O \rightarrow O) + score(O \rightarrow ORG) $$
>
> As probabilidades $score(X|token)$ representam a pontuação dada pela camada softmax para a etiqueta $X$ e o token. As transições $score(X \rightarrow Y)$ representam a pontuação para a transição do rótulo $X$ para o rótulo $Y$.  Utilizando números fictícios para este exemplo, poderíamos obter:
>
> $$ score(PER, PER, O, O, ORG) = 0.9 + 0.8 + 0.8 + 0.9 + 0.8 + 0.9 + (-0.5) + 0.8 + (-0.5) = 4.9$$
>
> E para outra sequência possível, PER, O, O, O, ORG :
>
> $$ score(PER, O, O, O, ORG) =  0.9 + 0.1 + 0.8 + 0.9 + 0.8 + (-0.5) + 0.8 + 0.8 + (-0.5) = 3.3 $$
>
> Essas pontuações são usadas para calcular a probabilidade da sequência de rótulos, e na inferência o modelo usa o algoritmo de Viterbi para encontrar a sequência de rótulos que maximiza a probabilidade.
>
> Durante o treinamento, a retropropagação ajustará as matrizes $\mathbf{W}_K$ e  $\mathbf{A}$ para aumentar a probabilidade dos rótulos verdadeiros e diminuir a probabilidade de rótulos incorretos. Por exemplo, o CRF aprende que as transições de O para PER ou ORG são menos prováveis, enquanto as transições de PER para PER e ORG para ORG são mais prováveis em uma tarefa de NER.
>
>  Para ilustrar melhor, vamos considerar um passo de otimização. Suponha que, durante a retropropagação, o gradiente da perda em relação à transição $\mathbf{A}[O, PER]$ seja de 0.2. Isso significa que aumentar o valor de $\mathbf{A}[O, PER]$ em uma pequena quantidade aumentaria a probabilidade da sequência correta. O otimizador, como Adam, então atualizaria $\mathbf{A}[O, PER]$ usando um learning rate, por exemplo, 0.01. O novo valor de $\mathbf{A}[O, PER]$ seria $\mathbf{A}[O, PER] + 0.01 * 0.2$. Este processo é repetido iterativamente, ajustando todos os parâmetros de $\mathbf{A}$ e $\mathbf{W}_K$ para que as sequências de rótulos corretas tenham as maiores pontuações possíveis.

**Teorema 1:** O treinamento de modelos de rotulação de sequência com CRF garante que o modelo aprenda a modelar as dependências entre rótulos, utilizando tanto as informações de cada token quanto a influência dos rótulos adjacentes na probabilidade global da sequência.
*Prova:*
I. A função de perda do CRF considera a sequência de rótulos como um todo, calculando a probabilidade conjunta da sequência de rótulos corretos.
II. As matrizes de transição $\mathbf{A}$  aprendem explicitamente as probabilidades de transição entre rótulos adjacentes, incorporando dependências contextuais entre os rótulos.
III. O processo de treinamento ajusta os parâmetros para maximizar a probabilidade dos rótulos corretos dada a sequência de entrada, levando o modelo a modelar as dependências entre rótulos.
IV. Portanto, o modelo CRF aprende a modelar as dependências entre os rótulos de uma sequência de forma eficaz, melhorando os resultados em comparação com abordagens que tratam cada token individualmente. ■

**Teorema 1.1:** O algoritmo de Viterbi garante que a sequência de rótulos mais provável seja encontrada durante a inferência com CRF, ao explorar todas as possíveis sequências de rótulos e retornar a sequência com a maior probabilidade.
*Prova:*
I. O algoritmo de Viterbi usa programação dinâmica para calcular a sequência de rótulos com maior probabilidade através de caminhos em um grafo de estados, onde cada nó corresponde a um rótulo em cada posição da sequência.
II. Ele armazena os melhores caminhos parciais até cada posição, usando resultados parciais para calcular a probabilidade total da sequência de rótulos.
III. Essa abordagem garante a exploração de todas as sequências de rótulos possíveis, ao invés de uma exploração parcial.
IV. Portanto, o algoritmo de Viterbi encontra a sequência de rótulos mais provável em tempo polinomial, em contraste com uma busca exaustiva, que é exponencial. ■

**Lema 3:** A complexidade computacional do algoritmo de Viterbi para uma sequência de $n$ tokens e $K$ rótulos é $O(nK^2)$.
*Prova:*
I. Para cada token na sequência de entrada, o algoritmo Viterbi itera sobre todos os $K$ possíveis rótulos.
II. Em cada iteração, o algoritmo considera a transição de todos os $K$ rótulos anteriores.
III. Isso resulta em uma complexidade de $O(K^2)$ para cada token.
IV. Como isso é repetido para todos os $n$ tokens, a complexidade total é $O(nK^2)$. ■

### Conclusão

Este capítulo abordou em detalhes o processo de *fine-tuning* para rotulação de sequências, com particular foco na utilização do *Conditional Random Field* (CRF) [^1]. Exploramos como as saídas dos modelos pré-treinados são usadas para gerar as probabilidades dos rótulos individuais e como o CRF modela as dependências entre esses rótulos [^1]. O processo de treinamento supervisionado com CRF, juntamente com o algoritmo de Viterbi na inferência, garante a atribuição de rótulos consistentes e de alta qualidade. O CRF é uma ferramenta poderosa para melhorar o desempenho em tarefas de rotulação de sequência e, junto com as abordagens discutidas nos capítulos anteriores, demonstra a flexibilidade e o poder dos modelos de linguagem pré-treinados e a sua capacidade de adaptação a uma variedade de tarefas no campo do Processamento de Linguagem Natural [^1].

### Referências

[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All rights reserved. Draft of August 20, 2024.
<!-- END -->
