## Fine-Tuning para Classificação: Tipos de Tarefas e Abordagens Específicas

### Introdução
Como vimos nos capítulos anteriores, o *fine-tuning* é um processo essencial para adaptar modelos de linguagem pré-treinados a tarefas específicas, e como o processo de treinamento se utiliza das saídas dos transformadores bidirecionais [^1]. Nesta seção, aprofundaremos a discussão explorando os tipos de tarefas de classificação para *fine-tuning*, que incluem a classificação de sequências, a classificação de pares de sequências e a rotulação de sequências [^1]. Cada tipo de tarefa demanda uma abordagem específica de *fine-tuning*, embora a lógica geral do processo seja similar. Analisaremos detalhadamente como a arquitetura e os parâmetros são adaptados para cada cenário, oferecendo exemplos práticos e provas que sustentam a compreensão dos conceitos [^1].

### Classificação de Sequências: Adaptações e Especificidades

A **classificação de sequências** é uma tarefa fundamental em NLP que envolve a atribuição de um único rótulo a um texto completo [^1]. Exemplos comuns incluem análise de sentimento, detecção de spam e classificação de tópicos de documentos [^1]. No processo de *fine-tuning* para essa tarefa, um *classifier head* é adicionado sobre a saída do modelo pré-treinado. O token especial `[CLS]` desempenha um papel crucial na representação de toda a sequência, como visto nas seções anteriores [^1].

#### Adaptação da Arquitetura
A arquitetura para classificação de sequências é ajustada para que o vetor de saída correspondente ao token `[CLS]`, $h_{CLS}$, seja usado como representação da sequência [^1]. Esse vetor é então passado para um classificador, como uma regressão logística ou uma rede neural, cuja função é mapear a representação da sequência para um conjunto de pontuações sobre as possíveis classes. A equação (11.11), como já visto anteriormente, formaliza este processo [^1]:
$$
\mathbf{y} = \text{softmax}(\mathbf{h}_{CLS}\mathbf{W}_C) \quad (11.11)
$$

Durante o *fine-tuning*, a matriz de pesos $\mathbf{W}_C$ é treinada usando dados rotulados [^1]. A retropropagação ajusta os pesos para minimizar a perda de entropia cruzada entre a distribuição de probabilidade prevista $\mathbf{y}$ e o rótulo verdadeiro.

#### Especificidades do Fine-Tuning
Uma especificidade notável do *fine-tuning* para classificação de sequências é que apenas o vetor $h_{CLS}$ é usado para a classificação, enquanto as demais saídas do modelo pré-treinado são ignoradas diretamente pelo *classifier head*, embora estas saídas tenham influência no resultado via os mecanismos de *self-attention* [^1]. Isso faz com que o modelo aprenda a codificar informações relevantes de toda a sequência no vetor $h_{CLS}$ para realizar a tarefa de classificação.

> 💡 **Exemplo Numérico (Classificação de Sentimento):**
>
> Suponha que temos um modelo pré-treinado e queremos realizar a classificação de sentimento em um texto. O texto de entrada é "Este filme é incrível!". Após tokenização e passagem pelo modelo, obtemos um vetor $h_{CLS}$ de dimensão, por exemplo, 768. A matriz de pesos $\mathbf{W}_C$ terá a dimensão $768 \times 2$, considerando duas classes de saída: positivo e negativo.
>
> Seja $h_{CLS} = [0.1, -0.2, 0.5, \ldots, 0.3]$ e a matriz $\mathbf{W}_C$ inicializada aleatoriamente. O resultado da operação $\mathbf{h}_{CLS}\mathbf{W}_C$ será um vetor de dimensão 2.
>
> Digamos que após o cálculo de $\mathbf{h}_{CLS}\mathbf{W}_C$ obtemos o vetor $[2.5, -1.2]$. Aplicamos a função softmax para obter uma distribuição de probabilidade:
>
> $\mathbf{y} = \text{softmax}([2.5, -1.2]) \approx [0.97, 0.03]$
>
> A probabilidade de a frase ser classificada como positiva é 0.97 e como negativa é 0.03. Durante o treinamento, a matriz $\mathbf{W}_C$ é ajustada por retropropagação para que a probabilidade da classe correta aumente.

**Observação 1:** É crucial observar que a escolha do otimizador, como Adam, e a taxa de aprendizado são parâmetros cruciais que afetam a convergência e o desempenho do modelo. Uma taxa de aprendizado muito alta pode levar a instabilidade e *overshooting*, enquanto uma taxa muito baixa pode tornar o treinamento muito lento. Além disso, métodos de regularização, como *dropout* e *weight decay*, são frequentemente usados para evitar *overfitting*, especialmente quando o número de parâmetros a serem treinados é grande.

**Teorema 1:** A utilização de uma camada *dropout* após a representação $h_{CLS}$ e antes da multiplicação pela matriz $\mathbf{W}_C$ na classificação de sequências pode melhorar a generalização do modelo, reduzindo o *overfitting*.

*Prova:*
I. A camada *dropout* desativa aleatoriamente uma proporção dos neurônios durante o treinamento [^1].
II. Essa desativação forçada impede que o modelo se torne excessivamente dependente de determinados neurônios ou características.
III. A aplicação do *dropout* após a representação $h_{CLS}$ impede que o modelo memorize os padrões exatos dos dados de treinamento.
IV. Isso força o modelo a aprender representações mais robustas e generalizáveis, levando a um melhor desempenho em dados não vistos.
V. Portanto, a adição de uma camada *dropout* ajuda a regularizar o modelo, melhorando sua capacidade de generalização. ■

> 💡 **Exemplo Numérico (Dropout):**
>
> Continuando o exemplo anterior, antes de realizar a multiplicação por $\mathbf{W}_C$, aplicamos uma camada dropout com probabilidade 0.2, por exemplo. Isto significa que 20% dos elementos do vetor $h_{CLS}$ são zerados aleatoriamente a cada iteração durante o treinamento.
>
> Se $h_{CLS} = [0.1, -0.2, 0.5, \ldots, 0.3]$, após o dropout poderíamos ter, por exemplo, $h_{CLS}^{dropout} = [0, -0.2, 0, \ldots, 0.3]$, com os valores 0 representando os neurônios desativados. O modelo, então, irá aprender a classificar a sequência com essa representação parcialmente zerada, forçando-o a não depender de neurônios específicos, generalizando mais.

### Classificação de Pares de Sequências: Concatenando Representações

A **classificação de pares de sequências** envolve a classificação da relação semântica entre dois segmentos de texto [^1]. Tarefas comuns incluem detecção de paráfrases, reconhecimento de inferência textual (NLI) e determinação da coerência discursiva [^1]. Para realizar o *fine-tuning* para essa tarefa, uma abordagem comum é concatenar as representações do token `[CLS]` de cada sequência e passar o resultado para um classificador [^1].

#### Adaptação da Arquitetura
Como vimos na Proposição 1.1 do capítulo anterior,  a arquitetura para classificação de pares de sequências é adaptada concatenando os vetores de saída $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$  correspondentes ao token `[CLS]` de cada sequência [^1]:
$$
h_{pair} = \text{concat}(h_{CLS}^{(1)}, h_{CLS}^{(2)})
$$
Em seguida, um *classifier head* linear é adicionado sobre $h_{pair}$, mapeando a representação concatenada para um vetor de pontuações sobre as classes. A saída do classificador é dada por:
$$
\mathbf{y} = \text{softmax}(h_{pair} \mathbf{W}_P)
$$

Durante o *fine-tuning*, a matriz de pesos $\mathbf{W}_P$ é treinada usando pares de sequências rotuladas, e o objetivo é minimizar a perda de entropia cruzada entre a distribuição de probabilidade prevista $\mathbf{y}$ e o rótulo verdadeiro [^1].

#### Especificidades do Fine-Tuning
Uma especificidade da classificação de pares de sequências é a necessidade de modelar a relação entre as duas sequências de entrada, ao invés de tratar cada uma de forma independente [^1]. A concatenação dos embeddings `[CLS]` permite que o classificador capture informações conjuntas das duas sequências. Entretanto, outras abordagens podem ser utilizadas para melhor capturar a relação entre as sentenças.

> 💡 **Exemplo Numérico (NLI):**
>
> Considere as sentenças "Um gato está dormindo no tapete." e "Há um felino descansando no tapete.". O modelo pré-treinado gera os vetores $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$, ambos de dimensão 768. Após a concatenação, $h_{pair}$ terá dimensão 1536. A matriz $\mathbf{W}_P$ terá a dimensão $1536 \times 3$, considerando três classes de saída: *entailment*, *contradiction* e *neutral*.
>
>  Digamos que $h_{pair}$ = $[0.2, -0.1, 0.4, \ldots, 0.3]$. O resultado da operação  $h_{pair} \mathbf{W}_P$ é um vetor de tamanho 3, por exemplo, $[2.1, -0.8, 0.3]$. A função softmax gera uma distribuição de probabilidade:
>
>  $\mathbf{y} = \text{softmax}([2.1, -0.8, 0.3]) \approx [0.8, 0.1, 0.1]$.
>
>  A probabilidade de *entailment* é 0.8, *contradiction* é 0.1 e *neutral* é 0.1. O modelo, durante o treinamento, ajustará os pesos da matriz $\mathbf{W}_P$ para aumentar a probabilidade da classe correta, neste caso, *entailment*.

**Lema 1.2:** Métodos alternativos para representar pares de sequências podem envolver a utilização da diferença ou da multiplicação ponto a ponto dos vetores $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$, ou a adição de uma camada de *attention* sobre a concatenação para permitir que o modelo aprenda quais partes de cada vetor são mais importantes para a classificação.

*Prova:*
I. A concatenação, embora eficaz, não é a única forma de combinar as informações de duas sequências.
II. A diferença dos vetores $h_{CLS}^{(1)} - h_{CLS}^{(2)}$ captura as relações diferenciais entre as duas sequências, que podem ser relevantes para a tarefa.
III. A multiplicação ponto a ponto $h_{CLS}^{(1)} \odot h_{CLS}^{(2)}$ pode dar ênfase a características comuns às duas sequências.
IV. A adição de uma camada de atenção sobre a concatenação permite que o modelo determine quais partes de cada sequência são mais relevantes para a classificação.
V. Cada uma dessas abordagens representa uma forma alternativa de modelar a relação entre as sequências. Portanto, todas elas são alternativas válidas. ■

> 💡 **Exemplo Numérico (Diferença):**
>
> Usando o mesmo exemplo NLI, podemos calcular a diferença dos vetores: $h_{diff} = h_{CLS}^{(1)} - h_{CLS}^{(2)}$.  Assumindo que  $h_{CLS}^{(1)} = [0.1, 0.2, 0.3, \ldots, 0.4]$ e $h_{CLS}^{(2)} = [0.2, 0.1, 0.2, \ldots, 0.5]$, então $h_{diff} = [-0.1, 0.1, 0.1, \ldots, -0.1]$. Este vetor de diferença $h_{diff}$ pode ser usado para classificar a relação entre as sentenças. O classificador usaria uma matriz $\mathbf{W}_{diff}$ para mapear $h_{diff}$ para as classes.
>
> Similarmente, a multiplicação ponto a ponto seria $h_{mult} = h_{CLS}^{(1)} \odot h_{CLS}^{(2)}$, resultando em um vetor onde cada elemento é a multiplicação dos elementos correspondentes em $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$.

**Teorema 1.1:** A inclusão de uma camada não-linear, como uma camada ReLU, entre a concatenação $h_{pair}$ e a matriz de pesos $\mathbf{W}_P$ pode melhorar a capacidade do modelo de aprender relações complexas entre os pares de sequências.

*Prova:*
I. A concatenação $h_{pair}$ cria uma representação linear da relação entre as duas sequências.
II. Uma camada não-linear, como a ReLU, adiciona complexidade à representação, permitindo ao modelo aprender relações mais abstratas.
III. Sem uma camada não-linear, o modelo estaria limitado a aprender apenas relações lineares, o que pode não ser suficiente para tarefas complexas.
IV. Portanto, a adição de uma camada não-linear aumenta a capacidade do modelo de representar relações mais intrincadas, o que pode melhorar o desempenho em tarefas de classificação de pares de sequências. ■

> 💡 **Exemplo Numérico (ReLU):**
>
> Após concatenar os vetores $h_{CLS}^{(1)}$ e $h_{CLS}^{(2)}$ e obter $h_{pair}$, aplicamos uma camada ReLU, que zera os valores negativos do vetor. Por exemplo, se $h_{pair}$ = $[0.2, -0.1, 0.4, \ldots, -0.3]$, então após a ReLU teremos $h_{pair}^{ReLU}$ = $[0.2, 0, 0.4, \ldots, 0]$. Este vetor $h_{pair}^{ReLU}$ é então usado para a classificação. A ReLU introduz não-linearidade, permitindo que o modelo aprenda relações mais complexas entre as sequências.

### Rotulação de Sequências: Classificando Tokens Individuais

A **rotulação de sequências**, também conhecida como classificação *token-level*, envolve a atribuição de um rótulo a cada token em uma sequência de texto [^1]. Uma tarefa comum nessa categoria é o **reconhecimento de entidades nomeadas (NER)**, que identifica e classifica entidades como pessoas, organizações e locais em um texto [^1]. O *fine-tuning* para rotulação de sequências requer um classificador que processe cada token independentemente.

#### Adaptação da Arquitetura

Nesse caso, a arquitetura do *fine-tuning* é adaptada para que cada vetor de saída $h_i$ do modelo pré-treinado, correspondente a cada token $i$, seja passado para um classificador individual [^1]. Este classificador gera uma distribuição de probabilidade sobre o conjunto de etiquetas possíveis para aquele token, como definido nas equações (11.12) e (11.13):
$$
\mathbf{y}_i = \text{softmax}(\mathbf{h}_i\mathbf{W}_K) \quad (11.12)
$$
$$
t_i = \text{argmax}(\mathbf{y}_i) \quad (11.13)
$$

Durante o *fine-tuning*, a matriz de pesos $\mathbf{W}_K$ é treinada com dados de sequência rotulados, e o objetivo é minimizar a perda de entropia cruzada média sobre todos os tokens da sequência. Em algumas abordagens, uma camada de *Conditional Random Field* (CRF) pode ser adicionada após a softmax para modelar transições entre as etiquetas [^1], como será discutido no capítulo 17.

#### Especificidades do Fine-Tuning
Uma especificidade notável da rotulação de sequências é que a classificação é realizada em cada token individualmente [^1]. A retropropagação é utilizada para ajustar os pesos $\mathbf{W}_K$ com base na perda calculada para cada token. Além disso, abordagens como o uso de CRF ajudam a modelar dependências entre as etiquetas, melhorando a qualidade dos resultados [^1].

> 💡 **Exemplo Numérico (NER):**
>
> Considere a frase "Elon Musk trabalha na Tesla.". Após a tokenização, o modelo gera os vetores $h_i$ para cada token. Assumindo que temos 3 rótulos: *PER* (pessoa), *ORG* (organização) e *O* (outro). Para cada token, a matriz $\mathbf{W}_K$ realiza a operação $\mathbf{h}_i\mathbf{W}_K$ mapeando $h_i$ para um vetor de tamanho 3, por exemplo.
>
> Para o token "Elon", suponha que $\mathbf{h}_{\text{Elon}} \mathbf{W}_K = [2.0, -0.5, 0.1]$, e após a softmax $\mathbf{y}_{\text{Elon}} = [0.9, 0.05, 0.05]$. Assim, "Elon" seria classificado como *PER*. Para o token "Tesla", suponha que $\mathbf{h}_{\text{Tesla}} \mathbf{W}_K = [-0.2, 1.8, 0.1]$, resultando em $\mathbf{y}_{\text{Tesla}} = [0.05, 0.9, 0.05]$. "Tesla" seria classificado como *ORG*.
>
> A saída final seria: "Elon"*PER* "Musk"*PER* "trabalha"*O* "na"*O* "Tesla"*ORG* ".". *O*. A matriz $\mathbf{W}_K$ é treinada de forma que cada token seja classificado corretamente de acordo com seu rótulo.

> 💡 **Exemplo Numérico (NER com CRF):**
>
> Considere uma sequência de texto com os tokens  `[“Apple”, “is”, “planning”, “to”, “open”, “a”, “new”, “store”, “in”, “London”]` e seus respectivos embeddings $h_i$. Após a aplicação da equação (11.12), obtemos probabilidades para as etiquetas *ORG*, *LOC*, *O* para cada token, representadas por  $\mathbf{y}_i$. Suponha que para os tokens "Apple" e "London" tenhamos:
>
> $$
> \mathbf{y}_{\text{Apple}} = [0.89, 0.01, 0.10]
> $$
> $$
> \mathbf{y}_{\text{London}} = [0.05, 0.94, 0.01]
> $$
>
> Usando argmax (equação 11.13), sem considerar o CRF, "Apple" seria rotulado como *ORG* e "London" como *LOC*.
>
> Ao adicionar uma camada CRF, o modelo considera as transições entre as etiquetas. Por exemplo, a transição de *ORG* para *LOC* é menos provável do que de *ORG* para *O* ou de *O* para *LOC*, mas em casos como este, pode ser mais provável se houver uma entidade do tipo *LOC* após *ORG*, como em "Apple em London".
>
> A camada CRF aprende a probabilidade de transição entre as etiquetas, ajustando os pesos para capturar as dependências entre as etiquetas de forma global, considerando toda a sequência de saída. Suponha que o CRF atribua uma probabilidade maior para a sequência completa  *ORG*, *O*, *O*, *O*, *O*, *O*, *O*, *O*, *O*, *LOC* do que para a sequência *ORG*, *LOC*, *O*, *O*, *O*, *O*, *O*, *O*, *O*, *LOC*, já que a primeira segue a ordem correta de nome de organização seguida por uma localização no texto.
>
> Embora a camada softmax seja aplicada token por token, o CRF considera as dependências e a distribuição global das etiquetas, ajustando os pesos para minimizar a perda considerando transições mais apropriadas.

*Prova:*
I. Sem CRF, cada token $i$ é rotulado independentemente, utilizando a função softmax e a função argmax, com base em $h_i$ e $\mathbf{W}_K$ [^1].
II. Com CRF, a rotulação é um processo global, considerando todas as etiquetas de forma conjunta [^1].
III.  A camada CRF aprende matrizes de transição entre as etiquetas, que indicam a probabilidade de uma etiqueta seguida pela outra.
IV. Para uma sequência de tokens, o modelo avalia a probabilidade de cada sequência de etiquetas possíveis, considerando a pontuação gerada pelo modelo, as probabilidades da camada softmax e as matrizes de transição.
V. O objetivo do treinamento com CRF é maximizar a probabilidade das etiquetas corretas para cada sequência, em vez de rotular tokens individualmente.
VI. A utilização de CRF, por meio das matrizes de transição, leva a melhores resultados em tarefas de rotulação de sequência, quando as dependências entre as etiquetas são importantes.
Portanto, o uso do CRF é uma melhoria na modelagem de dependências entre rótulos. ■

**Lema 1.3:** A adição de uma camada de *attention* antes da camada softmax na rotulação de sequências pode permitir que o modelo aprenda quais partes da sequência são mais relevantes para a rotulação de cada token específico.

*Prova:*
I. Cada token $h_i$ é processado individualmente pela camada softmax.
II. Uma camada de atenção pode aprender a ponderar as representações $h_j$ dos outros tokens, criando uma representação contextualizada para cada token $h_i$.
III. Essa representação contextualizada $h_i^{att}$ pode ser obtida por meio de uma combinação linear dos demais $h_j$ ponderados por um score de atenção, que depende de $h_i$ e $h_j$ [^1].
IV. A camada softmax, ao usar $h_i^{att}$ como entrada, passa a considerar o contexto do token $i$ e não apenas a sua representação isolada.
V. Essa representação contextualizada permite que o modelo capture dependências entre tokens que podem ser relevantes para a tarefa de rotulação, melhorando o desempenho do modelo.
VI. Portanto, a adição de atenção é uma forma de aumentar o desempenho, considerando a influência do contexto para a tarefa de rotulação. ■

> 💡 **Exemplo Numérico (Attention):**
>
> No exemplo NER anterior, antes de aplicar a softmax, uma camada de *attention* é adicionada. Para o token "Musk", por exemplo, a camada de atenção calcularia um peso para cada token da sentença, dando maior importância para os tokens "Elon" e "Tesla", já que eles fazem parte da mesma entidade nomeada. O vetor de saída contextualizado, $h_{\text{Musk}}^{att}$,  seria calculado ponderando os outros vetores $h_i$ pela atenção, fazendo com que a classificação de "Musk" tenha mais informações contextuais.
>  A ideia principal é que alguns tokens da sequência são mais importantes para o contexto de outros, e a camada de *attention* aprende a determinar essa importância.

### Conclusão
Este capítulo explorou os três tipos principais de tarefas de classificação para *fine-tuning*: classificação de sequências, classificação de pares de sequências e rotulação de sequências [^1]. Para cada tarefa, abordamos as adaptações necessárias na arquitetura do modelo e as especificidades do *fine-tuning* [^1]. Enquanto a classificação de sequência utiliza o vetor de saída do token `[CLS]` como representação de todo o texto, a classificação de pares de sequência usa a concatenação de dois vetores `[CLS]`. Já a rotulação de sequências classifica cada token individualmente [^1]. Cada tipo de tarefa exige uma abordagem específica, mas todos compartilham a lógica geral de usar um classificador sobre a saída do modelo pré-treinado [^1]. As abordagens específicas, como o uso de camadas CRF e *attention*, demonstram a flexibilidade e adaptabilidade dos métodos de *fine-tuning* e a sua capacidade de modelar diferentes tipos de problemas de NLP. Os exemplos numéricos, o Lema 1.2, a prova do funcionamento do CRF e a demonstração das vantagens de *dropout*, camada não linear e *attention* ajudam a consolidar o conhecimento sobre cada abordagem.

### Referências
[^1]: Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All rights reserved. Draft of August 20, 2024.
<!-- END -->
