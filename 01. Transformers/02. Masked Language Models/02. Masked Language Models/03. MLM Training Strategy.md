## Treinamento de Modelos de Linguagem Mascarados: Corrup√ß√£o e Predi√ß√£o

### Introdu√ß√£o
Este cap√≠tulo se dedica a explorar o processo de treinamento de **Masked Language Models (MLMs)**, com foco especial nas t√©cnicas de corrup√ß√£o de textos de entrada e na predi√ß√£o de tokens originais a partir de contextos corrompidos [^1]. Diferentemente dos modelos causais, que s√£o treinados para prever a pr√≥xima palavra em uma sequ√™ncia, os MLMs aprendem a partir da reconstru√ß√£o de textos em que algumas palavras foram intencionalmente omitidas ou substitu√≠das [^1, 4]. Este m√©todo de treinamento, que tamb√©m pode ser visto como um tipo de *denoising*, √© fundamental para a capacidade dos MLMs de gerar representa√ß√µes contextuais robustas e compreender rela√ß√µes complexas entre palavras [^4]. O processo de treinamento envolve o mascaramento aleat√≥rio de tokens, a sua substitui√ß√£o por outros tokens ou sua manuten√ß√£o inalterada, uma abordagem que introduz varia√ß√£o e robustez ao aprendizado do modelo [^5].

### T√©cnicas de Corrup√ß√£o de Textos de Entrada
O treinamento de MLMs inicia-se com a corrup√ß√£o dos textos de entrada atrav√©s de diferentes m√©todos [^4]. O principal m√©todo, e aquele que d√° nome a este tipo de modelos, envolve o mascaramento aleat√≥rio de tokens [^5]. Este processo, que pode ser descrito como um tipo de *denoising*, √© essencial para que o modelo aprenda a reconstruir a sequ√™ncia original a partir de vers√µes corrompidas ou incompletas [^4].

#### Mascaramento Aleat√≥rio
O **mascaramento aleat√≥rio** √© o m√©todo central de corrup√ß√£o do input em MLMs [^5]. Ele consiste em selecionar aleatoriamente um subconjunto de tokens em cada sequ√™ncia de entrada, que √© ent√£o manipulado de tr√™s formas distintas [^5]:

1.  **Substitui√ß√£o por `[MASK]`**: A maioria dos tokens selecionados (por exemplo, 80% no BERT) s√£o substitu√≠dos por um token especial denominado `[MASK]` [^5]. Este token serve como um marcador que indica ao modelo que um token est√° ausente naquela posi√ß√£o, e que ele deve ser previsto com base no contexto circundante [^5].
2.  **Substitui√ß√£o por Token Aleat√≥rio**: Uma porcentagem menor (por exemplo, 10% no BERT) dos tokens selecionados s√£o substitu√≠dos por outros tokens escolhidos aleatoriamente do vocabul√°rio [^5]. Esta t√©cnica introduz ru√≠do no input, o que obriga o modelo a aprender a lidar com informa√ß√µes erradas, tornando-o mais robusto e eficaz na generaliza√ß√£o [^5].
3.  **Tokens Inalterados**: Uma pequena fra√ß√£o dos tokens selecionados (por exemplo, 10% no BERT) s√£o deixados inalterados [^5]. A raz√£o para isto √© evitar que o modelo se concentre apenas na predi√ß√£o de tokens mascarados, mantendo uma vis√£o da sequ√™ncia como um todo e ajudando a evitar o overfitting [^5].

A probabilidade de um token ser selecionado para mascaramento √© geralmente uniforme em toda a sequ√™ncia [^5].

**Lema 3** (Import√¢ncia da Diversidade no Mascaramento) A diversidade introduzida pela combina√ß√£o de mascaramento, substitui√ß√£o por tokens aleat√≥rios e manuten√ß√£o de tokens originais √© crucial para o treinamento robusto de MLMs, prevenindo o sobreajuste e promovendo a generaliza√ß√£o do modelo em diferentes contextos.

*Prova:*
I. A substitui√ß√£o de 80% dos tokens por `[MASK]` for√ßa o modelo a aprender a prever palavras com base no contexto circundante, capturando as rela√ß√µes sem√¢nticas e sint√°ticas entre as palavras.
II. A substitui√ß√£o de 10% dos tokens por tokens aleat√≥rios introduz ru√≠do nos dados, o que obriga o modelo a aprender a lidar com erros e incertezas e a extrair as informa√ß√µes relevantes do contexto, resultando em um aprendizado mais robusto.
III. A manuten√ß√£o de 10% dos tokens inalterados evita que o modelo se concentre apenas na predi√ß√£o de tokens mascarados, garantindo que o modelo aprenda a representar toda a sequ√™ncia de entrada e evitando o overfitting.
IV. A combina√ß√£o destas tr√™s t√©cnicas permite que o modelo aprenda de forma mais geral e robusta, melhorando seu desempenho em tarefas de downstream e prevenindo o sobreajuste.
V. Portanto, a diversidade introduzida pela combina√ß√£o de mascaramento, substitui√ß√£o por tokens aleat√≥rios e manuten√ß√£o de tokens originais √© crucial para o treinamento robusto de MLMs.
‚ñ†
**Lema 3.1** (Impacto da Probabilidade de Mascaramento) A probabilidade de um token ser selecionado para mascaramento influencia o desempenho do modelo, onde uma probabilidade muito baixa pode resultar em subtreinamento e uma probabilidade muito alta pode dificultar a aprendizagem do modelo.
*Prova:*
I. Uma probabilidade muito baixa de mascaramento significa que poucos tokens s√£o alterados, dando ao modelo pouco para aprender sobre a reconstru√ß√£o da sequ√™ncia a partir do contexto, resultando em subtreinamento.
II. Uma probabilidade muito alta de mascaramento significa que muitos tokens s√£o mascarados ou substitu√≠dos, o que pode tornar a tarefa de reconstru√ß√£o muito dif√≠cil, dificultando a aprendizagem das rela√ß√µes entre os tokens.
III. A escolha de uma probabilidade √≥tima de mascaramento √© um compromisso entre a quantidade de informa√ß√£o que √© mascarada e a dificuldade da tarefa de reconstru√ß√£o, sendo o valor 15% (80% para `[MASK]`, 10% para token aleat√≥rio e 10% inalterado), um valor tipicamente utilizado na literatura.
IV. Portanto, a probabilidade de um token ser selecionado para mascaramento influencia o desempenho do modelo, e o valor ideal depende de v√°rios fatores.
‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Consideremos a seguinte sequ√™ncia de entrada: "O gato preto correu rapidamente pela rua.". Vamos aplicar o m√©todo de mascaramento aleat√≥rio com as probabilidades de 80%, 10% e 10% para os tokens selecionados. Suponha que selecionamos aleatoriamente dois tokens para serem modificados.
>
> 1. **Sele√ß√£o Aleat√≥ria:** Suponhamos que os tokens "preto" e "rua" sejam selecionados para mascaramento.
> 2. **Aplica√ß√£o da M√°scara:**
>     - "preto" √© substitu√≠do por `[MASK]` (80% dos tokens selecionados).
>     - "rua" √© substitu√≠do por um token aleat√≥rio, por exemplo "casa" (10% dos tokens selecionados).
>
> A sequ√™ncia de entrada transformada seria: "O gato `[MASK]` correu rapidamente pela casa.".
>
> O modelo ent√£o tentaria prever "preto" na posi√ß√£o do token `[MASK]` e "rua" na posi√ß√£o de "casa", com base no restante da sequ√™ncia. Se o modelo fosse capaz de prever essas palavras corretamente, isso aumentaria a probabilidade dessas palavras estarem corretas dado o contexto. Este processo √© repetido v√°rias vezes em diferentes sequ√™ncias e diferentes mascaramentos.
>
> Vamos agora simular uma segunda itera√ß√£o com a mesma frase original "O gato preto correu rapidamente pela rua.".
>
>  1. **Sele√ß√£o Aleat√≥ria:** Suponhamos que desta vez os tokens "gato" e "pela" sejam selecionados para mascaramento.
>  2. **Aplica√ß√£o da M√°scara:**
>      - "gato" √© substitu√≠do por `[MASK]` (80% dos tokens selecionados).
>      - "pela" permanece inalterado (10% dos tokens selecionados - neste caso, o token selecionado para permanecer inalterado √© o "pela").
>      - Outro token aleat√≥rio poderia ser selecionado, mas neste exemplo, o token "pela" foi o token selecionado para permanecer inalterado.
>
>  A sequ√™ncia de entrada transformada seria: "O `[MASK]` preto correu rapidamente pela rua.".
>
> O modelo ent√£o tentaria prever "gato" na posi√ß√£o do token `[MASK]`. Este exemplo demonstra como a aleatoriedade da sele√ß√£o de tokens e a variabilidade da sua manipula√ß√£o ajudam o modelo a aprender representa√ß√µes mais robustas das palavras e frases.

#### Gera√ß√£o de Distribui√ß√µes de Probabilidade
Durante o treinamento, o modelo gera uma distribui√ß√£o de probabilidade sobre o vocabul√°rio para cada token mascarado [^5]. Este processo √© realizado da seguinte forma:

1.  **Processamento Bidirecional**: A sequ√™ncia de entrada, com tokens mascarados ou substitu√≠dos, √© passada atrav√©s do codificador transformer bidirecional [^2]. A sa√≠da deste codificador √© uma sequ√™ncia de vetores contextuais, um para cada token da sequ√™ncia de entrada [^2, 8].
2.  **Language Modeling Head**: A sa√≠da do codificador para cada token mascarado √© ent√£o passada atrav√©s do Language Modeling (LM) Head [^5]. O LM Head √© composto por uma matriz de pesos, que transforma os vetores contextuais em logits, uma pontua√ß√£o que representa a probabilidade n√£o normalizada de cada token no vocabul√°rio ser a palavra original [^5].
3.  **Fun√ß√£o Softmax**: Uma fun√ß√£o softmax √© aplicada a esses logits para gerar uma distribui√ß√£o de probabilidade sobre o vocabul√°rio, onde cada valor representa a probabilidade de um token espec√≠fico do vocabul√°rio ser a palavra que foi mascarada [^5].

**Teorema 2** (Otimiza√ß√£o via Perda de Entropia Cruzada) A utiliza√ß√£o da perda de entropia cruzada como fun√ß√£o de perda no treinamento de MLMs garante que o modelo aprenda a atribuir altas probabilidades aos tokens corretos nas posi√ß√µes mascaradas, permitindo uma converg√™ncia eficiente e um aprendizado robusto.

*Prova:*
I. A entropia cruzada mede a diferen√ßa entre duas distribui√ß√µes de probabilidade, no caso a distribui√ß√£o predita pelo modelo e a distribui√ß√£o real dos tokens mascarados.
II. Durante o treinamento, o modelo √© otimizado para minimizar a perda de entropia cruzada.
III. Minimizar a entropia cruzada leva o modelo a ajustar seus par√¢metros para que a distribui√ß√£o de probabilidade predita se aproxime o m√°ximo poss√≠vel da distribui√ß√£o real dos tokens mascarados.
IV. Isto garante que o modelo atribua altas probabilidades aos tokens corretos e baixas probabilidades aos tokens incorretos, levando a uma predi√ß√£o mais precisa dos tokens mascarados.
V. Portanto, o uso da perda de entropia cruzada como fun√ß√£o de perda garante que o modelo aprenda a atribuir altas probabilidades aos tokens corretos nas posi√ß√µes mascaradas, permitindo uma converg√™ncia eficiente e um aprendizado robusto.
‚ñ†
**Teorema 2.1** (Converg√™ncia da Perda de Entropia Cruzada) Sob condi√ß√µes adequadas, a perda de entropia cruzada converge para um valor m√≠nimo durante o treinamento do MLM, indicando que o modelo aprendeu a prever os tokens mascarados de forma precisa.

*Prova:*
I. A perda de entropia cruzada √© uma fun√ß√£o convexa, o que significa que possui um m√≠nimo global.
II. Durante o processo de treinamento, os par√¢metros do modelo s√£o atualizados iterativamente usando um algoritmo de otimiza√ß√£o, como o gradiente descendente.
III. O gradiente descendente garante que o valor da perda diminua a cada itera√ß√£o, movendo-se na dire√ß√£o do m√≠nimo da fun√ß√£o de perda.
IV. Sob certas condi√ß√µes, como uma taxa de aprendizagem adequada e um tamanho de *batch* razo√°vel, o processo de otimiza√ß√£o ir√° convergir para um valor m√≠nimo da fun√ß√£o de perda, que corresponde ao modelo ter aprendido a prever os tokens mascarados de forma eficaz.
V. Portanto, sob condi√ß√µes adequadas, a perda de entropia cruzada converge para um valor m√≠nimo durante o treinamento do MLM.
‚ñ†

A distribui√ß√£o de probabilidade sobre o vocabul√°rio para cada token mascarado $i$ √© dada por [^5]:
$$u_i = h_i E^T$$
$$y_i = \text{softmax}(u_i)$$

Onde:
- $h_i$ √© o vetor de sa√≠da do √∫ltimo layer do codificador transformer para o token mascarado $i$.
- $E^T$ √© a matriz de *unembedding*, que transforma os vetores contextuais em um espa√ßo do tamanho do vocabul√°rio.
- $u_i$ √© o vetor de logits, uma pontua√ß√£o para cada palavra do vocabul√°rio.
- $y_i$ √© a distribui√ß√£o de probabilidade sobre o vocabul√°rio, resultante da aplica√ß√£o da fun√ß√£o softmax aos logits.

A fun√ß√£o de perda, neste caso, √© a perda de entropia cruzada calculada como:
$$L = -\frac{1}{M} \sum_{i \in M} \log(P(x_i|h))$$
Onde $M$ √© o conjunto de tokens mascarados, e $P(x_i|h)$ √© a probabilidade atribu√≠da ao token original $x_i$ pelo modelo dado o seu contexto $h$ [^6].

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o primeiro exemplo num√©rico, com a sequ√™ncia "O gato `[MASK]` correu rapidamente pela casa.", onde o modelo deve prever a palavra "preto" no lugar do token `[MASK]` e "rua" na posi√ß√£o de "casa".
>
> 1.  **Processamento pelo Transformer:** Ap√≥s a sequ√™ncia passar pelo codificador transformer, um vetor contextualizado $h_i$ √© gerado para o token `[MASK]`, o qual representa o contexto da sequ√™ncia. Este vetor ter√° dimens√µes correspondentes √† dimens√£o do modelo. Por exemplo, para o BERT base, este vetor ter√° 768 dimens√µes.
>
> 2.  **LM Head e Matriz de Unembedding:** O vetor $h_i$ √© multiplicado pela matriz de *unembedding* $E^T$ para gerar os logits $u_i$. A matriz $E^T$ tem dimens√µes $d \times V$ onde $d$ √© a dimens√£o do vetor contextual (e.g. 768 para o BERT base) e $V$ √© o tamanho do vocabul√°rio (e.g. 30522 para o BERT base).
>
>    Vamos supor, para simplifica√ß√£o, que a sa√≠da da matriz de unembedding para o token `[MASK]` seja:
>    $$u_{MASK} = [0.01, 0.02, 0.85, 0.03, 0.04,\ldots]$$
>     onde cada valor corresponde a um logit para cada token no vocabul√°rio. Vamos supor que o token "preto" corresponde ao √≠ndice 3 no vocabul√°rio.
>    Vamos supor que a sa√≠da da matriz de unembedding para o token "casa" seja:
>    $$u_{casa} = [0.4, 0.1, 0.3, 0.05, 0.25,\ldots]$$
>    Onde cada valor corresponde a um logit para cada token no vocabul√°rio, e que a palavra "rua" corresponde ao √≠ndice 1.
>
> 3.  **Softmax:** A fun√ß√£o softmax √© aplicada a esses logits para gerar a distribui√ß√£o de probabilidade:
>    $$y_{MASK} = \text{softmax}(u_{MASK}) = [0.001, 0.002, 0.90, 0.003, 0.004, \ldots]$$
>     $$y_{casa} = \text{softmax}(u_{casa}) = [0.30, 0.20, 0.15, 0.02, 0.10, \ldots]$$
>
>  Neste exemplo, a probabilidade predita pelo modelo de ser a palavra correcta "preto" (no √≠ndice 3) √© de 90%, enquanto para a palavra "casa" (no √≠ndice 1), a probabilidade de ser a palavra "rua" √© de 20%.
>
>  A perda de entropia cruzada √© calculada comparando as probabilidades preditas pelo modelo com os tokens originais "preto" e "rua", respectivamente. As probabilidades preditas pelo modelo s√£o comparadas com os valores de *one-hot-encoding* correspondentes √† palavra real. Se o modelo predisser a palavra correcta com uma probabilidade alta, a perda de entropia cruzada √© pequena, caso contr√°rio a perda ser√° alta.
>
>   - *One-hot-encoding* da palavra "preto" = $[0,0,1,0,0,\ldots]$, e "rua" = $[0,1,0,0,0,\ldots]$
>
>  A perda para o token mascarado seria: $-\log(0.90) \approx 0.105$
>
>  A perda para a palavra substituida seria: $-\log(0.20) \approx 1.609$
>
> A perda total √© a m√©dia das perdas para todos os tokens mascarados, ou seja: $(0.105 + 1.609)/2 \approx 0.857$
>
> Durante o treinamento, os pesos do modelo s√£o atualizados para minimizar essa perda. Este processo √© repetido para todas as frases do conjunto de treino, e para v√°rios *epochs*, at√© a perda convergir para um valor m√≠nimo.

**Proposi√ß√£o 1** (Varia√ß√µes na Fun√ß√£o de Perda) Embora a entropia cruzada seja comumente utilizada, outras fun√ß√µes de perda, como a focal loss, podem ser aplicadas para melhorar o treinamento em casos espec√≠ficos, como desequil√≠brio de classes.
*Prova:*
I. A perda de entropia cruzada atribui o mesmo peso a todos os erros, o que pode n√£o ser ideal quando algumas classes s√£o muito mais comuns do que outras (desequil√≠brio de classes).
II. A focal loss, por outro lado, atribui mais peso a erros de classes menos frequentes, o que pode melhorar o desempenho do modelo nessas classes.
III. Outras fun√ß√µes de perda, como a perda *hinge*, podem tamb√©m ser aplicadas dependendo do problema espec√≠fico.
IV. A escolha da fun√ß√£o de perda adequada √© crucial para o desempenho do modelo e deve ser feita com base nas caracter√≠sticas do problema e nos dados.
V. Portanto, outras fun√ß√µes de perda podem ser aplicadas para melhorar o treinamento em casos espec√≠ficos, como desequil√≠brio de classes.
‚ñ†

### O Papel da Predi√ß√£o da Pr√≥xima Senten√ßa (NSP)
Embora o objetivo principal do treinamento de MLMs seja a predi√ß√£o de tokens mascarados, alguns modelos, como o BERT, incluem tamb√©m um objetivo auxiliar de **Next Sentence Prediction (NSP)** [^6]. A tarefa do NSP √© prever se duas frases apresentadas ao modelo s√£o ou n√£o adjacentes no texto original [^6].

Para o NSP, um token especial `[CLS]` √© adicionado no in√≠cio de cada sequ√™ncia e um token `[SEP]` √© inserido entre as duas frases [^6]. A representa√ß√£o vetorial do token `[CLS]` √© utilizada por uma camada de classifica√ß√£o para determinar a rela√ß√£o entre as frases [^6]. O objetivo do NSP √© melhorar a capacidade do modelo de capturar rela√ß√µes inter-sentenciais e de compreender o fluxo l√≥gico de um texto [^6].

**Corol√°rio 3** (Sin√©rgia entre MLM e NSP) A combina√ß√£o dos objetivos de treinamento MLM e NSP permite que o modelo aprenda representa√ß√µes textuais mais abrangentes, que s√£o √∫teis tanto para a compreens√£o do contexto de palavras individuais quanto para a compreens√£o da rela√ß√£o entre senten√ßas, levando a um desempenho superior em diversas tarefas de processamento de linguagem natural.

*Prova:*
I. O objetivo MLM treina o modelo a prever palavras com base no contexto, aprendendo as rela√ß√µes sem√¢nticas e sint√°ticas entre as palavras em uma senten√ßa.
II. O objetivo NSP treina o modelo a determinar se duas senten√ßas s√£o adjacentes, aprendendo a compreender a rela√ß√£o entre as senten√ßas dentro de um texto.
III. Ao treinar com ambos objetivos simultaneamente, o modelo aprende a representar o texto em diferentes n√≠veis: a n√≠vel de palavras e a n√≠vel de senten√ßas.
IV. A capacidade de compreender tanto o contexto intra-sentencial como o contexto inter-sentencial permite que o modelo tenha uma vis√£o mais abrangente do texto, resultando em um melhor desempenho em diversas tarefas de processamento de linguagem natural.
V. Portanto, a combina√ß√£o dos objetivos de treinamento MLM e NSP permite que o modelo aprenda representa√ß√µes textuais mais abrangentes.
‚ñ†
**Corol√°rio 3.1** (Limita√ß√µes do NSP) Apesar de sua utilidade, o objetivo NSP apresenta algumas limita√ß√µes, como sua fraca rela√ß√£o com tarefas *downstream* e a poss√≠vel introdu√ß√£o de ru√≠do no treinamento, o que levou alguns modelos posteriores a removerem este objetivo.
*Prova:*
I. Estudos t√™m demonstrado que o objetivo NSP nem sempre contribui significativamente para o desempenho em tarefas *downstream*, por vezes at√© prejudicando a aprendizagem.
II. A tarefa NSP, por vezes, introduz ru√≠do no treinamento, pois as senten√ßas podem n√£o ter uma forte rela√ß√£o sem√¢ntica ou l√≥gica mesmo que sejam adjacentes no texto original.
III. Devido a estas limita√ß√µes, alguns modelos de linguagem posteriores t√™m optado por remover o objetivo NSP, focando-se exclusivamente no objetivo MLM.
IV. Portanto, o objetivo NSP apresenta algumas limita√ß√µes que devem ser consideradas na escolha da arquitetura e objetivo do modelo.
‚ñ†

### Conclus√£o
O treinamento de modelos de linguagem mascarados √© um processo sofisticado que envolve a corrup√ß√£o controlada de textos de entrada e a predi√ß√£o de tokens originais a partir de um contexto corrompido. O uso de diferentes t√©cnicas de mascaramento e substitui√ß√£o de tokens, combinadas com a minimiza√ß√£o da perda de entropia cruzada, garante que o modelo aprenda representa√ß√µes contextuais robustas e eficazes. Adicionalmente, o objetivo de treinamento NSP possibilita que o modelo aprenda tamb√©m a compreender a rela√ß√£o entre senten√ßas. O modelo resultante deste processo √© capaz de efetuar diversas tarefas de processamento de linguagem natural, incluindo a classifica√ß√£o de texto e a compreens√£o sem√¢ntica.

### Refer√™ncias
[^1]: Cap√≠tulo 9, 10 e 11 do livro texto.
[^2]: Se√ß√£o 11.1 do livro texto.
[^3]: Se√ß√£o 11.1.1 do livro texto.
[^4]: Se√ß√£o 11.2 do livro texto.
[^5]: Se√ß√£o 11.2.1 do livro texto.
[^6]: Se√ß√£o 11.2.2 do livro texto.
[^8]: Se√ß√£o 11.3 do livro texto.
<!-- END -->
