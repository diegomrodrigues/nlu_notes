## Adaptação de Modelos de Linguagem Mascarados via Fine-Tuning

### Introdução
Este capítulo aborda o processo de **fine-tuning** de **Masked Language Models (MLMs)**, uma etapa crucial para adaptar esses modelos pré-treinados a tarefas específicas de processamento de linguagem natural [^1]. Após o pré-treinamento, onde o modelo aprende representações contextuais gerais da linguagem, o fine-tuning permite que o modelo aprenda características específicas para a tarefa desejada através da adição de camadas de classificação ou regressão sobre a camada superior do modelo, e do treinamento dessas camadas adicionais com dados específicos para a tarefa [^1, 12]. Este processo, essencial para a aplicação prática dos MLMs, utiliza as representações contextuais aprendidas durante o pre-treinamento e as adapta a tarefas específicas, resultando em um modelo otimizado para a tarefa em questão [^1, 12].

### Fine-Tuning: Adaptação a Tarefas Específicas
O fine-tuning é o processo de adaptar um modelo pré-treinado a uma tarefa específica, utilizando um conjunto de dados anotado para a tarefa [^1]. Este processo envolve duas etapas principais:

1.  **Adição de Camadas Específicas:** Adiciona-se camadas específicas da tarefa em cima da camada superior do modelo pré-treinado. Estas camadas adicionais podem ser camadas de classificação, regressão, ou outras camadas adaptadas à natureza da tarefa [^12].
2.  **Treinamento das Camadas Adicionais:** Os parâmetros destas camadas adicionais são treinados usando um conjunto de dados anotado, enquanto os parâmetros do modelo pré-treinado podem ser atualizados ou congelados, dependendo da complexidade da tarefa e do tamanho dos dados disponíveis [^12].

A principal vantagem do fine-tuning reside na capacidade de utilizar as representações contextuais já aprendidas pelo modelo pré-treinado, adaptando-as à tarefa específica com um número menor de dados anotados, o que torna o processo mais eficiente e eficaz, comparado com o treinamento de um modelo do zero [^12].

**Lema 5** (Eficiência do Fine-Tuning) O processo de fine-tuning, ao capitalizar o conhecimento aprendido pelo modelo durante o pré-treinamento, reduz drasticamente a quantidade de dados anotados necessária para atingir um bom desempenho em tarefas específicas de *downstream*.

*Prova:*
I. Modelos pré-treinados como MLMs, aprendem representações contextuais gerais de linguagem, que podem ser aplicadas em diferentes tarefas.
II. O fine-tuning adapta este conhecimento a uma tarefa específica através da adição de camadas específicas da tarefa e do treinamento destas com dados anotados.
III. A reutilização do conhecimento pré-treinado evita a necessidade de treinar o modelo do zero com grandes quantidades de dados anotados, o que seria computacionalmente dispendioso e demorado.
IV. O fine-tuning requer apenas ajustar os parâmetros das camadas adicionais e, por vezes, ajustar os parâmetros do modelo pré-treinado, o que requer menos dados anotados para atingir uma boa performance.
V. Portanto, o processo de fine-tuning, ao capitalizar o conhecimento aprendido pelo modelo durante o pré-treinamento, reduz drasticamente a quantidade de dados anotados necessária para atingir um bom desempenho em tarefas específicas.
■
**Lema 5.1** (Transferência de Aprendizado) O processo de fine-tuning é um exemplo de transferência de aprendizado, onde o conhecimento aprendido em uma tarefa (pre-treinamento) é transferido e aplicado para resolver uma tarefa diferente (fine-tuning).

*Prova:*
I. No pre-treinamento, um modelo aprende representações de linguagem gerais a partir de uma grande quantidade de dados.
II. No fine-tuning, essas representações de linguagem aprendidas são aproveitadas para resolver tarefas específicas.
III. As camadas adicionais são ajustadas de forma a adaptar estas representações aprendidas ao problema em questão, transferindo o conhecimento do modelo pré-treinado para o novo problema.
IV. Esta capacidade de usar o conhecimento de uma tarefa para resolver outra é denominada de transferência de aprendizado.
V. Portanto, o processo de fine-tuning é um exemplo de transferência de aprendizado.
■

**Lema 5.2** (Convergência do Fine-Tuning) O processo de fine-tuning tende a convergir mais rapidamente do que o treinamento de um modelo do zero, dada a inicialização dos parâmetros com um modelo pré-treinado.

*Prova:*
I. Os modelos pré-treinados já possuem parâmetros otimizados para representar a estrutura geral da linguagem.
II. O fine-tuning inicia o ajuste dos parâmetros com esses valores otimizados, evitando um início aleatório como num modelo treinado do zero.
III. Dado que o ponto de partida para o fine-tuning é mais próximo de uma solução ótima para a tarefa de *downstream*, o processo de convergência é mais rápido.
IV. Portanto, o processo de fine-tuning tende a convergir mais rapidamente que o treinamento do zero.
■

### Métodos de Fine-Tuning
Os métodos de fine-tuning variam dependendo da natureza da tarefa e do tipo de problema a resolver [^13]. Os métodos mais comuns incluem:

#### Classificação de Sequência
Na **classificação de sequência**, o objetivo é classificar toda a sequência de texto com uma única label, como em tarefas de análise de sentimento, detecção de spam ou classificação de tópicos de documentos [^13].

1.  **Camada de Classificação:** Uma camada de classificação linear é adicionada no topo da representação do token `[CLS]`, que representa toda a sequência [^13].
2.  **Treinamento:** Os parâmetros da camada de classificação são treinados utilizando dados de treinamento anotados. Os parâmetros do modelo pré-treinado podem ser ajustados ou congelados dependendo dos dados disponíveis [^13].

**Teorema 4** (Representação do Token `[CLS]`) A representação vetorial do token `[CLS]`, aprendida durante o pre-treinamento, encapsula a informação relevante de toda a sequência de entrada, tornando-a uma representação eficaz para tarefas de classificação de sequência.

*Prova:*
I. O token `[CLS]` é adicionado no início de todas as sequências de entrada durante o pre-treinamento do MLM.
II. O modelo é treinado para gerar uma representação contextual para este token, usando todo o contexto da sequência de entrada.
III. Devido a sua posição, o vetor de saída do token `[CLS]` acaba por resumir a informação de toda a sequência de entrada, representando-a de forma condensada.
IV. Portanto, a representação vetorial do token `[CLS]`, aprendida durante o pre-treinamento, encapsula a informação relevante de toda a sequência de entrada, tornando-a uma representação eficaz para tarefas de classificação de sequência.
■

#### Classificação de Pares de Sequências
Na **classificação de pares de sequências**, o modelo tem como objetivo classificar a relação entre dois textos, como na inferência de linguagem natural, detecção de paráfrases ou coerência de discurso [^13].

1.  **Representação do Par:** Os pares de sequências são combinados em uma única sequência, com um token especial `[SEP]` para separar as duas sentenças [^14]. A sequência resultante é processada pelo modelo, gerando um vetor para o token `[CLS]`.
2. **Camada de Classificação:** Uma camada de classificação linear é adicionada no topo da representação do token `[CLS]`.
3.  **Treinamento:** Os parâmetros da camada de classificação são treinados usando dados de treino anotados para a tarefa específica. Os parâmetros do modelo pré-treinado podem ser atualizados ou congelados dependendo dos dados disponíveis [^14].

**Teorema 4.1** (Generalização através do `[CLS]`) O uso do vetor do token `[CLS]` para a classificação de pares de sentenças permite que o modelo generalize a compreensão do texto para problemas complexos, como a inferência lógica entre frases.

*Prova:*
I. O modelo é treinado com tarefas de preenchimento de máscara e também de previsão da próxima sentença.
II. A saída do token `[CLS]` aprende a representar relações contextuais e lógicas entre as duas frases, que é útil para a tarefa de previsão da próxima frase e também pode ser generalizada para outras tarefas como inferência lógica e detecção de paráfrases.
III. O uso de um único vetor para representar toda a relação entre duas frases permite que a camada de classificação aprenda relações complexas de uma forma mais compacta e eficiente.
IV. Portanto, o uso do vetor do token `[CLS]` permite que o modelo generalize a compreensão do texto para problemas complexos.
■
**Teorema 4.2** (Adaptabilidade do `[CLS]` para Pares de Sequências) A representação do token `[CLS]` é adaptável não somente para classificação de sequência única mas também para pares de sequências, ao agregar informações de ambas as sequências separadas por um token `[SEP]`.
*Prova:*
I. O token `[SEP]` introduz uma separação semântica entre as duas sequências, permitindo ao modelo distinguir e processar cada uma individualmente enquanto as relaciona.
II. O vetor do token `[CLS]` é calculado com base no contexto de ambas as sequências, capturando a interação entre elas.
III. O modelo, durante o pre-treinamento, aprende a utilizar o `[SEP]` para distinguir sentenças, criando uma representação contextual que é relevante para a classificação de pares de sequências.
IV. Portanto, a representação do token `[CLS]` é adaptável para pares de sequências, capturando informações de ambas através do token separador `[SEP]`.
■

#### Rotulação de Sequências
Na **rotulação de sequências**, o objetivo é atribuir um label a cada token de uma sequência, como no reconhecimento de entidades nomeadas, rotulação de partes do discurso, ou extração de informação [^15].

1. **Camada de Rotulação:** Uma camada de rotulação linear é adicionada no topo da representação de cada token na sequência [^17].
2. **Treinamento:** Os parâmetros da camada de rotulação são treinados utilizando dados de treino anotados com labels para cada token na sequência. Os parâmetros do modelo pré-treinado podem ser atualizados ou congelados [^17].
> 💡 **Exemplo Numérico:**
>
> Vamos considerar um exemplo de rotulação de sequência para Named Entity Recognition (NER). Suponha que temos a frase: "A Apple foi fundada por Steve Jobs na Califórnia". O objetivo é classificar cada token como "B-ORG" (início de organização), "I-ORG" (continuação de organização), "B-PER" (início de pessoa), "I-PER" (continuação de pessoa), "B-LOC" (início de local) ou "O" (nenhuma entidade).
>
> ```python
> import numpy as np
> import torch
> import torch.nn as nn
>
> # Exemplo de representação de embeddings (simulando a saída do MLM)
> # Cada token da frase acima vai ter um vetor de tamanho 768, como um output de um transformer.
> embeddings = torch.randn(1, 9, 768) # batch size 1, 9 tokens, embedding size 768
>
> # Número de classes NER: B-ORG, I-ORG, B-PER, I-PER, B-LOC, O
> num_classes = 6
>
> # Camada de classificação linear
> linear_layer = nn.Linear(768, num_classes)
>
> # Aplicar a camada linear aos embeddings
> logits = linear_layer(embeddings)
>
> # Softmax para obter as probabilidades
> probs = torch.softmax(logits, dim=2)
>
> # Imprimir os logits e as probabilidades para cada token
> print("Logits (antes do softmax):\n", logits.detach().numpy())
> print("\nProbabilidades (após o softmax):\n", probs.detach().numpy())
>
> # Supondo as labels verdadeiras
> true_labels = torch.tensor([[2, 0, 0, 3, 1, 4, 5, 5, 5]], dtype=torch.long) # B-ORG, O, O, B-PER, I-PER, B-LOC, O, O, O
>
> # Calcula a perda de CrossEntropy
> loss_function = nn.CrossEntropyLoss()
> loss = loss_function(logits.view(-1, num_classes), true_labels.view(-1))
>
> print("\nLoss:", loss.item())
>
> # O objetivo agora seria usar retropropagação para ajustar os pesos da camada linear
> # Isso ajustaria o modelo para prever as categorias corretas.
> ```
>
> O código acima demonstra como os vetores de saída do MLM são usados para predizer as labels de cada token. No exemplo, podemos ver os logits (valores antes do softmax), as probabilidades (após o softmax) e o valor da loss function. O fine-tuning ajusta os pesos da camada linear (e potencialmente do MLM) usando retropropagação para que o modelo consiga predizer as labels corretas.
>
> Para o exemplo específico da frase "A Apple foi fundada por Steve Jobs na Califórnia", um resultado possível seria:
> -  "A" : O (nenhuma entidade)
> -  "Apple" : B-ORG (inicio da entidade organização)
> -  "foi" : O (nenhuma entidade)
> -  "fundada" : O (nenhuma entidade)
> -  "por" : O (nenhuma entidade)
> -  "Steve" : B-PER (inicio da entidade pessoa)
> -  "Jobs" : I-PER (continuação da entidade pessoa)
> -  "na" : O (nenhuma entidade)
> -  "Califórnia" : B-LOC (inicio da entidade local)
> A perda de entropia cruzada é usada para quantificar o quão diferentes as probabilidades previstas estão dos valores reais, e os pesos são atualizados durante o treinamento.
>
>
>
>
> Vamos supor que temos um modelo MLM pré-treinado e desejamos adaptá-lo para análise de sentimentos de filmes (classificação de sequência). O modelo gera um vetor $h_{[CLS]}$ para a sequência de entrada.
>
> 1.  **Adicionar Camada de Classificação:** Adicionamos uma camada linear em cima da saída do modelo, tal que a camada tem a seguinte equação:
> $$z = h_{[CLS]} W_c + b_c$$
> Onde $W_c$ é a matriz de pesos da camada de classificação e $b_c$ é o *bias*. A dimensão de saída da camada é igual ao número de categorias (exemplo: positivo, negativo e neutro).
>
> 2.  **Softmax:** A saída da camada linear é passada através da função softmax para gerar uma distribuição de probabilidade sobre as categorias:
>  $$y = \text{softmax}(z)$$
>  A saída $y$ representa uma probabilidade para cada uma das classes de sentimento, onde a classe com maior probabilidade será a predição do modelo.
> 3.  **Retropropagação e Ajuste:** A perda é calculada utilizando a entropia cruzada entre as previsões $y$ e as labels reais, e os pesos da camada ($W_c$ e $b_c$) são ajustados com base no gradiente, utilizando retropropagação.  Este processo é repetido ao longo de várias iterações até o modelo convergir e prever os sentimentos corretamente. Os pesos do modelo pré-treinado podem também ser ajustados, ou permanecer congelados, conforme o modelo requerer.
>
> Para um exemplo de rotulação de sequências, vamos considerar o problema de nomeação de entidades (NER).
>
> 1.  **Camada de Rotulação:** Adicionamos uma camada de rotulação em cima de cada vetor de saída $h_i$ do modelo (um para cada token):
>   $$z_i = h_i W_l + b_l$$
>   Onde $W_l$ é a matriz de pesos da camada de rotulação, $b_l$ é o *bias*, e a saída é uma probabilidade para cada categoria de entidade nomeada.
> 2.  **Softmax:** A saída da camada linear é passada através da função softmax:
>    $$y_i = \text{softmax}(z_i)$$
>    A saída $y_i$ representa uma distribuição de probabilidade para a categoria de cada token da frase.
> 3.  **Retropropagação e Ajuste:** A perda de entropia cruzada é calculada comparando as predições com as labels reais e os parâmetros da camada de rotulação ($W_l$ e $b_l$) e, potencialmente, os parâmetros do modelo, são ajustados através do otimizador. Este processo é repetido ao longo de várias iterações até o modelo convergir.

>
**Corolário 5** (Adaptabilidade do Fine-Tuning) A capacidade de ajustar camadas de classificação ou regressão adicionadas sobre o modelo pré-treinado permite que o MLM seja adaptado a uma variedade de tarefas, desde classificação de textos até rotulação de sequências, com um custo computacional reduzido comparado ao treino do modelo do zero.

*Prova:*
I.  O fine-tuning permite adicionar camadas específicas da tarefa em cima da camada superior do modelo pré-treinado.
II. Estas camadas adicionadas podem variar desde camadas lineares para classificação e regressão a camadas de rotulação para problemas de rotulação de sequências.
III. Como as camadas adicionais são relativamente pequenas, o custo computacional do treinamento é pequeno comparado ao custo do treino do modelo original do zero.
IV. A reutilização das representações do modelo pre-treinado e a adaptação da camada de classificação ou regressão requer poucos dados anotados.
V. Portanto, a capacidade de ajustar camadas de classificação ou regressão adicionadas sobre o modelo pré-treinado permite que o MLM seja adaptado a uma variedade de tarefas com um custo computacional reduzido.
■
### Estratégias de Fine-Tuning
Existem diversas estratégias de fine-tuning, sendo as mais comuns:

1.  **Fine-tuning Completo:** Todos os parâmetros do modelo pré-treinado e das camadas adicionais são ajustados durante o treinamento [^12]. Esta estratégia é adequada para tarefas com grandes conjuntos de dados e onde a adaptação completa do modelo pode levar a melhores resultados [^12].
2.  **Fine-tuning Seletivo:** Apenas os parâmetros das camadas adicionais são ajustados, mantendo congelados os parâmetros do modelo pré-treinado [^12]. Esta estratégia é útil para tarefas com poucos dados anotados ou quando se pretende preservar o conhecimento geral adquirido durante o pre-treinamento [^12].
3.  **Fine-tuning em Camadas:** Um número limitado das últimas camadas do modelo pré-treinado é ajustado, além das camadas adicionais. Esta estratégia combina a adaptação específica da tarefa com a retenção do conhecimento geral das camadas inferiores do modelo [^12].

**Teorema 5** (Compromisso entre Fine-Tuning e Overfitting) A escolha entre fine-tuning completo, seletivo e em camadas representa um compromisso entre a capacidade de adaptar o modelo à tarefa específica e o risco de *overfitting* aos dados de treinamento.

*Prova:*
I. O fine-tuning completo permite uma maior capacidade de adaptação do modelo à tarefa específica, mas pode levar a *overfitting* se o conjunto de dados de fine-tuning for pequeno.
II. O fine-tuning seletivo, ao congelar os parâmetros do modelo pré-treinado, preserva o conhecimento geral de linguagem, mas pode limitar a capacidade do modelo em se adaptar a detalhes específicos da tarefa.
III. O fine-tuning em camadas encontra um equilíbrio entre os dois, permitindo a adaptação das camadas superiores do modelo à tarefa específica e retendo o conhecimento de baixo nível aprendido no pre-treinamento.
IV. A escolha da estratégia de fine-tuning ideal deve ser feita considerando o tamanho dos dados de fine-tuning, a complexidade da tarefa e a necessidade de adaptação ou retenção de conhecimento generalizado.
V. Portanto, a escolha entre fine-tuning completo, seletivo e em camadas representa um compromisso entre a capacidade de adaptar o modelo à tarefa específica e o risco de *overfitting* aos dados de treinamento.
■
**Teorema 5.1** (Regularização e Estabilidade do Fine-Tuning) A utilização de técnicas de regularização como dropout, weight decay e *early stopping* durante o processo de fine-tuning melhora a generalização do modelo e previne o *overfitting*, resultando em modelos mais estáveis e com melhor desempenho.

*Prova:*
I. O *dropout* impede a coadaptação dos neurónios, levando o modelo a ser mais robusto e resistente a *overfitting*.
II. O *weight decay* penaliza os pesos com valores muito altos, ajudando o modelo a ser mais simples e a generalizar melhor.
III. O *early stopping* para o treinamento quando a performance em um *validation set* deixa de aumentar, evitando que o modelo aprenda ruído dos dados de treino e evitando o *overfitting*.
IV. A combinação destas técnicas de regularização ajuda a estabilizar o treinamento e a obter modelos com melhor performance.
V. Portanto, a utilização de técnicas de regularização durante o processo de fine-tuning melhora a generalização do modelo e previne o *overfitting*.
■
**Teorema 5.2** (Impacto do Learning Rate no Fine-Tuning) A escolha de uma taxa de aprendizado (learning rate) adequada é crucial para o sucesso do fine-tuning. Taxas de aprendizado muito altas podem levar a instabilidade no treinamento e taxas muito baixas podem levar a convergência lenta ou a mínimos locais subótimos.

*Prova:*
I. Uma taxa de aprendizado muito alta pode fazer com que os parâmetros do modelo oscilem e não convirjam para um mínimo da função de perda.
II. Uma taxa de aprendizado muito baixa pode fazer com que o treinamento demore muito tempo a convergir ou que o modelo fique preso em mínimos locais.
III. A escolha de uma taxa de aprendizado adequada garante que o modelo aprenda de forma estável e rápida, resultando num modelo de melhor performance.
IV. Portanto, a escolha de uma taxa de aprendizado adequada é crucial para o sucesso do fine-tuning, assegurando que o modelo converge para um bom resultado.
■

###  Aplicações do Fine-Tuning
O fine-tuning é uma etapa essencial para a aplicação de MLMs em uma variedade de tarefas de processamento de linguagem natural, como:

*   **Análise de Sentimentos:** Classificação de textos em categorias de sentimento (positivo, negativo, neutro).
*   **Reconhecimento de Entidades Nomeadas (NER):** Identificação de entidades em textos (pessoas, organizações, locais).
*   **Classificação de Textos:** Categorização de textos em diferentes tópicos ou classes.
*   **Inferencia de Linguagem Natural (NLI):** Determinação da relação entre pares de sentenças (entailment, contradição, neutralidade).
*   **Extração de Informação:** Extração de informações específicas de textos, como datas, preços, ou outras entidades.
*   **Resposta a Perguntas:** Geração de respostas a perguntas formuladas em linguagem natural.

**Corolário 6** (Versatilidade do Fine-Tuning) A versatilidade do fine-tuning permite que modelos pré-treinados sejam adaptados a uma grande variedade de tarefas de processamento de linguagem natural, o que demonstra o seu poder e a sua aplicabilidade generalizada.

*Prova:*
I.  O fine-tuning permite usar o conhecimento aprendido durante o pre-treinamento para resolver diferentes tarefas através da adição de diferentes camadas (classificação, rotulação, etc).
II. Esta capacidade de transferir o conhecimento é crucial para a versatilidade dos modelos pré-treinados.
III. Ao ajustar os parâmetros para as diferentes tarefas, os modelos adaptam o conhecimento a um problema específico.
IV. Portanto, a versatilidade do fine-tuning permite que modelos pré-treinados sejam adaptados a uma grande variedade de tarefas, demonstrando o seu poder e a sua aplicabilidade generalizada.
■

**Corolário 6.1** (Customização do Fine-Tuning) A possibilidade de adaptar a arquitetura das camadas adicionais e a estratégia de fine-tuning permite a customização dos modelos pré-treinados para tarefas altamente especializadas e específicas.
*Prova:*
I. O fine-tuning permite a adição de camadas específicas para diferentes tarefas, não se limitando a classificação ou regressão.
II. A escolha da estratégia de fine-tuning permite adaptar o processo de treinamento a diferentes quantidades de dados e complexidades das tarefas.
III. A combinação dessas características permite a customização dos modelos pre-treinados para diferentes problemas, adaptando-os a tarefas muito específicas.
IV. Portanto, a possibilidade de adaptar a arquitetura das camadas adicionais e a estratégia de fine-tuning permite a customização dos modelos pré-treinados.
■

### Conclusão
O fine-tuning é uma etapa crucial na aplicação de **Masked Language Models** a tarefas específicas. Através do fine-tuning, os modelos pré-treinados são capazes de aprender as nuances e características específicas de cada tarefa, sem necessidade de grandes quantidades de dados anotados. A combinação da capacidade de aprender representações contextuais durante o pré-treinamento com a flexibilidade do fine-tuning torna os MLMs modelos poderosos e versáteis para diferentes tarefas de processamento de linguagem natural. A escolha adequada da estratégia de fine-tuning, juntamente com as técnicas de regularização, garantem que o modelo seja eficaz para o problema em questão e apresente um bom desempenho.

### Referências
[^1]: Capítulo 9, 10 e 11 do livro texto.
[^12]: Seção 11.4 do livro texto.
[^13]: Seção 11.4.1 do livro texto.
[^14]: Seção 11.4.2 do livro texto.
[^15]: Seção 11.5 do livro texto.
[^17]: Seção 11.5.3 do livro texto.
<!-- END -->
