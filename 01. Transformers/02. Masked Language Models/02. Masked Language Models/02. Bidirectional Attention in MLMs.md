## Mecanismos de Aten√ß√£o Bidirecional em Masked Language Models

### Introdu√ß√£o
Este cap√≠tulo explora em detalhes os mecanismos de aten√ß√£o bidirecional empregados em **Masked Language Models (MLMs)**, contrastando-os com os modelos causais discutidos anteriormente [^1]. Em modelos causais, como os apresentados nos cap√≠tulos anteriores, a aten√ß√£o √© restrita a tokens precedentes, seguindo um fluxo da esquerda para a direita [^1]. Por outro lado, os MLMs, com seu codificador transformer bidirecional, habilitam o mecanismo de aten√ß√£o a operar sobre toda a sequ√™ncia de entrada, acessando informa√ß√µes tanto do contexto anterior quanto do posterior de cada token [^1, 2]. Esta caracter√≠stica fundamental √© alcan√ßada pela remo√ß√£o da m√°scara de aten√ß√£o, que √© uma etapa essencial nos modelos causais [^2]. A habilidade de utilizar todo o contexto permite que os MLMs capturem rela√ß√µes complexas entre palavras de maneira mais eficaz, aprofundando sua capacidade de compreens√£o textual [^2].

### Aten√ß√£o Bidirecional vs. Aten√ß√£o Causal
A principal diferen√ßa na fun√ß√£o de aten√ß√£o entre MLMs e modelos causais reside na capacidade de considerar ou n√£o informa√ß√µes de tokens futuros [^1, 2]. Em modelos causais, a matriz de aten√ß√£o $Q K^T$ √© mascarada para impedir que um token atenda a tokens subsequentes, garantindo assim uma dire√ß√£o de processamento da esquerda para a direita [^2]. Nos MLMs, essa m√°scara √© removida, permitindo que cada token atenda a todos os outros tokens na sequ√™ncia [^2, 3].

Essa mudan√ßa √© matematicamente refletida na forma como a matriz de aten√ß√£o $A$ √© calculada [^3]. Em modelos causais, a matriz de aten√ß√£o $A$ √© definida como:
$$A = \text{softmax} \left( \text{mask} \left( \frac{Q K^T}{\sqrt{d_k}} \right) \right) V$$
onde `mask` √© uma fun√ß√£o que zera os elementos superiores da matriz $Q K^T$, impedindo que tokens atendam a tokens futuros.

Nos MLMs, essa fun√ß√£o de `mask` √© removida, e a matriz de aten√ß√£o √© calculada sem restri√ß√µes, como:
$$A = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) V$$ [^3]

Esta remo√ß√£o da m√°scara permite que cada token compute sua relev√¢ncia em rela√ß√£o a todos os outros tokens na sequ√™ncia, tanto anteriores quanto posteriores [^2, 3].

**Lema 2** (Impacto da Remo√ß√£o da M√°scara na Aten√ß√£o) A remo√ß√£o da m√°scara na matriz $QK^T$ em MLMs habilita o modelo a capturar depend√™ncias contextuais bidirecionais, permitindo a compreens√£o de rela√ß√µes de longo alcance entre palavras, que √© essencial para tarefas de compreens√£o de linguagem natural, em contraste com a vis√£o unidirecional dos modelos causais.

*Prova:*
I.  A aten√ß√£o computa a relev√¢ncia de cada token em rela√ß√£o a todos os outros, usando $Q$, $K$ e $V$.
II. Em modelos causais, a matriz $QK^T$ √© mascarada para restringir a aten√ß√£o a tokens passados.
III. Ao remover a m√°scara, cada token atende a todos os tokens na sequ√™ncia, tanto passados quanto futuros.
IV. Esta aten√ß√£o bidirecional permite que cada token seja contextualizado por toda a sequ√™ncia.
V. A contextualiza√ß√£o permite que MLMs aprendam depend√™ncias contextuais mais abrangentes, resultando em um entendimento mais profundo do texto, o que n√£o √© poss√≠vel em modelos com aten√ß√£o causal.
VI.  Portanto, a remo√ß√£o da m√°scara na matriz $QK^T$ em MLMs habilita o modelo a capturar depend√™ncias contextuais bidirecionais.
‚ñ†

**Lema 2.1** (Conex√£o com Matriz de Adjac√™ncia) A matriz de aten√ß√£o em MLMs, ao capturar as depend√™ncias bidirecionais, pode ser vista como uma forma de representa√ß√£o de uma matriz de adjac√™ncia em um grafo onde os n√≥s s√£o os tokens e as arestas representam a relev√¢ncia entre os tokens.

*Prova:*
I. Cada elemento $A_{ij}$ na matriz de aten√ß√£o $A$ representa o qu√£o importante o token $j$ √© para o token $i$.
II. Em um grafo, uma matriz de adjac√™ncia $M$ representa as conex√µes entre os n√≥s, onde $M_{ij}$ indica a presen√ßa de uma aresta entre o n√≥ $i$ e o n√≥ $j$ (e, em grafos ponderados, o peso dessa aresta).
III. A matriz de aten√ß√£o $A$, da mesma forma, pondera a "conex√£o" ou relev√¢ncia entre cada par de tokens.
IV. Portanto, a matriz de aten√ß√£o $A$ pode ser interpretada como uma matriz de adjac√™ncia ponderada, onde as arestas representam a relev√¢ncia de um token em rela√ß√£o aos outros.
‚ñ†

### C√°lculo da Aten√ß√£o em MLMs
O c√°lculo da aten√ß√£o em MLMs segue as mesmas etapas b√°sicas dos modelos causais, mas com a diferen√ßa crucial de n√£o utilizar mascaramento [^3]. Para uma sequ√™ncia de entrada com embeddings de tokens $x_1, x_2, ..., x_n$, os seguintes passos s√£o executados:

1.  **Transforma√ß√£o Linear**: Os embeddings de entrada s√£o transformados em tr√™s matrizes: consulta ($Q$), chave ($K$) e valor ($V$) usando transforma√ß√µes lineares aprendidas [^2].
    $$ Q = X W_Q $$
    $$ K = X W_K $$
    $$ V = X W_V $$
    Onde $X$ √© a matriz dos embeddings e $W_Q, W_K, W_V$ s√£o as matrizes de transforma√ß√£o aprendidas.

2.  **C√°lculo da Pontua√ß√£o de Aten√ß√£o**: A matriz de aten√ß√£o n√£o normalizada ($Q K^T$) √© calculada multiplicando a matriz de consulta ($Q$) pela transposta da matriz de chave ($K^T$) [^3].
    $$ QK^T =  \begin{bmatrix}
    q_1 \\
    q_2 \\
    \vdots \\
    q_n
    \end{bmatrix}
    \begin{bmatrix}
    k_1 & k_2 & \cdots & k_n
    \end{bmatrix} =
    \begin{bmatrix}
    q_1k_1 & q_1k_2 & \cdots & q_1k_n \\
    q_2k_1 & q_2k_2 & \cdots & q_2k_n \\
    \vdots & \vdots & \ddots & \vdots \\
    q_nk_1 & q_nk_2 & \cdots & q_nk_n \\
    \end{bmatrix}
    $$
3. **Escalonamento**: A matriz de pontua√ß√£o √© escalonada dividindo-a por $\sqrt{d_k}$, onde $d_k$ √© a dimensionalidade das matrizes $Q$ e $K$, para evitar valores muito grandes durante o c√°lculo do softmax [^3].
   $$ \frac{QK^T}{\sqrt{d_k}} $$
4. **Normaliza√ß√£o (Softmax)**: Uma fun√ß√£o softmax √© aplicada a cada linha da matriz escalonada para obter pesos de aten√ß√£o normalizados que somam 1 [^3].
   $$ A = \text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right) $$
    Cada entrada $A_{ij}$ na matriz de aten√ß√£o $A$ representa o qu√£o importante o token $j$ √© para o token $i$.

5.  **Pondera√ß√£o dos Valores**: Os pesos de aten√ß√£o calculados s√£o utilizados para ponderar os valores ($V$), resultando nos vetores de sa√≠da contextualizados ($h$).
    $$ h = A V $$

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar uma sequ√™ncia de entrada simplificada: "o c√£o corre". Vamos representar cada palavra com vetores de embedding unidimensionais para facilitar a compreens√£o:
>
>  - "o": $x_1 = [1]$
>  - "c√£o": $x_2 = [2]$
>  - "corre": $x_3 = [3]$
>
>  Para simplificar, vamos usar transforma√ß√µes lineares $W_Q$, $W_K$ e $W_V$ que n√£o alteram os embeddings, ou seja $W_Q = W_K = W_V = 1$. Portanto, $Q=K=V=X$, onde $X$ √© a matriz de embeddings:
>
> $$ Q = K = V = X = \begin{bmatrix}
> 1 \\
> 2 \\
> 3
> \end{bmatrix}$$
>
> 1. Calculamos $QK^T$:
> $$QK^T = \begin{bmatrix}
> 1 \\
> 2 \\
> 3
> \end{bmatrix}
> \begin{bmatrix}
> 1 & 2 & 3
> \end{bmatrix} =
> \begin{bmatrix}
> 1 & 2 & 3 \\
> 2 & 4 & 6 \\
> 3 & 6 & 9
> \end{bmatrix}$$
>
> 2. Dividimos $QK^T$ por $\sqrt{d_k}$. Como $d_k = 1$, ent√£o $\sqrt{d_k} = 1$. Logo,
> $$ \frac{QK^T}{\sqrt{d_k}} = QK^T = \begin{bmatrix}
> 1 & 2 & 3 \\
> 2 & 4 & 6 \\
> 3 & 6 & 9
> \end{bmatrix}$$
>
> 3. Aplicamos a fun√ß√£o softmax para normalizar as pontua√ß√µes de aten√ß√£o.  A fun√ß√£o softmax √© aplicada a cada linha da matriz. Por exemplo, para a primeira linha:
>  $$softmax([1, 2, 3]) = [\frac{e^1}{e^1 + e^2 + e^3}, \frac{e^2}{e^1 + e^2 + e^3}, \frac{e^3}{e^1 + e^2 + e^3}] \approx [0.09, 0.24, 0.67]$$
> Aplicando o softmax a todas as linhas, obtemos a matriz de aten√ß√£o:
>
>  $$ A = \text{softmax} \begin{bmatrix}
> 1 & 2 & 3 \\
> 2 & 4 & 6 \\
> 3 & 6 & 9
> \end{bmatrix} \approx \begin{bmatrix}
> 0.09 & 0.24 & 0.67 \\
> 0.09 & 0.24 & 0.67 \\
> 0.09 & 0.24 & 0.67
> \end{bmatrix}$$
>
>
> 4.  Calculamos os vetores contextuais $h$:
>
> $$h = AV = \begin{bmatrix}
> 0.09 & 0.24 & 0.67 \\
> 0.09 & 0.24 & 0.67 \\
> 0.09 & 0.24 & 0.67
> \end{bmatrix}
> \begin{bmatrix}
> 1 \\
> 2 \\
> 3
> \end{bmatrix} = \begin{bmatrix}
> 2.58 \\
> 2.58 \\
> 2.58
> \end{bmatrix}$$
>
> Neste exemplo simplificado, todos os tokens acabam com o mesmo vetor contextualizado, pois todos os tokens contribu√≠ram da mesma forma para a aten√ß√£o, o que n√£o acontece em casos reais. Este exemplo demonstra o fluxo de c√°lculo da aten√ß√£o bidirecional. Em modelos reais, os valores e vetores seriam muito maiores e mais complexos, permitindo que o modelo aprenda rela√ß√µes de contextualiza√ß√£o mais complexas.
>
> ```mermaid
> graph LR
>     A[Input Embeddings X] --> B(Linear Transform Q);
>      A --> C(Linear Transform K);
>      A --> D(Linear Transform V);
>     B --> E(QK^T);
>     C --> E;
>     E --> F(Scale by sqrt(dk));
>     F --> G(Softmax);
>     G --> H(Attention Matrix A);
>     H --> I(AV);
>     D --> I;
>     I --> J[Contextualized Output h];
>
> ```

**Teorema 1** (Invari√¢ncia por Permuta√ß√£o na Aten√ß√£o Bidirecional) Dada uma sequ√™ncia de tokens $X = [x_1, x_2, ..., x_n]$ e uma permuta√ß√£o $\pi$ dos √≠ndices $\{1, 2, ..., n\}$, a matriz de aten√ß√£o $A$ calculada para a sequ√™ncia permutada $\pi(X) = [x_{\pi(1)}, x_{\pi(2)}, ..., x_{\pi(n)}]$ tem o mesmo conjunto de valores de aten√ß√£o que a matriz de aten√ß√£o calculada para $X$, embora com as linhas e colunas permutadas correspondentes √† permuta√ß√£o $\pi$.

*Prova:*
I.  A matriz de aten√ß√£o $A$ √© calculada como $\text{softmax} \left( \frac{Q K^T}{\sqrt{d_k}} \right)$.
II.  As matrizes $Q$, $K$ e $V$ s√£o transforma√ß√µes lineares dos embeddings $X$, ou seja, $Q = XW_Q$, $K = XW_K$, $V = XW_V$.
III. Uma permuta√ß√£o $\pi$ dos √≠ndices da sequ√™ncia $X$ resulta em uma permuta√ß√£o das linhas de $Q$, $K$ e $V$.
IV. O c√°lculo de $QK^T$ para a sequ√™ncia permutada resulta em uma matriz onde as linhas e colunas est√£o permutadas de acordo com $\pi$.
V. A aplica√ß√£o da fun√ß√£o softmax a $QK^T$ preserva a propriedade de permuta√ß√£o, com as linhas da matriz de aten√ß√£o $A$ permutadas de acordo com $\pi$.
VI. Portanto, os valores de aten√ß√£o em $A$ s√£o os mesmos, independente da permuta√ß√£o dos tokens em $X$, apenas a ordem das linhas e colunas √© afetada pela permuta√ß√£o $\pi$.
‚ñ†

### Consequ√™ncias da Aten√ß√£o Bidirecional
A aten√ß√£o bidirecional permite que os MLMs capturem depend√™ncias de longo alcance entre palavras [^2]. Em compara√ß√£o com modelos causais, onde o contexto √© limitado √†s palavras precedentes, os MLMs podem considerar tanto as palavras anteriores quanto as seguintes ao prever um token mascarado [^2, 3]. Essa capacidade de "olhar para frente" e "olhar para tr√°s" oferece uma representa√ß√£o mais completa do contexto [^2].

Al√©m disso, a remo√ß√£o da m√°scara na matriz de aten√ß√£o $Q K^T$ torna os MLMs mais adequados para tarefas que exigem uma compreens√£o profunda do contexto, como classifica√ß√£o de texto, infer√™ncia de linguagem natural, e nomea√ß√£o de entidades [^2]. No entanto, essa bidirecionalidade tamb√©m torna os MLMs menos adequados para tarefas de gera√ß√£o de texto, onde a predi√ß√£o sequencial da esquerda para a direita √© crucial [^2].

**Proposi√ß√£o 2** (Trade-off entre Aten√ß√£o Bidirecional e Gera√ß√£o Sequencial) A natureza bidirecional da aten√ß√£o em MLMs, embora vantajosa para tarefas de compreens√£o, dificulta a gera√ß√£o sequencial de texto. Essa limita√ß√£o contrasta com os modelos causais que, por terem uma aten√ß√£o sequencial, s√£o mais adequados para gera√ß√£o de texto, mesmo que menos eficazes em compreens√£o do contexto.

*Prova:*
I. A aten√ß√£o bidirecional permite que cada token atenda a todos os outros tokens na sequ√™ncia, permitindo a compreens√£o do contexto tanto do lado esquerdo quanto do lado direito do token.
II. A gera√ß√£o de texto, em modelos causais, necessita da predi√ß√£o de cada token sequencialmente a partir do contexto anterior, o que a aten√ß√£o bidirecional impede ao permitir que a predi√ß√£o de um token dependa de tokens futuros.
III. Modelos causais, com sua aten√ß√£o unidirecional, s√£o projetados especificamente para este processo de predi√ß√£o sequencial.
IV. Portanto, a aten√ß√£o bidirecional, que √© ideal para compreens√£o, dificulta a gera√ß√£o sequencial, enquanto a aten√ß√£o causal, ideal para gera√ß√£o, limita a compreens√£o contextual.
V. Logo, existe um trade-off entre aten√ß√£o bidirecional e gera√ß√£o sequencial.
‚ñ†

**Corol√°rio 2.1** (Aplica√ß√µes Espec√≠ficas de Modelos Bidirecionais)  A natureza bidirecional da aten√ß√£o em MLMs torna-os particularmente adequados para tarefas de an√°lise de sentimentos, resumo de texto, e resposta a perguntas, onde o contexto global √© fundamental para a compreens√£o.

*Prova:*
I.  Modelos de aten√ß√£o bidirecional, como MLMs, capturam depend√™ncias de longo alcance e rela√ß√µes contextuais em toda a sequ√™ncia de entrada.
II. Tarefas de an√°lise de sentimentos, resumo de texto e resposta a perguntas geralmente requerem uma compreens√£o hol√≠stica do texto, onde o contexto geral √© crucial.
III. MLMs, devido a sua capacidade de usar todo o contexto, podem fornecer representa√ß√µes mais ricas e relevantes para a tomada de decis√µes nessas tarefas.
IV. Portanto, a aten√ß√£o bidirecional torna MLMs mais adequados para tarefas que se beneficiam de uma compreens√£o contextual abrangente.
‚ñ†

### Conclus√£o
A fun√ß√£o de aten√ß√£o em **Masked Language Models** difere fundamentalmente dos modelos causais por permitir o uso de todo o contexto para a representa√ß√£o de cada token. Ao remover a m√°scara da matriz de aten√ß√£o $Q K^T$, os MLMs habilitam a captura de depend√™ncias contextuais bidirecionais. Este mecanismo, embora complexo, √© fundamental para a capacidade dos MLMs de realizar tarefas de compreens√£o com efic√°cia. A an√°lise detalhada do processo de aten√ß√£o, desde a transforma√ß√£o linear at√© a pondera√ß√£o dos valores, ilustra a sofistica√ß√£o desses modelos. No entanto, √© essencial notar o trade-off entre a aten√ß√£o bidirecional, que favorece a compreens√£o, e a aten√ß√£o unidirecional, que favorece a gera√ß√£o sequencial, delineando o escopo de aplica√ß√£o de cada modelo.

### Refer√™ncias
[^1]: Cap√≠tulo 9, 10 e 11 do livro texto.
[^2]: Se√ß√£o 11.1 do livro texto.
[^3]: Se√ß√£o 11.1.1 do livro texto.
<!-- END -->
