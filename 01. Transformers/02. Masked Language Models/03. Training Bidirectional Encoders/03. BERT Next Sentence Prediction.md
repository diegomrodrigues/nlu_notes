## Treinamento de Encoders Bidirecionais: Predi√ß√£o da Pr√≥xima Senten√ßa e Regimes de Treinamento

### Introdu√ß√£o
Em continuidade aos t√≥picos anteriores sobre **Masked Language Modeling (MLM)** e a fun√ß√£o de perda de entropia cruzada [^1, 2], este cap√≠tulo abordar√° um aspecto adicional do treinamento original do BERT: a **Next Sentence Prediction (NSP)**, e como ela √© utilizada em conjunto com o MLM, al√©m de discutir os diferentes regimes de treinamento usados em modelos bidirecionais [^1]. Embora a NSP tenha sido descontinuada em modelos mais recentes como o RoBERTa, entender seu papel no BERT original oferece uma vis√£o mais completa do processo de treinamento.

### Conceitos Fundamentais

#### Next Sentence Prediction (NSP)

O objetivo principal do **Masked Language Modeling (MLM)** √© aprender representa√ß√µes eficazes no n√≠vel da palavra atrav√©s da previs√£o de tokens mascarados em uma sequ√™ncia [^2]. No entanto, muitos problemas de processamento de linguagem natural envolvem entender a rela√ß√£o entre duas ou mais senten√ßas. Para lidar com esse tipo de problema, o modelo original BERT inclu√≠a uma tarefa secund√°ria chamada **Next Sentence Prediction (NSP)** [^1]. O objetivo do NSP √© treinar o modelo a reconhecer se duas senten√ßas de entrada s√£o adjacentes em um texto, ou se s√£o de diferentes partes do corpus.

Durante o treinamento, o modelo √© apresentado com pares de senten√ßas:

1.  **Pares Positivos:** 50% dos pares s√£o compostos por senten√ßas adjacentes reais do corpus de treinamento [^6].
2.  **Pares Negativos:** Os outros 50% s√£o compostos por uma primeira senten√ßa, e uma segunda senten√ßa selecionada aleatoriamente de outra parte do corpus [^6].

O modelo deve classificar cada par de senten√ßas como "isNext" (as senten√ßas s√£o adjacentes) ou "notNext" (as senten√ßas n√£o s√£o adjacentes). Para realizar a classifica√ß√£o, o modelo usa o vetor de sa√≠da associado a um token especial `[CLS]`, adicionado ao in√≠cio de cada par de senten√ßas [^7]. O vetor de sa√≠da do token `[CLS]` √© ent√£o alimentado em uma camada de classifica√ß√£o com pesos trein√°veis, que gera uma probabilidade de que o par de senten√ßas √© "isNext" ou "notNext" [^7].

> üí° **Exemplo Num√©rico:**
>
> Considere o seguinte par de senten√ßas:
>
> **Senten√ßa 1 (Premissa):** "O sol nasce no leste."
>
> **Senten√ßa 2 (Hip√≥tese):** "A noite cai no oeste."
>
> **Pares Positivos:** Se as senten√ßas 1 e 2 forem adjacentes no corpus de treinamento, o par seria classificado como "isNext".
>
> **Pares Negativos:** Se a senten√ßa 2 fosse selecionada aleatoriamente de outra parte do corpus, como "Gatos gostam de leite", o par seria classificado como "notNext".
>
> Para realizar essa classifica√ß√£o, a sequ√™ncia de entrada seria: `[CLS] O sol nasce no leste. [SEP] A noite cai no oeste.`, onde `[CLS]` √© o token especial que √© usado para a classifica√ß√£o, e `[SEP]` √© o token separador usado para separar as duas senten√ßas.
>
> A sa√≠da associada ao token `[CLS]` seria ent√£o utilizada para gerar a predi√ß√£o "isNext" ou "notNext". Suponha que a sa√≠da da camada do Transformer para o token `[CLS]` seja um vetor de dimens√£o 768, denotado como $h_{[CLS]} \in \mathbb{R}^{768}$. Este vetor √© ent√£o passado por uma camada de classifica√ß√£o linear, com pesos $W \in \mathbb{R}^{2 \times 768}$ e bias $b \in \mathbb{R}^{2}$, para gerar as probabilidades de "isNext" e "notNext". A probabilidade de "isNext" pode ser calculada como:
>
> $\hat{y} = \text{softmax}(W h_{[CLS]} + b)$
>
> Se o primeiro elemento do vetor $\hat{y}$ for maior que o segundo, o modelo prediz "isNext". Caso contr√°rio, prediz "notNext". Digamos que ap√≥s o c√°lculo, $\hat{y} = [0.9, 0.1]$. Isso significa que o modelo est√° 90% confiante de que a segunda senten√ßa segue a primeira. A fun√ß√£o de perda para este exemplo seria baseada na entropia cruzada. Se o r√≥tulo verdadeiro fosse "isNext" (1), a perda seria:
>
> $L = -[1 \cdot \log(0.9) + 0 \cdot \log(0.1)] \approx 0.105$.

Para facilitar o treinamento da NSP, o BERT introduz dois tokens especiais na representa√ß√£o de entrada [^6]:

1.  `[CLS]` (Classification): √â um token adicionado no in√≠cio de cada par de senten√ßas. O vetor de sa√≠da do token `[CLS]` na camada final do encoder √© usado como uma representa√ß√£o da sequ√™ncia inteira para classifica√ß√£o [^7].
2.  `[SEP]` (Separator): √â um token adicionado entre as duas senten√ßas e ao final da segunda senten√ßa para separ√°-las [^6].

Adicionalmente, embeddings representando o primeiro e segundo segmentos de texto s√£o adicionados aos embeddings das palavras e aos embeddings de posi√ß√£o para ajudar o modelo a distinguir as senten√ßas.

A fun√ß√£o de perda para o NSP √© uma entropia cruzada que calcula a diferen√ßa entre a previs√£o do modelo e o r√≥tulo correto ("isNext" ou "notNext") [^7]. No treinamento original do BERT, a perda do NSP era usada em conjunto com a perda do MLM para aprimorar o aprendizado do modelo [^6].
$$
    L_{NSP} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y_i}) + (1 - y_i) \log(1 - \hat{y_i})]
$$
Onde:
- $L_{NSP}$ √© a perda de predi√ß√£o da pr√≥xima senten√ßa
- $N$ √© o n√∫mero de pares de senten√ßas no batch
- $y_i$ √© o r√≥tulo da classe (0 ou 1)
- $\hat{y_i}$ √© a probabilidade prevista pelo modelo

**Lema 3**: O objetivo da tarefa de NSP √© treinar o modelo a identificar se duas senten√ßas est√£o relacionadas em um contexto discursivo, o que √© diferente do objetivo do MLM que √© focado em entender as rela√ß√µes sint√°ticas e sem√¢nticas dentro de uma √∫nica senten√ßa.

*Proof Outline:* A tarefa MLM se concentra em prever palavras dentro de uma mesma frase, exigindo que o modelo compreenda as depend√™ncias locais entre palavras. O NSP, por outro lado, exige a compreens√£o das rela√ß√µes de contexto e coes√£o entre senten√ßas, que s√£o depend√™ncias de longo alcance. Portanto, o MLM √© mais √∫til para modelar as depend√™ncias locais, enquanto o NSP modela depend√™ncias intersentenciais, ou seja, conex√µes discursivas entre senten√ßas.

**Proposi√ß√£o 1**: Embora o NSP tenha como objetivo capturar rela√ß√µes intersentenciais, a natureza bin√°ria da tarefa pode ser uma limita√ß√£o, uma vez que n√£o diferencia entre os diferentes tipos de rela√ß√µes (e.g., explica√ß√£o, elabora√ß√£o, contraste) que podem existir entre as senten√ßas.

*Proof Outline*: A tarefa de NSP se resume a prever se duas senten√ßas s√£o consecutivas ou n√£o, ou seja, se existe uma rela√ß√£o intersentencial de qualquer natureza. Essa abordagem ignora a complexidade das rela√ß√µes discursivas que podem existir entre as senten√ßas.

**Prova da Proposi√ß√£o 1:**

I. A tarefa NSP √© formulada como uma classifica√ß√£o bin√°ria: "isNext" (as senten√ßas s√£o adjacentes) ou "notNext" (as senten√ßas n√£o s√£o adjacentes).

II. A classifica√ß√£o bin√°ria n√£o oferece informa√ß√µes sobre o tipo espec√≠fico de rela√ß√£o entre as senten√ßas (e.g., causa, explica√ß√£o, contraste, etc.).

III. Por exemplo, as senten√ßas "O c√©u est√° azul" e "√â um dia agrad√°vel" podem ser consecutivas, mas a rela√ß√£o entre elas √© uma associa√ß√£o circunstancial. Uma senten√ßa como "Choveu muito, ent√£o a rua est√° molhada" possui uma rela√ß√£o de causa e efeito. Ambas podem receber o mesmo r√≥tulo "isNext"

IV. A tarefa NSP ignora essas diferen√ßas sem√¢nticas, agrupando as rela√ß√µes discursivas em uma √∫nica classe, sem discriminar os diferentes tipos de rela√ß√µes.

V.  Portanto, a natureza bin√°ria da tarefa NSP limita sua capacidade de capturar a complexidade das rela√ß√µes intersentenciais. ‚ñ†

#### Regimes de Treinamento e Abandono da NSP
O modelo BERT original era treinado com a combina√ß√£o das perdas MLM e NSP [^7]. No entanto, modelos posteriores, como o RoBERTa, optaram por descontinuar o uso da NSP, devido a sua relativa inefic√°cia [^6]. O regime de treinamento do RoBERTa utiliza sequ√™ncias cont√≠nuas de senten√ßas do corpus, com um token separador adicional inserido quando as sequ√™ncias s√£o extra√≠das de diferentes documentos, o que se mostrou mais eficiente e eficaz [^6].

O regime de treinamento do BERT e do RoBERTa s√£o diferentes:

1.  **BERT:**
    -   Usa pares de senten√ßas gerados com a t√©cnica do NSP com 50% de probabilidade de serem consecutivas e 50% n√£o consecutivas [^6].
    -   As sequ√™ncias t√™m um comprimento m√°ximo de 512 tokens, o que limita a quantidade de contexto que o modelo pode utilizar [^6].
    - Combina as perdas do MLM e do NSP [^7].
2.  **RoBERTa:**
    -   N√£o utiliza a tarefa de NSP.
    -   Usa sequ√™ncias cont√≠nuas de senten√ßas do corpus de treinamento, sem a necessidade de construir pares de senten√ßas.
    -   Quando a sequ√™ncia atinge o tamanho m√°ximo, o modelo simplesmente continua com a pr√≥xima sequ√™ncia do corpus [^6].
    - Adiciona um separador adicional quando a sequ√™ncia alcan√ßa o final de um documento e come√ßa a pr√≥xima,
    - Utiliza tamanhos de batch maiores do que BERT (entre 8K e 32K) [^6].

O abandono da NSP pelo RoBERTa indica que a tarefa de MLM por si s√≥ √© capaz de treinar modelos de linguagem eficazes, e que a tarefa de NSP pode n√£o contribuir significativamente para o aprendizado das representa√ß√µes contextuais de modelos bidirecionais, ou at√© mesmo, pode prejudicar o aprendizado.
> üí° **Exemplo Num√©rico:**
>
> **Treinamento no BERT:**
>
> - Pares de senten√ßas s√£o formados (50% consecutivos e 50% n√£o consecutivos). Suponha que um par de senten√ßas como o do exemplo anterior seja concatenado, resultando na sequ√™ncia `[CLS] O sol nasce no leste. [SEP] A noite cai no oeste. [SEP]`, e essa sequ√™ncia contenha 10 tokens (incluindo os tokens especiais).
> - Cada par √© truncado em 512 tokens, usando o token [SEP] como separador, e um token [CLS] como primeiro token. Se essa sequ√™ncia fosse menor que 512, ela poderia ser preenchida com tokens de padding `[PAD]`.
> - A perda total √© a soma da perda MLM (tokens mascarados) e da perda da NSP (r√≥tulo de par de senten√ßas). Suponha que a perda do MLM para este exemplo seja 0.5 e a perda da NSP seja 0.1. A perda total seria 0.6.
>
> **Treinamento no RoBERTa:**
>
> - As senten√ßas s√£o encadeadas sequencialmente para formar uma sequ√™ncia com at√© 512 tokens. Por exemplo, as senten√ßas "O sol nasce no leste. A noite cai no oeste. Gatos gostam de leite." poderiam ser concatenadas em uma √∫nica sequ√™ncia de texto at√© atingir 512 tokens.
> - Quando a sequ√™ncia cruza a fronteira entre dois documentos, um token [SEP] adicional √© inserido. Por exemplo, se as senten√ßas acima vierem de dois documentos diferentes, a sequ√™ncia seria: "O sol nasce no leste. A noite cai no oeste. [SEP] Gatos gostam de leite."
>  - A perda total √© calculada com o MLM (tokens mascarados) com 15% dos tokens. Suponha que nessa sequ√™ncia de exemplo, 15% dos tokens foram mascarados, e a perda do MLM calculada seja 0.3.
> - S√£o utilizados batches maiores. Por exemplo, um batch no RoBERTa poderia ter 8192 sequ√™ncias como essa em cada atualiza√ß√£o de peso, enquanto no BERT o batch poderia ter apenas 256.

**Corol√°rio 2**: A decis√£o de remover a tarefa de NSP em modelos como o RoBERTa demonstra a import√¢ncia de avaliar empiricamente cada componente do processo de treinamento para determinar sua contribui√ß√£o real para o desempenho final do modelo.

*Proof Outline:* O abandono da NSP em modelos posteriores foi baseado em estudos emp√≠ricos que demonstraram que a tarefa n√£o contribu√≠a significativamente para a qualidade das representa√ß√µes contextuais, e podia at√© mesmo prejudicar a performance do modelo. Isso demonstra a import√¢ncia de testar e verificar cuidadosamente a utilidade de cada componente do treinamento.

**Prova do Corol√°rio 2:**
I.  O modelo BERT foi inicialmente treinado com a combina√ß√£o de duas perdas: MLM e NSP.

II. Modelos subsequentes, como RoBERTa, foram treinados sem a tarefa NSP, utilizando apenas a perda MLM.

III. Estudos emp√≠ricos comparando modelos treinados com e sem NSP demonstraram que a tarefa de NSP n√£o proporcionava ganhos significativos de desempenho, e em alguns casos, prejudicava o aprendizado.

IV. Esses resultados emp√≠ricos indicam que a efic√°cia de um componente de treinamento, como a NSP, deve ser medida por sua contribui√ß√£o real para o desempenho do modelo, e n√£o apenas assumida como ben√©fica.

V. Portanto, a remo√ß√£o da NSP em modelos como o RoBERTa ressalta a necessidade de avalia√ß√µes emp√≠ricas rigorosas para cada componente de treinamento. ‚ñ†

**Lema 3.1:** A substitui√ß√£o do NSP por sequ√™ncias cont√≠nuas no RoBERTa permite que o modelo capture rela√ß√µes intersentenciais de maneira mais natural, pois as senten√ßas n√£o s√£o tratadas como pares isolados, mas como parte de um fluxo discursivo.

*Proof Outline:*  Ao concatenar senten√ßas, o modelo √© exposto a sequ√™ncias mais longas que preservam as rela√ß√µes de contexto. A remo√ß√£o da necessidade de classificar pares discretos, que √© inerente a NSP, evita a perda de informa√ß√£o sobre a conex√£o sequencial entre as frases.

**Prova do Lema 3.1:**
I. No BERT, a tarefa NSP treina o modelo usando pares de senten√ßas que podem ou n√£o ser adjacentes. Cada par √© processado isoladamente.
II. No RoBERTa, as senten√ßas s√£o concatenadas sequencialmente para formar uma sequ√™ncia cont√≠nua.
III. Sequ√™ncias cont√≠nuas mant√™m as rela√ß√µes contextuais entre as senten√ßas, pois elas s√£o processadas como parte de um fluxo natural de texto, sem quebras artificiais.
IV. Ao contr√°rio do BERT, que trata as senten√ßas como pares discretos, o RoBERTa permite que o modelo aprenda as conex√µes discursivas entre as senten√ßas de forma mais direta.
V. A remo√ß√£o da classifica√ß√£o de pares isolados, evita o problema de "quebra" de contexto, permitindo que as rela√ß√µes de contexto sejam capturadas de forma mais natural.
VI. Portanto, o uso de sequ√™ncias cont√≠nuas no RoBERTa permite capturar rela√ß√µes intersentenciais de maneira mais natural do que a abordagem de pares do NSP. ‚ñ†

#### Outros Aspectos do Treinamento
Al√©m do MLM e NSP, outros aspectos importantes do treinamento incluem:
-   **Tamanho do Vocabul√°rio:** Modelos como BERT utilizam um vocabul√°rio de 30.000 tokens [^3], enquanto o XLM-RoBERTa utiliza um vocabul√°rio de 250.000 tokens [^3]. A escolha do tamanho do vocabul√°rio pode afetar o desempenho do modelo em diversas tarefas. Modelos com vocabul√°rios maiores t√™m melhor capacidade de representar palavras raras e podem lidar melhor com idiomas de baixa representa√ß√£o [^3].
> üí° **Exemplo Num√©rico:** Um modelo com vocabul√°rio de 30.000 tokens pode ter dificuldade em representar palavras menos comuns em portugu√™s, como "esdr√∫xulo" ou "procrastinar", pois essas palavras podem ser tokenizadas em sub-palavras, como "es", "dru", "xulo", o que dificulta seu aprendizado. J√° um modelo com vocabul√°rio de 250.000 tokens pode ter essas palavras em seu vocabul√°rio, permitindo que o modelo aprenda sua representa√ß√£o de forma mais eficaz.
- **Tokeniza√ß√£o:** O modelo recebe como entrada tokens que s√£o geralmente sub-palavras geradas por modelos como o WordPiece ou o SentencePiece [^3].
> üí° **Exemplo Num√©rico:** A palavra "estudando" pode ser tokenizada como "estud", "##ando" pelo WordPiece, e os modelos aprendem a representa√ß√£o dessas sub-palavras em vez de palavras inteiras.
-   **N√∫mero de Camadas e Cabe√ßas de Aten√ß√£o:** Modelos como o BERT possuem 12 camadas de transformadores e 12 cabe√ßas de aten√ß√£o [^3], enquanto modelos como o XLM-RoBERTa possuem 24 camadas de transformadores e 16 cabe√ßas de aten√ß√£o [^3]. Esses par√¢metros afetam a capacidade de aprendizado e a complexidade do modelo.
> üí° **Exemplo Num√©rico:** Um modelo com 12 camadas de transformadores ter√° menor capacidade de modelar rela√ß√µes complexas entre palavras se comparado a um modelo com 24 camadas. Cada camada do transformer pode capturar diferentes n√≠veis de abstra√ß√£o e complexidade na representa√ß√£o da linguagem.
- **Dura√ß√£o do Treinamento:** O treinamento de modelos grandes requer um tempo e quantidade de dados consider√°vel [^7], o BERT precisou de 40 passagens sobre os dados para convergir, por exemplo [^7].
> üí° **Exemplo Num√©rico:** Suponha que o BERT foi treinado em um corpus de 3.3 bilh√µes de tokens, e cada passagem sobre esse corpus demore 10 horas. Ser√£o necess√°rias 400 horas de treinamento para treinar o modelo.

**Teorema 2**: A escolha de hiperpar√¢metros como taxa de mascaramento, tamanho do vocabul√°rio, n√∫mero de camadas, arquitetura da rede e tamanho do batch tem um impacto significativo no desempenho final do modelo, exigindo uma abordagem emp√≠rica e cuidadosa na sua defini√ß√£o.

*Proof Outline:* O desempenho de um modelo de linguagem √© influenciado pela intera√ß√£o complexa de diferentes hiperpar√¢metros e componentes do treinamento. Ao variar os hiperpar√¢metros, podemos observar mudan√ßas na velocidade de treinamento, na generaliza√ß√£o do modelo e na capacidade de aprendizado de caracter√≠sticas complexas. Portanto, a escolha adequada de cada par√¢metro deve ser resultado de um ajuste emp√≠rico utilizando experimentos controlados em um dado conjunto de dados e arquitetura.

**Prova do Teorema 2:**
I. O desempenho de um modelo de linguagem √© uma fun√ß√£o de v√°rios hiperpar√¢metros como taxa de mascaramento, tamanho do vocabul√°rio, n√∫mero de camadas, arquitetura da rede, tamanho do batch, learning rate, etc.
II. Cada hiperpar√¢metro interage com os demais de forma complexa e n√£o linear.
III. Altera√ß√µes em um hiperpar√¢metro podem afetar a converg√™ncia do treinamento, a capacidade de generaliza√ß√£o, o tempo computacional, e a qualidade da representa√ß√£o aprendida.
IV. A escolha √≥tima de hiperpar√¢metros √© geralmente desconhecida a priori.
V. Portanto, a defini√ß√£o desses par√¢metros precisa ser determinada de maneira emp√≠rica atrav√©s de uma busca por hiperpar√¢metros (e.g., grid search, random search, Bayesian optimization), que consiste em experimentar diversas combina√ß√µes de par√¢metros no conjunto de dados e arquitetura, e avaliar o desempenho em um conjunto de valida√ß√£o.
VI. Portanto, a escolha de hiperpar√¢metros tem um impacto significativo no desempenho final, e exige uma abordagem emp√≠rica e cuidadosa. ‚ñ†

**Teorema 2.1:** O tamanho do batch de treinamento tem um impacto direto no tempo de treinamento, na qualidade do aprendizado, e na capacidade de generaliza√ß√£o do modelo. Batches maiores geralmente permitem treinamento mais r√°pido e maior paralelismo computacional, mas podem levar a modelos menos precisos em alguns casos.

*Proof Outline:* Batches maiores podem levar a uma estimativa do gradiente menos ruidosa e mais est√°vel, o que pode acelerar a converg√™ncia do treinamento. No entanto, batches muito grandes podem tamb√©m fazer com que o modelo aprenda m√≠nimos locais sub-√≥timos. O tamanho ideal do batch √© dependente da tarefa, da quantidade de dados, e do poder computacional dispon√≠vel.

**Prova do Teorema 2.1:**
I. Durante o treinamento de uma rede neural, os gradientes s√£o calculados a partir dos dados no batch.
II. Batches maiores fornecem uma estimativa menos ruidosa do gradiente verdadeiro da fun√ß√£o de perda, pois a m√©dia dos gradientes nos exemplos do batch √© uma aproxima√ß√£o mais precisa do gradiente verdadeiro.
III. Batches maiores permitem maior paralelismo computacional, o que reduz o tempo de treinamento.
IV. Entretanto, batches muito grandes podem tamb√©m levar o modelo a ficar preso em m√≠nimos locais ruins, resultando em menor qualidade de aprendizado.
V. Batches pequenos podem apresentar estimativas ruidosas, que podem ser √∫teis para escapar de m√≠nimos locais, mas tornam o treinamento mais inst√°vel e lento.
VI. O tamanho √≥timo do batch √© uma compensa√ß√£o entre a estabilidade e a velocidade do treinamento, e a generaliza√ß√£o do modelo.
VII. Portanto, o tamanho do batch tem um impacto significativo no treinamento do modelo. ‚ñ†
> üí° **Exemplo Num√©rico:**
> Suponha que voc√™ esteja treinando um modelo com um batch size de 32. O seu modelo leva 1000 itera√ß√µes para convergir. Agora suponha que voc√™ aumente o batch size para 128. O n√∫mero de itera√ß√µes para converg√™ncia cai para 300, mas cada itera√ß√£o √© 4 vezes mais cara em termos de computa√ß√£o. O tempo de treinamento total ser√° reduzido, mas o modelo pode apresentar uma menor qualidade em termos de generaliza√ß√£o.

### Conclus√£o
Embora a tarefa de **Next Sentence Prediction (NSP)** tenha sido fundamental no treinamento do modelo BERT original, modelos mais recentes como RoBERTa demonstraram que a tarefa pode ser omitida sem preju√≠zo, ou at√© mesmo com melhorias de desempenho [^6]. O treinamento por **Masked Language Modeling (MLM)** continua sendo um componente essencial nos modelos bidirecionais [^1, 2], e a escolha do regime de treinamento e dos demais hiperpar√¢metros, como taxa de mascaramento, tamanho do vocabul√°rio e arquitetura da rede, tem um impacto crucial no desempenho final do modelo. Compreender as nuances do treinamento do BERT e suas evolu√ß√µes, como RoBERTa, √© fundamental para aplicar e adaptar esses modelos em problemas de processamento de linguagem natural [^1].

### Refer√™ncias
[^1]: Cap√≠tulo 11, "Masked Language Models"
[^2]: Se√ß√£o 11.2, "Training Bidirectional Encoders"
[^3]: Se√ß√£o 11.1, "Bidirectional Transformer Encoders"
[^6]: Se√ß√£o 11.2.2, "Next Sentence Prediction"
[^7]: Se√ß√£o 11.2.3, "Training Regimes"
<!-- END -->
