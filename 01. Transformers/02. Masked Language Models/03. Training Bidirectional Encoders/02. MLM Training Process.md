## Treinamento de Encoders Bidirecionais com Masked Language Modeling: Amostragem e Perda

### IntroduÃ§Ã£o
Em continuidade ao tÃ³pico anterior, que introduziu o **Masked Language Modeling (MLM)** como mÃ©todo central para o treinamento de encoders bidirecionais [^1], este capÃ­tulo aprofundarÃ¡ os detalhes da amostragem de tokens e a funÃ§Ã£o de perda utilizada no treinamento. O MLM, conforme previamente discutido, corrompe intencionalmente a entrada de texto, e o modelo Ã© treinado para reconstruir a versÃ£o original, atuando como um *denoising* [^4]. Este processo envolve a escolha aleatÃ³ria de tokens, a aplicaÃ§Ã£o de modificaÃ§Ãµes e, finalmente, a previsÃ£o dos tokens originais usando o contexto restante [^1].

### Conceitos Fundamentais
#### Amostragem de Tokens e CorrupÃ§Ã£o
Durante o processo de treinamento do MLM, uma fraÃ§Ã£o especÃ­fica dos tokens de entrada em cada sequÃªncia Ã© selecionada aleatoriamente [^5]. No modelo BERT, por exemplo, 15% dos tokens sÃ£o amostrados para aprendizado [^5]. Este processo de amostragem Ã© crucial para o treinamento eficaz do modelo. Uma vez selecionados, esses tokens sÃ£o submetidos a um processo de corrupÃ§Ã£o com diferentes probabilidade:
-   **SubstituiÃ§Ã£o por `[MASK]`:** 80% dos tokens selecionados sÃ£o substituÃ­dos por um token especial denominado `[MASK]` [^5]. Este token indica ao modelo que ele deve prever o token original naquela posiÃ§Ã£o.
-   **SubstituiÃ§Ã£o por Token AleatÃ³rio:** 10% dos tokens selecionados sÃ£o substituÃ­dos por outro token aleatÃ³rio do vocabulÃ¡rio [^5]. Este processo adiciona ruÃ­do ao processo de treinamento e forÃ§a o modelo a aprender a depender do contexto para a recuperaÃ§Ã£o da palavra original.
-   **ManutenÃ§Ã£o do Token Original:** Os 10% restantes dos tokens selecionados sÃ£o deixados inalterados [^5]. Isso garante que o modelo nÃ£o seja excessivamente treinado em reconstruir apenas tokens corrompidos e permite que ele aprenda tambÃ©m com contexto nÃ£o modificado.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere a sequÃªncia de entrada: "A Ã¡guia voa alto no cÃ©u azul."
>
> 1. **Amostragem:** Suponha que os tokens "Ã¡guia" e "azul" sejam aleatoriamente selecionados como parte dos 15% de tokens para aprendizado.
> 2. **CorrupÃ§Ã£o:**
>     - O token "Ã¡guia", com probabilidade de 80%, Ã© substituÃ­do por `[MASK]`: "A `[MASK]` voa alto no cÃ©u azul."
>     - O token "azul", com probabilidade de 10%, Ã© substituÃ­do por um token aleatÃ³rio do vocabulÃ¡rio, como "verde": "A `[MASK]` voa alto no cÃ©u verde."
>      - Se considerarmos outros tokens nÃ£o selecionados para aprendizado, como "voa", eles permanecem inalterados.
>
> O resultado final Ã© uma sequÃªncia onde uma parte dos tokens selecionados Ã© mascarada, outra parte substituÃ­da por tokens aleatÃ³rios, e os tokens restantes permanecem inalterados.

A escolha da porcentagem de tokens a serem mascarados (15%) e a distribuiÃ§Ã£o das modificaÃ§Ãµes (80%, 10%, 10%) sÃ£o hiperparÃ¢metros ajustÃ¡veis que podem afetar o desempenho do modelo [^5]. A taxa de mascaramento (15%) Ã© um balanceamento entre a complexidade da tarefa de reconstruÃ§Ã£o e a preservaÃ§Ã£o de informaÃ§Ãµes contextuais. Uma taxa muito alta poderia dificultar a aprendizagem devido Ã  falta de informaÃ§Ã£o, enquanto uma taxa muito baixa poderia levar a um aprendizado menos robusto [^4].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar o efeito de diferentes taxas de mascaramento. Suponha que temos uma sequÃªncia de 100 tokens.
>
> - **CenÃ¡rio 1: Taxa de Mascaramento de 5%**: Apenas 5 tokens seriam selecionados. Isso pode nÃ£o ser suficiente para o modelo aprender dependÃªncias complexas na sequÃªncia.
> - **CenÃ¡rio 2: Taxa de Mascaramento de 15%**: 15 tokens seriam selecionados, o que Ã© o padrÃ£o no BERT. Isso oferece um bom equilÃ­brio entre contexto e desafio de reconstruÃ§Ã£o.
> - **CenÃ¡rio 3: Taxa de Mascaramento de 30%**: 30 tokens seriam selecionados. Isso pode tornar a tarefa de reconstruÃ§Ã£o mais difÃ­cil, mas pode forÃ§ar o modelo a aprender representaÃ§Ãµes contextuais mais robustas. No entanto, pode tambÃ©m levar a uma convergÃªncia mais lenta e a um risco de underfitting.
>
>  A escolha da taxa ideal de mascaramento Ã© um hiperparÃ¢metro que deve ser ajustado empiricamente com base em dados e arquitetura do modelo.

**Lema 2**: A aleatoriedade na seleÃ§Ã£o dos tokens e a diversidade na forma de corrupÃ§Ã£o (mascaramento, substituiÃ§Ã£o, nÃ£o modificaÃ§Ã£o) contribuem para a generalizaÃ§Ã£o do modelo e evitam o sobreajuste, alÃ©m de forÃ§ar o modelo a aprender representaÃ§Ãµes mais robustas e contextuais.

*Proof Outline*: Ao selecionar aleatoriamente os tokens, o modelo Ã© exposto a diferentes partes da sequÃªncia durante o treinamento, evitando que se especialize em regiÃµes especÃ­ficas. A diversidade nas corrupÃ§Ãµes (80% mascarado, 10% aleatÃ³rio, 10% original) tambÃ©m forÃ§a o modelo a desenvolver mais habilidades de prediÃ§Ã£o, melhorando a sua capacidade de reconstruÃ§Ã£o em diferentes contextos e sob diferentes condiÃ§Ãµes, o que tambÃ©m diminui as chances de sobreajuste.

**Lema 2.1**: A distribuiÃ§Ã£o de probabilidades para os diferentes tipos de corrupÃ§Ã£o (80% `[MASK]`, 10% aleatÃ³rio, 10% original) pode ser ajustada para otimizar o desempenho do modelo. Um ajuste fino desses hiperparÃ¢metros pode impactar a velocidade e a qualidade do aprendizado do modelo.

*Proof Outline:* A escolha da probabilidade de cada tipo de corrupÃ§Ã£o Ã© um balanceamento entre a dificuldade da tarefa de reconstruÃ§Ã£o e a capacidade do modelo de aprender representaÃ§Ãµes robustas. Ao aumentar a probabilidade de substituir por `[MASK]`, a tarefa de reconstruÃ§Ã£o se torna mais desafiadora, o que pode levar a uma melhor generalizaÃ§Ã£o, mas tambÃ©m pode tornar o treinamento mais lento. Por outro lado, ao aumentar a probabilidade de manter o token original, o modelo pode aprender mais rapidamente, mas pode ser menos robusto. Ajustar essas probabilidades empiricamente durante o treinamento permite encontrar um balanÃ§o Ã³timo para um dado conjunto de dados e arquitetura de modelo.

#### FunÃ§Ã£o de Perda: Entropia Cruzada

ApÃ³s a amostragem e corrupÃ§Ã£o dos tokens, o modelo bidirecional processa a sequÃªncia modificada e gera uma distribuiÃ§Ã£o de probabilidade sobre todo o vocabulÃ¡rio para cada token mascarado ou substituÃ­do [^5]. A funÃ§Ã£o de perda de entropia cruzada (cross-entropy loss) Ã© usada para avaliar o quÃ£o bem o modelo consegue prever os tokens originais [^4].

A perda de entropia cruzada Ã© calculada comparando a distribuiÃ§Ã£o de probabilidade prevista pelo modelo com a distribuiÃ§Ã£o de probabilidade do token original [^4, 5]. Em outras palavras, a funÃ§Ã£o de perda compara o quÃ£o perto a previsÃ£o do modelo estÃ¡ do token original, sendo a funÃ§Ã£o de perda definida como:

$$
    L_{MLM} = -\frac{1}{M} \sum_{i \in M} \log P(x_i|h)
$$

onde:
-   $L_{MLM}$ Ã© a perda total para todos os tokens mascarados.
-   $M$ Ã© o conjunto de tokens selecionados.
-   $x_i$ Ã© o token original na posiÃ§Ã£o $i$.
-   $P(x_i|h)$ Ã© a probabilidade que o modelo atribui ao token original $x_i$, dado o contexto $h$.
-  $h$ representa a representaÃ§Ã£o contextualizada de toda sequÃªncia, incluindo tokens nÃ£o mascarados e token especiais como o [CLS].

O objetivo do treinamento Ã© minimizar essa funÃ§Ã£o de perda, o que Ã© feito ajustando os parÃ¢metros do modelo por meio de tÃ©cnicas de otimizaÃ§Ã£o [^4, 5]. Isso forÃ§a o modelo a melhorar a probabilidade de prever o token correto dado o contexto da sequÃªncia.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Retornando ao exemplo da frase "A `[MASK]` voa alto no cÃ©u verde":
>
> 1. **PrediÃ§Ã£o do Modelo:** O modelo gera uma distribuiÃ§Ã£o de probabilidade sobre todo o vocabulÃ¡rio para o token `[MASK]`. Suponha que o modelo preveja:
>      - $P(\text{Ã¡guia}|h) = 0.7$
>      - $P(\text{pÃ¡ssaro}|h) = 0.1$
>      - $P(\text{nuvem}|h) = 0.05$
>      - ... e assim por diante para todo o vocabulÃ¡rio.
> 2. **CÃ¡lculo da Perda:** O token original era "Ã¡guia". A perda para este token Ã©:
>    $$ L_i = -\log P(\text{Ã¡guia}|h) = -\log(0.7) \approx 0.36$$
> 3.  Se o outro token modificado Ã© "verde" e o token original era "azul", vamos supor que o modelo tenha calculado uma probabilidade de $P(\text{azul}|h) = 0.4$. A perda seria
>      $$ L_j = -\log P(\text{azul}|h) = -\log(0.4) \approx 0.91$$
> 4.  A perda total $L_{MLM}$ Ã© calculada como a mÃ©dia das perdas de todos os tokens selecionados $i$ para aprendizado:
>     $$L_{MLM} = \frac{L_i + L_j}{2} = \frac{0.36 + 0.91}{2} = 0.635$$
>
>  O objetivo do treinamento Ã© ajustar os parÃ¢metros do modelo para reduzir essa perda, fazendo com que o modelo preveja o token original com uma probabilidade mais alta.
>
> 5. **VisualizaÃ§Ã£o:** Podemos visualizar a entropia cruzada em um grÃ¡fico, onde o eixo x representa a probabilidade prevista pelo modelo para o token correto e o eixo y representa o valor da perda. Quanto menor a probabilidade, maior a perda.
>  ```mermaid
>  graph LR
>      A[Probabilidade Prevista] --> B(Perda);
>      B -- log(0.7) ~ 0.36 --> C[Baixa Perda];
>      B -- log(0.4) ~ 0.91 --> D[Alta Perda];
>  ```

A minimizaÃ§Ã£o da perda de entropia cruzada na prÃ¡tica leva a que o modelo aloque maiores probabilidades para os tokens que realmente ocorrem na entrada, dada o contexto fornecido.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos supor que durante o treinamento, o modelo processa um novo batch e agora as prediÃ§Ãµes para os mesmos tokens sÃ£o:
>
> 1. **Nova PrediÃ§Ã£o para "Ã¡guia":**
>    -  $P(\text{Ã¡guia}|h) = 0.9$
>    -  $P(\text{pÃ¡ssaro}|h) = 0.05$
>     - $P(\text{nuvem}|h) = 0.01$
> 2. **Nova PrediÃ§Ã£o para "azul":**
>     - $P(\text{azul}|h) = 0.7$
>     - $P(\text{verde}|h) = 0.1$
>     - $P(\text{branco}|h) = 0.05$
>
>
> Calculando as novas perdas:
>   $$L'_i = -\log(0.9) \approx 0.11$$
>    $$L'_j = -\log(0.7) \approx 0.36$$
> E a perda total:
>   $$L'_{MLM} = \frac{0.11 + 0.36}{2} = 0.235$$
> Note que a nova perda Ã© menor do que a perda anterior (0.235 < 0.635). Isso significa que o modelo estÃ¡ melhorando sua capacidade de prever as palavras corretas dado o contexto.

**CorolÃ¡rio 1**: A perda MLM, $L_{MLM}$, nÃ£o apenas avalia a qualidade das prediÃ§Ãµes dos tokens mascarados, mas tambÃ©m indiretamente guia o modelo a aprender representaÃ§Ãµes contextuais Ãºteis para a prediÃ§Ã£o das palavras.

*Prova:*
 I. A perda $L_{MLM}$ Ã© definida como:
 $$L_{MLM} = -\frac{1}{M} \sum_{i \in M} \log P(x_i|h)$$
 onde $P(x_i|h)$ Ã© a probabilidade prevista pelo modelo para o token original $x_i$, dadas as representaÃ§Ãµes contextuais $h$.
 II. Para minimizar $L_{MLM}$, o modelo precisa maximizar $P(x_i|h)$, que Ã© a probabilidade atribuÃ­da ao token correto.
 III. Para maximizar $P(x_i|h)$, o modelo deve aprender representaÃ§Ãµes contextuais $h$ que capturem as nuances do significado da sequÃªncia de entrada, permitindo que ele identifique qual token Ã© mais provÃ¡vel dado aquele contexto.
 IV. Ao minimizar $L_{MLM}$, o modelo Ã© forÃ§ado a ajustar seus parÃ¢metros para melhorar as representaÃ§Ãµes contextuais $h$, de modo que a probabilidade de prever o token original correto $x_i$ seja maximizada.
 V. Portanto, a perda $L_{MLM}$ nÃ£o sÃ³ avalia a prediÃ§Ã£o, mas tambÃ©m guia o modelo no aprendizado de representaÃ§Ãµes contextuais mais adequadas, que sÃ£o entÃ£o usadas em tarefas subsequentes.
â– 

**Teorema 1**: O treinamento por MLM pode ser visto como uma forma de aprendizado auto-supervisionado, onde o prÃ³prio texto de entrada gera os rÃ³tulos para o aprendizado do modelo.

*Proof Outline:* No MLM, os tokens originais servem como rÃ³tulos, enquanto os tokens corrompidos (mascarados, substituÃ­dos ou mantidos) atuam como as entradas. O modelo Ã© treinado para prever os tokens originais a partir das entradas corrompidas. Como os rÃ³tulos sÃ£o derivados do prÃ³prio texto de entrada, nÃ£o hÃ¡ necessidade de rÃ³tulos explÃ­citos fornecidos externamente, o que caracteriza o aprendizado auto-supervisionado.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Em um conjunto de dados de texto, podemos pegar qualquer sequÃªncia de texto nÃ£o rotulada. Por exemplo: "O rato roeu a roupa do rei de Roma."
> 1. **Entrada Corrompida:** Aplicando o MLM, a sequÃªncia poderia se tornar "O rato `[MASK]` a roupa do `[MASK]` de Roma".
> 2. **RÃ³tulos Derivados:** Os rÃ³tulos para o treinamento sÃ£o os tokens originais, "roeu" e "rei".
> 3. **Auto-SupervisÃ£o:** O modelo Ã© entÃ£o treinado para prever esses rÃ³tulos ("roeu", "rei") com base na entrada corrompida, sem a necessidade de rÃ³tulos fornecidos externamente.
>
> Esse processo demonstra como o MLM usa a estrutura inerente do texto para gerar dados de treinamento supervisionados a partir de dados nÃ£o rotulados.

### ConclusÃ£o
O treinamento por MLM envolve a amostragem e corrupÃ§Ã£o aleatÃ³ria de tokens, seguido pelo uso da funÃ§Ã£o de perda de entropia cruzada para orientar o aprendizado do modelo [^4, 5]. Este processo garante que o modelo aprenda representaÃ§Ãµes contextuais robustas e seja capaz de prever tokens a partir de informaÃ§Ãµes parciais e corrompidas [^1]. A escolha criteriosa de hiperparÃ¢metros, como a taxa de mascaramento e o tipo de corrupÃ§Ã£o, Ã© fundamental para o desempenho do modelo, e a funÃ§Ã£o de perda de entropia cruzada Ã© a mÃ©trica chave para avaliar e otimizar as previsÃµes do modelo [^5]. A combinaÃ§Ã£o destes elementos forma a base do treinamento eficaz de encoders bidirecionais em modelos de linguagem modernos.

### ReferÃªncias
[^1]: CapÃ­tulo 11, "Masked Language Models"
[^4]: SeÃ§Ã£o 11.2, "Training Bidirectional Encoders"
[^5]: SeÃ§Ã£o 11.2.1, "Masking Words"
<!-- END -->
