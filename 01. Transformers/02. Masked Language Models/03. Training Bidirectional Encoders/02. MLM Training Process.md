## Treinamento de Encoders Bidirecionais com Masked Language Modeling: Amostragem e Perda

### Introdu√ß√£o
Em continuidade ao t√≥pico anterior, que introduziu o **Masked Language Modeling (MLM)** como m√©todo central para o treinamento de encoders bidirecionais [^1], este cap√≠tulo aprofundar√° os detalhes da amostragem de tokens e a fun√ß√£o de perda utilizada no treinamento. O MLM, conforme previamente discutido, corrompe intencionalmente a entrada de texto, e o modelo √© treinado para reconstruir a vers√£o original, atuando como um *denoising* [^4]. Este processo envolve a escolha aleat√≥ria de tokens, a aplica√ß√£o de modifica√ß√µes e, finalmente, a previs√£o dos tokens originais usando o contexto restante [^1].

### Conceitos Fundamentais
#### Amostragem de Tokens e Corrup√ß√£o
Durante o processo de treinamento do MLM, uma fra√ß√£o espec√≠fica dos tokens de entrada em cada sequ√™ncia √© selecionada aleatoriamente [^5]. No modelo BERT, por exemplo, 15% dos tokens s√£o amostrados para aprendizado [^5]. Este processo de amostragem √© crucial para o treinamento eficaz do modelo. Uma vez selecionados, esses tokens s√£o submetidos a um processo de corrup√ß√£o com diferentes probabilidade:
-   **Substitui√ß√£o por `[MASK]`:** 80% dos tokens selecionados s√£o substitu√≠dos por um token especial denominado `[MASK]` [^5]. Este token indica ao modelo que ele deve prever o token original naquela posi√ß√£o.
-   **Substitui√ß√£o por Token Aleat√≥rio:** 10% dos tokens selecionados s√£o substitu√≠dos por outro token aleat√≥rio do vocabul√°rio [^5]. Este processo adiciona ru√≠do ao processo de treinamento e for√ßa o modelo a aprender a depender do contexto para a recupera√ß√£o da palavra original.
-   **Manuten√ß√£o do Token Original:** Os 10% restantes dos tokens selecionados s√£o deixados inalterados [^5]. Isso garante que o modelo n√£o seja excessivamente treinado em reconstruir apenas tokens corrompidos e permite que ele aprenda tamb√©m com contexto n√£o modificado.

> üí° **Exemplo Num√©rico:**
>
> Considere a sequ√™ncia de entrada: "A √°guia voa alto no c√©u azul."
>
> 1. **Amostragem:** Suponha que os tokens "√°guia" e "azul" sejam aleatoriamente selecionados como parte dos 15% de tokens para aprendizado.
> 2. **Corrup√ß√£o:**
>     - O token "√°guia", com probabilidade de 80%, √© substitu√≠do por `[MASK]`: "A `[MASK]` voa alto no c√©u azul."
>     - O token "azul", com probabilidade de 10%, √© substitu√≠do por um token aleat√≥rio do vocabul√°rio, como "verde": "A `[MASK]` voa alto no c√©u verde."
>      - Se considerarmos outros tokens n√£o selecionados para aprendizado, como "voa", eles permanecem inalterados.
>
> O resultado final √© uma sequ√™ncia onde uma parte dos tokens selecionados √© mascarada, outra parte substitu√≠da por tokens aleat√≥rios, e os tokens restantes permanecem inalterados.

A escolha da porcentagem de tokens a serem mascarados (15%) e a distribui√ß√£o das modifica√ß√µes (80%, 10%, 10%) s√£o hiperpar√¢metros ajust√°veis que podem afetar o desempenho do modelo [^5]. A taxa de mascaramento (15%) √© um balanceamento entre a complexidade da tarefa de reconstru√ß√£o e a preserva√ß√£o de informa√ß√µes contextuais. Uma taxa muito alta poderia dificultar a aprendizagem devido √† falta de informa√ß√£o, enquanto uma taxa muito baixa poderia levar a um aprendizado menos robusto [^4].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar o efeito de diferentes taxas de mascaramento. Suponha que temos uma sequ√™ncia de 100 tokens.
>
> - **Cen√°rio 1: Taxa de Mascaramento de 5%**: Apenas 5 tokens seriam selecionados. Isso pode n√£o ser suficiente para o modelo aprender depend√™ncias complexas na sequ√™ncia.
> - **Cen√°rio 2: Taxa de Mascaramento de 15%**: 15 tokens seriam selecionados, o que √© o padr√£o no BERT. Isso oferece um bom equil√≠brio entre contexto e desafio de reconstru√ß√£o.
> - **Cen√°rio 3: Taxa de Mascaramento de 30%**: 30 tokens seriam selecionados. Isso pode tornar a tarefa de reconstru√ß√£o mais dif√≠cil, mas pode for√ßar o modelo a aprender representa√ß√µes contextuais mais robustas. No entanto, pode tamb√©m levar a uma converg√™ncia mais lenta e a um risco de underfitting.
>
>  A escolha da taxa ideal de mascaramento √© um hiperpar√¢metro que deve ser ajustado empiricamente com base em dados e arquitetura do modelo.

**Lema 2**: A aleatoriedade na sele√ß√£o dos tokens e a diversidade na forma de corrup√ß√£o (mascaramento, substitui√ß√£o, n√£o modifica√ß√£o) contribuem para a generaliza√ß√£o do modelo e evitam o sobreajuste, al√©m de for√ßar o modelo a aprender representa√ß√µes mais robustas e contextuais.

*Proof Outline*: Ao selecionar aleatoriamente os tokens, o modelo √© exposto a diferentes partes da sequ√™ncia durante o treinamento, evitando que se especialize em regi√µes espec√≠ficas. A diversidade nas corrup√ß√µes (80% mascarado, 10% aleat√≥rio, 10% original) tamb√©m for√ßa o modelo a desenvolver mais habilidades de predi√ß√£o, melhorando a sua capacidade de reconstru√ß√£o em diferentes contextos e sob diferentes condi√ß√µes, o que tamb√©m diminui as chances de sobreajuste.

**Lema 2.1**: A distribui√ß√£o de probabilidades para os diferentes tipos de corrup√ß√£o (80% `[MASK]`, 10% aleat√≥rio, 10% original) pode ser ajustada para otimizar o desempenho do modelo. Um ajuste fino desses hiperpar√¢metros pode impactar a velocidade e a qualidade do aprendizado do modelo.

*Proof Outline:* A escolha da probabilidade de cada tipo de corrup√ß√£o √© um balanceamento entre a dificuldade da tarefa de reconstru√ß√£o e a capacidade do modelo de aprender representa√ß√µes robustas. Ao aumentar a probabilidade de substituir por `[MASK]`, a tarefa de reconstru√ß√£o se torna mais desafiadora, o que pode levar a uma melhor generaliza√ß√£o, mas tamb√©m pode tornar o treinamento mais lento. Por outro lado, ao aumentar a probabilidade de manter o token original, o modelo pode aprender mais rapidamente, mas pode ser menos robusto. Ajustar essas probabilidades empiricamente durante o treinamento permite encontrar um balan√ßo √≥timo para um dado conjunto de dados e arquitetura de modelo.

#### Fun√ß√£o de Perda: Entropia Cruzada

Ap√≥s a amostragem e corrup√ß√£o dos tokens, o modelo bidirecional processa a sequ√™ncia modificada e gera uma distribui√ß√£o de probabilidade sobre todo o vocabul√°rio para cada token mascarado ou substitu√≠do [^5]. A fun√ß√£o de perda de entropia cruzada (cross-entropy loss) √© usada para avaliar o qu√£o bem o modelo consegue prever os tokens originais [^4].

A perda de entropia cruzada √© calculada comparando a distribui√ß√£o de probabilidade prevista pelo modelo com a distribui√ß√£o de probabilidade do token original [^4, 5]. Em outras palavras, a fun√ß√£o de perda compara o qu√£o perto a previs√£o do modelo est√° do token original, sendo a fun√ß√£o de perda definida como:

$$
    L_{MLM} = -\frac{1}{M} \sum_{i \in M} \log P(x_i|h)
$$

onde:
-   $L_{MLM}$ √© a perda total para todos os tokens mascarados.
-   $M$ √© o conjunto de tokens selecionados.
-   $x_i$ √© o token original na posi√ß√£o $i$.
-   $P(x_i|h)$ √© a probabilidade que o modelo atribui ao token original $x_i$, dado o contexto $h$.
-  $h$ representa a representa√ß√£o contextualizada de toda sequ√™ncia, incluindo tokens n√£o mascarados e token especiais como o [CLS].

O objetivo do treinamento √© minimizar essa fun√ß√£o de perda, o que √© feito ajustando os par√¢metros do modelo por meio de t√©cnicas de otimiza√ß√£o [^4, 5]. Isso for√ßa o modelo a melhorar a probabilidade de prever o token correto dado o contexto da sequ√™ncia.

> üí° **Exemplo Num√©rico:**
>
> Retornando ao exemplo da frase "A `[MASK]` voa alto no c√©u verde":
>
> 1. **Predi√ß√£o do Modelo:** O modelo gera uma distribui√ß√£o de probabilidade sobre todo o vocabul√°rio para o token `[MASK]`. Suponha que o modelo preveja:
>      - $P(\text{√°guia}|h) = 0.7$
>      - $P(\text{p√°ssaro}|h) = 0.1$
>      - $P(\text{nuvem}|h) = 0.05$
>      - ... e assim por diante para todo o vocabul√°rio.
> 2. **C√°lculo da Perda:** O token original era "√°guia". A perda para este token √©:
>    $$ L_i = -\log P(\text{√°guia}|h) = -\log(0.7) \approx 0.36$$
> 3.  Se o outro token modificado √© "verde" e o token original era "azul", vamos supor que o modelo tenha calculado uma probabilidade de $P(\text{azul}|h) = 0.4$. A perda seria
>      $$ L_j = -\log P(\text{azul}|h) = -\log(0.4) \approx 0.91$$
> 4.  A perda total $L_{MLM}$ √© calculada como a m√©dia das perdas de todos os tokens selecionados $i$ para aprendizado:
>     $$L_{MLM} = \frac{L_i + L_j}{2} = \frac{0.36 + 0.91}{2} = 0.635$$
>
>  O objetivo do treinamento √© ajustar os par√¢metros do modelo para reduzir essa perda, fazendo com que o modelo preveja o token original com uma probabilidade mais alta.
>
> 5. **Visualiza√ß√£o:** Podemos visualizar a entropia cruzada em um gr√°fico, onde o eixo x representa a probabilidade prevista pelo modelo para o token correto e o eixo y representa o valor da perda. Quanto menor a probabilidade, maior a perda.
>  ```mermaid
>  graph LR
>      A[Probabilidade Prevista] --> B(Perda);
>      B -- log(0.7) ~ 0.36 --> C[Baixa Perda];
>      B -- log(0.4) ~ 0.91 --> D[Alta Perda];
>  ```

A minimiza√ß√£o da perda de entropia cruzada na pr√°tica leva a que o modelo aloque maiores probabilidades para os tokens que realmente ocorrem na entrada, dada o contexto fornecido.

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que durante o treinamento, o modelo processa um novo batch e agora as predi√ß√µes para os mesmos tokens s√£o:
>
> 1. **Nova Predi√ß√£o para "√°guia":**
>    -  $P(\text{√°guia}|h) = 0.9$
>    -  $P(\text{p√°ssaro}|h) = 0.05$
>     - $P(\text{nuvem}|h) = 0.01$
> 2. **Nova Predi√ß√£o para "azul":**
>     - $P(\text{azul}|h) = 0.7$
>     - $P(\text{verde}|h) = 0.1$
>     - $P(\text{branco}|h) = 0.05$
>
>
> Calculando as novas perdas:
>   $$L'_i = -\log(0.9) \approx 0.11$$
>    $$L'_j = -\log(0.7) \approx 0.36$$
> E a perda total:
>   $$L'_{MLM} = \frac{0.11 + 0.36}{2} = 0.235$$
> Note que a nova perda √© menor do que a perda anterior (0.235 < 0.635). Isso significa que o modelo est√° melhorando sua capacidade de prever as palavras corretas dado o contexto.

**Corol√°rio 1**: A perda MLM, $L_{MLM}$, n√£o apenas avalia a qualidade das predi√ß√µes dos tokens mascarados, mas tamb√©m indiretamente guia o modelo a aprender representa√ß√µes contextuais √∫teis para a predi√ß√£o das palavras.

*Prova:*
 I. A perda $L_{MLM}$ √© definida como:
 $$L_{MLM} = -\frac{1}{M} \sum_{i \in M} \log P(x_i|h)$$
 onde $P(x_i|h)$ √© a probabilidade prevista pelo modelo para o token original $x_i$, dadas as representa√ß√µes contextuais $h$.
 II. Para minimizar $L_{MLM}$, o modelo precisa maximizar $P(x_i|h)$, que √© a probabilidade atribu√≠da ao token correto.
 III. Para maximizar $P(x_i|h)$, o modelo deve aprender representa√ß√µes contextuais $h$ que capturem as nuances do significado da sequ√™ncia de entrada, permitindo que ele identifique qual token √© mais prov√°vel dado aquele contexto.
 IV. Ao minimizar $L_{MLM}$, o modelo √© for√ßado a ajustar seus par√¢metros para melhorar as representa√ß√µes contextuais $h$, de modo que a probabilidade de prever o token original correto $x_i$ seja maximizada.
 V. Portanto, a perda $L_{MLM}$ n√£o s√≥ avalia a predi√ß√£o, mas tamb√©m guia o modelo no aprendizado de representa√ß√µes contextuais mais adequadas, que s√£o ent√£o usadas em tarefas subsequentes.
‚ñ†

**Teorema 1**: O treinamento por MLM pode ser visto como uma forma de aprendizado auto-supervisionado, onde o pr√≥prio texto de entrada gera os r√≥tulos para o aprendizado do modelo.

*Proof Outline:* No MLM, os tokens originais servem como r√≥tulos, enquanto os tokens corrompidos (mascarados, substitu√≠dos ou mantidos) atuam como as entradas. O modelo √© treinado para prever os tokens originais a partir das entradas corrompidas. Como os r√≥tulos s√£o derivados do pr√≥prio texto de entrada, n√£o h√° necessidade de r√≥tulos expl√≠citos fornecidos externamente, o que caracteriza o aprendizado auto-supervisionado.

> üí° **Exemplo Num√©rico:**
>
> Em um conjunto de dados de texto, podemos pegar qualquer sequ√™ncia de texto n√£o rotulada. Por exemplo: "O rato roeu a roupa do rei de Roma."
> 1. **Entrada Corrompida:** Aplicando o MLM, a sequ√™ncia poderia se tornar "O rato `[MASK]` a roupa do `[MASK]` de Roma".
> 2. **R√≥tulos Derivados:** Os r√≥tulos para o treinamento s√£o os tokens originais, "roeu" e "rei".
> 3. **Auto-Supervis√£o:** O modelo √© ent√£o treinado para prever esses r√≥tulos ("roeu", "rei") com base na entrada corrompida, sem a necessidade de r√≥tulos fornecidos externamente.
>
> Esse processo demonstra como o MLM usa a estrutura inerente do texto para gerar dados de treinamento supervisionados a partir de dados n√£o rotulados.

### Conclus√£o
O treinamento por MLM envolve a amostragem e corrup√ß√£o aleat√≥ria de tokens, seguido pelo uso da fun√ß√£o de perda de entropia cruzada para orientar o aprendizado do modelo [^4, 5]. Este processo garante que o modelo aprenda representa√ß√µes contextuais robustas e seja capaz de prever tokens a partir de informa√ß√µes parciais e corrompidas [^1]. A escolha criteriosa de hiperpar√¢metros, como a taxa de mascaramento e o tipo de corrup√ß√£o, √© fundamental para o desempenho do modelo, e a fun√ß√£o de perda de entropia cruzada √© a m√©trica chave para avaliar e otimizar as previs√µes do modelo [^5]. A combina√ß√£o destes elementos forma a base do treinamento eficaz de encoders bidirecionais em modelos de linguagem modernos.

### Refer√™ncias
[^1]: Cap√≠tulo 11, "Masked Language Models"
[^4]: Se√ß√£o 11.2, "Training Bidirectional Encoders"
[^5]: Se√ß√£o 11.2.1, "Masking Words"
<!-- END -->
