## Treinamento Multil√≠ngue e Balanceamento de Dados em Modelos de Linguagem Mascarados

### Introdu√ß√£o
Expandindo nosso estudo sobre o treinamento de encoders bidirecionais com **Masked Language Modeling (MLM)** e seus regimes [^1, 2, 3], este cap√≠tulo abordar√° as complexidades e os desafios espec√≠ficos encontrados no treinamento de modelos de linguagem *multil√≠ngues*. Em particular, discutiremos a import√¢ncia do balanceamento de dados e como ajustar as probabilidades de amostragem para que idiomas menos representados sejam devidamente considerados durante o treinamento, evitando o vi√©s do modelo para idiomas mais frequentes [^8].

### Conceitos Fundamentais
#### Desafios do Treinamento Multil√≠ngue
Modelos de linguagem monol√≠ngues s√£o treinados em um √∫nico idioma, como ingl√™s, o que pode resultar em representa√ß√µes e desempenhos √≥timos para esse idioma. No entanto, a necessidade de modelos que possam lidar com diversos idiomas levou ao desenvolvimento de modelos multil√≠ngues. O treinamento de modelos multil√≠ngues introduz uma nova camada de complexidade, devido √† varia√ß√£o na disponibilidade de dados para diferentes idiomas.

Em geral, a quantidade de dados textuais dispon√≠vel para o ingl√™s √© muito superior √† dispon√≠vel para outros idiomas. Isso pode levar a modelos de linguagem multil√≠ngues enviesados, com um desempenho muito melhor em ingl√™s do que em outros idiomas. O vi√©s pode manifestar-se de v√°rias formas:
- **Vocabul√°rio enviesado**: O vocabul√°rio do modelo pode ser dominado por tokens do ingl√™s, resultando em representa√ß√µes menos eficazes para outros idiomas [^8].
- **Desempenho desigual**: O modelo pode apresentar um desempenho inferior em idiomas menos representados, pois n√£o aprendeu representa√ß√µes contextuais robustas o suficiente para eles [^8].
- **Vi√©s de transfer√™ncia**: O modelo pode transferir caracter√≠sticas lingu√≠sticas do ingl√™s para outros idiomas, fazendo com que esses idiomas pare√ßam ter "sotaque" ingl√™s [^8].

Para superar esses desafios, uma das principais estrat√©gias √© garantir que a amostragem dos dados de treinamento seja feita de forma a balancear a representa√ß√£o dos diferentes idiomas, especialmente aqueles com menor disponibilidade de texto.

#### Balanceamento de Dados Multil√≠ngues
O balanceamento de dados para modelos de linguagem multil√≠ngues envolve ajustar as probabilidades de amostragem de cada idioma, de modo a garantir que os idiomas menos representados sejam devidamente considerados [^8]. O processo geral consiste nos seguintes passos:
1. **Divis√£o dos Dados:** Os dados de treinamento s√£o divididos em subcorpora, com base nos idiomas presentes [^8].
2. **C√°lculo da Frequ√™ncia:** O n√∫mero de senten√ßas ($n_i$) √© calculado para cada idioma ($i$) [^8].
3. **Ajuste das Probabilidades:** As probabilidades de amostragem dos idiomas s√£o reajustadas, com o objetivo de dar maior peso aos idiomas menos frequentes [^8].

A probabilidade de selecionar uma senten√ßa do idioma $i$, cuja frequ√™ncia original √© $n_i$, √© reajustada usando a seguinte f√≥rmula:

$$
    q_i = \frac{p_i^{\alpha}}{\sum_{j=1}^N p_j^{\alpha}} \quad \text{com} \quad p_i = \frac{n_i}{\sum_{k=1}^N n_k}
$$

Onde:
-   $q_i$ √© a nova probabilidade de selecionar uma senten√ßa do idioma $i$.
-   $p_i$ √© a probabilidade original de selecionar uma senten√ßa do idioma $i$, dada pela frequ√™ncia relativa das senten√ßas daquele idioma no corpus.
-   $n_i$ √© o n√∫mero de senten√ßas no idioma $i$.
-   $N$ √© o n√∫mero total de idiomas no corpus.
-   $\alpha$ √© um hiperpar√¢metro que controla o grau de balanceamento.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um corpus de treinamento com 3 idiomas: Ingl√™s (EN), Espanhol (ES) e Portugu√™s (PT).
>
> -   Ingl√™s (EN): $n_{EN}$ = 1.000.000 senten√ßas
> -   Espanhol (ES): $n_{ES}$ = 200.000 senten√ßas
> -   Portugu√™s (PT): $n_{PT}$ = 50.000 senten√ßas
>
> O n√∫mero total de senten√ßas no corpus √© $N = 1.250.000$. As probabilidades originais de sele√ß√£o s√£o:
>
> -   $p_{EN} = \frac{1.000.000}{1.250.000} = 0.8$
> -   $p_{ES} = \frac{200.000}{1.250.000} = 0.16$
> -   $p_{PT} = \frac{50.000}{1.250.000} = 0.04$
>
> Se usarmos um hiperpar√¢metro $\alpha = 0.3$, as novas probabilidades de sele√ß√£o s√£o:
>
> -   $p_{EN}^\alpha = 0.8^{0.3} \approx 0.928$
> -   $p_{ES}^\alpha = 0.16^{0.3} \approx 0.503$
> -   $p_{PT}^\alpha = 0.04^{0.3} \approx 0.331$
>
> A soma das probabilidades $p_{EN}^\alpha + p_{ES}^\alpha + p_{PT}^\alpha = 1.762$.  Dividindo as probabilidades pela soma, temos:
>
> -   $q_{EN} = \frac{0.928}{1.762} \approx 0.527$
> -   $q_{ES} = \frac{0.503}{1.762} \approx 0.285$
> -   $q_{PT} = \frac{0.331}{1.762} \approx 0.188$
>
> Comparando, a probabilidade do ingl√™s caiu de 80% para 52.7%, enquanto a probabilidade do portugu√™s subiu de 4% para 18.8%. Isso significa que ao reajustar as probabilidades de amostragem, o modelo ter√° uma chance maior de ser treinado com senten√ßas em portugu√™s.

O hiperpar√¢metro $\alpha$ desempenha um papel crucial nesse processo. Valores menores de $\alpha$ aumentam a influ√™ncia dos idiomas menos frequentes, enquanto valores maiores diminuem essa influ√™ncia. Quando $\alpha=1$, as probabilidades de amostragem n√£o s√£o ajustadas, e o modelo seria treinado com a frequ√™ncia relativa original. Um valor comum para $\alpha$ √© 0.3, que oferece um bom balan√ßo entre o aprendizado de idiomas mais representados e menos representados [^8].

> üí° **Exemplo Num√©rico:**
>
> Vamos analisar o efeito do hiperpar√¢metro $\alpha$ nas probabilidades de amostragem com os mesmos idiomas e frequ√™ncia do exemplo anterior: Ingl√™s (EN), Espanhol (ES) e Portugu√™s (PT).
>
> -   Ingl√™s (EN): $n_{EN}$ = 1.000.000 senten√ßas
> -   Espanhol (ES): $n_{ES}$ = 200.000 senten√ßas
> -   Portugu√™s (PT): $n_{PT}$ = 50.000 senten√ßas
>
> As probabilidades originais de sele√ß√£o s√£o:
>
> -   $p_{EN} = 0.8$
> -   $p_{ES} = 0.16$
> -   $p_{PT} = 0.04$
>
>
> Vamos calcular as novas probabilidades de sele√ß√£o com diferentes valores de $\alpha$.
>
> **Cen√°rio 1: $\alpha = 1$** (Nenhum Balanceamento)
>
> -   $p_{EN}^1 = 0.8$
> -   $p_{ES}^1 = 0.16$
> -   $p_{PT}^1 = 0.04$
>
> As probabilidades de amostragem permanecem as mesmas.
>
> **Cen√°rio 2: $\alpha = 0.3$** (Balanceamento Moderado)
>
> -   $q_{EN} \approx 0.527$
> -   $q_{ES} \approx 0.285$
> -   $q_{PT} \approx 0.188$
>
>  Os idiomas menos frequentes s√£o upsampled, o ingl√™s perde import√¢ncia e o portugu√™s passa a ser amostrado mais frequentemente.
>
> **Cen√°rio 3: $\alpha = 0.1$** (Balanceamento Forte)
>
> -   $p_{EN}^{0.1} \approx 0.961$
> -   $p_{ES}^{0.1} \approx 0.711$
> -   $p_{PT}^{0.1} \approx 0.631$
>
> A soma das probabilidades √© 2.303, e as probabilidades ajustadas s√£o:
>
> -   $q_{EN} \approx 0.417$
> -   $q_{ES} \approx 0.309$
> -   $q_{PT} \approx 0.274$
>
>  O balanceamento √© mais forte. O ingl√™s √© reduzido para 41.7%, enquanto a chance de usar senten√ßas em portugu√™s aumenta ainda mais, para 27.4%.
>
> Esse exemplo demonstra como $\alpha$ controla a intensidade do balanceamento. Ao ajustar este hiperpar√¢metro, √© poss√≠vel encontrar o balanceamento que melhor atende √†s necessidades da tarefa e dos dados dispon√≠veis.

> üí° **Exemplo Num√©rico:**
> Vamos criar um exemplo usando Python e NumPy para simular o efeito do balanceamento de dados com diferentes valores de $\alpha$.
>
> ```python
> import numpy as np
>
> # N√∫mero de senten√ßas por idioma
> n_en = 1000000
> n_es = 200000
> n_pt = 50000
>
> # Total de senten√ßas
> n_total = n_en + n_es + n_pt
>
> # Probabilidades originais
> p_en = n_en / n_total
> p_es = n_es / n_total
> p_pt = n_pt / n_total
>
> print(f"Probabilidades Originais: EN={p_en:.3f}, ES={p_es:.3f}, PT={p_pt:.3f}")
>
> # Fun√ß√£o para calcular as probabilidades ajustadas
> def calculate_adjusted_probabilities(p, alpha):
>     p_alpha = np.power(p, alpha)
>     q = p_alpha / np.sum(p_alpha)
>     return q
>
> # Valores de alpha
> alphas = [1, 0.5, 0.3, 0.1]
>
> for alpha in alphas:
>     q = calculate_adjusted_probabilities(np.array([p_en, p_es, p_pt]), alpha)
>     print(f"Alpha={alpha}: EN={q[0]:.3f}, ES={q[1]:.3f}, PT={q[2]:.3f}")
> ```
>
> Sa√≠da:
> ```
> Probabilidades Originais: EN=0.800, ES=0.160, PT=0.040
> Alpha=1: EN=0.800, ES=0.160, PT=0.040
> Alpha=0.5: EN=0.676, ES=0.258, PT=0.066
> Alpha=0.3: EN=0.527, ES=0.285, PT=0.188
> Alpha=0.1: EN=0.417, ES=0.309, PT=0.274
> ```
>
> Este exemplo mostra como as probabilidades de amostragem mudam com diferentes valores de $\alpha$, confirmando que valores menores aumentam a probabilidade de amostragem para idiomas menos representados.

**Lema 4:** Ao ajustar as probabilidades de amostragem, √© poss√≠vel garantir que o modelo aprenda representa√ß√µes contextuais mais equilibradas, com desempenho melhorado em idiomas menos representados.

*Prova*:
I. A probabilidade de amostragem de cada idioma √© determinada pela equa√ß√£o $q_i = \frac{p_i^{\alpha}}{\sum_{j=1}^N p_j^{\alpha}}$ onde $p_i = \frac{n_i}{\sum_{k=1}^N n_k}$.
II. Reduzir $\alpha < 1$ amplifica a probabilidade de amostragem dos idiomas com menor $p_i$ (e $n_i$), ou seja, os idiomas menos representados.
III. Aumentar a frequ√™ncia de amostragem de idiomas com menos dados aumenta a exposi√ß√£o do modelo a esses idiomas durante o treinamento.
IV. Uma maior exposi√ß√£o a dados menos representados permite que o modelo aprenda representa√ß√µes mais robustas para esses idiomas.
V.  Representa√ß√µes mais robustas levam a um melhor desempenho em idiomas menos representados.
VI. O resultado final √© um desempenho mais equilibrado entre os idiomas. ‚ñ†

**Teorema 3**: O balanceamento de dados no treinamento multil√≠ngue pode ser visto como uma forma de regulariza√ß√£o, que evita que o modelo fique muito enviesado para os idiomas com mais dados e favorece o aprendizado de representa√ß√µes mais gerais e balanceadas.

*Prova*:
I. O treinamento de modelos de linguagem em grandes conjuntos de dados pode levar a overfitting (sobreajuste) do modelo nos dados mais frequentes.
II. Em modelos multil√≠ngues, os idiomas de maior frequ√™ncia (como ingl√™s) podem levar o modelo a focar mais na representa√ß√£o desses idiomas em detrimento de outros com menos dados.
III. Ao reajustar as probabilidades de amostragem para privilegiar os idiomas com menor frequ√™ncia, a tarefa de treinamento se torna mais desafiadora para o modelo, e o modelo √© for√ßado a aprender representa√ß√µes mais generalizadas e menos enviesadas para os idiomas de maior frequ√™ncia.
IV. O ajuste das probabilidades de amostragem age como um mecanismo de regulariza√ß√£o, pois impede que o modelo se especialize excessivamente em um subconjunto dos dados (idiomas de maior frequ√™ncia).
V. O balanceamento de dados, portanto, induz um treinamento que leva a modelos com melhor capacidade de generaliza√ß√£o e mais robustos em todos os idiomas. ‚ñ†

**Teorema 3.1** O hiperpar√¢metro $\alpha$ pode ser ajustado dinamicamente durante o treinamento, utilizando t√©cnicas de *annealing*, para um balanceamento mais adaptativo.

*Prova*:
I. Inicialmente, durante o treinamento, um valor maior de $\alpha$ (pr√≥ximo a 1) pode ser usado, para permitir que o modelo aprenda padr√µes gerais nos idiomas com mais dados.
II. Conforme o treinamento avan√ßa, o valor de $\alpha$ pode ser gradualmente reduzido para dar mais peso aos idiomas menos frequentes e refinar as representa√ß√µes.
III. Essa redu√ß√£o gradual pode ser realizada usando uma fun√ß√£o de *annealing*, onde $\alpha$ √© ajustado com base no n√∫mero de √©pocas ou itera√ß√µes.
IV. Essa estrat√©gia de ajuste din√¢mico de $\alpha$ permite que o modelo explore melhor o espa√ßo de solu√ß√µes, primeiro aprendendo padr√µes gerais e depois refinando representa√ß√µes espec√≠ficas para cada idioma.
V. O uso de annealing pode levar a uma melhor converg√™ncia e a modelos multil√≠ngues com melhor desempenho.‚ñ†

#### Impacto do Balanceamento no Desempenho
Ajustar as probabilidades de amostragem e o valor do $\alpha$ pode resultar em melhorias significativas no desempenho de modelos multil√≠ngues, especialmente para idiomas com menos recursos [^8]. O balanceamento adequado permite que o modelo aprenda representa√ß√µes mais robustas para todos os idiomas, o que resulta em uma capacidade maior de generaliza√ß√£o para novas tarefas e diferentes contextos [^8].
> üí° **Exemplo Num√©rico:**
>
> Um modelo multil√≠ngue n√£o balanceado pode ter uma precis√£o de 90% em tarefas de classifica√ß√£o de texto em ingl√™s, mas apenas 60% em portugu√™s e espanhol. Ap√≥s o balanceamento de dados, o mesmo modelo pode ter uma precis√£o de 80% nos tr√™s idiomas. Embora a precis√£o em ingl√™s tenha diminu√≠do ligeiramente, o desempenho nos outros idiomas melhorou substancialmente, resultando em um sistema mais equitativo e mais √∫til para diversas aplica√ß√µes em diferentes idiomas.
>
> O uso de m√©tricas de avalia√ß√£o balanceadas √© importante, como o F1-score ponderado por classe para refletir o desempenho geral do modelo.
>
> Vamos detalhar um pouco mais este exemplo com dados hipot√©ticos. Suponha que temos os seguintes resultados de um modelo multil√≠ngue antes e depois do balanceamento:
>
> **Antes do Balanceamento:**
>
> | Idioma   | Precis√£o | Recall | F1-Score |
> |----------|----------|--------|----------|
> | Ingl√™s   | 0.90     | 0.92   | 0.91     |
> | Espanhol | 0.60     | 0.55   | 0.57     |
> | Portugu√™s| 0.60     | 0.65   | 0.62     |
>
> **Ap√≥s o Balanceamento (Œ± = 0.3):**
>
> | Idioma   | Precis√£o | Recall | F1-Score |
> |----------|----------|--------|----------|
> | Ingl√™s   | 0.82     | 0.85   | 0.83     |
> | Espanhol | 0.78     | 0.75   | 0.76     |
> | Portugu√™s| 0.79     | 0.80   | 0.79     |
>
> Podemos ver que o F1-score para ingl√™s diminuiu um pouco (de 0.91 para 0.83) ap√≥s o balanceamento, mas o desempenho para espanhol (de 0.57 para 0.76) e portugu√™s (de 0.62 para 0.79) melhorou substancialmente. O resultado √© um modelo mais equilibrado e eficiente em todos os idiomas. O F1-score m√©dio antes do balanceamento √© (0.91 + 0.57 + 0.62)/3 = 0.70, e ap√≥s o balanceamento √© (0.83 + 0.76 + 0.79)/3 = 0.79.

Al√©m do balanceamento de dados, outras t√©cnicas podem ser usadas para melhorar o desempenho de modelos multil√≠ngues, como a tradu√ß√£o reversa de dados, a cria√ß√£o de dados sint√©ticos e o uso de t√©cnicas de aprendizado por transfer√™ncia (transfer learning).

**Proposi√ß√£o 1**: T√©cnicas de aumento de dados (data augmentation), como a tradu√ß√£o reversa, podem ser usadas em conjunto com o balanceamento de amostragem para mitigar os efeitos da falta de dados em idiomas menos representados.

*Prova*:
I. A tradu√ß√£o reversa envolve traduzir dados de um idioma de alta frequ√™ncia (por exemplo, ingl√™s) para um idioma de baixa frequ√™ncia (por exemplo, portugu√™s).
II. Isso cria dados sint√©ticos no idioma de baixa frequ√™ncia, aumentando o tamanho efetivo do corpus desse idioma.
III. O balanceamento de amostragem aumenta a probabilidade de selecionar dados de idiomas de baixa frequ√™ncia durante o treinamento.
IV.  O aumento de dados por tradu√ß√£o reversa em conjunto com o balanceamento de amostragem permite que modelos sejam treinados em um corpus maior e mais representativo, reduzindo o problema da falta de dados.
V.  Essa abordagem combinada de aumento de dados e balanceamento de amostragem pode mitigar os efeitos da falta de dados em idiomas menos representados, melhorando o desempenho geral do modelo multil√≠ngue. ‚ñ†

### Conclus√£o
O treinamento de modelos de linguagem multil√≠ngues exige a considera√ß√£o de desafios espec√≠ficos, como o desequil√≠brio de dados entre diferentes idiomas. Ajustar as probabilidades de amostragem e o hiperpar√¢metro $\alpha$ √© uma t√©cnica eficaz para garantir que idiomas menos representados sejam devidamente considerados, levando a modelos mais robustos e com desempenho mais equilibrado em diferentes idiomas [^8]. O balanceamento de dados √©, portanto, uma componente fundamental para o treinamento de modelos de linguagem multil√≠ngues, e uma √°rea de pesquisa ativa, que deve ser levada em conta para que modelos de linguagem possam atender a todos os usu√°rios independentemente do idioma.

### Refer√™ncias
[^1]: Cap√≠tulo 11, "Masked Language Models"
[^2]: Se√ß√£o 11.2, "Training Bidirectional Encoders"
[^3]: Se√ß√£o 11.1, "Bidirectional Transformer Encoders"
[^8]: Se√ß√£o 11.2.3, "Training Regimes"
<!-- END -->
