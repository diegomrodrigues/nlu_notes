## Treinamento de Encoders Bidirecionais com Masked Language Modeling

### IntroduÃ§Ã£o
Este capÃ­tulo discute o treinamento de modelos de linguagem bidirecionais, em contraste com os modelos causais (left-to-right) abordados em capÃ­tulos anteriores [^1]. Especificamente, o foco reside no **Masked Language Modeling (MLM)**, que permite o uso de contexto Ã  direita e Ã  esquerda para o aprendizado [^1]. Este mÃ©todo difere fundamentalmente do treinamento de modelos causais, que preveem a prÃ³xima palavra em uma sequÃªncia, pois o MLM visa preencher lacunas dentro do texto. O MLM Ã© uma forma de *denoising*, onde o modelo aprende a reconstruir a entrada original a partir de uma versÃ£o corrompida [^4].

### Conceitos Fundamentais
#### Masked Language Modeling (MLM)
O **Masked Language Modeling (MLM)** Ã© a abordagem principal para treinar encoders bidirecionais como o BERT [^4]. Em vez de prever a prÃ³xima palavra em uma sequÃªncia, o MLM mascara aleatoriamente alguns tokens em uma sequÃªncia de entrada e treina o modelo para prever os tokens mascarados originais [^1, 4]. O processo de treinamento do MLM envolve as seguintes etapas:

1.  **SeleÃ§Ã£o de Tokens:** Uma amostra aleatÃ³ria de tokens Ã© selecionada de cada sequÃªncia de treinamento [^4].
2.  **CorrupÃ§Ã£o de Tokens:** Cada token selecionado Ã© processado de uma das trÃªs formas:
    -   80% dos tokens sÃ£o substituÃ­dos por um token especial denominado `[MASK]` [^5].
    -   10% dos tokens sÃ£o substituÃ­dos por um token aleatÃ³rio do vocabulÃ¡rio [^5].
    -   10% dos tokens permanecem inalterados [^5].
3.  **PrevisÃ£o de Tokens Mascarados:** O modelo, utilizando um encoder bidirecional, processa a sequÃªncia modificada e tenta prever os tokens originais que foram mascarados, substituÃ­dos ou mantidos [^5].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere a frase de entrada: "O gato estÃ¡ sentado no tapete."
>
> 1.  **SeleÃ§Ã£o de Tokens:** Suponha que os tokens selecionados aleatoriamente sejam "gato" e "tapete".
> 2.  **CorrupÃ§Ã£o de Tokens:**
>     -   Para "gato", com probabilidade de 80%, ele serÃ¡ substituÃ­do por `[MASK]`, entÃ£o a sequÃªncia se torna "O `[MASK]` estÃ¡ sentado no tapete.".
>     -   Para "tapete", com 10% de probabilidade, ele serÃ¡ substituÃ­do por um token aleatÃ³rio do vocabulÃ¡rio, por exemplo "mesa", fazendo a frase ficar "O `[MASK]` estÃ¡ sentado no mesa.".
>     -   Se considerarmos um outro token qualquer na frase, por exemplo "estÃ¡", com 10% de chance ele permaneceria inalterado.
>     -   O processo com os tokens "gato" e "tapete" seria feito de forma independente
> 3. **PrevisÃ£o de Tokens Mascarados:** O modelo bidirecional receberÃ¡ "O `[MASK]` estÃ¡ sentado no mesa." (ou "O `[MASK]` estÃ¡ sentado no tapete."). O objetivo do modelo Ã© prever que o token mascarado `[MASK]` seja "gato" e que o token substituido "mesa" seja "tapete".
>
> Isso ilustra a aplicaÃ§Ã£o de mascaramento e substituiÃ§Ã£o em um exemplo concreto, preparando a entrada para o modelo fazer a previsÃ£o.

A intuiÃ§Ã£o por trÃ¡s do MLM Ã© que, ao prever as palavras mascaradas, o modelo aprende representaÃ§Ãµes contextuais ricas das palavras [^1]. O modelo nÃ£o prediz os tokens diretamente. Em vez disso, ele produz uma distribuiÃ§Ã£o de probabilidade sobre todo o vocabulÃ¡rio para cada token mascarado. A funÃ§Ã£o de perda de entropia cruzada (cross-entropy loss) Ã© utilizada para otimizar o modelo, comparando a previsÃ£o do modelo com a palavra real mascarada [^4, 5].

$$
    L_{MLM} = -\frac{1}{M} \sum_{i \in M} \log P(x_i|h)
$$

Onde $M$ Ã© o conjunto de tokens mascarados, $x_i$ Ã© o token original, e $h$ Ã© a representaÃ§Ã£o contextualizada da sequÃªncia de entrada [^6].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que, para o exemplo anterior, o modelo tenha um vocabulÃ¡rio de 10.000 palavras.
>
> ApÃ³s processar a sequÃªncia "O `[MASK]` estÃ¡ sentado no tapete.", o modelo gera uma distribuiÃ§Ã£o de probabilidade sobre as 10.000 palavras do vocabulÃ¡rio para o token mascarado `[MASK]`.
>
> Seja $P(\text{gato}|h) = 0.6$ a probabilidade que o modelo atribui Ã  palavra "gato" para o token mascarado. Se "gato" Ã© o token correto ($x_i$), a contribuiÃ§Ã£o para a perda de entropia cruzada nesse caso seria:
>
> $$ L_i = -\log(0.6) \approx 0.22 $$
>
> Se, por exemplo, o modelo atribuÃ­sse uma baixa probabilidade a "gato", como $P(\text{gato}|h) = 0.1$, entÃ£o a perda seria:
>
> $$ L_i = -\log(0.1) \approx 1 $$
>
> Este valor de perda maior indica que o modelo fez uma previsÃ£o pior. A perda $L_{MLM}$ Ã© a mÃ©dia das perdas individuais de todos os tokens mascarados, e o objetivo Ã© minimizar essa perda durante o treinamento.

Ã‰ importante notar que, embora todos os tokens da sequÃªncia de entrada participem do processo de auto-atenÃ§Ã£o (self-attention), apenas os tokens amostrados contribuem para a perda de treinamento [^5]. Isso significa que apenas 15% dos tokens sÃ£o utilizados para o aprendizado, o que torna este mÃ©todo de treinamento computacionalmente menos eficiente em comparaÃ§Ã£o com outros mÃ©todos, embora seja eficaz na criaÃ§Ã£o de representaÃ§Ãµes contextuais [^6].

**Lema 1** A escolha da taxa de mascaramento (15% neste caso) Ã© um hiperparÃ¢metro crucial que afeta o desempenho do modelo. Taxas de mascaramento mais altas podem levar o modelo a aprender representaÃ§Ãµes mais robustas, mas podem tornar o treinamento mais difÃ­cil devido Ã  maior incerteza. Por outro lado, taxas de mascaramento mais baixas podem resultar em um aprendizado mais fÃ¡cil, mas com representaÃ§Ãµes menos ricas em contexto.

*Proof Outline*: O resultado decorre diretamente da natureza do treinamento por denoising. Uma maior taxa de mascaramento forÃ§a o modelo a depender mais do contexto para reconstruir o texto original, o que leva a representaÃ§Ãµes mais robustas, mas tambÃ©m aumenta a dificuldade de treinamento, pois a quantidade de informaÃ§Ã£o disponÃ­vel Ã© menor. Uma taxa menor de mascaramento faz o oposto, facilitando a aprendizagem mas com menos aproveitamento do contexto.

**Teorema 1** (CaracterizaÃ§Ã£o da Perda MLM): A perda de Masked Language Modeling, $L_{MLM}$, pode ser vista como uma aproximaÃ§Ã£o da divergÃªncia de Kullback-Leibler entre a distribuiÃ§Ã£o real do token mascarado e a distribuiÃ§Ã£o prevista pelo modelo.

*Prova:*
I. A funÃ§Ã£o de perda de entropia cruzada para um Ãºnico token mascarado Ã© definida como:
   $$L_i = -\log P(x_i|h)$$
   onde $x_i$ Ã© o token original e $h$ Ã© a representaÃ§Ã£o contextualizada da sequÃªncia de entrada.

II. A perda de entropia cruzada pode ser expressa em termos de distribuiÃ§Ãµes de probabilidade:
   $$H(p, q) = -\sum_{x_i} p(x_i) \log q(x_i)$$
   onde $p(x_i)$ Ã© a distribuiÃ§Ã£o empÃ­rica do token real $x_i$ (um vetor one-hot) e $q(x_i)$ Ã© a distribuiÃ§Ã£o de probabilidade prevista pelo modelo $P(x_i|h)$.

III. Para um Ãºnico token correto $x_i$, a distribuiÃ§Ã£o empÃ­rica $p(x_i)$ Ã© 1 para o token correto e 0 para todos os outros. Portanto, o somatÃ³rio na expressÃ£o da entropia cruzada se reduz a um Ãºnico termo, correspondente ao token correto:
    $$H(p, q) = -1 * \log q(x_i) = -\log P(x_i|h)$$

IV. A divergÃªncia de Kullback-Leibler Ã© definida como:
   $$D_{KL}(p||q) = \sum_{x_i} p(x_i) \log \frac{p(x_i)}{q(x_i)} = \sum_{x_i} p(x_i) \log p(x_i) - \sum_{x_i} p(x_i) \log q(x_i)$$

V. A entropia da distribuiÃ§Ã£o empÃ­rica $p$ (one-hot) Ã© zero, ou seja, $H(p) = - \sum_{x_i} p(x_i) \log p(x_i) = 0$. A entropia cruzada $H(p, q)$ pode ser escrita em termos da divergÃªncia KL:
     $$H(p, q) = D_{KL}(p||q) + H(p) = D_{KL}(p||q)$$

VI. Como a entropia cruzada Ã© igual Ã  divergÃªncia KL quando a distribuiÃ§Ã£o $p$ Ã© um vetor one-hot, minimizar a perda de entropia cruzada Ã© equivalente a minimizar a divergÃªncia KL entre a distribuiÃ§Ã£o empÃ­rica do token real e a distribuiÃ§Ã£o prevista pelo modelo.
     Portanto, a perda $L_{MLM}$ pode ser vista como uma aproximaÃ§Ã£o da divergÃªncia de Kullback-Leibler entre a distribuiÃ§Ã£o real do token mascarado e a distribuiÃ§Ã£o prevista pelo modelo.
â– 

**Teorema 1.1** (RelaÃ§Ã£o com a mÃ¡xima verossimilhanÃ§a): O treinamento do MLM, atravÃ©s da minimizaÃ§Ã£o da perda $L_{MLM}$, busca maximizar a verossimilhanÃ§a dos tokens mascarados, dado o contexto.

*Prova:*
I. A perda $L_{MLM}$ Ã© definida como a mÃ©dia das perdas de entropia cruzada sobre os tokens mascarados $M$:
   $$L_{MLM} = -\frac{1}{M} \sum_{i \in M} \log P(x_i|h)$$

II. Minimizar $L_{MLM}$ Ã© equivalente a maximizar a soma dos logaritmos das probabilidades $P(x_i|h)$ com sinal invertido:
   $$\min L_{MLM} \Leftrightarrow \min \left( -\frac{1}{M} \sum_{i \in M} \log P(x_i|h) \right) \Leftrightarrow \max \left( \frac{1}{M} \sum_{i \in M} \log P(x_i|h) \right)$$

III. Maximizar a soma dos logaritmos das probabilidades Ã© equivalente a maximizar o produto das probabilidades (pelas propriedades dos logaritmos):
   $$\max \left( \frac{1}{M} \sum_{i \in M} \log P(x_i|h) \right) \Leftrightarrow \max \left( \prod_{i \in M} P(x_i|h)^{1/M} \right)$$

IV. Como maximizar o produto das probabilidades Ã© equivalente a maximizar a verossimilhanÃ§a dos dados (tokens mascarados) dado o modelo, o treinamento do MLM busca encontrar parÃ¢metros que maximizem a probabilidade dos tokens mascarados dado o contexto ($h$).
   Portanto, minimizar a perda $L_{MLM}$ Ã© equivalente a maximizar a verossimilhanÃ§a dos tokens mascarados dado o contexto.
â– 

#### Denoising como Paradigma de Aprendizagem
O MLM Ã© um exemplo de um paradigma de aprendizado mais geral conhecido como *denoising* [^4]. Em *denoising*, a entrada de treinamento Ã© intencionalmente corrompida, e o modelo aprende a recuperar a entrada original nÃ£o corrompida. As formas de corrupÃ§Ã£o podem variar, incluindo mascaramento, substituiÃ§Ã£o, reordenaÃ§Ã£o, deleÃ§Ã£o e inserÃ§Ãµes extras de texto [^4]. O objetivo Ã© que o modelo aprenda representaÃ§Ãµes robustas, recuperando o conteÃºdo original mesmo na presenÃ§a de ruÃ­do [^4].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Imagine uma tarefa de *denoising* onde o objetivo Ã© recuperar uma sequÃªncia de texto apÃ³s a inserÃ§Ã£o de ruÃ­do aleatÃ³rio.
>
> **Entrada original:** "A inteligÃªncia artificial estÃ¡ transformando o mundo."
>
> **Entrada corrompida:** "A *&%$#@ncia arteficial &^%# transformando mundo o." (Aqui, `*&%$#@` e `&^%#` representam tokens aleatÃ³rios ou ruÃ­do)
>
> O modelo de *denoising* seria treinado para mapear a entrada corrompida de volta para a entrada original. Isso forÃ§a o modelo a aprender a abstrair o ruÃ­do e a entender o significado subjacente do texto.
>
> AlÃ©m disso, poderÃ­amos ter outros tipos de corrupÃ§Ã£o, como:
> -   **DeleÃ§Ã£o:** "A inteligÃªncia estÃ¡ transformando o mundo" (a palavra "artificial" foi removida)
> -   **ReordenaÃ§Ã£o:** "mundo o transformando estÃ¡ artificial inteligÃªncia A"
> - **InserÃ§Ã£o:** "A inteligÃªncia artificial rapidamente estÃ¡ transformando o mundo." (a palavra "rapidamente" foi inserida)
>
> Em todos esses casos, o objetivo do modelo Ã© reconstruir a sequÃªncia original, aprendendo representaÃ§Ãµes robustas e contextuais.

### ConclusÃ£o
O MLM, como paradigma de *denoising*, permite o treinamento eficaz de encoders bidirecionais, possibilitando que o modelo aprenda a entender e a reconstruir texto a partir de dados parcialmente mascarados. Esse processo leva a representaÃ§Ãµes contextuais de palavras de alta qualidade, que podem ser aplicadas a diversas tarefas de processamento de linguagem natural. O uso da perda de entropia cruzada na previsÃ£o dos tokens mascarados impulsiona o processo de aprendizagem do modelo, habilitando-o a extrair padrÃµes linguÃ­sticos complexos [^5]. A flexibilidade do MLM e sua capacidade de utilizar contexto Ã  direita e Ã  esquerda o tornam uma abordagem fundamental para modelos de linguagem bidirecionais, contrastando com o aprendizado sequencial dos modelos causais.

### ReferÃªncias
[^1]: CapÃ­tulo 11, "Masked Language Models"
[^4]: SeÃ§Ã£o 11.2, "Training Bidirectional Encoders"
[^5]: SeÃ§Ã£o 11.2.1, "Masking Words"
[^6]: SeÃ§Ã£o 11.2.1, "Masking Words"
<!-- END -->
