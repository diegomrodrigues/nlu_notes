## Treinamento de Encoders Bidirecionais com Masked Language Modeling

### Introdu√ß√£o
Este cap√≠tulo discute o treinamento de modelos de linguagem bidirecionais, em contraste com os modelos causais (left-to-right) abordados em cap√≠tulos anteriores [^1]. Especificamente, o foco reside no **Masked Language Modeling (MLM)**, que permite o uso de contexto √† direita e √† esquerda para o aprendizado [^1]. Este m√©todo difere fundamentalmente do treinamento de modelos causais, que preveem a pr√≥xima palavra em uma sequ√™ncia, pois o MLM visa preencher lacunas dentro do texto. O MLM √© uma forma de *denoising*, onde o modelo aprende a reconstruir a entrada original a partir de uma vers√£o corrompida [^4].

### Conceitos Fundamentais
#### Masked Language Modeling (MLM)
O **Masked Language Modeling (MLM)** √© a abordagem principal para treinar encoders bidirecionais como o BERT [^4]. Em vez de prever a pr√≥xima palavra em uma sequ√™ncia, o MLM mascara aleatoriamente alguns tokens em uma sequ√™ncia de entrada e treina o modelo para prever os tokens mascarados originais [^1, 4]. O processo de treinamento do MLM envolve as seguintes etapas:

1.  **Sele√ß√£o de Tokens:** Uma amostra aleat√≥ria de tokens √© selecionada de cada sequ√™ncia de treinamento [^4].
2.  **Corrup√ß√£o de Tokens:** Cada token selecionado √© processado de uma das tr√™s formas:
    -   80% dos tokens s√£o substitu√≠dos por um token especial denominado `[MASK]` [^5].
    -   10% dos tokens s√£o substitu√≠dos por um token aleat√≥rio do vocabul√°rio [^5].
    -   10% dos tokens permanecem inalterados [^5].
3.  **Previs√£o de Tokens Mascarados:** O modelo, utilizando um encoder bidirecional, processa a sequ√™ncia modificada e tenta prever os tokens originais que foram mascarados, substitu√≠dos ou mantidos [^5].

> üí° **Exemplo Num√©rico:**
>
> Considere a frase de entrada: "O gato est√° sentado no tapete."
>
> 1.  **Sele√ß√£o de Tokens:** Suponha que os tokens selecionados aleatoriamente sejam "gato" e "tapete".
> 2.  **Corrup√ß√£o de Tokens:**
>     -   Para "gato", com probabilidade de 80%, ele ser√° substitu√≠do por `[MASK]`, ent√£o a sequ√™ncia se torna "O `[MASK]` est√° sentado no tapete.".
>     -   Para "tapete", com 10% de probabilidade, ele ser√° substitu√≠do por um token aleat√≥rio do vocabul√°rio, por exemplo "mesa", fazendo a frase ficar "O `[MASK]` est√° sentado no mesa.".
>     -   Se considerarmos um outro token qualquer na frase, por exemplo "est√°", com 10% de chance ele permaneceria inalterado.
>     -   O processo com os tokens "gato" e "tapete" seria feito de forma independente
> 3. **Previs√£o de Tokens Mascarados:** O modelo bidirecional receber√° "O `[MASK]` est√° sentado no mesa." (ou "O `[MASK]` est√° sentado no tapete."). O objetivo do modelo √© prever que o token mascarado `[MASK]` seja "gato" e que o token substituido "mesa" seja "tapete".
>
> Isso ilustra a aplica√ß√£o de mascaramento e substitui√ß√£o em um exemplo concreto, preparando a entrada para o modelo fazer a previs√£o.

A intui√ß√£o por tr√°s do MLM √© que, ao prever as palavras mascaradas, o modelo aprende representa√ß√µes contextuais ricas das palavras [^1]. O modelo n√£o prediz os tokens diretamente. Em vez disso, ele produz uma distribui√ß√£o de probabilidade sobre todo o vocabul√°rio para cada token mascarado. A fun√ß√£o de perda de entropia cruzada (cross-entropy loss) √© utilizada para otimizar o modelo, comparando a previs√£o do modelo com a palavra real mascarada [^4, 5].

$$
    L_{MLM} = -\frac{1}{M} \sum_{i \in M} \log P(x_i|h)
$$

Onde $M$ √© o conjunto de tokens mascarados, $x_i$ √© o token original, e $h$ √© a representa√ß√£o contextualizada da sequ√™ncia de entrada [^6].

> üí° **Exemplo Num√©rico:**
>
> Suponha que, para o exemplo anterior, o modelo tenha um vocabul√°rio de 10.000 palavras.
>
> Ap√≥s processar a sequ√™ncia "O `[MASK]` est√° sentado no tapete.", o modelo gera uma distribui√ß√£o de probabilidade sobre as 10.000 palavras do vocabul√°rio para o token mascarado `[MASK]`.
>
> Seja $P(\text{gato}|h) = 0.6$ a probabilidade que o modelo atribui √† palavra "gato" para o token mascarado. Se "gato" √© o token correto ($x_i$), a contribui√ß√£o para a perda de entropia cruzada nesse caso seria:
>
> $$ L_i = -\log(0.6) \approx 0.22 $$
>
> Se, por exemplo, o modelo atribu√≠sse uma baixa probabilidade a "gato", como $P(\text{gato}|h) = 0.1$, ent√£o a perda seria:
>
> $$ L_i = -\log(0.1) \approx 1 $$
>
> Este valor de perda maior indica que o modelo fez uma previs√£o pior. A perda $L_{MLM}$ √© a m√©dia das perdas individuais de todos os tokens mascarados, e o objetivo √© minimizar essa perda durante o treinamento.

√â importante notar que, embora todos os tokens da sequ√™ncia de entrada participem do processo de auto-aten√ß√£o (self-attention), apenas os tokens amostrados contribuem para a perda de treinamento [^5]. Isso significa que apenas 15% dos tokens s√£o utilizados para o aprendizado, o que torna este m√©todo de treinamento computacionalmente menos eficiente em compara√ß√£o com outros m√©todos, embora seja eficaz na cria√ß√£o de representa√ß√µes contextuais [^6].

**Lema 1** A escolha da taxa de mascaramento (15% neste caso) √© um hiperpar√¢metro crucial que afeta o desempenho do modelo. Taxas de mascaramento mais altas podem levar o modelo a aprender representa√ß√µes mais robustas, mas podem tornar o treinamento mais dif√≠cil devido √† maior incerteza. Por outro lado, taxas de mascaramento mais baixas podem resultar em um aprendizado mais f√°cil, mas com representa√ß√µes menos ricas em contexto.

*Proof Outline*: O resultado decorre diretamente da natureza do treinamento por denoising. Uma maior taxa de mascaramento for√ßa o modelo a depender mais do contexto para reconstruir o texto original, o que leva a representa√ß√µes mais robustas, mas tamb√©m aumenta a dificuldade de treinamento, pois a quantidade de informa√ß√£o dispon√≠vel √© menor. Uma taxa menor de mascaramento faz o oposto, facilitando a aprendizagem mas com menos aproveitamento do contexto.

**Teorema 1** (Caracteriza√ß√£o da Perda MLM): A perda de Masked Language Modeling, $L_{MLM}$, pode ser vista como uma aproxima√ß√£o da diverg√™ncia de Kullback-Leibler entre a distribui√ß√£o real do token mascarado e a distribui√ß√£o prevista pelo modelo.

*Prova:*
I. A fun√ß√£o de perda de entropia cruzada para um √∫nico token mascarado √© definida como:
   $$L_i = -\log P(x_i|h)$$
   onde $x_i$ √© o token original e $h$ √© a representa√ß√£o contextualizada da sequ√™ncia de entrada.

II. A perda de entropia cruzada pode ser expressa em termos de distribui√ß√µes de probabilidade:
   $$H(p, q) = -\sum_{x_i} p(x_i) \log q(x_i)$$
   onde $p(x_i)$ √© a distribui√ß√£o emp√≠rica do token real $x_i$ (um vetor one-hot) e $q(x_i)$ √© a distribui√ß√£o de probabilidade prevista pelo modelo $P(x_i|h)$.

III. Para um √∫nico token correto $x_i$, a distribui√ß√£o emp√≠rica $p(x_i)$ √© 1 para o token correto e 0 para todos os outros. Portanto, o somat√≥rio na express√£o da entropia cruzada se reduz a um √∫nico termo, correspondente ao token correto:
    $$H(p, q) = -1 * \log q(x_i) = -\log P(x_i|h)$$

IV. A diverg√™ncia de Kullback-Leibler √© definida como:
   $$D_{KL}(p||q) = \sum_{x_i} p(x_i) \log \frac{p(x_i)}{q(x_i)} = \sum_{x_i} p(x_i) \log p(x_i) - \sum_{x_i} p(x_i) \log q(x_i)$$

V. A entropia da distribui√ß√£o emp√≠rica $p$ (one-hot) √© zero, ou seja, $H(p) = - \sum_{x_i} p(x_i) \log p(x_i) = 0$. A entropia cruzada $H(p, q)$ pode ser escrita em termos da diverg√™ncia KL:
     $$H(p, q) = D_{KL}(p||q) + H(p) = D_{KL}(p||q)$$

VI. Como a entropia cruzada √© igual √† diverg√™ncia KL quando a distribui√ß√£o $p$ √© um vetor one-hot, minimizar a perda de entropia cruzada √© equivalente a minimizar a diverg√™ncia KL entre a distribui√ß√£o emp√≠rica do token real e a distribui√ß√£o prevista pelo modelo.
     Portanto, a perda $L_{MLM}$ pode ser vista como uma aproxima√ß√£o da diverg√™ncia de Kullback-Leibler entre a distribui√ß√£o real do token mascarado e a distribui√ß√£o prevista pelo modelo.
‚ñ†

**Teorema 1.1** (Rela√ß√£o com a m√°xima verossimilhan√ßa): O treinamento do MLM, atrav√©s da minimiza√ß√£o da perda $L_{MLM}$, busca maximizar a verossimilhan√ßa dos tokens mascarados, dado o contexto.

*Prova:*
I. A perda $L_{MLM}$ √© definida como a m√©dia das perdas de entropia cruzada sobre os tokens mascarados $M$:
   $$L_{MLM} = -\frac{1}{M} \sum_{i \in M} \log P(x_i|h)$$

II. Minimizar $L_{MLM}$ √© equivalente a maximizar a soma dos logaritmos das probabilidades $P(x_i|h)$ com sinal invertido:
   $$\min L_{MLM} \Leftrightarrow \min \left( -\frac{1}{M} \sum_{i \in M} \log P(x_i|h) \right) \Leftrightarrow \max \left( \frac{1}{M} \sum_{i \in M} \log P(x_i|h) \right)$$

III. Maximizar a soma dos logaritmos das probabilidades √© equivalente a maximizar o produto das probabilidades (pelas propriedades dos logaritmos):
   $$\max \left( \frac{1}{M} \sum_{i \in M} \log P(x_i|h) \right) \Leftrightarrow \max \left( \prod_{i \in M} P(x_i|h)^{1/M} \right)$$

IV. Como maximizar o produto das probabilidades √© equivalente a maximizar a verossimilhan√ßa dos dados (tokens mascarados) dado o modelo, o treinamento do MLM busca encontrar par√¢metros que maximizem a probabilidade dos tokens mascarados dado o contexto ($h$).
   Portanto, minimizar a perda $L_{MLM}$ √© equivalente a maximizar a verossimilhan√ßa dos tokens mascarados dado o contexto.
‚ñ†

#### Denoising como Paradigma de Aprendizagem
O MLM √© um exemplo de um paradigma de aprendizado mais geral conhecido como *denoising* [^4]. Em *denoising*, a entrada de treinamento √© intencionalmente corrompida, e o modelo aprende a recuperar a entrada original n√£o corrompida. As formas de corrup√ß√£o podem variar, incluindo mascaramento, substitui√ß√£o, reordena√ß√£o, dele√ß√£o e inser√ß√µes extras de texto [^4]. O objetivo √© que o modelo aprenda representa√ß√µes robustas, recuperando o conte√∫do original mesmo na presen√ßa de ru√≠do [^4].

> üí° **Exemplo Num√©rico:**
>
> Imagine uma tarefa de *denoising* onde o objetivo √© recuperar uma sequ√™ncia de texto ap√≥s a inser√ß√£o de ru√≠do aleat√≥rio.
>
> **Entrada original:** "A intelig√™ncia artificial est√° transformando o mundo."
>
> **Entrada corrompida:** "A *&%$#@ncia arteficial &^%# transformando mundo o." (Aqui, `*&%$#@` e `&^%#` representam tokens aleat√≥rios ou ru√≠do)
>
> O modelo de *denoising* seria treinado para mapear a entrada corrompida de volta para a entrada original. Isso for√ßa o modelo a aprender a abstrair o ru√≠do e a entender o significado subjacente do texto.
>
> Al√©m disso, poder√≠amos ter outros tipos de corrup√ß√£o, como:
> -   **Dele√ß√£o:** "A intelig√™ncia est√° transformando o mundo" (a palavra "artificial" foi removida)
> -   **Reordena√ß√£o:** "mundo o transformando est√° artificial intelig√™ncia A"
> - **Inser√ß√£o:** "A intelig√™ncia artificial rapidamente est√° transformando o mundo." (a palavra "rapidamente" foi inserida)
>
> Em todos esses casos, o objetivo do modelo √© reconstruir a sequ√™ncia original, aprendendo representa√ß√µes robustas e contextuais.

### Conclus√£o
O MLM, como paradigma de *denoising*, permite o treinamento eficaz de encoders bidirecionais, possibilitando que o modelo aprenda a entender e a reconstruir texto a partir de dados parcialmente mascarados. Esse processo leva a representa√ß√µes contextuais de palavras de alta qualidade, que podem ser aplicadas a diversas tarefas de processamento de linguagem natural. O uso da perda de entropia cruzada na previs√£o dos tokens mascarados impulsiona o processo de aprendizagem do modelo, habilitando-o a extrair padr√µes lingu√≠sticos complexos [^5]. A flexibilidade do MLM e sua capacidade de utilizar contexto √† direita e √† esquerda o tornam uma abordagem fundamental para modelos de linguagem bidirecionais, contrastando com o aprendizado sequencial dos modelos causais.

### Refer√™ncias
[^1]: Cap√≠tulo 11, "Masked Language Models"
[^4]: Se√ß√£o 11.2, "Training Bidirectional Encoders"
[^5]: Se√ß√£o 11.2.1, "Masking Words"
[^6]: Se√ß√£o 11.2.1, "Masking Words"
<!-- END -->
