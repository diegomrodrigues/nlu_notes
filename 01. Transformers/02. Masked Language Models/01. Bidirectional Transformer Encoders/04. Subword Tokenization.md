## Codificadores Bidirecionais Transformers e Modelos de Linguagem Mascarados

### Introdu√ß√£o
Este cap√≠tulo aprofunda-se no conceito de **codificadores bidirecionais transformers** e sua aplica√ß√£o em **modelos de linguagem mascarados (Masked Language Models - MLM)**, complementando os conceitos de modelos de linguagem causais e transformers apresentados em cap√≠tulos anteriores [^1]. Anteriormente, exploramos o funcionamento dos transformers causais, que processam o texto sequencialmente, da esquerda para a direita [^1]. Agora, introduzimos uma abordagem alternativa que permite o processamento de sequ√™ncias em ambas as dire√ß√µes, oferecendo uma compreens√£o contextual mais rica. Os modelos bidirecionais, como o BERT, utilizam a t√©cnica de *masked language modeling* para realizar o treinamento, diferentemente dos modelos causais, que predizem o pr√≥ximo token na sequ√™ncia. O foco principal deste cap√≠tulo √© a arquitetura, o treinamento e a aplica√ß√£o dos codificadores bidirecionais em diversas tarefas de processamento de linguagem natural.

### Conceitos Fundamentais

#### 11.1 Bidirectional Transformer Encoders

O n√∫cleo deste cap√≠tulo s√£o os **codificadores bidirecionais transformer**, arquiteturas que, ao contr√°rio dos modelos causais, n√£o s√£o projetadas para gerar sequ√™ncias de texto, mas sim para produzir **representa√ß√µes contextualizadas** dos *tokens* de entrada [^2]. Modelos como o BERT, RoBERTa e SpanBERT, s√£o exemplos de arquiteturas que empregam este tipo de codificador [^1]. Em contraste com os *transformers* causais, que processam informa√ß√µes sequencialmente da esquerda para a direita, os codificadores bidirecionais utilizam **autoaten√ß√£o** para mapear sequ√™ncias de *embeddings* de entrada $(x_1, \ldots, x_n)$ para sequ√™ncias de *embeddings* de sa√≠da $(h_1, \ldots, h_n)$, onde cada vetor de sa√≠da $h_i$ √© contextualizado pela informa√ß√£o de toda a sequ√™ncia de entrada [^2]. Esta abordagem possibilita que a representa√ß√£o de cada *token* incorpore informa√ß√µes de todo o contexto da sequ√™ncia, o que se mostra especialmente √∫til em tarefas que requerem uma compreens√£o global da senten√ßa, como tarefas de classifica√ß√£o ou decis√£o baseada no contexto do token [^2].

Enquanto os modelos causais s√£o frequentemente chamados de "decoder-only" (pois correspondem ao decodificador do modelo *encoder-decoder*), os modelos de linguagem mascarados s√£o referidos como "encoder-only", pois produzem uma codifica√ß√£o para cada *token* de entrada, mas n√£o s√£o geralmente utilizados para gera√ß√£o de texto por meio de decodifica√ß√£o/amostragem [^2]. √â crucial notar que esses modelos n√£o s√£o concebidos para gera√ß√£o de texto; em vez disso, eles s√£o utilizados para tarefas de an√°lise e interpreta√ß√£o [^2].

**Lema 11.1:** *A representa√ß√£o contextualizada* $h_i$ *de um token* $x_i$ *em um codificador bidirecional transformer √© uma fun√ß√£o de todos os tokens na sequ√™ncia de entrada* $(x_1, \ldots, x_n)$.

*Prova:*
I. Em um codificador bidirecional transformer, a representa√ß√£o $h_i$ de um token $x_i$ √© obtida atrav√©s da camada de autoaten√ß√£o.
II. A camada de autoaten√ß√£o calcula uma matriz de aten√ß√£o $A$, que determina quanto cada token "atende" aos outros tokens.
III.  A matriz de aten√ß√£o √© calculada utilizando as matrizes de *queries* $Q$, *keys* $K$ e *values* $V$, todas derivadas da sequ√™ncia de entrada $(x_1, \ldots, x_n)$.
IV.  Especificamente, $A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)$, onde $Q$, $K$ e $V$ s√£o fun√ß√µes da sequ√™ncia de entrada $(x_1, \ldots, x_n)$.
V.   A representa√ß√£o $h_i$ √© ent√£o calculada como uma combina√ß√£o linear de $V$, ponderada por $A$.
VI.  Portanto, como a matriz de aten√ß√£o $A$ √© influenciada por todos os *tokens* na sequ√™ncia de entrada, a representa√ß√£o $h_i$ tamb√©m √© influenciada por todos os *tokens* $(x_1, \ldots, x_n)$.
VII. Conclu√≠mos que $h_i$ √© uma fun√ß√£o de todos os *tokens* na sequ√™ncia de entrada.‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere uma sequ√™ncia de entrada com 3 tokens:  $x = [x_1, x_2, x_3]$. Vamos supor que ap√≥s o *embedding* esses tokens sejam representados como vetores $x_1 = [1, 0, 0]$, $x_2 = [0, 1, 0]$, e $x_3 = [0, 0, 1]$. Para simplificar, vamos trabalhar com um √∫nico *head* de aten√ß√£o e com uma dimens√£o $d_k = 3$. As matrizes $Q$, $K$ e $V$ s√£o calculadas por meio de transforma√ß√µes lineares de $x$. Vamos supor que:
>
> $Q = XW_Q$, $K = XW_K$, $V = XW_V$
>
> Onde $X$ √© a matriz de entrada $[x_1, x_2, x_3]^T$ e $W_Q$, $W_K$ e $W_V$ s√£o matrizes de pesos.
>
> Vamos supor tamb√©m que:
>
> $W_Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$,  $W_K = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$, $W_V = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.
>
> Logo, $Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$, $K = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$, e $V = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.
>
> A matriz de aten√ß√£o sem a normaliza√ß√£o $\frac{QK^T}{\sqrt{d_k}}$ √©:
>
>  $QK^T = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$.
>
> Dividindo por $\sqrt{d_k} = \sqrt{3} \approx 1.732$, temos $\begin{bmatrix} 0 & 0 & 0.577 \\ 0.577 & 0 & 0 \\ 0 & 0.577 & 0 \end{bmatrix}$.
>
> Aplicando a fun√ß√£o *softmax* por linha, obtemos:
>
> $A = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix}$.
>
>  Finalmente, a sa√≠da √© calculada como $AV$:
>
> $AV = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix}$.
>
> Observe que a representa√ß√£o contextualizada de $x_1$ ($h_1 = [0.20, 0.20, 0.60]$) √© influenciada n√£o s√≥ por $x_1$ mas tamb√©m por $x_2$ e $x_3$ atrav√©s da matriz de aten√ß√£o.

**Lema 11.1.1:** *A representa√ß√£o contextualizada* $h_i$ *em modelos bidirecionais pode ser vista como uma m√©dia ponderada dos embeddings de entrada* $x_j$, *onde os pesos s√£o determinados pela matriz de aten√ß√£o* $A$.

*Prova:*
I. Da prova do Lema 11.1, sabemos que $h_i$ √© uma fun√ß√£o de todos os tokens na sequ√™ncia de entrada $(x_1, \ldots, x_n)$.
II. A representa√ß√£o $h_i$ √© calculada como $AV$, onde $A$ √© a matriz de aten√ß√£o e $V$ √© a matriz de *values*.
III. A matriz de *values* $V$ √© uma transforma√ß√£o linear da matriz de entrada $X$, ou seja, $V=XW_V$ onde $W_V$ s√£o pesos.
IV. Portanto, $h_i$ √© uma combina√ß√£o linear dos vetores de $V$ ponderados pelos pesos na matriz de aten√ß√£o $A$.
V. Cada vetor em $V$ √© uma transforma√ß√£o linear de um token na entrada $x_j$.
VI. Assim, podemos interpretar $h_i$ como uma m√©dia ponderada dos embeddings de entrada $x_j$ transformados linearmente, onde os pesos s√£o dados pela matriz de aten√ß√£o $A$.
VII. Conclu√≠mos que $h_i$ pode ser vista como uma m√©dia ponderada dos embeddings de entrada. ‚ñ†

#### 11.1.1 A Arquitetura para Modelos Mascarados Bidirecionais

A arquitetura dos modelos de linguagem bidirecionais difere dos *transformers* causais em dois aspectos principais [^2]:

1.  **A fun√ß√£o de aten√ß√£o n√£o √© causal:** Ao contr√°rio dos modelos causais, a aten√ß√£o de um *token* $i$ pode considerar *tokens* seguintes $i+1$ [^2].
2.  **O treinamento √© ligeiramente diferente:** A predi√ß√£o √© feita sobre algo no meio do texto, em vez de prever o pr√≥ximo *token* no final da sequ√™ncia [^2].

Em termos de arquitetura, a principal diferen√ßa reside na remo√ß√£o da m√°scara de aten√ß√£o. Em modelos causais, uma m√°scara √© aplicada √† matriz $Q K^T$ para evitar que a aten√ß√£o considere *tokens* futuros [^2]. Em codificadores bidirecionais, esta m√°scara √© removida, permitindo que a aten√ß√£o seja calculada sobre toda a sequ√™ncia [^3]. A computa√ß√£o da aten√ß√£o √© definida como:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$

onde $Q$, $K$ e $V$ s√£o as matrizes de *queries*, *keys* e *values*, respectivamente, e $d_k$ √© a dimens√£o das *keys*. A diferen√ßa crucial √© a aus√™ncia da m√°scara que impedia o acesso a *tokens* futuros. A arquitetura do bloco transformer, incluindo as camadas *feedforward*, normaliza√ß√£o de camadas, etc., permanece a mesma [^3].

**Observa√ß√£o 11.1:** A remo√ß√£o da m√°scara de aten√ß√£o √© o que habilita o processamento bidirecional. A aten√ß√£o, agora n√£o limitada pela causalidade, permite que cada token considere todo o contexto, tanto √† esquerda quanto √† direita, resultando em representa√ß√µes contextuais mais ricas.
> üí° **Exemplo Num√©rico:**
>
> Considere uma frase "o gato pulou alto". Os tokens seriam: `[o, gato, pulou, alto]`.
>
> Em um modelo causal, ao calcular a representa√ß√£o contextualizada para "pulou", apenas os tokens anteriores `[o, gato]` seriam considerados.
>
> Em um modelo bidirecional, ao calcular a representa√ß√£o contextualizada para "pulou", todos os tokens `[o, gato, pulou, alto]` seriam considerados.
>
> Isto permite que o modelo capture melhor o significado da palavra "pulou", pois ele tem acesso ao contexto completo da frase. Em modelos causais, o modelo s√≥ teria acesso ao contexto da esquerda para a direita, o que pode levar a representa√ß√µes menos ricas.

A entrada do modelo consiste em *subword tokens*, geralmente gerados por algoritmos como *WordPiece* ou *SentencePiece Unigram LM* [^3]. √â importante notar que todo o processamento posterior ocorre no n√≠vel de *subword tokens*, o que exige que, em algumas tarefas que requerem no√ß√£o de palavras, seja feito um mapeamento de volta para as palavras [^3].

**Teorema 11.1:** *A utiliza√ß√£o de subword tokens em modelos de linguagem mascarados permite que o modelo lide com palavras raras e out-of-vocabulary (OOV) de forma mais eficaz do que modelos que utilizam vocabul√°rio baseado em palavras.*

*Prova:*
I. Modelos de linguagem baseados em palavras possuem um vocabul√°rio fixo, o que significa que qualquer palavra n√£o presente no vocabul√°rio √© considerada OOV.
II. Algoritmos de tokeniza√ß√£o baseados em *subwords*, como *WordPiece* e *SentencePiece*, decomp√µem palavras em unidades menores, como *subwords*.
III. Isso diminui o tamanho do vocabul√°rio, pois cada palavra √© composta por combina√ß√µes de *subwords*.
IV.  Palavras raras e OOV s√£o, geralmente, composi√ß√µes de *subwords* que o modelo j√° conhece, o que permite represent√°-las mesmo que elas n√£o tenham sido vistas durante o treinamento.
V.  Assim, o modelo √© capaz de generalizar para palavras nunca vistas, ou seja, o problema de OOV √© mitigado, permitindo um processamento mais robusto.
VI. Portanto, modelos que utilizam *subword tokens* lidam com palavras raras e OOV de forma mais eficaz. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um vocabul√°rio baseado em palavras com as seguintes palavras:
>
> `["gato", "cachorro", "corre", "pula", "rapidamente"]`.
>
> Se a frase "o gatinho corre rapidamente" aparecer, a palavra "gatinho" seria considerada OOV, pois n√£o est√° no vocabul√°rio.
>
> Agora, usando um modelo com *subword tokens*, como o WordPiece, ter√≠amos um vocabul√°rio com:
>
> `["o", "gat", "##inho", "corre", "rapida", "##mente"]`.
>
> Nesse caso, a frase "o gatinho corre rapidamente" seria tokenizada como:
>
> `["o", "gat", "##inho", "corre", "rapida", "##mente"]`
>
> Observe que "gatinho" foi quebrado em "gat" e "##inho", ambos presentes no vocabul√°rio. O modelo consegue processar toda a frase sem OOV, aproveitando o contexto de tokens menores para representar palavras que n√£o viu no treinamento.
>
> ```mermaid
> graph LR
>     A[Frase: "o gatinho corre rapidamente"] --> B(Tokeniza√ß√£o por Palavras);
>     B --> C[Vocabul√°rio: gato, cachorro, corre, pula, rapidamente];
>     B --> D[Tokens: o, OOV, corre, rapidamente];
>     A --> E(Tokeniza√ß√£o por Subwords);
>      E --> F[Vocabul√°rio: o, gat, ##inho, corre, rapida, ##mente];
>     E --> G[Tokens: o, gat, ##inho, corre, rapida, ##mente];
>     C --> H[OOV: gatinho];
>     style H fill:#f9f,stroke:#333,stroke-width:2px
>     style D fill:#ccf,stroke:#333,stroke-width:2px
>     style G fill:#aaf,stroke:#333,stroke-width:2px
> ```

**Lema 11.2:** *A tokeniza√ß√£o utilizando subwords resulta em um vocabul√°rio menor quando comparado a tokeniza√ß√£o por palavras, com a mesma cobertura textual.*

*Prova:*
I.  A tokeniza√ß√£o por palavras associa um token √∫nico para cada palavra presente no vocabul√°rio, resultando em um vocabul√°rio de tamanho igual ao n√∫mero de palavras distintas.
II. A tokeniza√ß√£o por subwords quebra as palavras em unidades menores, onde v√°rias palavras podem compartilhar a mesma unidade, como afixos ou ra√≠zes.
III. Em geral, o n√∫mero de unidades √∫nicas (subwords) √© menor do que o n√∫mero de palavras √∫nicas em um corpus textual, especialmente se o vocabul√°rio contiver palavras com estruturas morfol√≥gicas similares.
IV. Portanto, a tokeniza√ß√£o por subwords resulta em um vocabul√°rio menor, pois diversas palavras podem ser representadas por combina√ß√µes das mesmas unidades subword.
V. Conclu√≠mos que o vocabul√°rio de subwords √© menor do que o vocabul√°rio de palavras, com a mesma cobertura textual. ‚ñ†

#### Exemplos de Modelos Bidirecionais

O modelo original BERT ( *Bidirectional Encoder Representations from Transformers* ), por exemplo, possui as seguintes caracter√≠sticas [^3]:

*   Um vocabul√°rio de *subwords* de 30,000 *tokens*, gerado utilizando o algoritmo *WordPiece* [^3].
*   Camadas ocultas de dimensionalidade $d = 768$ [^3].
*   12 camadas de blocos *transformer*, cada uma com 12 camadas de aten√ß√£o *multi-head* [^3].
*   Aproximadamente 100 milh√µes de par√¢metros [^3].

Outro exemplo not√°vel √© o modelo XLM-RoBERTa, um modelo multil√≠ngue treinado em 100 l√≠nguas, que possui [^3]:

*   Um vocabul√°rio de *subwords* de 250.000 *tokens*, gerado utilizando o algoritmo *SentencePiece Unigram LM* [^3].
*   24 camadas de blocos *transformer*, cada uma com 16 camadas de aten√ß√£o *multi-head* [^3].
*   Camadas ocultas de tamanho 1024 [^4].
*   Janela de contexto de 512 *tokens* [^4].
*   Aproximadamente 550 milh√µes de par√¢metros [^4].

Comparativamente, modelos de linguagem mascarados tendem a ser menores em termos de n√∫mero de par√¢metros em compara√ß√£o com modelos causais, como o Llama 3, que possui 405 bilh√µes de par√¢metros [^4].

**Corol√°rio 11.1:** *A menor quantidade de par√¢metros em modelos de linguagem mascarados em compara√ß√£o com modelos causais, como o Llama 3, pode ser atribu√≠da, em parte, ao fato de que os modelos mascarados s√£o encoders e n√£o decoders. Decoders, como o Llama 3, tendem a ter uma capacidade maior por serem capazes de gerar texto de maneira autogressiva.*
*Prova:*
I. Modelos de linguagem mascarados (MLMs) s√£o arquiteturas do tipo "encoder-only", projetadas para gerar representa√ß√µes contextuais de sequ√™ncias de entrada, e n√£o para gera√ß√£o de texto.
II. Modelos causais, por outro lado, s√£o decodificadores projetados para gerar texto de forma auto-regressiva, token por token.
III. Arquiteturas decodificadoras precisam modelar toda a distribui√ß√£o de probabilidade sobre o espa√ßo de sa√≠da, o que requer uma capacidade maior da rede, e por consequ√™ncia, mais par√¢metros.
IV.  Modelos encoders, como MLMs, n√£o precisam modelar esta distribui√ß√£o, e focam em aprender representa√ß√µes contextuais, o que requer menos par√¢metros.
V. Portanto, a menor quantidade de par√¢metros em modelos de linguagem mascarados em compara√ß√£o com modelos causais como o Llama 3 pode ser atribu√≠da ao fato de que modelos MLMs s√£o encoders e n√£o decoders. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos comparar um modelo encoder (BERT) com um modelo decoder (GPT):
>
> **BERT (Encoder):**
> - N√∫mero de camadas: 12
> - Dimens√£o da camada oculta: 768
> - N√∫mero de par√¢metros: ~100 milh√µes
> - Tarefa: Gerar representa√ß√µes contextuais de texto
>
> **GPT (Decoder):**
> - N√∫mero de camadas: 12
> - Dimens√£o da camada oculta: 768
> - N√∫mero de par√¢metros: ~117 milh√µes (GPT-1)
> - Tarefa: Gerar texto autogressivamente
>
> Modelos de decoder, como o GPT, muitas vezes t√™m estruturas mais complexas e necessitam de mais par√¢metros para modelar a probabilidade de sequ√™ncias de texto, o que √© refletido em seus par√¢metros. Modelos como o Llama 3 t√™m ainda mais camadas e par√¢metros para conseguirem gerar texto coerente.
>
> | Modelo    | Tipo      | Camadas | Dimens√£o Oculta | Par√¢metros | Tarefa                                  |
> |-----------|-----------|---------|-----------------|------------|-----------------------------------------|
> | BERT      | Encoder   | 12      | 768             | ~100M      | Representa√ß√µes contextuais             |
> | GPT-1     | Decoder   | 12      | 768             | ~117M      | Gera√ß√£o de texto auto-regressiva        |
> | Llama 3   | Decoder   |  V√°rias  | V√°rias          | 405B     | Gera√ß√£o de texto auto-regressiva        |

**Proposi√ß√£o 11.1:** *O n√∫mero de camadas e a dimens√£o das camadas ocultas s√£o fatores que afetam significativamente a capacidade de um modelo de linguagem mascarado capturar nuances contextuais e complexidade lingu√≠stica.*
*Justificativa:*
I. A profundidade da rede (n√∫mero de camadas) permite que o modelo aprenda representa√ß√µes hier√°rquicas da linguagem, com camadas mais baixas capturando caracter√≠sticas mais b√°sicas e camadas mais altas capturando abstra√ß√µes mais complexas.
II. A dimens√£o das camadas ocultas define a capacidade de cada n√≥ da rede representar informa√ß√µes. Dimens√µes maiores permitem que mais informa√ß√µes sejam armazenadas e processadas por cada n√≥.
III. Portanto, um aumento no n√∫mero de camadas e/ou na dimens√£o das camadas ocultas possibilita que o modelo capture nuances contextuais e complexidade lingu√≠stica de forma mais eficaz.

**Teorema 11.2:** *A utiliza√ß√£o de aten√ß√£o multi-head permite que o modelo capture diferentes tipos de rela√ß√µes entre os tokens, melhorando a qualidade das representa√ß√µes contextuais.*

*Prova:*
I. Em um modelo de aten√ß√£o single-head, a aten√ß√£o √© computada apenas uma vez para cada par de tokens na sequ√™ncia.
II. Modelos com aten√ß√£o multi-head computam a aten√ß√£o paralelamente em *h* cabe√ßas, cada uma com suas pr√≥prias proje√ß√µes de *queries*, *keys*, e *values*, ou seja, cada head pode focar em um aspecto diferente das rela√ß√µes entre os tokens.
III. As sa√≠das de cada cabe√ßa s√£o concatenadas e transformadas linearmente, resultando em uma representa√ß√£o que combina diferentes padr√µes de aten√ß√£o.
IV. Ao capturar diferentes padr√µes de aten√ß√£o, o modelo consegue representar as rela√ß√µes entre os tokens de uma forma mais completa e rica.
V. Assim, a utiliza√ß√£o de aten√ß√£o multi-head permite que o modelo capture diferentes tipos de rela√ß√µes entre os tokens, melhorando a qualidade das representa√ß√µes contextuais. ‚ñ†

#### 11.1.1.1 Remo√ß√£o do Mascaramento na Matriz de Aten√ß√£o

Em continuidade ao conceito apresentado, a diferen√ßa fundamental na arquitetura dos codificadores bidirecionais reside na forma como a aten√ß√£o √© computada [^3]. Nos *transformers* causais, como visto anteriormente, uma m√°scara √© aplicada na matriz de aten√ß√£o $Q K^T$ para garantir que cada *token* apenas atenda aos *tokens* precedentes [^1, 2]. Essa m√°scara √© essencial para a natureza autoregressiva dos modelos causais, onde a predi√ß√£o de um *token* depende exclusivamente dos *tokens* anteriores. Em contrapartida, os codificadores bidirecionais removem essa m√°scara, permitindo que a aten√ß√£o se estenda sobre toda a sequ√™ncia de entrada [^3].

A remo√ß√£o da m√°scara, conforme detalhado na se√ß√£o anterior, habilita cada *token* a acessar informa√ß√µes tanto dos *tokens* √† esquerda quanto √† direita, levando a representa√ß√µes contextuais mais ricas e profundas.  A computa√ß√£o da aten√ß√£o, no entanto, mant√©m-se id√™ntica √† dos modelos causais, utilizando a mesma f√≥rmula:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$
Onde,
$Q$, $K$, e $V$ representam as matrizes de *queries*, *keys* e *values* respectivamente, e  $d_k$ representa a dimens√£o das *keys*.

A altera√ß√£o principal √© a aus√™ncia da m√°scara aplicada √† matriz $Q K^T$, e essa aus√™ncia √© o que possibilita o codificador bidirecional processar a informa√ß√£o de forma n√£o causal, permitindo que a aten√ß√£o de cada *token* se estenda por toda a sequ√™ncia, resultando em um modelo que pode aprender rela√ß√µes contextuais mais complexas.

**Lema 11.2:** *A aus√™ncia da m√°scara na matriz de aten√ß√£o em codificadores bidirecionais permite que cada token "veja" toda a sequ√™ncia de entrada durante a computa√ß√£o da aten√ß√£o, ao contr√°rio dos modelos causais onde a informa√ß√£o flui apenas em uma dire√ß√£o.*
*Prova:*
I. Em modelos causais, a m√°scara de aten√ß√£o impede que um token $x_i$ atenda aos tokens $x_j$ com $j > i$.
II. Em codificadores bidirecionais, a remo√ß√£o da m√°scara permite que o token $x_i$ atenda a todos os tokens na sequ√™ncia, incluindo aqueles com $j > i$.
III. Assim, a representa√ß√£o contextualizada de $x_i$ em um codificador bidirecional incorpora informa√ß√µes de todo o contexto, n√£o apenas dos tokens precedentes, como nos modelos causais.
IV. Portanto, a aus√™ncia da m√°scara permite que cada token veja toda a sequ√™ncia de entrada. ‚ñ†

#### 11.1.1.2 Implementa√ß√£o da Arquitetura Bidirecional
A implementa√ß√£o da arquitetura bidirecional √© notavelmente simples [^2]. Como mencionado, a principal altera√ß√£o em rela√ß√£o ao *transformer* causal reside na remo√ß√£o da m√°scara na matriz $Q K^T$ [^2]. Essa remo√ß√£o elimina a restri√ß√£o de causalidade e permite que cada *token* atenda a todos os outros *tokens* na sequ√™ncia, tanto √† esquerda quanto √† direita [^2].

Com essa mudan√ßa, a computa√ß√£o da aten√ß√£o para modelos bidirecionais torna-se id√™ntica √† Equa√ß√£o 11.1, por√©m sem a aplica√ß√£o da m√°scara [^3]:
$$ A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
Onde:
*   $Q$ representa a matriz de *queries*.
*   $K$ representa a matriz de *keys*.
*   $V$ representa a matriz de *values*.
*   $d_k$ representa a dimens√£o das *keys*.

Al√©m disso, a arquitetura do bloco *transformer*, que inclui a camada *feedforward*, a normaliza√ß√£o de camada, e outros componentes, permanece inalterada [^3].  Essa estrutura permite que o codificador bidirecional utilize a mesma base arquitetural dos *transformers* causais, mas com a capacidade adicional de processar informa√ß√µes em ambas as dire√ß√µes, proporcionando representa√ß√µes contextuais mais ricas e abrangentes [^2].

> üí° **Exemplo Num√©rico:**
>
> Em termos de computa√ß√£o, imagine uma sequ√™ncia de entrada representada pelas matrizes Q, K e V. Em um transformer causal, uma m√°scara seria aplicada na matriz $QK^T$ antes do softmax, para zerar as intera√ß√µes entre um token e os tokens subsequentes. Por exemplo, um token na posi√ß√£o i poderia apenas atender aos tokens nas posi√ß√µes menores ou iguais a i.
> Em um transformer bidirecional, essa m√°scara n√£o √© aplicada, ou seja, todos os tokens podem atender a todos os outros tokens na sequ√™ncia, sem restri√ß√µes. A computa√ß√£o da matriz de aten√ß√£o, e o restante da arquitetura, como a camada feedforward, permanece a mesma.
>
> ```mermaid
> graph LR
>     A[Input Sequence: Q, K, V] --> B{Causal Transformer};
>     B -->|Apply Mask| C[Masked QK^T];
>     C --> D[Softmax(Masked QK^T/sqrt(dk))V];
>     A --> E{Bidirectional Transformer};
>     E --> F[Softmax(QK^T/sqrt(dk))V];
>     style C fill:#ccf,stroke:#333,stroke-width:2px
>     style D fill:#aaf,stroke:#333,stroke-width:2px
>
> ```

#### 11.1.1.3 Input em Subword Tokens

Assim como nos modelos causais, a entrada do modelo bidirecional n√£o √© composta por palavras, mas por uma sequ√™ncia de *subword tokens* [^3].  Esses *tokens* s√£o geralmente computados por algoritmos como *WordPiece* ou *SentencePiece Unigram LM* [^3]. O uso de *subword tokens* oferece diversas vantagens, incluindo a capacidade de lidar com palavras raras e *out-of-vocabulary* (OOV), al√©m de reduzir o tamanho do vocabul√°rio.

√â importante ressaltar que todo o processamento subsequente, incluindo a computa√ß√£o da aten√ß√£o e as camadas subsequentes do modelo, ocorre no n√≠vel dos *subword tokens*, n√£o em n√≠vel de palavra [^3]. Isso implica que, para tarefas que exigem uma no√ß√£o clara de palavras (como tarefas de *parsing* sint√°tico), um mapeamento adicional de *subword tokens* de volta para palavras pode ser necess√°rio.

> üí° **Exemplo Num√©rico:**
> Considere a frase "O c√£ozinho late muito".
>
> **Tokeniza√ß√£o com palavras:**
> `["O", "c√£ozinho", "late", "muito"]`
>
> **Tokeniza√ß√£o com *subwords* (exemplo com WordPiece):**
> `["O", "c√£o", "##zinho", "late", "muito"]`
>
> Observe que a palavra "c√£ozinho" foi dividida em "c√£o" e "##zinho".
>
> Em um codificador bidirecional transformer, a computa√ß√£o da aten√ß√£o e demais opera√ß√µes seriam feitas sobre essa sequ√™ncia de subwords, e n√£o sobre palavras completas. Se uma tarefa necessitar da representa√ß√£o de "c√£ozinho" como uma unidade, o mapeamento de volta de subwords para a palavra seria necess√°rio.

**Corol√°rio 11.2:** *A utiliza√ß√£o de subword tokens n√£o apenas mitiga o problema de OOV, mas tamb√©m pode auxiliar o modelo a capturar rela√ß√µes morfol√≥gicas e sem√¢nticas entre palavras que compartilham a mesma raiz ou afixos, j√° que essas partes menores da palavra podem ter representa√ß√µes vetoriais semelhantes.*
*Justificativa:*
I. Subword tokens que comp√µem palavras morfologicamente relacionadas (ex: "c√£o", "c√£ezinho") ter√£o representa√ß√µes vetoriais similares no espa√ßo de embeddings.
II. Essa similaridade permite que o modelo aprenda rela√ß√µes entre as palavras, mesmo que elas n√£o tenham aparecido exatamente no mesmo contexto.
III. Consequentemente, modelos que usam subwords conseguem generalizar melhor a rela√ß√£o entre palavras morfologicamente relacionadas, melhorando o entendimento da estrutura da linguagem.
IV. Portanto, a utiliza√ß√£o de subword tokens n√£o apenas mitiga o problema de OOV, mas tamb√©m pode auxiliar o modelo a capturar rela√ß√µes morfol√≥gicas e sem√¢nticas.
> üí° **Exemplo Num√©rico:**
> Considere as palavras "correr", "correndo" e "correu". Usando WordPiece, elas poderiam ser tokenizadas como:
> - correr: ["correr"]
> - correndo: ["cor", "##rendo"]
> - correu: ["cor", "##reu"]
>
> Note que os subwords "cor" s√£o compartilhados entre as 3 palavras, o que permite que o modelo aprenda semelhan√ßas entre as palavras, mesmo que elas tenham afixos diferentes.

**Proposi√ß√£o 11.3:** *O uso de subwords pode gerar representa√ß√µes mais robustas em rela√ß√£o a erros de ortografia, pois mesmo palavras com erros podem conter subwords que s√£o familiares ao modelo.*
*Justificativa:*
I. Palavras com erros ortogr√°ficos podem ser decompostas em subwords, alguns dos quais podem corresponder a subwords corretos.
II.  Modelos treinados em subwords podem ser capazes de associar esses subwords corretos a representa√ß√µes mais pr√≥ximas da palavra correta.
III. Assim, mesmo que uma palavra tenha erros, o modelo ainda pode capturar parte do seu significado atrav√©s da representa√ß√£o de subwords.
IV. Portanto, o uso de subwords pode gerar representa√ß√µes mais robustas em rela√ß√£o a erros de ortografia.

#### 11.1.2 A Fun√ß√£o de Aten√ß√£o: Uma Perspectiva Matem√°tica

A fun√ß√£o de aten√ß√£o, central para o funcionamento dos codificadores bidirecionais, pode ser analisada em detalhe sob uma perspectiva matem√°tica. Como vimos anteriormente, essa fun√ß√£o √© a base do mecanismo de autoaten√ß√£o, que permite aos modelos *transformer* capturar depend√™ncias complexas entre *tokens* de uma sequ√™ncia [^2]. Em termos matem√°ticos, a fun√ß√£o de aten√ß√£o opera atrav√©s de tr√™s matrizes de proje√ß√£o: *Query* ($Q$), *Key* ($K$) e *Value* ($V$) [^3]. Cada matriz √© obtida por meio de uma transforma√ß√£o linear da sequ√™ncia de entrada $X$ atrav√©s de matrizes de pesos espec√≠ficas, $W_Q$, $W_K$ e $W_V$ respectivamente:

$$
Q = X W_Q \\
K = X W_K \\
V = X W_V
$$
onde $X$ representa a matriz de *embeddings* de entrada, e $W_Q, W_K, W_V$ s√£o as matrizes de pesos trein√°veis.

A matriz de aten√ß√£o $A$ √© calculada por meio do *softmax* dos produtos escalares entre as *queries* e as *keys*, escalonados pela raiz quadrada da dimens√£o das *keys* ($d_k$) [^3]:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)
$$
Nesse contexto, $Q K^T$ resulta em uma matriz onde cada elemento $(i, j)$ representa a similaridade entre o *token* $i$ e o *token* $j$, antes da aplica√ß√£o do *softmax* [^3]. A divis√£o por $\sqrt{d_k}$ tem como objetivo estabilizar os gradientes durante o treinamento, evitando que os valores do produto escalar se tornem muito grandes e que o *softmax* se sature [^3]. A fun√ß√£o *softmax* garante que os valores na matriz $A$ sejam transformados em probabilidades que somam 1, representando o peso de aten√ß√£o de cada *token* em rela√ß√£o aos outros.

A sa√≠da final da camada de aten√ß√£o, $H$,  √© dada pela combina√ß√£o linear dos *values* ponderados pela matriz de aten√ß√£o [^3]:

$$ H = AV $$

Esta sa√≠da $H$ √© a representa√ß√£o contextualizada de cada *token* na sequ√™ncia de entrada, que √© usada nas camadas subsequentes do modelo.

**Proposi√ß√£o 11.2:** *A matriz de aten√ß√£o* $$A$ *√© calculada usando a fun√ß√£o softmax sobre as pontua√ß√µes de aten√ß√£o.*

*Demonstra√ß√£o:*

As pontua√ß√µes de aten√ß√£o s√£o obtidas atrav√©s de um produto interno entre as matrizes de consulta, chave e valor.  Especificamente, para cada *token* na sequ√™ncia de entrada, a sua pontua√ß√£o de aten√ß√£o com rela√ß√£o a outro *token* √© dada por:

$$ score(q_i, k_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}} $$

onde $q_i$ √© o vetor de consulta do *token* $i$, $k_j$ √© o vetor chave do *token* $j$, e $d_k$ √© a dimens√£o dos vetores chave. O fator $\sqrt{d_k}$ √© usado para escalar os resultados e evitar que os valores fiquem muito grandes durante o treinamento.

A matriz de aten√ß√£o $A$ √© ent√£o calculada aplicando a fun√ß√£o *softmax* sobre estas pontua√ß√µes para todos os *tokens* na sequ√™ncia de entrada:

$$ A_{ij} = \frac{\exp(score(q_i, k_j))}{\sum_{k=1}^n \exp(score(q_i, k_k))} $$

onde $n$ √© o comprimento da sequ√™ncia de entrada. O resultado $A_{ij}$ representa a import√¢ncia do *token* $j$ para o *token* $i$.

A matriz $A$, sendo resultado da fun√ß√£o softmax, tem as seguintes propriedades:

1.  Os seus elementos s√£o n√£o-negativos, $A_{ij} \geq 0$, o que garante que n√£o haver√° aten√ß√£o negativa.
2.  A soma das entradas de cada linha √© igual a 1, $\sum_{j} A_{ij} = 1$, o que define uma distribui√ß√£o de probabilidade sobre os *tokens* da sequ√™ncia de entrada, dando a propor√ß√£o da aten√ß√£o para cada *token* de entrada.

$\blacksquare$

**Teorema 11.1:** *O mecanismo de auto-aten√ß√£o √© invariante √† permuta√ß√£o dos tokens de entrada quando as matrizes de consulta, chave e valor s√£o todas id√™nticas.*

*Demonstra√ß√£o:*

Suponha que as matrizes de consulta, chave e valor sejam todas iguais, isto √©, $Q = K = V$. Neste caso, a matriz de pontua√ß√µes √© dada por:

$$ S = \frac{QQ^T}{\sqrt{d_k}} $$

Onde $Q^T$ √© a transposta de $Q$.

A matriz de aten√ß√£o $A$ √© ent√£o:

$$ A = softmax(S) $$

Se permutarmos a ordem dos *tokens* de entrada, isto equivale a multiplicar a matriz $Q$ por uma matriz de permuta√ß√£o $P$. Assim, a matriz de consulta permuta ser√° $Q' = PQ$.
A nova matriz de pontua√ß√µes $S'$ ser√°:

$$ S' = \frac{(PQ)(PQ)^T}{\sqrt{d_k}} =  \frac{PQQ^TP^T}{\sqrt{d_k}} $$

Como $P$ √© uma matriz de permuta√ß√£o, $P^T$ √© a sua inversa, portanto, $PP^T = I$, onde $I$ √© a matriz identidade.  Ent√£o:

$$ S' =  \frac{PQQ^T}{\sqrt{d_k}}P^T $$

$$ S' = PS P^T $$

A nova matriz de aten√ß√£o √© ent√£o:

$$ A' = softmax(S') = softmax(PSP^T) $$

Como $softmax$ preserva a ordem dos valores, a matriz de aten√ß√£o permutada $A'$ corresponder√° √† matriz $A$ original com as linhas e colunas permutadas na mesma ordem da matriz $P$. Isso significa que a aten√ß√£o dada aos tokens (a sa√≠da de aten√ß√£o) n√£o √© afetada pela permuta√ß√£o.

A sa√≠da $H$ ser√°:

$$ H' = A' V' = A' P V = P A P^T P V = P A V $$

Assim, a sa√≠da $H'$ √© apenas uma permuta√ß√£o da sa√≠da original $H = AV$ e a rela√ß√£o de aten√ß√£o entre os *tokens* n√£o se altera, demonstrando que o mecanismo de auto-aten√ß√£o √© invariante √† ordem de entrada quando as matrizes de consulta, chave e valor s√£o iguais.

$\blacksquare$

A auto-aten√ß√£o permite que o modelo pondere a import√¢ncia de cada *token* em rela√ß√£o a outros *tokens* dentro da mesma sequ√™ncia, facilitando a captura de depend√™ncias e rela√ß√µes complexas entre eles. Isto √© crucial para entender o contexto e o significado da entrada.

<!-- END -->
