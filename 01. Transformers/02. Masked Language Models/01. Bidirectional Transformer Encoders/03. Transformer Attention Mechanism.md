## Codificadores Bidirecionais Transformers e Modelos de Linguagem Mascarados

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda-se no conceito de **codificadores bidirecionais transformers** e sua aplicaÃ§Ã£o em **modelos de linguagem mascarados (Masked Language Models - MLM)**, complementando os conceitos de modelos de linguagem causais e transformers apresentados em capÃ­tulos anteriores [^1]. Anteriormente, exploramos o funcionamento dos transformers causais, que processam o texto sequencialmente, da esquerda para a direita [^1]. Agora, introduzimos uma abordagem alternativa que permite o processamento de sequÃªncias em ambas as direÃ§Ãµes, oferecendo uma compreensÃ£o contextual mais rica. Os modelos bidirecionais, como o BERT, utilizam a tÃ©cnica de *masked language modeling* para realizar o treinamento, diferentemente dos modelos causais, que predizem o prÃ³ximo token na sequÃªncia. O foco principal deste capÃ­tulo Ã© a arquitetura, o treinamento e a aplicaÃ§Ã£o dos codificadores bidirecionais em diversas tarefas de processamento de linguagem natural.

### Conceitos Fundamentais

#### 11.1 Bidirectional Transformer Encoders

O nÃºcleo deste capÃ­tulo sÃ£o os **codificadores bidirecionais transformer**, arquiteturas que, ao contrÃ¡rio dos modelos causais, nÃ£o sÃ£o projetadas para gerar sequÃªncias de texto, mas sim para produzir **representaÃ§Ãµes contextualizadas** dos *tokens* de entrada [^2]. Modelos como o BERT, RoBERTa e SpanBERT, sÃ£o exemplos de arquiteturas que empregam este tipo de codificador [^1]. Em contraste com os *transformers* causais, que processam informaÃ§Ãµes sequencialmente da esquerda para a direita, os codificadores bidirecionais utilizam **autoatenÃ§Ã£o** para mapear sequÃªncias de *embeddings* de entrada $(x_1, ..., x_n)$ para sequÃªncias de *embeddings* de saÃ­da $(h_1, ..., h_n)$, onde cada vetor de saÃ­da $h_i$ Ã© contextualizado pela informaÃ§Ã£o de toda a sequÃªncia de entrada [^2]. Esta abordagem possibilita que a representaÃ§Ã£o de cada *token* incorpore informaÃ§Ãµes de todo o contexto da sequÃªncia, o que se mostra especialmente Ãºtil em tarefas que requerem uma compreensÃ£o global da sentenÃ§a, como tarefas de classificaÃ§Ã£o ou decisÃ£o baseada no contexto do token [^2].

Enquanto os modelos causais sÃ£o frequentemente chamados de "decoder-only" (pois correspondem ao decodificador do modelo *encoder-decoder*), os modelos de linguagem mascarados sÃ£o referidos como "encoder-only", pois produzem uma codificaÃ§Ã£o para cada *token* de entrada, mas nÃ£o sÃ£o geralmente utilizados para geraÃ§Ã£o de texto por meio de decodificaÃ§Ã£o/amostragem [^2]. Ã‰ crucial notar que esses modelos nÃ£o sÃ£o concebidos para geraÃ§Ã£o de texto; em vez disso, eles sÃ£o utilizados para tarefas de anÃ¡lise e interpretaÃ§Ã£o [^2].

**Lema 11.1:** *A representaÃ§Ã£o contextualizada* $h_i$ *de um token* $x_i$ *em um codificador bidirecional transformer Ã© uma funÃ§Ã£o de todos os tokens na sequÃªncia de entrada* $(x_1, ..., x_n)$.

*Prova:*
I. Em um codificador bidirecional transformer, a representaÃ§Ã£o $h_i$ de um token $x_i$ Ã© obtida atravÃ©s da camada de autoatenÃ§Ã£o.
II. A camada de autoatenÃ§Ã£o calcula uma matriz de atenÃ§Ã£o $A$, que determina quanto cada token "atende" aos outros tokens.
III.  A matriz de atenÃ§Ã£o Ã© calculada utilizando as matrizes de *queries* $Q$, *keys* $K$ e *values* $V$, todas derivadas da sequÃªncia de entrada $(x_1, \ldots, x_n)$.
IV.  Especificamente, $A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)$, onde $Q$, $K$ e $V$ sÃ£o funÃ§Ãµes da sequÃªncia de entrada $(x_1, \ldots, x_n)$.
V.   A representaÃ§Ã£o $h_i$ Ã© entÃ£o calculada como uma combinaÃ§Ã£o linear de $V$, ponderada por $A$.
VI.  Portanto, como a matriz de atenÃ§Ã£o $A$ Ã© influenciada por todos os *tokens* na sequÃªncia de entrada, a representaÃ§Ã£o $h_i$ tambÃ©m Ã© influenciada por todos os *tokens* $(x_1, \ldots, x_n)$.
VII. ConcluÃ­mos que $h_i$ Ã© uma funÃ§Ã£o de todos os *tokens* na sequÃªncia de entrada.â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere uma sequÃªncia de entrada com 3 tokens:  $x = [x_1, x_2, x_3]$. Vamos supor que apÃ³s o *embedding* esses tokens sejam representados como vetores $x_1 = [1, 0, 0]$, $x_2 = [0, 1, 0]$, e $x_3 = [0, 0, 1]$. Para simplificar, vamos trabalhar com um Ãºnico *head* de atenÃ§Ã£o e com uma dimensÃ£o $d_k = 3$. As matrizes $Q$, $K$ e $V$ sÃ£o calculadas por meio de transformaÃ§Ãµes lineares de $x$. Vamos supor que:
>
> $Q = XW_Q$, $K = XW_K$, $V = XW_V$
>
> Onde $X$ Ã© a matriz de entrada $[x_1, x_2, x_3]^T$ e $W_Q$, $W_K$ e $W_V$ sÃ£o matrizes de pesos.
>
> Vamos supor tambÃ©m que:
>
> $W_Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$,  $W_K = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$, $W_V = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.
>
> Logo, $Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$, $K = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$, e $V = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.
>
> A matriz de atenÃ§Ã£o sem a normalizaÃ§Ã£o $\frac{QK^T}{\sqrt{d_k}}$ Ã©:
>
>  $QK^T = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$.
>
> Dividindo por $\sqrt{d_k} = \sqrt{3} \approx 1.732$, temos $\begin{bmatrix} 0 & 0 & 0.577 \\ 0.577 & 0 & 0 \\ 0 & 0.577 & 0 \end{bmatrix}$.
>
> Aplicando a funÃ§Ã£o *softmax* por linha, obtemos:
>
> $A = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix}$.
>
>  Finalmente, a saÃ­da Ã© calculada como $AV$:
>
> $AV = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix}$.
>
> Observe que a representaÃ§Ã£o contextualizada de $x_1$ ($h_1 = [0.20, 0.20, 0.60]$) Ã© influenciada nÃ£o sÃ³ por $x_1$ mas tambÃ©m por $x_2$ e $x_3$ atravÃ©s da matriz de atenÃ§Ã£o.

#### 11.1.1 A Arquitetura para Modelos Mascarados Bidirecionais

A arquitetura dos modelos de linguagem bidirecionais difere dos *transformers* causais em dois aspectos principais [^2]:

1.  **A funÃ§Ã£o de atenÃ§Ã£o nÃ£o Ã© causal:** Ao contrÃ¡rio dos modelos causais, a atenÃ§Ã£o de um *token* $i$ pode considerar *tokens* seguintes $i+1$ [^2].
2.  **O treinamento Ã© ligeiramente diferente:** A prediÃ§Ã£o Ã© feita sobre algo no meio do texto, em vez de prever o prÃ³ximo *token* no final da sequÃªncia [^2].

Em termos de arquitetura, a principal diferenÃ§a reside na remoÃ§Ã£o da mÃ¡scara de atenÃ§Ã£o. Em modelos causais, uma mÃ¡scara Ã© aplicada Ã  matriz $Q K^T$ para evitar que a atenÃ§Ã£o considere *tokens* futuros [^2]. Em codificadores bidirecionais, esta mÃ¡scara Ã© removida, permitindo que a atenÃ§Ã£o seja calculada sobre toda a sequÃªncia [^3]. A computaÃ§Ã£o da atenÃ§Ã£o Ã© definida como:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$

onde $Q$, $K$ e $V$ sÃ£o as matrizes de *queries*, *keys* e *values*, respectivamente, e $d_k$ Ã© a dimensÃ£o das *keys*. A diferenÃ§a crucial Ã© a ausÃªncia da mÃ¡scara que impedia o acesso a *tokens* futuros. A arquitetura do bloco transformer, incluindo as camadas *feedforward*, normalizaÃ§Ã£o de camadas, etc., permanece a mesma [^3].

**ObservaÃ§Ã£o 11.1:** A remoÃ§Ã£o da mÃ¡scara de atenÃ§Ã£o Ã© o que habilita o processamento bidirecional. A atenÃ§Ã£o, agora nÃ£o limitada pela causalidade, permite que cada token considere todo o contexto, tanto Ã  esquerda quanto Ã  direita, resultando em representaÃ§Ãµes contextuais mais ricas.
> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere uma frase "o gato pulou alto". Os tokens seriam: `[o, gato, pulou, alto]`.
>
> Em um modelo causal, ao calcular a representaÃ§Ã£o contextualizada para "pulou", apenas os tokens anteriores `[o, gato]` seriam considerados.
>
> Em um modelo bidirecional, ao calcular a representaÃ§Ã£o contextualizada para "pulou", todos os tokens `[o, gato, pulou, alto]` seriam considerados.
>
> Isto permite que o modelo capture melhor o significado da palavra "pulou", pois ele tem acesso ao contexto completo da frase. Em modelos causais, o modelo sÃ³ teria acesso ao contexto da esquerda para a direita, o que pode levar a representaÃ§Ãµes menos ricas.

A entrada do modelo consiste em *subword tokens*, geralmente gerados por algoritmos como *WordPiece* ou *SentencePiece Unigram LM* [^3]. Ã‰ importante notar que todo o processamento posterior ocorre no nÃ­vel de *subword tokens*, o que exige que, em algumas tarefas que requerem noÃ§Ã£o de palavras, seja feito um mapeamento de volta para as palavras [^3].

**Teorema 11.1:** *A utilizaÃ§Ã£o de subword tokens em modelos de linguagem mascarados permite que o modelo lide com palavras raras e out-of-vocabulary (OOV) de forma mais eficaz do que modelos que utilizam vocabulÃ¡rio baseado em palavras.*

*Prova:*
I. Modelos de linguagem baseados em palavras possuem um vocabulÃ¡rio fixo, o que significa que qualquer palavra nÃ£o presente no vocabulÃ¡rio Ã© considerada OOV.
II. Algoritmos de tokenizaÃ§Ã£o baseados em *subwords*, como *WordPiece* e *SentencePiece*, decompÃµem palavras em unidades menores, como *subwords*.
III. Isso diminui o tamanho do vocabulÃ¡rio, pois cada palavra Ã© composta por combinaÃ§Ãµes de *subwords*.
IV.  Palavras raras e OOV sÃ£o, geralmente, composiÃ§Ãµes de *subwords* que o modelo jÃ¡ conhece, o que permite representÃ¡-las mesmo que elas nÃ£o tenham sido vistas durante o treinamento.
V.  Assim, o modelo Ã© capaz de generalizar para palavras nunca vistas, ou seja, o problema de OOV Ã© mitigado, permitindo um processamento mais robusto.
VI. Portanto, modelos que utilizam *subword tokens* lidam com palavras raras e OOV de forma mais eficaz. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um vocabulÃ¡rio baseado em palavras com as seguintes palavras:
>
> `["gato", "cachorro", "corre", "pula", "rapidamente"]`.
>
> Se a frase "o gatinho corre rapidamente" aparecer, a palavra "gatinho" seria considerada OOV, pois nÃ£o estÃ¡ no vocabulÃ¡rio.
>
> Agora, usando um modelo com *subword tokens*, como o WordPiece, terÃ­amos um vocabulÃ¡rio com:
>
> `["o", "gat", "##inho", "corre", "rapida", "##mente"]`.
>
> Nesse caso, a frase "o gatinho corre rapidamente" seria tokenizada como:
>
> `["o", "gat", "##inho", "corre", "rapida", "##mente"]`
>
> Observe que "gatinho" foi quebrado em "gat" e "##inho", ambos presentes no vocabulÃ¡rio. O modelo consegue processar toda a frase sem OOV, aproveitando o contexto de tokens menores para representar palavras que nÃ£o viu no treinamento.
>
> ```mermaid
> graph LR
>     A[Frase: "o gatinho corre rapidamente"] --> B(TokenizaÃ§Ã£o por Palavras);
>     B --> C[VocabulÃ¡rio: gato, cachorro, corre, pula, rapidamente];
>     B --> D[Tokens: o, OOV, corre, rapidamente];
>     A --> E(TokenizaÃ§Ã£o por Subwords);
>      E --> F[VocabulÃ¡rio: o, gat, ##inho, corre, rapida, ##mente];
>     E --> G[Tokens: o, gat, ##inho, corre, rapida, ##mente];
>     C --> H[OOV: gatinho];
>     style H fill:#f9f,stroke:#333,stroke-width:2px
>     style D fill:#ccf,stroke:#333,stroke-width:2px
>     style G fill:#aaf,stroke:#333,stroke-width:2px
> ```

#### Exemplos de Modelos Bidirecionais

O modelo original BERT ( *Bidirectional Encoder Representations from Transformers* ), por exemplo, possui as seguintes caracterÃ­sticas [^3]:

*   Um vocabulÃ¡rio de *subwords* de 30,000 *tokens*, gerado utilizando o algoritmo *WordPiece* [^3].
*   Camadas ocultas de dimensionalidade $d = 768$ [^3].
*   12 camadas de blocos *transformer*, cada uma com 12 camadas de atenÃ§Ã£o *multi-head* [^3].
*   Aproximadamente 100 milhÃµes de parÃ¢metros [^3].

Outro exemplo notÃ¡vel Ã© o modelo XLM-RoBERTa, um modelo multilÃ­ngue treinado em 100 lÃ­nguas, que possui [^3]:

*   Um vocabulÃ¡rio de *subwords* de 250.000 *tokens*, gerado utilizando o algoritmo *SentencePiece Unigram LM* [^3].
*   24 camadas de blocos *transformer*, cada uma com 16 camadas de atenÃ§Ã£o *multi-head* [^3].
*   Camadas ocultas de tamanho 1024 [^4].
*   Janela de contexto de 512 *tokens* [^4].
*   Aproximadamente 550 milhÃµes de parÃ¢metros [^4].

Comparativamente, modelos de linguagem mascarados tendem a ser menores em termos de nÃºmero de parÃ¢metros em comparaÃ§Ã£o com modelos causais, como o Llama 3, que possui 405 bilhÃµes de parÃ¢metros [^4].

**CorolÃ¡rio 11.1:** *A menor quantidade de parÃ¢metros em modelos de linguagem mascarados em comparaÃ§Ã£o com modelos causais, como o Llama 3, pode ser atribuÃ­da, em parte, ao fato de que os modelos mascarados sÃ£o encoders e nÃ£o decoders. Decoders, como o Llama 3, tendem a ter uma capacidade maior por serem capazes de gerar texto de maneira autogressiva.*
*Prova:*
I. Modelos de linguagem mascarados (MLMs) sÃ£o arquiteturas do tipo "encoder-only", projetadas para gerar representaÃ§Ãµes contextuais de sequÃªncias de entrada, e nÃ£o para geraÃ§Ã£o de texto.
II. Modelos causais, por outro lado, sÃ£o decodificadores projetados para gerar texto de forma auto-regressiva, token por token.
III. Arquiteturas decodificadoras precisam modelar toda a distribuiÃ§Ã£o de probabilidade sobre o espaÃ§o de saÃ­da, o que requer uma capacidade maior da rede, e por consequÃªncia, mais parÃ¢metros.
IV.  Modelos encoders, como MLMs, nÃ£o precisam modelar esta distribuiÃ§Ã£o, e focam em aprender representaÃ§Ãµes contextuais, o que requer menos parÃ¢metros.
V. Portanto, a menor quantidade de parÃ¢metros em modelos de linguagem mascarados em comparaÃ§Ã£o com modelos causais como o Llama 3 pode ser atribuÃ­da ao fato de que modelos MLMs sÃ£o encoders e nÃ£o decoders. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos comparar um modelo encoder (BERT) com um modelo decoder (GPT):
>
> **BERT (Encoder):**
> - NÃºmero de camadas: 12
> - DimensÃ£o da camada oculta: 768
> - NÃºmero de parÃ¢metros: ~100 milhÃµes
> - Tarefa: Gerar representaÃ§Ãµes contextuais de texto
>
> **GPT (Decoder):**
> - NÃºmero de camadas: 12
> - DimensÃ£o da camada oculta: 768
> - NÃºmero de parÃ¢metros: ~117 milhÃµes (GPT-1)
> - Tarefa: Gerar texto autogressivamente
>
> Modelos de decoder, como o GPT, muitas vezes tÃªm estruturas mais complexas e necessitam de mais parÃ¢metros para modelar a probabilidade de sequÃªncias de texto, o que Ã© refletido em seus parÃ¢metros. Modelos como o Llama 3 tÃªm ainda mais camadas e parÃ¢metros para conseguirem gerar texto coerente.
>
> | Modelo    | Tipo      | Camadas | DimensÃ£o Oculta | ParÃ¢metros | Tarefa                                  |
> |-----------|-----------|---------|-----------------|------------|-----------------------------------------|
> | BERT      | Encoder   | 12      | 768             | ~100M      | RepresentaÃ§Ãµes contextuais             |
> | GPT-1     | Decoder   | 12      | 768             | ~117M      | GeraÃ§Ã£o de texto auto-regressiva        |
> | Llama 3   | Decoder   |  VÃ¡rias  | VÃ¡rias          | 405B     | GeraÃ§Ã£o de texto auto-regressiva        |

**ProposiÃ§Ã£o 11.1:** *O nÃºmero de camadas e a dimensÃ£o das camadas ocultas sÃ£o fatores que afetam significativamente a capacidade de um modelo de linguagem mascarado capturar nuances contextuais e complexidade linguÃ­stica.*
*Justificativa:*
I. A profundidade da rede (nÃºmero de camadas) permite que o modelo aprenda representaÃ§Ãµes hierÃ¡rquicas da linguagem, com camadas mais baixas capturando caracterÃ­sticas mais bÃ¡sicas e camadas mais altas capturando abstraÃ§Ãµes mais complexas.
II. A dimensÃ£o das camadas ocultas define a capacidade de cada nÃ³ da rede representar informaÃ§Ãµes. DimensÃµes maiores permitem que mais informaÃ§Ãµes sejam armazenadas e processadas por cada nÃ³.
III. Portanto, um aumento no nÃºmero de camadas e/ou na dimensÃ£o das camadas ocultas possibilita que o modelo capture nuances contextuais e complexidade linguÃ­stica de forma mais eficaz.

#### 11.1.1.1 RemoÃ§Ã£o do Mascaramento na Matriz de AtenÃ§Ã£o

Em continuidade ao conceito apresentado, a diferenÃ§a fundamental na arquitetura dos codificadores bidirecionais reside na forma como a atenÃ§Ã£o Ã© computada [^3]. Nos *transformers* causais, como visto anteriormente, uma mÃ¡scara Ã© aplicada na matriz de atenÃ§Ã£o $Q K^T$ para garantir que cada *token* apenas atenda aos *tokens* precedentes [^1, 2]. Essa mÃ¡scara Ã© essencial para a natureza autoregressiva dos modelos causais, onde a prediÃ§Ã£o de um *token* depende exclusivamente dos *tokens* anteriores. Em contrapartida, os codificadores bidirecionais removem essa mÃ¡scara, permitindo que a atenÃ§Ã£o se estenda sobre toda a sequÃªncia de entrada [^3].

A remoÃ§Ã£o da mÃ¡scara, conforme detalhado na seÃ§Ã£o anterior, habilita cada *token* a acessar informaÃ§Ãµes tanto dos *tokens* Ã  esquerda quanto Ã  direita, levando a representaÃ§Ãµes contextuais mais ricas e profundas.  A computaÃ§Ã£o da atenÃ§Ã£o, no entanto, mantÃ©m-se idÃªntica Ã  dos modelos causais, utilizando a mesma fÃ³rmula:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$
Onde,
$Q$, $K$, e $V$ representam as matrizes de *queries*, *keys* e *values* respectivamente, e  $d_k$ representa a dimensÃ£o das *keys*.

A alteraÃ§Ã£o principal Ã© a ausÃªncia da mÃ¡scara aplicada Ã  matriz $Q K^T$, e essa ausÃªncia Ã© o que possibilita o codificador bidirecional processar a informaÃ§Ã£o de forma nÃ£o causal, permitindo que a atenÃ§Ã£o de cada *token* se estenda por toda a sequÃªncia, resultando em um modelo que pode aprender relaÃ§Ãµes contextuais mais complexas.

**Lema 11.2:** *A ausÃªncia da mÃ¡scara na matriz de atenÃ§Ã£o em codificadores bidirecionais permite que cada token "veja" toda a sequÃªncia de entrada durante a computaÃ§Ã£o da atenÃ§Ã£o, ao contrÃ¡rio dos modelos causais onde a informaÃ§Ã£o flui apenas em uma direÃ§Ã£o.*
*Prova:*
I. Em modelos causais, a mÃ¡scara de atenÃ§Ã£o impede que um token $x_i$ atenda aos tokens $x_j$ com $j > i$.
II. Em codificadores bidirecionais, a remoÃ§Ã£o da mÃ¡scara permite que o token $x_i$ atenda a todos os tokens na sequÃªncia, incluindo aqueles com $j > i$.
III. Assim, a representaÃ§Ã£o contextualizada de $x_i$ em um codificador bidirecional incorpora informaÃ§Ãµes de todo o contexto, nÃ£o apenas dos tokens precedentes, como nos modelos causais.
IV. Portanto, a ausÃªncia da mÃ¡scara permite que cada token veja toda a sequÃªncia de entrada. â– 

#### 11.1.1.2 ImplementaÃ§Ã£o da Arquitetura Bidirecional
A implementaÃ§Ã£o da arquitetura bidirecional Ã© notavelmente simples [^2]. Como mencionado, a principal alteraÃ§Ã£o em relaÃ§Ã£o ao *transformer* causal reside na remoÃ§Ã£o da mÃ¡scara na matriz $Q K^T$ [^2]. Essa remoÃ§Ã£o elimina a restriÃ§Ã£o de causalidade e permite que cada *token* atenda a todos os outros *tokens* na sequÃªncia, tanto Ã  esquerda quanto Ã  direita [^2].

Com essa mudanÃ§a, a computaÃ§Ã£o da atenÃ§Ã£o para modelos bidirecionais torna-se idÃªntica Ã  EquaÃ§Ã£o 11.1, porÃ©m sem a aplicaÃ§Ã£o da mÃ¡scara [^3]:
$$ A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
Onde:
*   $Q$ representa a matriz de *queries*.
*   $K$ representa a matriz de *keys*.
*   $V$ representa a matriz de *values*.
*   $d_k$ representa a dimensÃ£o das *keys*.

AlÃ©m disso, a arquitetura do bloco *transformer*, que inclui a camada *feedforward*, a normalizaÃ§Ã£o de camada, e outros componentes, permanece inalterada [^3].  Essa estrutura permite que o codificador bidirecional utilize a mesma base arquitetural dos *transformers* causais, mas com a capacidade adicional de processar informaÃ§Ãµes em ambas as direÃ§Ãµes, proporcionando representaÃ§Ãµes contextuais mais ricas e abrangentes [^2].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Em termos de computaÃ§Ã£o, imagine uma sequÃªncia de entrada representada pelas matrizes Q, K e V. Em um transformer causal, uma mÃ¡scara seria aplicada na matriz $QK^T$ antes do softmax, para zerar as interaÃ§Ãµes entre um token e os tokens subsequentes. Por exemplo, um token na posiÃ§Ã£o i poderia apenas atender aos tokens nas posiÃ§Ãµes menores ou iguais a i.
> Em um transformer bidirecional, essa mÃ¡scara nÃ£o Ã© aplicada, ou seja, todos os tokens podem atender a todos os outros tokens na sequÃªncia, sem restriÃ§Ãµes. A computaÃ§Ã£o da matriz de atenÃ§Ã£o, e o restante da arquitetura, como a camada feedforward, permanece a mesma.
>
> ```mermaid
> graph LR
>     A[Input Sequence: Q, K, V] --> B{Causal Transformer};
>     B -->|Apply Mask| C[Masked QK^T];
>     C --> D[Softmax(Masked QK^T/sqrt(dk))V];
>     A --> E{Bidirectional Transformer};
>     E --> F[Softmax(QK^T/sqrt(dk))V];
>     style C fill:#ccf,stroke:#333,stroke-width:2px
>     style D fill:#aaf,stroke:#333,stroke-width:2px
>
> ```

#### 11.1.1.3 Input em Subword Tokens

Assim como nos modelos causais, a entrada do modelo bidirecional nÃ£o Ã© composta por palavras, mas por uma sequÃªncia de *subword tokens* [^3].  Esses *tokens* sÃ£o geralmente computados por algoritmos como *WordPiece* ou *SentencePiece Unigram LM* [^3]. O uso de *subword tokens* oferece diversas vantagens, incluindo a capacidade de lidar com palavras raras e *out-of-vocabulary* (OOV), alÃ©m de reduzir o tamanho do vocabulÃ¡rio.

Ã‰ importante ressaltar que todo o processamento subsequente, incluindo a computaÃ§Ã£o da atenÃ§Ã£o e as camadas subsequentes do modelo, ocorre no nÃ­vel dos *subword tokens*, nÃ£o em nÃ­vel de palavra [^3]. Isso implica que, para tarefas que exigem uma noÃ§Ã£o clara de palavras (como tarefas de *parsing* sintÃ¡tico), um mapeamento adicional de *subword tokens* de volta para palavras pode ser necessÃ¡rio.

> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere a frase "O cÃ£ozinho late muito".
>
> **TokenizaÃ§Ã£o com palavras:**
> `["O", "cÃ£ozinho", "late", "muito"]`
>
> **TokenizaÃ§Ã£o com *subwords* (exemplo com WordPiece):**
> `["O", "cÃ£o", "##zinho", "late", "muito"]`
>
> Observe que a palavra "cÃ£ozinho" foi dividida em "cÃ£o" e "##zinho".
>
> Em um codificador bidirecional transformer, a computaÃ§Ã£o da atenÃ§Ã£o e demais operaÃ§Ãµes seriam feitas sobre essa sequÃªncia de subwords, e nÃ£o sobre palavras completas. Se uma tarefa necessitar da representaÃ§Ã£o de "cÃ£ozinho" como uma unidade, o mapeamento de volta de subwords para a palavra seria necessÃ¡rio.

**CorolÃ¡rio 11.2:** *A utilizaÃ§Ã£o de subword tokens nÃ£o apenas mitiga o problema de OOV, mas tambÃ©m pode auxiliar o modelo a capturar relaÃ§Ãµes morfolÃ³gicas e semÃ¢nticas entre palavras que compartilham a mesma raiz ou afixos, jÃ¡ que essas partes menores da palavra podem ter representaÃ§Ãµes vetoriais semelhantes.*
*Justificativa:*
I. Subword tokens que compÃµem palavras morfologicamente relacionadas (ex: "cÃ£o", "cÃ£ezinho") terÃ£o representaÃ§Ãµes vetoriais similares no espaÃ§o de embeddings.
II. Essa similaridade permite que o modelo aprenda relaÃ§Ãµes entre as palavras, mesmo que elas nÃ£o tenham aparecido exatamente no mesmo contexto.
III. Consequentemente, modelos que usam subwords conseguem generalizar melhor a relaÃ§Ã£o entre palavras morfologicamente relacionadas, melhorando o entendimento da estrutura da linguagem.
IV. Portanto, a utilizaÃ§Ã£o de subword tokens nÃ£o apenas mitiga o problema de OOV, mas tambÃ©m pode auxiliar o modelo a capturar relaÃ§Ãµes morfolÃ³gicas e semÃ¢nticas.
> ğŸ’¡ **Exemplo NumÃ©rico:**
> Considere as palavras "correr", "correndo" e "correu". Usando WordPiece, elas poderiam ser tokenizadas como:
> - correr: ["correr"]
> - correndo: ["cor", "##rendo"]
> - correu: ["cor", "##reu"]
>
> Note que os subwords "cor" sÃ£o compartilhados entre as 3 palavras, o que permite que o modelo aprenda semelhanÃ§as entre as palavras, mesmo que elas tenham afixos diferentes.

#### 11.1.2 A FunÃ§Ã£o de AtenÃ§Ã£o: Uma Perspectiva MatemÃ¡tica

A funÃ§Ã£o de atenÃ§Ã£o, central para o funcionamento dos codificadores bidirecionais, pode ser analisada em detalhe sob uma perspectiva matemÃ¡tica. Como vimos anteriormente, essa funÃ§Ã£o Ã© a base do mecanismo de autoatenÃ§Ã£o, que permite aos modelos *transformer* capturar dependÃªncias complexas entre *tokens* de uma sequÃªncia [^2]. Em termos matemÃ¡ticos, a funÃ§Ã£o de atenÃ§Ã£o opera atravÃ©s de trÃªs matrizes de projeÃ§Ã£o: *Query* ($Q$), *Key* ($K$) e *Value* ($V$) [^3]. Cada matriz Ã© obtida por meio de uma transformaÃ§Ã£o linear da sequÃªncia de entrada $X$ atravÃ©s de matrizes de pesos especÃ­ficas, $W_Q$, $W_K$ e $W_V$ respectivamente:

$$
Q = X W_Q \\
K = X W_K \\
V = X W_V
$$
onde $X$ representa a matriz de *embeddings* de entrada, e $W_Q, W_K, W_V$ sÃ£o as matrizes de pesos treinÃ¡veis.

A matriz de atenÃ§Ã£o $A$ Ã© calculada por meio do *softmax* dos produtos escalares entre as *queries* e as *keys*, escalonados pela raiz quadrada da dimensÃ£o das *keys* ($d_k$) [^3]:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)
$$
Nesse contexto, $Q K^T$ resulta em uma matriz onde cada elemento $(i, j)$ representa a similaridade entre o *token* $i$ e o *token* $j$, antes da aplicaÃ§Ã£o do *softmax* [^3]. A divisÃ£o por $\sqrt{d_k}$ tem como objetivo estabilizar os gradientes durante o treinamento, evitando que os valores do produto escalar se tornem muito grandes e que o *softmax* se sature [^3]. A funÃ§Ã£o *softmax* garante que os valores na matriz $A$ sejam transformados em probabilidades que somam 1, representando o peso de atenÃ§Ã£o de cada *token* em relaÃ§Ã£o aos outros.

A saÃ­da final da camada de atenÃ§Ã£o, $H$,  Ã© dada pela combinaÃ§Ã£o linear dos *values* ponderados pela matriz de atenÃ§Ã£o [^3]:

$$ H = AV $$

Esta saÃ­da $H$ Ã© a representaÃ§Ã£o contextualizada de cada *token* na sequÃªncia de entrada, que Ã© usada nas camadas subsequentes do modelo.

**ProposiÃ§Ã£o 11.2:** *A matriz de atenÃ§Ã£o* $A$ *em um codificador bidirecional, calculada atravÃ©s da funÃ§Ã£o softmax sobre os produtos escalares das queries e keys, captura as relaÃ§Ãµes de importÃ¢ncia entre os tokens em uma sequÃªncia de entrada, permitindo que cada token "atenda" aos outros de forma seletiva durante o processo de contextualizaÃ§Ã£o.*
*Justificativa:*
I. A matriz de atenÃ§Ã£o $A$ Ã© calculada como $\text{softmax}(\frac{QK^T}{\sqrt{d_k}})$.
II. O produto matricial $QK^T$ calcula os produtos escalares entre os vetores de *queries* e *keys*, representando a similaridade entre os *tokens*.
III. A funÃ§Ã£o softmax transforma esses produtos escalares em probabilidades, resultando em um mapa de importÃ¢ncia de cada *token* em relaÃ§Ã£o aos outros.
IV. Durante o cÃ¡lculo da representaÃ§Ã£o contextualizada, os vetores de *values* sÃ£o ponderados por essa matriz de atenÃ§Ã£o, ou seja, cada *token* Ã© influenciado pelos demais de acordo com suas relaÃ§Ãµes de importÃ¢ncia, definidas pelo produto escaler e o *softmax*.
V. Assim, $A$ captura relaÃ§Ãµes de importÃ¢ncia entre os *tokens* permitindo que o modelo "atenda" seletivamente a diferentes partes da sequÃªncia.

**Exemplo NumÃ©rico:**
Considerando um caso simplificado com uma sequÃªncia de entrada $X$ contendo dois tokens,  e $d_k = 2$, vamos ilustrar a funÃ§Ã£o de atenÃ§Ã£o com nÃºmeros:

1. **Entrada:**
   $X = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ (representando dois tokens, cada um com 2 dimensÃµes).

2. **Pesos Lineares:**
   Vamos supor que as matrizes de pesos treinÃ¡veis sejam:
   $W_Q = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix},  W_K = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix},  W_V = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$

3. **CÃ¡lculo de Q, K, V:**
   $Q = XW_Q = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}, K = XW_K = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}, V = XW_V = \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix}$

4. **CÃ¡lculo da Matriz de AtenÃ§Ã£o (QK^T/sqrt(dk)):**
   $QK^T = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$
  $\frac{QK^T}{\sqrt{d_k}} =  \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} / \sqrt{2} = \begin{bmatrix} 0 & 0.707 \\ 0.707 & 0 \end{bmatrix}$

5.  **AplicaÃ§Ã£o do Softmax:**
   $A = \text{softmax} \left(\begin{bmatrix} 0 & 0.707 \\ 0.707 & 0 \end{bmatrix} \right) = \begin{bmatrix} 0.269 & 0.731 \\ 0.731 & 0.269 \end{bmatrix}$

6.  **CÃ¡lculo da SaÃ­da (AV):**
   $H = AV = \begin{bmatrix} 0.269 & 0.731 \\ 0.731 & 0.269 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} = \begin{bmatrix} 1 & -0.462 \\ 1 & 0.462 \end{bmatrix}$

Observe que a matriz de atenÃ§Ã£o $A$ indica quea primeira palavra da frase tem atenÃ§Ã£o de 1 sobre ela mesma, e a segunda palavra da frase tem atenÃ§Ã£o de 0.462 sobre a primeira palavra, e a primeira palavra da frase tem atenÃ§Ã£o de -0.462 sobre a segunda palavra. Podemos observar que a matriz de atenÃ§Ã£o $A$ Ã© uma matriz que indica a atenÃ§Ã£o de cada palavra sobre todas as outras palavras da frase, e a matriz $B$ Ã© uma matriz que indica a combinaÃ§Ã£o linear dos vetores de embeddings, que sÃ£o os vetores que representam as palavras da frase, no espaÃ§o vetorial.

A matriz de saÃ­da $O$ Ã© a matriz resultante da multiplicaÃ§Ã£o das matrizes de atenÃ§Ã£o $A$ e de combinaÃ§Ãµes lineares $B$. A matriz de saÃ­da $O$ Ã© a matriz que representa a atenÃ§Ã£o de cada palavra sobre todas as outras palavras da frase, apÃ³s a combinaÃ§Ã£o linear dos vetores de embeddings.

$O = A \cdot B = \begin{bmatrix} 1 & -0.462 \\ 1 & 0.462 \end{bmatrix}$

A matriz de saÃ­da $O$ indica que a primeira palavra da frase tem atenÃ§Ã£o de 1 sobre ela mesma, e a segunda palavra da frase tem atenÃ§Ã£o de 1 sobre a primeira palavra. A primeira palavra da frase tem atenÃ§Ã£o de -0.462 sobre a segunda palavra, e a segunda palavra da frase tem atenÃ§Ã£o de 0.462 sobre ela mesma.

Em geral, o mecanismo de atenÃ§Ã£o Ã© uma funÃ§Ã£o que recebe como entrada uma sequÃªncia de vetores de embeddings, e retorna uma sequÃªncia de vetores de saÃ­da, onde cada vetor de saÃ­da Ã© uma combinaÃ§Ã£o linear dos vetores de embeddings, onde os pesos da combinaÃ§Ã£o linear sÃ£o determinados pela atenÃ§Ã£o de cada palavra sobre todas as outras palavras da frase.

Em resumo, o mecanismo de atenÃ§Ã£o Ã© um mecanismo que permite que o modelo preste atenÃ§Ã£o nas palavras mais importantes da frase, e que combine os vetores de embeddings de acordo com a atenÃ§Ã£o de cada palavra sobre todas as outras palavras da frase.

O mecanismo de atenÃ§Ã£o Ã© usado em diversos modelos de linguagem natural, como o Transformer, o BERT, o GPT, e outros modelos. O mecanismo de atenÃ§Ã£o Ã© uma das principais razÃµes do sucesso desses modelos.

<!-- END -->
