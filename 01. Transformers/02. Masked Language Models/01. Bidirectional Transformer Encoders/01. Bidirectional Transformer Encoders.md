## Codificadores Bidirecionais Transformers e Modelos de Linguagem Mascarados

### IntroduÃ§Ã£o
Este capÃ­tulo aprofunda-se no conceito de **codificadores bidirecionais transformers** e sua aplicaÃ§Ã£o em **modelos de linguagem mascarados (Masked Language Models - MLM)**, complementando os conceitos de modelos de linguagem causais e transformers apresentados em capÃ­tulos anteriores [^1]. Anteriormente, exploramos o funcionamento dos transformers causais, que processam o texto sequencialmente, da esquerda para a direita [^1]. Agora, introduzimos uma abordagem alternativa que permite o processamento de sequÃªncias em ambas as direÃ§Ãµes, oferecendo uma compreensÃ£o contextual mais rica. Os modelos bidirecionais, como o BERT, utilizam a tÃ©cnica de *masked language modeling* para realizar o treinamento, diferentemente dos modelos causais, que predizem o prÃ³ximo token na sequÃªncia. O foco principal deste capÃ­tulo Ã© a arquitetura, o treinamento e a aplicaÃ§Ã£o dos codificadores bidirecionais em diversas tarefas de processamento de linguagem natural.

### Conceitos Fundamentais

#### 11.1 Bidirectional Transformer Encoders

O nÃºcleo deste capÃ­tulo sÃ£o os **codificadores bidirecionais transformer**, arquiteturas que, ao contrÃ¡rio dos modelos causais, nÃ£o sÃ£o projetadas para gerar sequÃªncias de texto, mas sim para produzir **representaÃ§Ãµes contextualizadas** dos *tokens* de entrada [^2]. Modelos como o BERT, RoBERTa e SpanBERT, sÃ£o exemplos de arquiteturas que empregam este tipo de codificador [^1]. Em contraste com os *transformers* causais, que processam informaÃ§Ãµes sequencialmente da esquerda para a direita, os codificadores bidirecionais utilizam **autoatenÃ§Ã£o** para mapear sequÃªncias de *embeddings* de entrada $(x_1, ..., x_n)$ para sequÃªncias de *embeddings* de saÃ­da $(h_1, ..., h_n)$, onde cada vetor de saÃ­da $h_i$ Ã© contextualizado pela informaÃ§Ã£o de toda a sequÃªncia de entrada [^2]. Esta abordagem possibilita que a representaÃ§Ã£o de cada *token* incorpore informaÃ§Ãµes de todo o contexto da sequÃªncia, o que se mostra especialmente Ãºtil em tarefas que requerem uma compreensÃ£o global da sentenÃ§a, como tarefas de classificaÃ§Ã£o ou decisÃ£o baseada no contexto do token [^2].

Enquanto os modelos causais sÃ£o frequentemente chamados de "decoder-only" (pois correspondem ao decodificador do modelo *encoder-decoder*), os modelos de linguagem mascarados sÃ£o referidos como "encoder-only", pois produzem uma codificaÃ§Ã£o para cada *token* de entrada, mas nÃ£o sÃ£o geralmente utilizados para geraÃ§Ã£o de texto por meio de decodificaÃ§Ã£o/amostragem [^2]. Ã‰ crucial notar que esses modelos nÃ£o sÃ£o concebidos para geraÃ§Ã£o de texto; em vez disso, eles sÃ£o utilizados para tarefas de anÃ¡lise e interpretaÃ§Ã£o [^2].

**Lema 11.1:** *A representaÃ§Ã£o contextualizada* $h_i$ *de um token* $x_i$ *em um codificador bidirecional transformer Ã© uma funÃ§Ã£o de todos os tokens na sequÃªncia de entrada* $(x_1, ..., x_n)$.

*Prova:*
I. Em um codificador bidirecional transformer, a representaÃ§Ã£o $h_i$ de um token $x_i$ Ã© obtida atravÃ©s da camada de autoatenÃ§Ã£o.
II. A camada de autoatenÃ§Ã£o calcula uma matriz de atenÃ§Ã£o $A$, que determina quanto cada token "atende" aos outros tokens.
III.  A matriz de atenÃ§Ã£o Ã© calculada utilizando as matrizes de *queries* $Q$, *keys* $K$ e *values* $V$, todas derivadas da sequÃªncia de entrada $(x_1, ..., x_n)$.
IV.  Especificamente, $A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)$, onde $Q$, $K$ e $V$ sÃ£o funÃ§Ãµes da sequÃªncia de entrada $(x_1, ..., x_n)$.
V.   A representaÃ§Ã£o $h_i$ Ã© entÃ£o calculada como uma combinaÃ§Ã£o linear de $V$, ponderada por $A$.
VI.  Portanto, como a matriz de atenÃ§Ã£o $A$ Ã© influenciada por todos os *tokens* na sequÃªncia de entrada, a representaÃ§Ã£o $h_i$ tambÃ©m Ã© influenciada por todos os *tokens* $(x_1, ..., x_n)$.
VII. ConcluÃ­mos que $h_i$ Ã© uma funÃ§Ã£o de todos os *tokens* na sequÃªncia de entrada.â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere uma sequÃªncia de entrada com 3 tokens:  $x = [x_1, x_2, x_3]$. Vamos supor que apÃ³s o *embedding* esses tokens sejam representados como vetores $x_1 = [1, 0, 0]$, $x_2 = [0, 1, 0]$, e $x_3 = [0, 0, 1]$. Para simplificar, vamos trabalhar com um Ãºnico *head* de atenÃ§Ã£o e com uma dimensÃ£o $d_k = 3$. As matrizes $Q$, $K$ e $V$ sÃ£o calculadas por meio de transformaÃ§Ãµes lineares de $x$. Vamos supor que:
>
> $Q = XW_Q$, $K = XW_K$, $V = XW_V$
>
> Onde $X$ Ã© a matriz de entrada $[x_1, x_2, x_3]^T$ e $W_Q$, $W_K$ e $W_V$ sÃ£o matrizes de pesos.
>
> Vamos supor tambÃ©m que:
>
> $W_Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$,  $W_K = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$, $W_V = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.
>
> Logo, $Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$, $K = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$, e $V = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.
>
> A matriz de atenÃ§Ã£o sem a normalizaÃ§Ã£o $\frac{QK^T}{\sqrt{d_k}}$ Ã©:
>
>  $QK^T = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$.
>
> Dividindo por $\sqrt{d_k} = \sqrt{3} \approx 1.732$, temos $\begin{bmatrix} 0 & 0 & 0.577 \\ 0.577 & 0 & 0 \\ 0 & 0.577 & 0 \end{bmatrix}$.
>
> Aplicando a funÃ§Ã£o *softmax* por linha, obtemos:
>
> $A = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix}$.
>
>  Finalmente, a saÃ­da Ã© calculada como $AV$:
>
> $AV = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix}$.
>
> Observe que a representaÃ§Ã£o contextualizada de $x_1$ ($h_1 = [0.20, 0.20, 0.60]$) Ã© influenciada nÃ£o sÃ³ por $x_1$ mas tambÃ©m por $x_2$ e $x_3$ atravÃ©s da matriz de atenÃ§Ã£o.

#### 11.1.1 A Arquitetura para Modelos Mascarados Bidirecionais

A arquitetura dos modelos de linguagem bidirecionais difere dos *transformers* causais em dois aspectos principais [^2]:

1.  **A funÃ§Ã£o de atenÃ§Ã£o nÃ£o Ã© causal:** Ao contrÃ¡rio dos modelos causais, a atenÃ§Ã£o de um *token* $i$ pode considerar *tokens* seguintes $i+1$ [^2].
2.  **O treinamento Ã© ligeiramente diferente:** A prediÃ§Ã£o Ã© feita sobre algo no meio do texto, em vez de prever o prÃ³ximo *token* no final da sequÃªncia [^2].

Em termos de arquitetura, a principal diferenÃ§a reside na remoÃ§Ã£o da mÃ¡scara de atenÃ§Ã£o. Em modelos causais, uma mÃ¡scara Ã© aplicada Ã  matriz $Q K^T$ para evitar que a atenÃ§Ã£o considere *tokens* futuros [^2]. Em codificadores bidirecionais, esta mÃ¡scara Ã© removida, permitindo que a atenÃ§Ã£o seja calculada sobre toda a sequÃªncia [^3]. A computaÃ§Ã£o da atenÃ§Ã£o Ã© definida como:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$

onde $Q$, $K$ e $V$ sÃ£o as matrizes de *queries*, *keys* e *values*, respectivamente, e $d_k$ Ã© a dimensÃ£o das *keys*. A diferenÃ§a crucial Ã© a ausÃªncia da mÃ¡scara que impedia o acesso a *tokens* futuros. A arquitetura do bloco transformer, incluindo as camadas *feedforward*, normalizaÃ§Ã£o de camadas, etc., permanece a mesma [^3].

**ObservaÃ§Ã£o 11.1:** A remoÃ§Ã£o da mÃ¡scara de atenÃ§Ã£o Ã© o que habilita o processamento bidirecional. A atenÃ§Ã£o, agora nÃ£o limitada pela causalidade, permite que cada token considere todo o contexto, tanto Ã  esquerda quanto Ã  direita, resultando em representaÃ§Ãµes contextuais mais ricas.
> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere uma frase "o gato pulou alto". Os tokens seriam: `[o, gato, pulou, alto]`.
>
> Em um modelo causal, ao calcular a representaÃ§Ã£o contextualizada para "pulou", apenas os tokens anteriores `[o, gato]` seriam considerados.
>
> Em um modelo bidirecional, ao calcular a representaÃ§Ã£o contextualizada para "pulou", todos os tokens `[o, gato, pulou, alto]` seriam considerados.
>
> Isto permite que o modelo capture melhor o significado da palavra "pulou", pois ele tem acesso ao contexto completo da frase. Em modelos causais, o modelo sÃ³ teria acesso ao contexto da esquerda para a direita, o que pode levar a representaÃ§Ãµes menos ricas.

A entrada do modelo consiste em *subword tokens*, geralmente gerados por algoritmos como *WordPiece* ou *SentencePiece Unigram LM* [^3]. Ã‰ importante notar que todo o processamento posterior ocorre no nÃ­vel de *subword tokens*, o que exige que, em algumas tarefas que requerem noÃ§Ã£o de palavras, seja feito um mapeamento de volta para as palavras [^3].

**Teorema 11.1:** *A utilizaÃ§Ã£o de subword tokens em modelos de linguagem mascarados permite que o modelo lide com palavras raras e out-of-vocabulary (OOV) de forma mais eficaz do que modelos que utilizam vocabulÃ¡rio baseado em palavras.*

*Prova:*
I. Modelos de linguagem baseados em palavras possuem um vocabulÃ¡rio fixo, o que significa que qualquer palavra nÃ£o presente no vocabulÃ¡rio Ã© considerada OOV.
II. Algoritmos de tokenizaÃ§Ã£o baseados em *subwords*, como *WordPiece* e *SentencePiece*, decompÃµem palavras em unidades menores, como *subwords*.
III. Isso diminui o tamanho do vocabulÃ¡rio, pois cada palavra Ã© composta por combinaÃ§Ãµes de *subwords*.
IV.  Palavras raras e OOV sÃ£o, geralmente, composiÃ§Ãµes de *subwords* que o modelo jÃ¡ conhece, o que permite representÃ¡-las mesmo que elas nÃ£o tenham sido vistas durante o treinamento.
V.  Assim, o modelo Ã© capaz de generalizar para palavras nunca vistas, ou seja, o problema de OOV Ã© mitigado, permitindo um processamento mais robusto.
VI. Portanto, modelos que utilizam *subword tokens* lidam com palavras raras e OOV de forma mais eficaz. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um vocabulÃ¡rio baseado em palavras com as seguintes palavras:
>
> `["gato", "cachorro", "corre", "pula", "rapidamente"]`.
>
> Se a frase "o gatinho corre rapidamente" aparecer, a palavra "gatinho" seria considerada OOV, pois nÃ£o estÃ¡ no vocabulÃ¡rio.
>
> Agora, usando um modelo com *subword tokens*, como o WordPiece, terÃ­amos um vocabulÃ¡rio com:
>
> `["o", "gat", "##inho", "corre", "rapida", "##mente"]`.
>
> Nesse caso, a frase "o gatinho corre rapidamente" seria tokenizada como:
>
> `["o", "gat", "##inho", "corre", "rapida", "##mente"]`
>
> Observe que "gatinho" foi quebrado em "gat" e "##inho", ambos presentes no vocabulÃ¡rio. O modelo consegue processar toda a frase sem OOV, aproveitando o contexto de tokens menores para representar palavras que nÃ£o viu no treinamento.

#### Exemplos de Modelos Bidirecionais

O modelo original BERT ( *Bidirectional Encoder Representations from Transformers* ), por exemplo, possui as seguintes caracterÃ­sticas [^3]:

*   Um vocabulÃ¡rio de *subwords* de 30,000 *tokens*, gerado utilizando o algoritmo *WordPiece* [^3].
*   Camadas ocultas de dimensionalidade $d = 768$ [^3].
*   12 camadas de blocos *transformer*, cada uma com 12 camadas de atenÃ§Ã£o *multi-head* [^3].
*   Aproximadamente 100 milhÃµes de parÃ¢metros [^3].

Outro exemplo notÃ¡vel Ã© o modelo XLM-RoBERTa, um modelo multilÃ­ngue treinado em 100 lÃ­nguas, que possui [^3]:

*   Um vocabulÃ¡rio de *subwords* de 250.000 *tokens*, gerado utilizando o algoritmo *SentencePiece Unigram LM* [^3].
*   24 camadas de blocos *transformer*, cada uma com 16 camadas de atenÃ§Ã£o *multi-head* [^3].
*   Camadas ocultas de tamanho 1024 [^4].
*   Janela de contexto de 512 *tokens* [^4].
*   Aproximadamente 550 milhÃµes de parÃ¢metros [^4].

Comparativamente, modelos de linguagem mascarados tendem a ser menores em termos de nÃºmero de parÃ¢metros em comparaÃ§Ã£o com modelos causais, como o Llama 3, que possui 405 bilhÃµes de parÃ¢metros [^4].

**CorolÃ¡rio 11.1:** *A menor quantidade de parÃ¢metros em modelos de linguagem mascarados em comparaÃ§Ã£o com modelos causais, como o Llama 3, pode ser atribuÃ­da, em parte, ao fato de que os modelos mascarados sÃ£o encoders e nÃ£o decoders. Decoders, como o Llama 3, tendem a ter uma capacidade maior por serem capazes de gerar texto de maneira autogressiva.*
*Prova:*
I. Modelos de linguagem mascarados (MLMs) sÃ£o arquiteturas do tipo "encoder-only", projetadas para gerar representaÃ§Ãµes contextuais de sequÃªncias de entrada, e nÃ£o para geraÃ§Ã£o de texto.
II. Modelos causais, por outro lado, sÃ£o decodificadores projetados para gerar texto de forma auto-regressiva, token por token.
III. Arquiteturas decodificadoras precisam modelar toda a distribuiÃ§Ã£o de probabilidade sobre o espaÃ§o de saÃ­da, o que requer uma capacidade maior da rede, e por consequÃªncia, mais parÃ¢metros.
IV.  Modelos encoders, como MLMs, nÃ£o precisam modelar esta distribuiÃ§Ã£o, e focam em aprender representaÃ§Ãµes contextuais, o que requer menos parÃ¢metros.
V. Portanto, a menor quantidade de parÃ¢metros em modelos de linguagem mascarados em comparaÃ§Ã£o com modelos causais como o Llama 3 pode ser atribuÃ­da ao fato de que modelos MLMs sÃ£o encoders e nÃ£o decoders. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos comparar um modelo encoder (BERT) com um modelo decoder (GPT):
>
> **BERT (Encoder):**
> - NÃºmero de camadas: 12
> - DimensÃ£o da camada oculta: 768
> - NÃºmero de parÃ¢metros: ~100 milhÃµes
> - Tarefa: Gerar representaÃ§Ãµes contextuais de texto
>
> **GPT (Decoder):**
> - NÃºmero de camadas: 12
> - DimensÃ£o da camada oculta: 768
> - NÃºmero de parÃ¢metros: ~117 milhÃµes (GPT-1)
> - Tarefa: Gerar texto autogressivamente
>
> Modelos de decoder, como o GPT, muitas vezes tÃªm estruturas mais complexas e necessitam de mais parÃ¢metros para modelar a probabilidade de sequÃªncias de texto, o que Ã© refletido em seus parÃ¢metros. Modelos como o Llama 3 tÃªm ainda mais camadas e parÃ¢metros para conseguirem gerar texto coerente.

### ConclusÃ£o

Os codificadores bidirecionais *transformer* representam um avanÃ§o significativo no processamento de linguagem natural, permitindo que modelos capturem dependÃªncias contextuais ricas em sequÃªncias de texto. AtravÃ©s da utilizaÃ§Ã£o de **autoatenÃ§Ã£o** sem a restriÃ§Ã£o de causalidade, esses modelos podem processar informaÃ§Ãµes de ambas as direÃ§Ãµes, gerando representaÃ§Ãµes contextualizadas dos *tokens* de entrada, que sÃ£o cruciais para tarefas de interpretaÃ§Ã£o e classificaÃ§Ã£o. A tÃ©cnica de *masked language modeling*, utilizada no treinamento desses modelos, possibilita que a rede aprenda a prever *tokens* faltantes a partir do contexto ao seu redor. Este capÃ­tulo serve como base para a compreensÃ£o de outros tÃ³picos, como *fine-tuning* e *contextual embeddings*, que exploraremos em seÃ§Ãµes futuras.

### ReferÃªncias
[^1]: CapÃ­tulo 9 e 10
[^2]: PÃ¡gina 2
[^3]: PÃ¡gina 3
[^4]: PÃ¡gina 4
<!-- END -->
