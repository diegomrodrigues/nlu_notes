## Codificadores Bidirecionais Transformers e Modelos de Linguagem Mascarados

### Introdu√ß√£o
Este cap√≠tulo aprofunda-se no conceito de **codificadores bidirecionais transformers** e sua aplica√ß√£o em **modelos de linguagem mascarados (Masked Language Models - MLM)**, complementando os conceitos de modelos de linguagem causais e transformers apresentados em cap√≠tulos anteriores [^1]. Anteriormente, exploramos o funcionamento dos transformers causais, que processam o texto sequencialmente, da esquerda para a direita [^1]. Agora, introduzimos uma abordagem alternativa que permite o processamento de sequ√™ncias em ambas as dire√ß√µes, oferecendo uma compreens√£o contextual mais rica. Os modelos bidirecionais, como o BERT, utilizam a t√©cnica de *masked language modeling* para realizar o treinamento, diferentemente dos modelos causais, que predizem o pr√≥ximo token na sequ√™ncia. O foco principal deste cap√≠tulo √© a arquitetura, o treinamento e a aplica√ß√£o dos codificadores bidirecionais em diversas tarefas de processamento de linguagem natural.

### Conceitos Fundamentais

#### 11.1 Bidirectional Transformer Encoders

O n√∫cleo deste cap√≠tulo s√£o os **codificadores bidirecionais transformer**, arquiteturas que, ao contr√°rio dos modelos causais, n√£o s√£o projetadas para gerar sequ√™ncias de texto, mas sim para produzir **representa√ß√µes contextualizadas** dos *tokens* de entrada [^2]. Modelos como o BERT, RoBERTa e SpanBERT, s√£o exemplos de arquiteturas que empregam este tipo de codificador [^1]. Em contraste com os *transformers* causais, que processam informa√ß√µes sequencialmente da esquerda para a direita, os codificadores bidirecionais utilizam **autoaten√ß√£o** para mapear sequ√™ncias de *embeddings* de entrada $(x_1, ..., x_n)$ para sequ√™ncias de *embeddings* de sa√≠da $(h_1, ..., h_n)$, onde cada vetor de sa√≠da $h_i$ √© contextualizado pela informa√ß√£o de toda a sequ√™ncia de entrada [^2]. Esta abordagem possibilita que a representa√ß√£o de cada *token* incorpore informa√ß√µes de todo o contexto da sequ√™ncia, o que se mostra especialmente √∫til em tarefas que requerem uma compreens√£o global da senten√ßa, como tarefas de classifica√ß√£o ou decis√£o baseada no contexto do token [^2].

Enquanto os modelos causais s√£o frequentemente chamados de "decoder-only" (pois correspondem ao decodificador do modelo *encoder-decoder*), os modelos de linguagem mascarados s√£o referidos como "encoder-only", pois produzem uma codifica√ß√£o para cada *token* de entrada, mas n√£o s√£o geralmente utilizados para gera√ß√£o de texto por meio de decodifica√ß√£o/amostragem [^2]. √â crucial notar que esses modelos n√£o s√£o concebidos para gera√ß√£o de texto; em vez disso, eles s√£o utilizados para tarefas de an√°lise e interpreta√ß√£o [^2].

**Lema 11.1:** *A representa√ß√£o contextualizada* $h_i$ *de um token* $x_i$ *em um codificador bidirecional transformer √© uma fun√ß√£o de todos os tokens na sequ√™ncia de entrada* $(x_1, ..., x_n)$.

*Prova:*
I. Em um codificador bidirecional transformer, a representa√ß√£o $h_i$ de um token $x_i$ √© obtida atrav√©s da camada de autoaten√ß√£o.
II. A camada de autoaten√ß√£o calcula uma matriz de aten√ß√£o $A$, que determina quanto cada token "atende" aos outros tokens.
III.  A matriz de aten√ß√£o √© calculada utilizando as matrizes de *queries* $Q$, *keys* $K$ e *values* $V$, todas derivadas da sequ√™ncia de entrada $(x_1, ..., x_n)$.
IV.  Especificamente, $A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)$, onde $Q$, $K$ e $V$ s√£o fun√ß√µes da sequ√™ncia de entrada $(x_1, ..., x_n)$.
V.   A representa√ß√£o $h_i$ √© ent√£o calculada como uma combina√ß√£o linear de $V$, ponderada por $A$.
VI.  Portanto, como a matriz de aten√ß√£o $A$ √© influenciada por todos os *tokens* na sequ√™ncia de entrada, a representa√ß√£o $h_i$ tamb√©m √© influenciada por todos os *tokens* $(x_1, ..., x_n)$.
VII. Conclu√≠mos que $h_i$ √© uma fun√ß√£o de todos os *tokens* na sequ√™ncia de entrada.‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere uma sequ√™ncia de entrada com 3 tokens:  $x = [x_1, x_2, x_3]$. Vamos supor que ap√≥s o *embedding* esses tokens sejam representados como vetores $x_1 = [1, 0, 0]$, $x_2 = [0, 1, 0]$, e $x_3 = [0, 0, 1]$. Para simplificar, vamos trabalhar com um √∫nico *head* de aten√ß√£o e com uma dimens√£o $d_k = 3$. As matrizes $Q$, $K$ e $V$ s√£o calculadas por meio de transforma√ß√µes lineares de $x$. Vamos supor que:
>
> $Q = XW_Q$, $K = XW_K$, $V = XW_V$
>
> Onde $X$ √© a matriz de entrada $[x_1, x_2, x_3]^T$ e $W_Q$, $W_K$ e $W_V$ s√£o matrizes de pesos.
>
> Vamos supor tamb√©m que:
>
> $W_Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$,  $W_K = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$, $W_V = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.
>
> Logo, $Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$, $K = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$, e $V = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$.
>
> A matriz de aten√ß√£o sem a normaliza√ß√£o $\frac{QK^T}{\sqrt{d_k}}$ √©:
>
>  $QK^T = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix}$.
>
> Dividindo por $\sqrt{d_k} = \sqrt{3} \approx 1.732$, temos $\begin{bmatrix} 0 & 0 & 0.577 \\ 0.577 & 0 & 0 \\ 0 & 0.577 & 0 \end{bmatrix}$.
>
> Aplicando a fun√ß√£o *softmax* por linha, obtemos:
>
> $A = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix}$.
>
>  Finalmente, a sa√≠da √© calculada como $AV$:
>
> $AV = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 0.20 & 0.20 & 0.60 \\ 0.60 & 0.20 & 0.20 \\ 0.20 & 0.60 & 0.20 \end{bmatrix}$.
>
> Observe que a representa√ß√£o contextualizada de $x_1$ ($h_1 = [0.20, 0.20, 0.60]$) √© influenciada n√£o s√≥ por $x_1$ mas tamb√©m por $x_2$ e $x_3$ atrav√©s da matriz de aten√ß√£o.
>
> ```mermaid
> graph LR
>     A[Input Tokens: x1, x2, x3] -->|Embeddings| B(Embeddings: x1=[1,0,0], x2=[0,1,0], x3=[0,0,1]);
>     B --> |Linear Transformation (Wq, Wk, Wv)| C[Q, K, V];
>     C --> | Q * K^T / sqrt(dk)| D[QK^T];
>     D --> |Softmax| E[Attention Matrix A];
>     E --> |A * V| F[Contextualized Output h1, h2, h3];
> ```

#### 11.1.1 A Arquitetura para Modelos Mascarados Bidirecionais

A arquitetura dos modelos de linguagem bidirecionais difere dos *transformers* causais em dois aspectos principais [^2]:

1.  **A fun√ß√£o de aten√ß√£o n√£o √© causal:** Ao contr√°rio dos modelos causais, a aten√ß√£o de um *token* $i$ pode considerar *tokens* seguintes $i+1$ [^2].
2.  **O treinamento √© ligeiramente diferente:** A predi√ß√£o √© feita sobre algo no meio do texto, em vez de prever o pr√≥ximo *token* no final da sequ√™ncia [^2].

Em termos de arquitetura, a principal diferen√ßa reside na remo√ß√£o da m√°scara de aten√ß√£o. Em modelos causais, uma m√°scara √© aplicada √† matriz $Q K^T$ para evitar que a aten√ß√£o considere *tokens* futuros [^2]. Em codificadores bidirecionais, esta m√°scara √© removida, permitindo que a aten√ß√£o seja calculada sobre toda a sequ√™ncia [^3]. A computa√ß√£o da aten√ß√£o √© definida como:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$

onde $Q$, $K$ e $V$ s√£o as matrizes de *queries*, *keys* e *values*, respectivamente, e $d_k$ √© a dimens√£o das *keys*. A diferen√ßa crucial √© a aus√™ncia da m√°scara que impedia o acesso a *tokens* futuros. A arquitetura do bloco transformer, incluindo as camadas *feedforward*, normaliza√ß√£o de camadas, etc., permanece a mesma [^3].

**Observa√ß√£o 11.1:** A remo√ß√£o da m√°scara de aten√ß√£o √© o que habilita o processamento bidirecional. A aten√ß√£o, agora n√£o limitada pela causalidade, permite que cada token considere todo o contexto, tanto √† esquerda quanto √† direita, resultando em representa√ß√µes contextuais mais ricas.
> üí° **Exemplo Num√©rico:**
>
> Considere uma frase "o gato pulou alto". Os tokens seriam: `[o, gato, pulou, alto]`.
>
> Em um modelo causal, ao calcular a representa√ß√£o contextualizada para "pulou", apenas os tokens anteriores `[o, gato]` seriam considerados.
>
> Em um modelo bidirecional, ao calcular a representa√ß√£o contextualizada para "pulou", todos os tokens `[o, gato, pulou, alto]` seriam considerados.
>
> Isto permite que o modelo capture melhor o significado da palavra "pulou", pois ele tem acesso ao contexto completo da frase. Em modelos causais, o modelo s√≥ teria acesso ao contexto da esquerda para a direita, o que pode levar a representa√ß√µes menos ricas.

A entrada do modelo consiste em *subword tokens*, geralmente gerados por algoritmos como *WordPiece* ou *SentencePiece Unigram LM* [^3]. √â importante notar que todo o processamento posterior ocorre no n√≠vel de *subword tokens*, o que exige que, em algumas tarefas que requerem no√ß√£o de palavras, seja feito um mapeamento de volta para as palavras [^3].

**Teorema 11.1:** *A utiliza√ß√£o de subword tokens em modelos de linguagem mascarados permite que o modelo lide com palavras raras e out-of-vocabulary (OOV) de forma mais eficaz do que modelos que utilizam vocabul√°rio baseado em palavras.*

*Prova:*
I. Modelos de linguagem baseados em palavras possuem um vocabul√°rio fixo, o que significa que qualquer palavra n√£o presente no vocabul√°rio √© considerada OOV.
II. Algoritmos de tokeniza√ß√£o baseados em *subwords*, como *WordPiece* e *SentencePiece*, decomp√µem palavras em unidades menores, como *subwords*.
III. Isso diminui o tamanho do vocabul√°rio, pois cada palavra √© composta por combina√ß√µes de *subwords*.
IV.  Palavras raras e OOV s√£o, geralmente, composi√ß√µes de *subwords* que o modelo j√° conhece, o que permite represent√°-las mesmo que elas n√£o tenham sido vistas durante o treinamento.
V.  Assim, o modelo √© capaz de generalizar para palavras nunca vistas, ou seja, o problema de OOV √© mitigado, permitindo um processamento mais robusto.
VI. Portanto, modelos que utilizam *subword tokens* lidam com palavras raras e OOV de forma mais eficaz. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um vocabul√°rio baseado em palavras com as seguintes palavras:
>
> `["gato", "cachorro", "corre", "pula", "rapidamente"]`.
>
> Se a frase "o gatinho corre rapidamente" aparecer, a palavra "gatinho" seria considerada OOV, pois n√£o est√° no vocabul√°rio.
>
> Agora, usando um modelo com *subword tokens*, como o WordPiece, ter√≠amos um vocabul√°rio com:
>
> `["o", "gat", "##inho", "corre", "rapida", "##mente"]`.
>
> Nesse caso, a frase "o gatinho corre rapidamente" seria tokenizada como:
>
> `["o", "gat", "##inho", "corre", "rapida", "##mente"]`
>
> Observe que "gatinho" foi quebrado em "gat" e "##inho", ambos presentes no vocabul√°rio. O modelo consegue processar toda a frase sem OOV, aproveitando o contexto de tokens menores para representar palavras que n√£o viu no treinamento.
>
> ```mermaid
> graph LR
>     A[Frase: "o gatinho corre rapidamente"] --> B(Tokeniza√ß√£o por Palavras);
>     B --> C[Vocabul√°rio: gato, cachorro, corre, pula, rapidamente];
>     B --> D[Tokens: o, OOV, corre, rapidamente];
>     A --> E(Tokeniza√ß√£o por Subwords);
>      E --> F[Vocabul√°rio: o, gat, ##inho, corre, rapida, ##mente];
>     E --> G[Tokens: o, gat, ##inho, corre, rapida, ##mente];
>     C --> H[OOV: gatinho];
>     style H fill:#f9f,stroke:#333,stroke-width:2px
>     style D fill:#ccf,stroke:#333,stroke-width:2px
>     style G fill:#aaf,stroke:#333,stroke-width:2px
> ```

#### Exemplos de Modelos Bidirecionais

O modelo original BERT ( *Bidirectional Encoder Representations from Transformers* ), por exemplo, possui as seguintes caracter√≠sticas [^3]:

*   Um vocabul√°rio de *subwords* de 30,000 *tokens*, gerado utilizando o algoritmo *WordPiece* [^3].
*   Camadas ocultas de dimensionalidade $d = 768$ [^3].
*   12 camadas de blocos *transformer*, cada uma com 12 camadas de aten√ß√£o *multi-head* [^3].
*   Aproximadamente 100 milh√µes de par√¢metros [^3].

Outro exemplo not√°vel √© o modelo XLM-RoBERTa, um modelo multil√≠ngue treinado em 100 l√≠nguas, que possui [^3]:

*   Um vocabul√°rio de *subwords* de 250.000 *tokens*, gerado utilizando o algoritmo *SentencePiece Unigram LM* [^3].
*   24 camadas de blocos *transformer*, cada uma com 16 camadas de aten√ß√£o *multi-head* [^3].
*   Camadas ocultas de tamanho 1024 [^4].
*   Janela de contexto de 512 *tokens* [^4].
*   Aproximadamente 550 milh√µes de par√¢metros [^4].

Comparativamente, modelos de linguagem mascarados tendem a ser menores em termos de n√∫mero de par√¢metros em compara√ß√£o com modelos causais, como o Llama 3, que possui 405 bilh√µes de par√¢metros [^4].

**Corol√°rio 11.1:** *A menor quantidade de par√¢metros em modelos de linguagem mascarados em compara√ß√£o com modelos causais, como o Llama 3, pode ser atribu√≠da, em parte, ao fato de que os modelos mascarados s√£o encoders e n√£o decoders. Decoders, como o Llama 3, tendem a ter uma capacidade maior por serem capazes de gerar texto de maneira autogressiva.*
*Prova:*
I. Modelos de linguagem mascarados (MLMs) s√£o arquiteturas do tipo "encoder-only", projetadas para gerar representa√ß√µes contextuais de sequ√™ncias de entrada, e n√£o para gera√ß√£o de texto.
II. Modelos causais, por outro lado, s√£o decodificadores projetados para gerar texto de forma auto-regressiva, token por token.
III. Arquiteturas decodificadoras precisam modelar toda a distribui√ß√£o de probabilidade sobre o espa√ßo de sa√≠da, o que requer uma capacidade maior da rede, e por consequ√™ncia, mais par√¢metros.
IV.  Modelos encoders, como MLMs, n√£o precisam modelar esta distribui√ß√£o, e focam em aprender representa√ß√µes contextuais, o que requer menos par√¢metros.
V. Portanto, a menor quantidade de par√¢metros em modelos de linguagem mascarados em compara√ß√£o com modelos causais como o Llama 3 pode ser atribu√≠da ao fato de que modelos MLMs s√£o encoders e n√£o decoders. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos comparar um modelo encoder (BERT) com um modelo decoder (GPT):
>
> **BERT (Encoder):**
> - N√∫mero de camadas: 12
> - Dimens√£o da camada oculta: 768
> - N√∫mero de par√¢metros: ~100 milh√µes
> - Tarefa: Gerar representa√ß√µes contextuais de texto
>
> **GPT (Decoder):**
> - N√∫mero de camadas: 12
> - Dimens√£o da camada oculta: 768
> - N√∫mero de par√¢metros: ~117 milh√µes (GPT-1)
> - Tarefa: Gerar texto autogressivamente
>
> Modelos de decoder, como o GPT, muitas vezes t√™m estruturas mais complexas e necessitam de mais par√¢metros para modelar a probabilidade de sequ√™ncias de texto, o que √© refletido em seus par√¢metros. Modelos como o Llama 3 t√™m ainda mais camadas e par√¢metros para conseguirem gerar texto coerente.
>
> | Modelo    | Tipo      | Camadas | Dimens√£o Oculta | Par√¢metros | Tarefa                                  |
> |-----------|-----------|---------|-----------------|------------|-----------------------------------------|
> | BERT      | Encoder   | 12      | 768             | ~100M      | Representa√ß√µes contextuais             |
> | GPT-1     | Decoder   | 12      | 768             | ~117M      | Gera√ß√£o de texto auto-regressiva        |
> | Llama 3   | Decoder   |  V√°rias  | V√°rias          | 405B     | Gera√ß√£o de texto auto-regressiva        |

**Proposi√ß√£o 11.1:** *O n√∫mero de camadas e a dimens√£o das camadas ocultas s√£o fatores que afetam significativamente a capacidade de um modelo de linguagem mascarado capturar nuances contextuais e complexidade lingu√≠stica.*
*Justificativa:*
I. A profundidade da rede (n√∫mero de camadas) permite que o modelo aprenda representa√ß√µes hier√°rquicas da linguagem, com camadas mais baixas capturando caracter√≠sticas mais b√°sicas e camadas mais altas capturando abstra√ß√µes mais complexas.
II. A dimens√£o das camadas ocultas define a capacidade de cada n√≥ da rede representar informa√ß√µes. Dimens√µes maiores permitem que mais informa√ß√µes sejam armazenadas e processadas por cada n√≥.
III. Portanto, um aumento no n√∫mero de camadas e/ou na dimens√£o das camadas ocultas possibilita que o modelo capture nuances contextuais e complexidade lingu√≠stica de forma mais eficaz.

#### 11.1.1.1 Remo√ß√£o do Mascaramento na Matriz de Aten√ß√£o

Em continuidade ao conceito apresentado, a diferen√ßa fundamental na arquitetura dos codificadores bidirecionais reside na forma como a aten√ß√£o √© computada [^3]. Nos *transformers* causais, como visto anteriormente, uma m√°scara √© aplicada na matriz de aten√ß√£o $Q K^T$ para garantir que cada *token* apenas atenda aos *tokens* precedentes [^1, 2]. Essa m√°scara √© essencial para a natureza autoregressiva dos modelos causais, onde a predi√ß√£o de um *token* depende exclusivamente dos *tokens* anteriores. Em contrapartida, os codificadores bidirecionais removem essa m√°scara, permitindo que a aten√ß√£o se estenda sobre toda a sequ√™ncia de entrada [^3].

A remo√ß√£o da m√°scara, conforme detalhado na se√ß√£o anterior, habilita cada *token* a acessar informa√ß√µes tanto dos *tokens* √† esquerda quanto √† direita, levando a representa√ß√µes contextuais mais ricas e profundas.  A computa√ß√£o da aten√ß√£o, no entanto, mant√©m-se id√™ntica √† dos modelos causais, utilizando a mesma f√≥rmula:

$$
A = \text{softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right) V
$$
Onde,
$Q$, $K$, e $V$ representam as matrizes de *queries*, *keys* e *values* respectivamente, e  $d_k$ representa a dimens√£o das *keys*.

A altera√ß√£o principal √© a aus√™ncia da m√°scara aplicada √† matriz $Q K^T$, e essa aus√™ncia √© o que possibilita o codificador bidirecional processar a informa√ß√£o de forma n√£o causal, permitindo que a aten√ß√£o de cada *token* se estenda por toda a sequ√™ncia, resultando em um modelo que pode aprender rela√ß√µes contextuais mais complexas.

**Lema 11.2:** *A aus√™ncia da m√°scara na matriz de aten√ß√£o em codificadores bidirecionais permite que cada token "veja" toda a sequ√™ncia de entrada durante a computa√ß√£o da aten√ß√£o, ao contr√°rio dos modelos causais onde a informa√ß√£o flui apenas em uma dire√ß√£o.*
*Prova:*
I. Em modelos causais, a m√°scara de aten√ß√£o impede que um token $x_i$ atenda aos tokens $x_j$ com $j > i$.
II. Em codificadores bidirecionais, a remo√ß√£o da m√°scara permite que o token $x_i$ atenda a todos os tokens na sequ√™ncia, incluindo aqueles com $j > i$.
III. Assim, a representa√ß√£o contextualizada de $x_i$ em um codificador bidirecional incorpora informa√ß√µes de todo o contexto, n√£o apenas dos tokens precedentes, como nos modelos causais.
IV. Portanto, a aus√™ncia da m√°scara permite que cada token veja toda a sequ√™ncia de entrada. ‚ñ†

#### 11.1.1.2 Implementa√ß√£o da Arquitetura Bidirecional
A implementa√ß√£o da arquitetura bidirecional √© notavelmente simples [^2]. Como mencionado, a principal altera√ß√£o em rela√ß√£o ao *transformer* causal reside na remo√ß√£o da m√°scara na matriz $Q K^T$ [^2]. Essa remo√ß√£o elimina a restri√ß√£o de causalidade e permite que cada *token* atenda a todos os outros *tokens* na sequ√™ncia, tanto √† esquerda quanto √† direita [^2].

Com essa mudan√ßa, a computa√ß√£o da aten√ß√£o para modelos bidirecionais torna-se id√™ntica √† Equa√ß√£o 11.1, por√©m sem a aplica√ß√£o da m√°scara [^3]:
$$ A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
Onde:
*   $Q$ representa a matriz de *queries*.
*   $K$ representa a matriz de *keys*.
*   $V$ representa a matriz de *values*.
*   $d_k$ representa a dimens√£o das *keys*.

Al√©m disso, a arquitetura do bloco *transformer*, que inclui a camada *feedforward*, a normaliza√ß√£o de camada, e outros componentes, permanece inalterada [^3].  Essa estrutura permite que o codificador bidirecional utilize a mesma base arquitetural dos *transformers* causais, mas com a capacidade adicional de processar informa√ß√µes em ambas as dire√ß√µes, proporcionando representa√ß√µes contextuais mais ricas e abrangentes [^2].

> üí° **Exemplo Num√©rico:**
>
> Em termos de computa√ß√£o, imagine uma sequ√™ncia de entrada representada pelas matrizes Q, K e V. Em um transformer causal, uma m√°scara seria aplicada na matriz $QK^T$ antes do softmax, para zerar as intera√ß√µes entre um token e os tokens subsequentes. Por exemplo, um token na posi√ß√£o i poderia apenas atender aos tokens nas posi√ß√µes menores ou iguais a i.
> Em um transformer bidirecional, essa m√°scara n√£o √© aplicada, ou seja, todos os tokens podem atender a todos os outros tokens na sequ√™ncia, sem restri√ß√µes. A computa√ß√£o da matriz de aten√ß√£o, e o restante da arquitetura, como a camada feedforward, permanece a mesma.
>
> ```mermaid
> graph LR
>     A[Input Sequence: Q, K, V] --> B{Causal Transformer};
>     B -->|Apply Mask| C[Masked QK^T];
>     C --> D[Softmax(Masked QK^T/sqrt(dk))V];
>     A --> E{Bidirectional Transformer};
>     E --> F[Softmax(QK^T/sqrt(dk))V];
>     style C fill:#ccf,stroke:#333,stroke-width:2px
>     style D fill:#aaf,stroke:#333,stroke-width:2px
>
> ```

#### 11.1.1.3 Input em Subword Tokens

Assim como nos modelos causais, a entrada do modelo bidirecional n√£o √© composta por palavras, mas por uma sequ√™ncia de *subword tokens* [^3].  Esses *tokens* s√£o geralmente computados por algoritmos como *WordPiece* ou *SentencePiece Unigram LM* [^3]. O uso de *subword tokens* oferece diversas vantagens, incluindo a capacidade de lidar com palavras raras e *out-of-vocabulary* (OOV), al√©m de reduzir o tamanho do vocabul√°rio.

√â importante ressaltar que todo o processamento subsequente, incluindo a computa√ß√£o da aten√ß√£o e as camadas subsequentes do modelo, ocorre no n√≠vel dos *subword tokens*, n√£o em n√≠vel de palavra [^3]. Isso implica que, para tarefas que exigem uma no√ß√£o clara de palavras (como tarefas de *parsing* sint√°tico), um mapeamento adicional de *subword tokens* de volta para palavras pode ser necess√°rio.

> üí° **Exemplo Num√©rico:**
> Considere a frase "O c√£ozinho late muito".
>
> **Tokeniza√ß√£o com palavras:**
> `["O", "c√£ozinho", "late", "muito"]`
>
> **Tokeniza√ß√£o com *subwords* (exemplo com WordPiece):**
> `["O", "c√£o", "##zinho", "late", "muito"]`
>
> Observe que a palavra "c√£ozinho" foi dividida em "c√£o" e "##zinho".
>
> Em um codificador bidirecional transformer, a computa√ß√£o da aten√ß√£o e demais opera√ß√µes seriam feitas sobre essa sequ√™ncia de subwords, e n√£o sobre palavras completas. Se uma tarefa necessitar da representa√ß√£o de "c√£ozinho" como uma unidade, o mapeamento de volta de subwords para a palavra seria necess√°rio.

**Corol√°rio 11.2:** *A utiliza√ß√£o de subword tokens n√£o apenas mitiga o problema de OOV, mas tamb√©m pode auxiliar o modelo a capturar rela√ß√µes morfol√≥gicas e sem√¢nticas entre palavras que compartilham a mesma raiz ou afixos, j√° que essas partes menores da palavra podem ter representa√ß√µes vetoriais semelhantes.*
*Justificativa:*
I. Subword tokens que comp√µem palavras morfologicamente relacionadas (ex: "c√£o", "c√£ezinho") ter√£o representa√ß√µes vetoriais similares no espa√ßo de embeddings.
II. Essa similaridade permite que o modelo aprenda rela√ß√µes entre as palavras, mesmo que elas n√£o tenham aparecido exatamente no mesmo contexto.
III. Consequentemente, modelos que usam subwords conseguem generalizar melhor a rela√ß√£o entre palavras morfologicamente relacionadas, melhorando o entendimento da estrutura da linguagem.
IV. Portanto, a utiliza√ß√£o de subword tokens n√£o apenas mitiga o problema de OOV, mas tamb√©m pode auxiliar o modelo a capturar rela√ß√µes morfol√≥gicas e sem√¢nticas.
> üí° **Exemplo Num√©rico:**
> Considere as palavras "correr", "correndo" e "correu". Usando WordPiece, elas poderiam ser tokenizadas como:
> - correr: ["correr"]
> - correndo: ["cor", "##rendo"]
> - correu: ["cor", "##reu"]
>
> Note que os subwords "cor" s√£o compartilhados entre as 3 palavras, o que permite que o modelo aprenda semelhan√ßas entre as palavras, mesmo que elas tenham afixos diferentes.

### Conclus√£o

Nesta se√ß√£o, exploramos a arquitetura de codificadores bidirecionais *transformer*, com √™nfase na remo√ß√£o da m√°scara de aten√ß√£o e no uso de *subword tokens* [^2,3]. A arquitetura bidirecional, sem a restri√ß√£o da causalidade na aten√ß√£o, permite que cada *token* capture um contexto mais rico, resultando em representa√ß√µes mais eficazes para diversas tarefas de processamento de linguagem natural. A implementa√ß√£o, baseada na remo√ß√£o da m√°scara na matriz de aten√ß√£o, √© simples e eficaz, permitindo que o modelo utilize a mesma base arquitetural dos *transformers* causais. Em continuidade, vamos detalhar como esses codificadores s√£o utilizados no treinamento dos modelos de linguagem mascarados.

### Refer√™ncias
[^1]: Cap√≠tulo 9 e 10
[^2]: P√°gina 2
[^3]: P√°gina 3
[^4]: P√°gina 4
<!-- END -->
