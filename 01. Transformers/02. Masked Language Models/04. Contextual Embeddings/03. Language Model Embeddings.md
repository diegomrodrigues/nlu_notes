## Contextual Embeddings: Deriva√ß√£o e Camadas do Modelo

### Introdu√ß√£o
No contexto do estudo avan√ßado de Large Language Models (LLMs) e Natural Language Understanding (NLU), os **contextual embeddings** representam um avan√ßo fundamental na forma como as palavras e tokens s√£o representados [^1]. Conforme explorado nos cap√≠tulos anteriores, esses embeddings, diferentemente das representa√ß√µes est√°ticas, s√£o din√¢micos e sens√≠veis ao contexto em que ocorrem, capturando as sutilezas sem√¢nticas e sint√°ticas que s√£o cruciais para uma compreens√£o mais profunda da linguagem [^1]. Este cap√≠tulo aprofunda a discuss√£o sobre como esses embeddings s√£o derivados, focando no papel das camadas do modelo *transformer* e nas diferentes abordagens para a gera√ß√£o dessas representa√ß√µes contextuais.

### Conceitos Fundamentais
Como j√° estabelecido, os modelos *bidirectional transformer encoders*, como o BERT, geram *contextual embeddings* que representam inst√¢ncias de palavras em um determinado contexto, em vez de um √∫nico *embedding* para cada palavra [^1]. Os *embeddings* contextuais s√£o criados a partir das sa√≠das de cada camada do *transformer*, que atuam como extratores de caracter√≠sticas hier√°rquicas [^1]. Cada camada processa a informa√ß√£o da camada anterior, adaptando os *embeddings* para refletir rela√ß√µes mais complexas e contextuais na sequ√™ncia de entrada [^1].

**Camadas do Transformer**: O modelo *transformer* √© composto por m√∫ltiplas camadas de autoaten√ß√£o e *feedforward networks*, que s√£o repetidas em sequ√™ncia [^1]. Cada camada transforma a representa√ß√£o vetorial de cada token, permitindo que o modelo aprenda representa√ß√µes mais complexas da rela√ß√£o entre as palavras no contexto. As primeiras camadas capturam informa√ß√µes mais b√°sicas, como aspectos sint√°ticos e padr√µes de palavras, enquanto as camadas superiores capturam informa√ß√µes mais abstratas e sem√¢nticas [^1].

**Deriva√ß√£o dos Contextual Embeddings**: A deriva√ß√£o dos *contextual embeddings* pode ser feita de duas formas principais:
1.  **√öltima Camada:** Utiliza-se o vetor de sa√≠da da √∫ltima camada do modelo para representar cada token da sequ√™ncia. Este vetor √© considerado uma representa√ß√£o contextualizada que incorpora informa√ß√µes de todas as camadas anteriores [^1]. Formalmente, para uma sequ√™ncia de entrada $x_1, ..., x_n$, o *embedding* contextual de um token $x_i$ √© dado por $h_i^L$, onde $h_i^L$ √© o vetor de sa√≠da do token $x_i$ na √∫ltima camada $L$ do *transformer* [^1].

2.  **M√©dia das √öltimas Camadas**: Uma abordagem alternativa envolve calcular a m√©dia dos vetores de sa√≠da das √∫ltimas $k$ camadas do modelo. Esta t√©cnica procura criar uma representa√ß√£o mais robusta e est√°vel, agregando informa√ß√µes de diversas camadas superiores, onde a informa√ß√£o sem√¢ntica √© mais proeminente. O *embedding* contextual para um token $x_i$ nesta abordagem √© calculado como:

$$ z_i = \frac{1}{k} \sum_{l=L-k+1}^{L} h_i^l $$
onde $h_i^l$ √© o vetor de sa√≠da do token $x_i$ na camada $l$.

   > üí° **Exemplo Num√©rico:** Suponha que um modelo *transformer* tenha 12 camadas e desejamos gerar um *embedding* contextual para a palavra "banco" na frase "O banco da pra√ßa estava cheio". Usando a abordagem da *√∫ltima camada*, o *embedding* contextual seria o vetor $h_{banco}^{12}$. Usando a abordagem da *m√©dia das √∫ltimas quatro camadas*, o *embedding* contextual seria o vetor:
   >  $$ z_{banco} = \frac{1}{4}(h_{banco}^9 + h_{banco}^{10} + h_{banco}^{11} + h_{banco}^{12})$$
   > onde $h_{banco}^9$, $h_{banco}^{10}$, $h_{banco}^{11}$ e $h_{banco}^{12}$ s√£o os vetores de sa√≠da da palavra "banco" nas camadas 9, 10, 11 e 12, respectivamente.
   >
   > Para ilustrar numericamente, suponha que os vetores de sa√≠da das √∫ltimas 4 camadas para a palavra "banco" sejam:
   >
   > $h_{banco}^9 = [0.1, 0.2, -0.3, 0.4]$
   >
   > $h_{banco}^{10} = [0.2, 0.1, -0.2, 0.5]$
   >
   > $h_{banco}^{11} = [0.15, 0.25, -0.35, 0.45]$
   >
   > $h_{banco}^{12} = [0.25, 0.15, -0.25, 0.55]$
   >
   > Ent√£o, o *embedding* contextual da m√©dia das √∫ltimas 4 camadas seria:
   >
   > $$ z_{banco} = \frac{1}{4}([0.1, 0.2, -0.3, 0.4] + [0.2, 0.1, -0.2, 0.5] + [0.15, 0.25, -0.35, 0.45] + [0.25, 0.15, -0.25, 0.55]) $$
   >
   > $$ z_{banco} = \frac{1}{4}[0.7, 0.7, -1.1, 1.9] = [0.175, 0.175, -0.275, 0.475] $$
   >
   > Este vetor $z_{banco}$ seria o *embedding* contextual para a palavra "banco" usando a m√©dia das √∫ltimas 4 camadas. Note que o *embedding* da √∫ltima camada ($h_{banco}^{12}$) √© diferente, com valores que podem variar, dependendo da camada. A m√©dia busca combinar estas representa√ß√µes.

**Camadas Intermedi√°rias**: Embora as camadas superiores sejam frequentemente usadas para gerar *contextual embeddings*, as camadas intermedi√°rias do *transformer* tamb√©m capturam informa√ß√µes √∫teis e podem ser relevantes para tarefas espec√≠ficas. Por exemplo, camadas inferiores podem capturar informa√ß√µes mais sint√°ticas e superficiais, enquanto as camadas mais altas capturam rela√ß√µes sem√¢nticas mais abstratas [^1].

**Proposi√ß√£o 1.** *Uma outra abordagem para a gera√ß√£o de embeddings contextuais √© a concatena√ß√£o dos vetores das √∫ltimas k camadas, em vez da m√©dia. Isso resulta em um vetor de maior dimensionalidade, capaz de potencialmente capturar mais detalhes sobre o contexto*. Formalmente, o *embedding* contextual para um token $x_i$ nesta abordagem √© calculado como:
$$c_i = \text{concat}(h_i^{L-k+1}, h_i^{L-k+2}, ..., h_i^L)$$
onde  $h_i^l$  √© o vetor de sa√≠da do token  $x_i$  na camada  $l$, e "concat" representa a opera√ß√£o de concatena√ß√£o de vetores.
>Apesar de gerar representa√ß√µes mais ricas em detalhes, essa abordagem aumenta a dimensionalidade dos embeddings, o que pode ser computacionalmente mais custoso em tarefas subsequentes, requerendo potencialmente mais mem√≥ria e tempo de processamento.

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo anterior, se concatenarmos as √∫ltimas 2 camadas (camadas 11 e 12), teremos:
   >
   > $h_{banco}^{11} = [0.15, 0.25, -0.35, 0.45]$
   >
   > $h_{banco}^{12} = [0.25, 0.15, -0.25, 0.55]$
   >
   > A concatena√ß√£o seria:
   >
   > $c_{banco} = \text{concat}([0.15, 0.25, -0.35, 0.45], [0.25, 0.15, -0.25, 0.55]) = [0.15, 0.25, -0.35, 0.45, 0.25, 0.15, -0.25, 0.55]$
   >
   > O vetor resultante $c_{banco}$ tem dimens√£o 8, o dobro da dimens√£o dos vetores de cada camada individual, demonstrando o aumento da dimensionalidade que a concatena√ß√£o provoca.

### Desenvolvimento
**An√°lise das Camadas do Transformer**: Cada camada do *transformer* desempenha um papel √∫nico na gera√ß√£o de *embeddings* contextuais. As camadas iniciais se concentram em reconhecer padr√µes locais, como *n-grams* e informa√ß√µes sint√°ticas b√°sicas, enquanto as camadas mais altas integram informa√ß√µes de toda a sequ√™ncia de entrada para capturar rela√ß√µes sem√¢nticas complexas.

**Escolha da Camada para a Deriva√ß√£o**: A escolha de qual camada ou combina√ß√£o de camadas usar para a deriva√ß√£o dos *embeddings* contextuais depende da tarefa espec√≠fica. Para tarefas que requerem uma compreens√£o mais sem√¢ntica e contextual da linguagem, as camadas superiores do *transformer* s√£o geralmente mais adequadas. Por outro lado, para tarefas que dependem de informa√ß√µes sint√°ticas ou de padr√µes locais, as camadas inferiores podem ser mais relevantes [^1]. A decis√£o de usar a *√∫ltima camada* ou a *m√©dia das √∫ltimas camadas* √© geralmente emp√≠rica, baseada no desempenho em dados de valida√ß√£o.

**Impacto da M√©dia das √öltimas Camadas**: A utiliza√ß√£o da m√©dia das √∫ltimas camadas pode levar a uma representa√ß√£o mais robusta por alguns motivos:
   1.  **Redu√ß√£o de Ru√≠do:** A m√©dia reduz o ru√≠do e a variabilidade presente em uma √∫nica camada. As informa√ß√µes capturadas em uma √∫nica camada podem ser sens√≠veis a padr√µes espec√≠ficos ou ru√≠dos na entrada, e a agrega√ß√£o de m√∫ltiplas camadas pode suavizar essa sensibilidade [^1].
   2.  **Integra√ß√£o de Informa√ß√£o Hier√°rquica**: Ao incluir vetores de diferentes camadas, a representa√ß√£o resultante integra informa√ß√µes capturadas em diferentes n√≠veis de abstra√ß√£o. As camadas superiores geralmente codificam informa√ß√£o mais sem√¢ntica, enquanto as camadas mais pr√≥ximas √† entrada modelam aspectos mais sint√°ticos e de superf√≠cie da linguagem. A combina√ß√£o dessas camadas proporciona uma representa√ß√£o mais equilibrada.
   3.  **Estabilidade:** A m√©dia das √∫ltimas camadas geralmente gera *embeddings* mais est√°veis e menos propensos a flutua√ß√µes, uma vez que agrega informa√ß√µes de diferentes est√°gios do processamento do modelo.
   4. **Desempenho Emp√≠rico**: A m√©dia das √∫ltimas camadas tem sido observada para melhorar o desempenho em v√°rias tarefas de PLN, o que sugere que ela captura informa√ß√µes relevantes para diversas aplica√ß√µes [^1].

**Adapta√ß√£o a Diferentes Tarefas**: Diferentes tarefas de PLN podem se beneficiar de diferentes abordagens para a gera√ß√£o de *embeddings* contextuais. Por exemplo, tarefas de classifica√ß√£o de texto podem se beneficiar de *embeddings* obtidos atrav√©s da m√©dia das √∫ltimas camadas, enquanto tarefas de *sequence labeling* podem se beneficiar da informa√ß√£o da √∫ltima camada, pois esta √© mais espec√≠fica ao token e menos "misturada" pela m√©dia das v√°rias camadas.

**Lema 3.** *A opera√ß√£o de m√©dia das √∫ltimas k camadas de um transformer gera um embedding contextual que preserva a informa√ß√£o do contexto, mas reduz ru√≠dos espec√≠ficos de uma √∫nica camada*.

*Prova:*
I. Seja $h_i^l$ o vetor de sa√≠da do token $x_i$ na camada $l$ do *transformer*. Cada $h_i^l$ √© uma representa√ß√£o de $x_i$ em diferentes n√≠veis de abstra√ß√£o, com cada camada $l$ capturando diferentes tipos de informa√ß√µes contextuais [^1].
II. A m√©dia das √∫ltimas $k$ camadas para o token $x_i$ √© dada por:
    $$ z_i = \frac{1}{k} \sum_{l=L-k+1}^{L} h_i^l $$
III. A opera√ß√£o de m√©dia √© uma opera√ß√£o linear que combina informa√ß√µes de diferentes camadas do *transformer* sem introduzir n√£o linearidades.
IV. Dado que cada camada $h_i^l$ cont√©m informa√ß√£o contextualizada sobre $x_i$, a combina√ß√£o linear da m√©dia preserva a informa√ß√£o contextual geral.
V. A m√©dia reduz ru√≠dos ou informa√ß√µes espec√≠ficas que podem estar presentes em uma √∫nica camada $h_i^l$. Isso acontece pois os ru√≠dos em cada camada tendem a ser aleat√≥rios e, portanto, ser√£o cancelados parcialmente pela m√©dia.
VI. Portanto, a m√©dia das √∫ltimas $k$ camadas gera um embedding contextual que preserva a informa√ß√£o do contexto, mas reduz ru√≠dos espec√≠ficos de uma √∫nica camada. $\blacksquare$

**Teorema 2.** *A combina√ß√£o linear da m√©dia dos embeddings das √∫ltimas camadas do transformer gera um embedding que captura informa√ß√µes em diferentes n√≠veis de abstra√ß√£o, representando de maneira mais robusta o contexto do token*.

*Prova:*
I. As primeiras camadas do *transformer* tendem a capturar informa√ß√µes de baixo n√≠vel, como padr√µes sint√°ticos e caracter√≠sticas locais dos tokens [^1].
II. As camadas superiores do *transformer* tendem a capturar informa√ß√µes de alto n√≠vel, como rela√ß√µes sem√¢nticas complexas e informa√ß√µes de contexto de longo alcance [^1].
III. A opera√ß√£o de m√©dia das √∫ltimas $k$ camadas √© dada por:
  $$ z_i = \frac{1}{k} \sum_{l=L-k+1}^{L} h_i^l $$
IV. Ao combinar informa√ß√µes de diferentes n√≠veis de abstra√ß√£o em $z_i$, o *embedding* resultante ter√° uma representa√ß√£o mais completa e robusta do contexto, uma vez que incluir√° informa√ß√µes de n√≠veis variados de abstra√ß√£o.
V.  A opera√ß√£o de m√©dia preserva a informa√ß√£o contextual de cada camada, conforme provado no Lema 3.
VI. Portanto, a combina√ß√£o linear da m√©dia das √∫ltimas camadas do *transformer* gera um *embedding* que captura informa√ß√µes em diferentes n√≠veis de abstra√ß√£o, representando de maneira mais robusta o contexto do token. $\blacksquare$

**Teorema 2.1** *A concatena√ß√£o das √∫ltimas k camadas do transformer gera um embedding contextual de alta dimensionalidade que ret√©m mais detalhes contextuais, mas potencialmente introduz complexidade computacional*.
*Prova:*
I.  Seja $h_i^l$ o vetor de sa√≠da do token $x_i$ na camada $l$ do *transformer*.
II.  A concatena√ß√£o das √∫ltimas $k$ camadas √© dada por: $c_i = \text{concat}(h_i^{L-k+1}, h_i^{L-k+2}, ..., h_i^L)$.
III.  Cada $h_i^l$ cont√©m informa√ß√µes contextuais distintas. A concatena√ß√£o preserva essas informa√ß√µes individualmente, resultando em uma representa√ß√£o mais detalhada em compara√ß√£o com a m√©dia.
IV. Se a dimensionalidade de cada vetor $h_i^l$ √© $d$, ent√£o a dimensionalidade de $c_i$ ser√° $k \cdot d$.
V.  Embora a concatena√ß√£o retenha mais informa√ß√µes, ela tamb√©m aumenta a complexidade computacional, tanto em termos de armazenamento quanto de processamento.
VI.  Portanto, a concatena√ß√£o das √∫ltimas $k$ camadas do *transformer* gera um *embedding* contextual de alta dimensionalidade que ret√©m mais detalhes contextuais, mas potencialmente introduz complexidade computacional. $\blacksquare$

### Conclus√£o
A gera√ß√£o de **contextual embeddings** √© um processo que envolve o aproveitamento das camadas do modelo *transformer*, cada uma desempenhando um papel distinto na captura de diferentes n√≠veis de informa√ß√µes contextuais [^1]. A escolha entre usar a *√∫ltima camada* ou a *m√©dia das √∫ltimas camadas*, ou outras combina√ß√µes, como a concatena√ß√£o, depende das especificidades da tarefa e das caracter√≠sticas dos dados. A m√©dia das √∫ltimas camadas surge como uma estrat√©gia eficaz para a cria√ß√£o de representa√ß√µes robustas, est√°veis e capazes de integrar informa√ß√µes hier√°rquicas de diferentes n√≠veis de abstra√ß√£o. A concatena√ß√£o de camadas, por outro lado, oferece uma representa√ß√£o mais detalhada mas a custo de maior dimensionalidade. A compreens√£o detalhada dessas abordagens √© essencial para aprimorar o desempenho de modelos de linguagem em diversas tarefas de PLN e NLU. [^1]

### Refer√™ncias
[^1]: Cap√≠tulo 11 do livro-texto fornecido.
<!-- END -->
