## Similaridade Sem√¢ntica e Anisotropia em Contextual Embeddings

### Introdu√ß√£o
Este cap√≠tulo explora em detalhes a aplica√ß√£o de **contextual embeddings** na medi√ß√£o da similaridade sem√¢ntica entre palavras e frases, com foco particular no fen√¥meno da **anisotropia** e nas t√©cnicas de normaliza√ß√£o para mitig√°-lo. Como vimos anteriormente [^1], os *embeddings* contextuais s√£o representa√ß√µes vetoriais que capturam o significado de uma palavra ou frase dentro de um contexto espec√≠fico, superando as limita√ß√µes dos *embeddings* est√°ticos. A similaridade sem√¢ntica, por sua vez, √© crucial para diversas tarefas de Processamento de Linguagem Natural (PLN), como recupera√ß√£o de informa√ß√£o, tradu√ß√£o autom√°tica, an√°lise de sentimentos e *word sense disambiguation* (WSD). Este cap√≠tulo tamb√©m aprofundar√° a compreens√£o de como a similaridade do cosseno √© utilizada para quantificar essa proximidade sem√¢ntica, destacando os desafios impostos pela anisotropia e as solu√ß√µes que envolvem a normaliza√ß√£o dos *embeddings* contextuais.

### Conceitos Fundamentais
Conforme discutido nos cap√≠tulos anteriores, os **contextual embeddings** s√£o representa√ß√µes vetoriais de palavras ou frases que s√£o din√¢micas e sens√≠veis ao contexto [^1]. Em modelos como o BERT, essas representa√ß√µes s√£o geradas pelas camadas do *transformer*, onde cada vetor representa a rela√ß√£o de um token com os outros tokens na sequ√™ncia de entrada [^1].

**Similaridade do Cosseno**: Uma das m√©tricas mais comuns para medir a similaridade entre dois vetores √© o **cosseno** do √¢ngulo entre eles [^1]. O cosseno √© definido como:
$$ \text{cosine}(a,b) = \frac{a \cdot b}{||a|| \cdot ||b||} $$
onde $a$ e $b$ s√£o os vetores, $\cdot$ representa o produto escalar, e $||\cdot||$ representa a norma do vetor. O cosseno varia de -1 a 1, onde 1 indica m√°xima similaridade, 0 indica ortogonalidade e -1 indica m√°xima dissimilaridade.

**Anisotropia em Embeddings Contextuais**: Apesar de sua utilidade, os *embeddings* contextuais podem apresentar **anisotropia**, ou seja, a tend√™ncia de todos os vetores apontarem para a mesma dire√ß√£o no espa√ßo vetorial [^1]. Essa propriedade faz com que vetores de palavras semanticamente distintas tenham alta similaridade do cosseno, o que dificulta a avalia√ß√£o precisa da similaridade sem√¢ntica. O problema da anisotropia ocorre porque os modelos *transformer* tendem a gerar *embeddings* com magnitudes muito altas em certas dimens√µes, dominando as outras dimens√µes e fazendo com que os vetores se aglomerem no mesmo espa√ßo.

**Normaliza√ß√£o Z-score**: Uma t√©cnica comum para mitigar a anisotropia √© a normaliza√ß√£o **z-score** (ou padroniza√ß√£o), que transforma os vetores para ter m√©dia zero e desvio padr√£o um em cada dimens√£o [^1]. A normaliza√ß√£o z-score √© dada por:
$$ z = \frac{x-\mu}{\sigma} $$
onde $x$ √© o vetor original, $\mu$ √© a m√©dia dos vetores em um corpus, e $\sigma$ √© o desvio padr√£o dos vetores no mesmo corpus [^1]. A normaliza√ß√£o z-score aumenta a dispers√£o dos vetores no espa√ßo vetorial, separando os vetores de palavras semanticamente distintas.

**Lema 1.** *A normaliza√ß√£o z-score preserva a dire√ß√£o do vetor, alterando apenas sua magnitude.*

*Prova:*
I. Seja $v$ um vetor original, $\mu$ a m√©dia dos vetores do corpus e $\sigma$ o desvio padr√£o dos vetores do corpus.
II. A normaliza√ß√£o z-score transforma $v$ em $z = \frac{v - \mu}{\sigma}$.
III. O vetor $v-\mu$ representa uma transla√ß√£o do vetor $v$ para a origem, assim preservando a dire√ß√£o de $v$ original.
IV. A divis√£o do vetor $v-\mu$ por um escalar $\sigma$ altera a magnitude do vetor, mas n√£o sua dire√ß√£o.
V. Portanto, a normaliza√ß√£o z-score altera a magnitude de um vetor, mas n√£o sua dire√ß√£o. $\blacksquare$

### Desenvolvimento
**Similaridade Sem√¢ntica com Cosseno**: O c√°lculo da similaridade sem√¢ntica entre duas palavras atrav√©s do cosseno entre seus *embeddings* contextuais envolve as seguintes etapas:
1. **Gera√ß√£o de Embeddings**: Obt√™m-se os *embeddings* contextuais para as palavras de interesse usando um modelo de linguagem pr√©-treinado [^1].
2. **Normaliza√ß√£o**: Os *embeddings* s√£o normalizados usando a t√©cnica de z-score para mitigar a anisotropia [^1].
3. **C√°lculo da Similaridade**: Calcula-se o cosseno entre os *embeddings* normalizados para quantificar a similaridade sem√¢ntica.

> üí° **Exemplo Num√©rico**: Considere as palavras "carro" e "autom√≥vel" em duas frases diferentes. Ap√≥s processar as frases com um modelo BERT, suponha que os vetores resultantes sejam:
>
> $v_{carro} = [1.2, -0.5, 0.8, 0.3, -0.1]$
> $v_{autom√≥vel} = [1.1, -0.4, 0.9, 0.4, 0.0]$
>
> Primeiro, calculamos a m√©dia ($\mu$) e o desvio padr√£o ($\sigma$) de todos os vetores no corpus (vamos simplificar e considerar $\mu = [0, 0, 0, 0, 0]$ e $\sigma = [1, 1, 1, 1, 1]$ para este exemplo). Em seguida, aplicamos a normaliza√ß√£o z-score:
>
> $z_{carro} = \frac{v_{carro} - \mu}{\sigma} = [1.2, -0.5, 0.8, 0.3, -0.1]$
> $z_{autom√≥vel} = \frac{v_{autom√≥vel} - \mu}{\sigma} = [1.1, -0.4, 0.9, 0.4, 0.0]$
>
> Agora, calculamos o produto escalar:
>
> $z_{carro} \cdot z_{autom√≥vel} = (1.2 \times 1.1) + (-0.5 \times -0.4) + (0.8 \times 0.9) + (0.3 \times 0.4) + (-0.1 \times 0.0) = 1.32 + 0.2 + 0.72 + 0.12 + 0 = 2.36$
>
> Calculamos as normas:
>
> $||z_{carro}|| = \sqrt{1.2^2 + (-0.5)^2 + 0.8^2 + 0.3^2 + (-0.1)^2} = \sqrt{1.44 + 0.25 + 0.64 + 0.09 + 0.01} = \sqrt{2.43} \approx 1.56$
> $||z_{autom√≥vel}|| = \sqrt{1.1^2 + (-0.4)^2 + 0.9^2 + 0.4^2 + 0.0^2} = \sqrt{1.21 + 0.16 + 0.81 + 0.16 + 0} = \sqrt{2.34} \approx 1.53$
>
> Finalmente, o cosseno da similaridade:
>
> $\text{cosine}(z_{carro}, z_{autom√≥vel}) = \frac{2.36}{1.56 \times 1.53} \approx \frac{2.36}{2.3868} \approx 0.988$
>
> Este valor indica uma alta similaridade sem√¢ntica entre "carro" e "autom√≥vel", o que √© esperado. Se os vetores n√£o fossem normalizados, a similaridade ainda seria alta, mas a compara√ß√£o seria menos precisa devido √† anisotropia.

**Anisotropia e o Problema do Cosseno**: A anisotropia dos *embeddings* contextuais pode levar a resultados enganosos na medi√ß√£o da similaridade sem√¢ntica [^1]. A tend√™ncia de todos os vetores apontarem para a mesma dire√ß√£o faz com que o cosseno seja muito alto mesmo para vetores de palavras semanticamente distintas. Isso ocorre porque a maioria dos vetores tem alta magnitude em algumas dimens√µes espec√≠ficas, que dominam o c√°lculo do produto escalar. A normaliza√ß√£o z-score busca corrigir este problema, uniformizando a escala e a dire√ß√£o dos vetores.

**Normaliza√ß√£o Z-score**: A normaliza√ß√£o z-score centraliza os vetores (subtraindo a m√©dia) e escala os vetores (dividindo pelo desvio padr√£o), o que permite que a similaridade do cosseno reflita mais precisamente a similaridade sem√¢ntica [^1]. A normaliza√ß√£o z-score preserva a dire√ß√£o do vetor, alterando apenas sua escala, como provado no Lema 1.
>
> üí° **Exemplo Num√©rico**: Suponha que os vetores antes da normaliza√ß√£o z-score s√£o:
>
> $v_1 = [100, 200, 150]$
> $v_2 = [90, 210, 140]$
> $v_3 = [10, 20, 15]$
>
> Vamos considerar os vetores $v_1$ e $v_2$ como vetores de palavras sem√¢nticamente similares, mas que os valores das dimens√µes s√£o muito grandes, e o vetor $v_3$ como o vetor de uma palavra semanticamente diferente, com valores de dimens√µes muito menores.
>
> Calculamos o cosseno entre $v_1$ e $v_2$ e $v_1$ e $v_3$ antes da normaliza√ß√£o:
>
> $\text{cosine}(v_1, v_2) = \frac{100*90 + 200*210 + 150*140}{\sqrt{100^2+200^2+150^2}*\sqrt{90^2+210^2+140^2}} = \frac{9000 + 42000 + 21000}{\sqrt{10000+40000+22500}*\sqrt{8100+44100+19600}} = \frac{72000}{\sqrt{72500}*\sqrt{71800}} \approx \frac{72000}{269.258*268.007} \approx 0.998$
>
> $\text{cosine}(v_1, v_3) = \frac{100*10 + 200*20 + 150*15}{\sqrt{100^2+200^2+150^2}*\sqrt{10^2+20^2+15^2}} = \frac{1000+4000+2250}{\sqrt{72500}*\sqrt{100+400+225}} = \frac{7250}{\sqrt{72500}*\sqrt{725}} \approx \frac{7250}{269.258*26.925} \approx 0.997$
>
> Note que o cosseno entre $v_1$ e $v_2$, apesar de mais parecido semanticamente, √© muito pr√≥ximo do cosseno entre $v_1$ e $v_3$.
>
> Agora vamos normalizar os vetores. Suponha que, analisando todo o corpus, temos que a m√©dia dos vetores √© $\mu=[50, 100, 70]$ e o desvio padr√£o √© $\sigma=[40, 50, 30]$
>
> $z_1 = \frac{v_1 - \mu}{\sigma} = \frac{[100, 200, 150]-[50,100,70]}{[40, 50, 30]} = [\frac{50}{40}, \frac{100}{50}, \frac{80}{30}] = [1.25, 2, 2.67]$
> $z_2 = \frac{v_2 - \mu}{\sigma} = \frac{[90, 210, 140]-[50,100,70]}{[40, 50, 30]} = [\frac{40}{40}, \frac{110}{50}, \frac{70}{30}] = [1, 2.2, 2.33]$
> $z_3 = \frac{v_3 - \mu}{\sigma} = \frac{[10, 20, 15]-[50,100,70]}{[40, 50, 30]} = [\frac{-40}{40}, \frac{-80}{50}, \frac{-55}{30}] = [-1, -1.6, -1.83]$
>
> Calculamos o cosseno entre $z_1$ e $z_2$ e $z_1$ e $z_3$ depois da normaliza√ß√£o:
>
> $\text{cosine}(z_1, z_2) = \frac{1.25*1 + 2*2.2 + 2.67*2.33}{\sqrt{1.25^2+2^2+2.67^2}*\sqrt{1^2+2.2^2+2.33^2}} = \frac{1.25 + 4.4 + 6.22}{\sqrt{1.56 + 4 + 7.13}*\sqrt{1 + 4.84 + 5.43}} = \frac{11.87}{\sqrt{12.69}*\sqrt{11.27}} \approx \frac{11.87}{3.56*3.35} \approx 0.993$
>
> $\text{cosine}(z_1, z_3) = \frac{1.25*-1 + 2*-1.6 + 2.67*-1.83}{\sqrt{1.25^2+2^2+2.67^2}*\sqrt{(-1)^2+(-1.6)^2+(-1.83)^2}} = \frac{-1.25-3.2-4.88}{\sqrt{1.56+4+7.13}*\sqrt{1+2.56+3.35}} = \frac{-9.33}{\sqrt{12.69}*\sqrt{6.91}} \approx \frac{-9.33}{3.56*2.63} \approx -0.997$
>
> Note que ap√≥s a normaliza√ß√£o a similaridade entre $z_1$ e $z_2$ continua alta, mas agora a similaridade entre $z_1$ e $z_3$ √© negativa, mostrando que a normaliza√ß√£o fez com que o cosseno refletisse melhor as diferen√ßas de significado sem√¢ntico.
>

> üí° **Exemplo Num√©rico**: Vamos considerar um exemplo pr√°tico usando Python e a biblioteca `numpy`. Suponha que temos tr√™s vetores de embeddings (representando as palavras "rei", "rainha" e "abacaxi") e queremos calcular a similaridade do cosseno entre eles antes e depois da normaliza√ß√£o z-score.
>
> ```python
> import numpy as np
>
> # Vetores de embeddings (exemplo)
> v_rei = np.array([10, 20, 15])
> v_rainha = np.array([9, 21, 14])
> v_abacaxi = np.array([1, 2, 1])
>
> # Calculando a similaridade do cosseno antes da normaliza√ß√£o
> def cosine_similarity(a, b):
>     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
>
> print(f"Cosseno(rei, rainha) antes da normaliza√ß√£o: {cosine_similarity(v_rei, v_rainha):.3f}")
> print(f"Cosseno(rei, abacaxi) antes da normaliza√ß√£o: {cosine_similarity(v_rei, v_abacaxi):.3f}")
>
> # Simula√ß√£o de m√©dia e desvio padr√£o do corpus
> media = np.array([5, 10, 7])
> desvio_padrao = np.array([4, 5, 3])
>
> # Normaliza√ß√£o z-score
> def z_score_normalization(v, media, desvio_padrao):
>     return (v - media) / desvio_padrao
>
> z_rei = z_score_normalization(v_rei, media, desvio_padrao)
> z_rainha = z_score_normalization(v_rainha, media, desvio_padrao)
> z_abacaxi = z_score_normalization(v_abacaxi, media, desvio_padrao)
>
> print(f"Cosseno(rei, rainha) ap√≥s a normaliza√ß√£o: {cosine_similarity(z_rei, z_rainha):.3f}")
> print(f"Cosseno(rei, abacaxi) ap√≥s a normaliza√ß√£o: {cosine_similarity(z_rei, z_abacaxi):.3f}")
> ```
>
> O c√≥digo acima produzir√° algo como:
> ```
> Cosseno(rei, rainha) antes da normaliza√ß√£o: 0.998
> Cosseno(rei, abacaxi) antes da normaliza√ß√£o: 0.996
> Cosseno(rei, rainha) ap√≥s a normaliza√ß√£o: 0.993
> Cosseno(rei, abacaxi) ap√≥s a normaliza√ß√£o: -0.973
> ```
> Observamos que antes da normaliza√ß√£o, os cossenos s√£o muito pr√≥ximos. Ap√≥s a normaliza√ß√£o, a similaridade entre "rei" e "rainha" ainda √© alta, mas a similaridade entre "rei" e "abacaxi" se torna negativa, refletindo melhor a diferen√ßa sem√¢ntica.

**Limita√ß√µes do Cosseno Mesmo Ap√≥s a Padroniza√ß√£o**: Apesar de eficaz, o cosseno pode subestimar a similaridade de palavras muito frequentes mesmo ap√≥s a padroniza√ß√£o [^1]. Isso ocorre porque as palavras frequentes tendem a ter *embeddings* mais dispersos, o que pode reduzir a similaridade do cosseno, mesmo quando semanticamente relacionadas.

**Teorema 3.** *A normaliza√ß√£o z-score atenua o problema da anisotropia em embeddings contextuais, mas n√£o o elimina completamente.*

*Prova:*
I. A anisotropia em *embeddings* contextuais √© causada por uma distribui√ß√£o n√£o uniforme de magnitudes nas dimens√µes dos vetores, fazendo com que todos os vetores tendam a apontar para a mesma dire√ß√£o [^1].
II. A normaliza√ß√£o z-score centraliza os vetores (subtraindo a m√©dia) e uniformiza a escala dos vetores (dividindo pelo desvio padr√£o), como provado no Lema 1.
III. A centraliza√ß√£o faz com que a m√©dia dos vetores se posicione na origem do espa√ßo vetorial.
IV. A uniformiza√ß√£o da escala reduz a varia√ß√£o das magnitudes nas dimens√µes dos vetores.
V. Ao fazer isso, a normaliza√ß√£o z-score aumenta a dispers√£o dos vetores no espa√ßo, reduzindo a influ√™ncia das dimens√µes com magnitudes elevadas.
VI. Embora a normaliza√ß√£o z-score atenue o problema da anisotropia, ela n√£o o elimina completamente pois n√£o consegue corrigir a similaridade de palavras muito frequentes. Portanto, outras t√©cnicas de normaliza√ß√£o podem ser necess√°rias para remover completamente os efeitos da anisotropia. $\blacksquare$

**Proposi√ß√£o 2.** *A similaridade sem√¢ntica pode ser medida atrav√©s de outras fun√ß√µes de dist√¢ncia, em vez do cosseno. M√©tricas como a dist√¢ncia euclidiana ou a similaridade de Jaccard podem complementar ou substituir a similaridade do cosseno, dependendo da tarefa espec√≠fica.*

> A dist√¢ncia euclidiana √© dada por $||a-b||$, que quantifica a dist√¢ncia "em linha reta" entre dois vetores. A similaridade de Jaccard, por sua vez, √© √∫til para medir a similaridade entre conjuntos e pode ser aplicada aos *embeddings* contextuais transformados em conjuntos de caracter√≠sticas.
>
> No entanto, a similaridade do cosseno, por ser uma m√©trica que compara o √¢ngulo entre vetores, √© mais apropriada para avaliar a similaridade sem√¢ntica pois captura informa√ß√µes de dire√ß√£o e n√£o de magnitude, e √© menos sens√≠vel a mudan√ßas na escala dos vetores.

**Corol√°rio 2.1** *A escolha da m√©trica de similaridade deve considerar as caracter√≠sticas espec√≠ficas dos embeddings e a tarefa em quest√£o. Em algumas situa√ß√µes, m√©tricas que enfatizam a magnitude, como a dist√¢ncia euclidiana, podem ser mais adequadas, enquanto em outras, m√©tricas que enfatizam a dire√ß√£o, como o cosseno, s√£o prefer√≠veis. A combina√ß√£o de diferentes m√©tricas pode tamb√©m oferecer uma avalia√ß√£o mais robusta da similaridade sem√¢ntica.*

**Teorema 4.** *A normaliza√ß√£o z-score transforma os vetores de forma que a m√©dia dos vetores normalizados seja sempre o vetor nulo, e que o desvio padr√£o das dimens√µes de cada vetor normalizado seja unit√°rio.*

*Prova:*
I. Seja $x_i$ a $i$-√©sima componente de um vetor $x$,  $\mu_i$ a m√©dia da $i$-√©sima dimens√£o dos vetores do corpus e $\sigma_i$ o desvio padr√£o da $i$-√©sima dimens√£o dos vetores do corpus.
II. Ap√≥s a normaliza√ß√£o z-score, a $i$-√©sima componente do vetor transformado $z$ √© dada por:
$z_i = \frac{x_i - \mu_i}{\sigma_i}$
III. A m√©dia do vetor transformado $z$ √© dada por:
$E[z_i] = E[\frac{x_i - \mu_i}{\sigma_i}] = \frac{1}{\sigma_i}(E[x_i] - \mu_i) = \frac{1}{\sigma_i}(\mu_i - \mu_i) = 0$
IV. Portanto, a m√©dia de todas as dimens√µes dos vetores normalizados √© zero, o que significa que a m√©dia dos vetores normalizados √© o vetor nulo.
V. O desvio padr√£o da $i$-√©sima componente do vetor normalizado $z$ √© dado por:
$DP[z_i] = DP[\frac{x_i - \mu_i}{\sigma_i}] = \frac{1}{\sigma_i}DP[x_i - \mu_i] = \frac{1}{\sigma_i}DP[x_i] = \frac{\sigma_i}{\sigma_i} = 1$
VI. Portanto, o desvio padr√£o das dimens√µes de cada vetor normalizado √© unit√°rio. $\blacksquare$

### Conclus√£o
A medi√ß√£o da similaridade sem√¢ntica com **contextual embeddings** e a fun√ß√£o cosseno √© uma t√©cnica poderosa em PLN. No entanto, o problema da **anisotropia** imp√µe desafios que requerem t√©cnicas de normaliza√ß√£o, como a z-score, para atenuar a tend√™ncia de vetores apontarem para a mesma dire√ß√£o e prejudicarem a avalia√ß√£o da similaridade sem√¢ntica [^1]. Embora a normaliza√ß√£o z-score seja eficaz, ela n√£o elimina completamente os problemas, especialmente para palavras muito frequentes, o que destaca a import√¢ncia de combinar o cosseno com outras medidas de similaridade e a escolha da normaliza√ß√£o mais adequada, dependendo da tarefa em m√£os. O estudo cont√≠nuo sobre representa√ß√µes contextuais e m√©tricas de similaridade √© essencial para aprimorar a precis√£o e a efic√°cia de modelos de linguagem em diversas aplica√ß√µes de PLN. [^1]

### Refer√™ncias
[^1]: Cap√≠tulo 11 do livro-texto fornecido.
<!-- END -->
