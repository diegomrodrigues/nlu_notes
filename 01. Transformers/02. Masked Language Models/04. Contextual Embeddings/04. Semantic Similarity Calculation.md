## Similaridade SemÃ¢ntica e Anisotropia em Contextual Embeddings

### IntroduÃ§Ã£o
Este capÃ­tulo explora em detalhes a aplicaÃ§Ã£o de **contextual embeddings** na mediÃ§Ã£o da similaridade semÃ¢ntica entre palavras e frases, com foco particular no fenÃ´meno da **anisotropia** e nas tÃ©cnicas de normalizaÃ§Ã£o para mitigÃ¡-lo. Como vimos anteriormente [^1], os *embeddings* contextuais sÃ£o representaÃ§Ãµes vetoriais que capturam o significado de uma palavra ou frase dentro de um contexto especÃ­fico, superando as limitaÃ§Ãµes dos *embeddings* estÃ¡ticos. A similaridade semÃ¢ntica, por sua vez, Ã© crucial para diversas tarefas de Processamento de Linguagem Natural (PLN), como recuperaÃ§Ã£o de informaÃ§Ã£o, traduÃ§Ã£o automÃ¡tica, anÃ¡lise de sentimentos e *word sense disambiguation* (WSD). Este capÃ­tulo tambÃ©m aprofundarÃ¡ a compreensÃ£o de como a similaridade do cosseno Ã© utilizada para quantificar essa proximidade semÃ¢ntica, destacando os desafios impostos pela anisotropia e as soluÃ§Ãµes que envolvem a normalizaÃ§Ã£o dos *embeddings* contextuais.

### Conceitos Fundamentais
Conforme discutido nos capÃ­tulos anteriores, os **contextual embeddings** sÃ£o representaÃ§Ãµes vetoriais de palavras ou frases que sÃ£o dinÃ¢micas e sensÃ­veis ao contexto [^1]. Em modelos como o BERT, essas representaÃ§Ãµes sÃ£o geradas pelas camadas do *transformer*, onde cada vetor representa a relaÃ§Ã£o de um token com os outros tokens na sequÃªncia de entrada [^1].

**Similaridade do Cosseno**: Uma das mÃ©tricas mais comuns para medir a similaridade entre dois vetores Ã© o **cosseno** do Ã¢ngulo entre eles [^1]. O cosseno Ã© definido como:
$$ \text{cosine}(a,b) = \frac{a \cdot b}{||a|| \cdot ||b||} $$
onde $a$ e $b$ sÃ£o os vetores, $\cdot$ representa o produto escalar, e $||\cdot||$ representa a norma do vetor. O cosseno varia de -1 a 1, onde 1 indica mÃ¡xima similaridade, 0 indica ortogonalidade e -1 indica mÃ¡xima dissimilaridade.

**Anisotropia em Embeddings Contextuais**: Apesar de sua utilidade, os *embeddings* contextuais podem apresentar **anisotropia**, ou seja, a tendÃªncia de todos os vetores apontarem para a mesma direÃ§Ã£o no espaÃ§o vetorial [^1]. Essa propriedade faz com que vetores de palavras semanticamente distintas tenham alta similaridade do cosseno, o que dificulta a avaliaÃ§Ã£o precisa da similaridade semÃ¢ntica. O problema da anisotropia ocorre porque os modelos *transformer* tendem a gerar *embeddings* com magnitudes muito altas em certas dimensÃµes, dominando as outras dimensÃµes e fazendo com que os vetores se aglomerem no mesmo espaÃ§o.

**NormalizaÃ§Ã£o Z-score**: Uma tÃ©cnica comum para mitigar a anisotropia Ã© a normalizaÃ§Ã£o **z-score** (ou padronizaÃ§Ã£o), que transforma os vetores para ter mÃ©dia zero e desvio padrÃ£o um em cada dimensÃ£o [^1]. A normalizaÃ§Ã£o z-score Ã© dada por:
$$ z = \frac{x-\mu}{\sigma} $$
onde $x$ Ã© o vetor original, $\mu$ Ã© a mÃ©dia dos vetores em um corpus, e $\sigma$ Ã© o desvio padrÃ£o dos vetores no mesmo corpus [^1]. A normalizaÃ§Ã£o z-score aumenta a dispersÃ£o dos vetores no espaÃ§o vetorial, separando os vetores de palavras semanticamente distintas.

**Lema 1.** *A normalizaÃ§Ã£o z-score preserva a direÃ§Ã£o do vetor, alterando apenas sua magnitude.*

*Prova:*
I. Seja $v$ um vetor original, $\mu$ a mÃ©dia dos vetores do corpus e $\sigma$ o desvio padrÃ£o dos vetores do corpus.
II. A normalizaÃ§Ã£o z-score transforma $v$ em $z = \frac{v - \mu}{\sigma}$.
III. O vetor $v-\mu$ representa uma translaÃ§Ã£o do vetor $v$ para a origem, assim preservando a direÃ§Ã£o de $v$ original.
IV. A divisÃ£o do vetor $v-\mu$ por um escalar $\sigma$ altera a magnitude do vetor, mas nÃ£o sua direÃ§Ã£o.
V. Portanto, a normalizaÃ§Ã£o z-score altera a magnitude de um vetor, mas nÃ£o sua direÃ§Ã£o. $\blacksquare$

### Desenvolvimento
**Similaridade SemÃ¢ntica com Cosseno**: O cÃ¡lculo da similaridade semÃ¢ntica entre duas palavras atravÃ©s do cosseno entre seus *embeddings* contextuais envolve as seguintes etapas:
1. **GeraÃ§Ã£o de Embeddings**: ObtÃªm-se os *embeddings* contextuais para as palavras de interesse usando um modelo de linguagem prÃ©-treinado [^1].
2. **NormalizaÃ§Ã£o**: Os *embeddings* sÃ£o normalizados usando a tÃ©cnica de z-score para mitigar a anisotropia [^1].
3. **CÃ¡lculo da Similaridade**: Calcula-se o cosseno entre os *embeddings* normalizados para quantificar a similaridade semÃ¢ntica.

> ğŸ’¡ **Exemplo NumÃ©rico**: Considere as palavras "carro" e "automÃ³vel" em duas frases diferentes. ApÃ³s processar as frases com um modelo BERT, suponha que os vetores resultantes sejam:
>
> $v_{carro} = [1.2, -0.5, 0.8, 0.3, -0.1]$
> $v_{automÃ³vel} = [1.1, -0.4, 0.9, 0.4, 0.0]$
>
> Primeiro, calculamos a mÃ©dia ($\mu$) e o desvio padrÃ£o ($\sigma$) de todos os vetores no corpus (vamos simplificar e considerar $\mu = [0, 0, 0, 0, 0]$ e $\sigma = [1, 1, 1, 1, 1]$ para este exemplo). Em seguida, aplicamos a normalizaÃ§Ã£o z-score:
>
> $z_{carro} = \frac{v_{carro} - \mu}{\sigma} = [1.2, -0.5, 0.8, 0.3, -0.1]$
> $z_{automÃ³vel} = \frac{v_{automÃ³vel} - \mu}{\sigma} = [1.1, -0.4, 0.9, 0.4, 0.0]$
>
> Agora, calculamos o produto escalar:
>
> $z_{carro} \cdot z_{automÃ³vel} = (1.2 \times 1.1) + (-0.5 \times -0.4) + (0.8 \times 0.9) + (0.3 \times 0.4) + (-0.1 \times 0.0) = 1.32 + 0.2 + 0.72 + 0.12 + 0 = 2.36$
>
> Calculamos as normas:
>
> $||z_{carro}|| = \sqrt{1.2^2 + (-0.5)^2 + 0.8^2 + 0.3^2 + (-0.1)^2} = \sqrt{1.44 + 0.25 + 0.64 + 0.09 + 0.01} = \sqrt{2.43} \approx 1.56$
> $||z_{automÃ³vel}|| = \sqrt{1.1^2 + (-0.4)^2 + 0.9^2 + 0.4^2 + 0.0^2} = \sqrt{1.21 + 0.16 + 0.81 + 0.16 + 0} = \sqrt{2.34} \approx 1.53$
>
> Finalmente, o cosseno da similaridade:
>
> $\text{cosine}(z_{carro}, z_{automÃ³vel}) = \frac{2.36}{1.56 \times 1.53} \approx \frac{2.36}{2.3868} \approx 0.988$
>
> Este valor indica uma alta similaridade semÃ¢ntica entre "carro" e "automÃ³vel", o que Ã© esperado. Se os vetores nÃ£o fossem normalizados, a similaridade ainda seria alta, mas a comparaÃ§Ã£o seria menos precisa devido Ã  anisotropia.

**Anisotropia e o Problema do Cosseno**: A anisotropia dos *embeddings* contextuais pode levar a resultados enganosos na mediÃ§Ã£o da similaridade semÃ¢ntica [^1]. A tendÃªncia de todos os vetores apontarem para a mesma direÃ§Ã£o faz com que o cosseno seja muito alto mesmo para vetores de palavras semanticamente distintas. Isso ocorre porque a maioria dos vetores tem alta magnitude em algumas dimensÃµes especÃ­ficas, que dominam o cÃ¡lculo do produto escalar. A normalizaÃ§Ã£o z-score busca corrigir este problema, uniformizando a escala e a direÃ§Ã£o dos vetores.

**NormalizaÃ§Ã£o Z-score**: A normalizaÃ§Ã£o z-score centraliza os vetores (subtraindo a mÃ©dia) e escala os vetores (dividindo pelo desvio padrÃ£o), o que permite que a similaridade do cosseno reflita mais precisamente a similaridade semÃ¢ntica [^1]. A normalizaÃ§Ã£o z-score preserva a direÃ§Ã£o do vetor, alterando apenas sua escala, como provado no Lema 1.
>
> ğŸ’¡ **Exemplo NumÃ©rico**: Suponha que os vetores antes da normalizaÃ§Ã£o z-score sÃ£o:
>
> $v_1 = [100, 200, 150]$
> $v_2 = [90, 210, 140]$
> $v_3 = [10, 20, 15]$
>
> Vamos considerar os vetores $v_1$ e $v_2$ como vetores de palavras semÃ¢nticamente similares, mas que os valores das dimensÃµes sÃ£o muito grandes, e o vetor $v_3$ como o vetor de uma palavra semanticamente diferente, com valores de dimensÃµes muito menores.
>
> Calculamos o cosseno entre $v_1$ e $v_2$ e $v_1$ e $v_3$ antes da normalizaÃ§Ã£o:
>
> $\text{cosine}(v_1, v_2) = \frac{100*90 + 200*210 + 150*140}{\sqrt{100^2+200^2+150^2}*\sqrt{90^2+210^2+140^2}} = \frac{9000 + 42000 + 21000}{\sqrt{10000+40000+22500}*\sqrt{8100+44100+19600}} = \frac{72000}{\sqrt{72500}*\sqrt{71800}} \approx \frac{72000}{269.258*268.007} \approx 0.998$
>
> $\text{cosine}(v_1, v_3) = \frac{100*10 + 200*20 + 150*15}{\sqrt{100^2+200^2+150^2}*\sqrt{10^2+20^2+15^2}} = \frac{1000+4000+2250}{\sqrt{72500}*\sqrt{100+400+225}} = \frac{7250}{\sqrt{72500}*\sqrt{725}} \approx \frac{7250}{269.258*26.925} \approx 0.997$
>
> Note que o cosseno entre $v_1$ e $v_2$, apesar de mais parecido semanticamente, Ã© muito prÃ³ximo do cosseno entre $v_1$ e $v_3$.
>
> Agora vamos normalizar os vetores. Suponha que, analisando todo o corpus, temos que a mÃ©dia dos vetores Ã© $\mu=[50, 100, 70]$ e o desvio padrÃ£o Ã© $\sigma=[40, 50, 30]$
>
> $z_1 = \frac{v_1 - \mu}{\sigma} = \frac{[100, 200, 150]-[50,100,70]}{[40, 50, 30]} = [\frac{50}{40}, \frac{100}{50}, \frac{80}{30}] = [1.25, 2, 2.67]$
> $z_2 = \frac{v_2 - \mu}{\sigma} = \frac{[90, 210, 140]-[50,100,70]}{[40, 50, 30]} = [\frac{40}{40}, \frac{110}{50}, \frac{70}{30}] = [1, 2.2, 2.33]$
> $z_3 = \frac{v_3 - \mu}{\sigma} = \frac{[10, 20, 15]-[50,100,70]}{[40, 50, 30]} = [\frac{-40}{40}, \frac{-80}{50}, \frac{-55}{30}] = [-1, -1.6, -1.83]$
>
> Calculamos o cosseno entre $z_1$ e $z_2$ e $z_1$ e $z_3$ depois da normalizaÃ§Ã£o:
>
> $\text{cosine}(z_1, z_2) = \frac{1.25*1 + 2*2.2 + 2.67*2.33}{\sqrt{1.25^2+2^2+2.67^2}*\sqrt{1^2+2.2^2+2.33^2}} = \frac{1.25 + 4.4 + 6.22}{\sqrt{1.56 + 4 + 7.13}*\sqrt{1 + 4.84 + 5.43}} = \frac{11.87}{\sqrt{12.69}*\sqrt{11.27}} \approx \frac{11.87}{3.56*3.35} \approx 0.993$
>
> $\text{cosine}(z_1, z_3) = \frac{1.25*-1 + 2*-1.6 + 2.67*-1.83}{\sqrt{1.25^2+2^2+2.67^2}*\sqrt{(-1)^2+(-1.6)^2+(-1.83)^2}} = \frac{-1.25-3.2-4.88}{\sqrt{1.56+4+7.13}*\sqrt{1+2.56+3.35}} = \frac{-9.33}{\sqrt{12.69}*\sqrt{6.91}} \approx \frac{-9.33}{3.56*2.63} \approx -0.997$
>
> Note que apÃ³s a normalizaÃ§Ã£o a similaridade entre $z_1$ e $z_2$ continua alta, mas agora a similaridade entre $z_1$ e $z_3$ Ã© negativa, mostrando que a normalizaÃ§Ã£o fez com que o cosseno refletisse melhor as diferenÃ§as de significado semÃ¢ntico.
>

> ğŸ’¡ **Exemplo NumÃ©rico**: Vamos considerar um exemplo prÃ¡tico usando Python e a biblioteca `numpy`. Suponha que temos trÃªs vetores de embeddings (representando as palavras "rei", "rainha" e "abacaxi") e queremos calcular a similaridade do cosseno entre eles antes e depois da normalizaÃ§Ã£o z-score.
>
> ```python
> import numpy as np
>
> # Vetores de embeddings (exemplo)
> v_rei = np.array([10, 20, 15])
> v_rainha = np.array([9, 21, 14])
> v_abacaxi = np.array([1, 2, 1])
>
> # Calculando a similaridade do cosseno antes da normalizaÃ§Ã£o
> def cosine_similarity(a, b):
>     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
>
> print(f"Cosseno(rei, rainha) antes da normalizaÃ§Ã£o: {cosine_similarity(v_rei, v_rainha):.3f}")
> print(f"Cosseno(rei, abacaxi) antes da normalizaÃ§Ã£o: {cosine_similarity(v_rei, v_abacaxi):.3f}")
>
> # SimulaÃ§Ã£o de mÃ©dia e desvio padrÃ£o do corpus
> media = np.array([5, 10, 7])
> desvio_padrao = np.array([4, 5, 3])
>
> # NormalizaÃ§Ã£o z-score
> def z_score_normalization(v, media, desvio_padrao):
>     return (v - media) / desvio_padrao
>
> z_rei = z_score_normalization(v_rei, media, desvio_padrao)
> z_rainha = z_score_normalization(v_rainha, media, desvio_padrao)
> z_abacaxi = z_score_normalization(v_abacaxi, media, desvio_padrao)
>
> print(f"Cosseno(rei, rainha) apÃ³s a normalizaÃ§Ã£o: {cosine_similarity(z_rei, z_rainha):.3f}")
> print(f"Cosseno(rei, abacaxi) apÃ³s a normalizaÃ§Ã£o: {cosine_similarity(z_rei, z_abacaxi):.3f}")
> ```
>
> O cÃ³digo acima produzirÃ¡ algo como:
> ```
> Cosseno(rei, rainha) antes da normalizaÃ§Ã£o: 0.998
> Cosseno(rei, abacaxi) antes da normalizaÃ§Ã£o: 0.996
> Cosseno(rei, rainha) apÃ³s a normalizaÃ§Ã£o: 0.993
> Cosseno(rei, abacaxi) apÃ³s a normalizaÃ§Ã£o: -0.973
> ```
> Observamos que antes da normalizaÃ§Ã£o, os cossenos sÃ£o muito prÃ³ximos. ApÃ³s a normalizaÃ§Ã£o, a similaridade entre "rei" e "rainha" ainda Ã© alta, mas a similaridade entre "rei" e "abacaxi" se torna negativa, refletindo melhor a diferenÃ§a semÃ¢ntica.

**LimitaÃ§Ãµes do Cosseno Mesmo ApÃ³s a PadronizaÃ§Ã£o**: Apesar de eficaz, o cosseno pode subestimar a similaridade de palavras muito frequentes mesmo apÃ³s a padronizaÃ§Ã£o [^1]. Isso ocorre porque as palavras frequentes tendem a ter *embeddings* mais dispersos, o que pode reduzir a similaridade do cosseno, mesmo quando semanticamente relacionadas.

**Teorema 3.** *A normalizaÃ§Ã£o z-score atenua o problema da anisotropia em embeddings contextuais, mas nÃ£o o elimina completamente.*

*Prova:*
I. A anisotropia em *embeddings* contextuais Ã© causada por uma distribuiÃ§Ã£o nÃ£o uniforme de magnitudes nas dimensÃµes dos vetores, fazendo com que todos os vetores tendam a apontar para a mesma direÃ§Ã£o [^1].
II. A normalizaÃ§Ã£o z-score centraliza os vetores (subtraindo a mÃ©dia) e uniformiza a escala dos vetores (dividindo pelo desvio padrÃ£o), como provado no Lema 1.
III. A centralizaÃ§Ã£o faz com que a mÃ©dia dos vetores se posicione na origem do espaÃ§o vetorial.
IV. A uniformizaÃ§Ã£o da escala reduz a variaÃ§Ã£o das magnitudes nas dimensÃµes dos vetores.
V. Ao fazer isso, a normalizaÃ§Ã£o z-score aumenta a dispersÃ£o dos vetores no espaÃ§o, reduzindo a influÃªncia das dimensÃµes com magnitudes elevadas.
VI. Embora a normalizaÃ§Ã£o z-score atenue o problema da anisotropia, ela nÃ£o o elimina completamente pois nÃ£o consegue corrigir a similaridade de palavras muito frequentes. Portanto, outras tÃ©cnicas de normalizaÃ§Ã£o podem ser necessÃ¡rias para remover completamente os efeitos da anisotropia. $\blacksquare$

**ProposiÃ§Ã£o 2.** *A similaridade semÃ¢ntica pode ser medida atravÃ©s de outras funÃ§Ãµes de distÃ¢ncia, em vez do cosseno. MÃ©tricas como a distÃ¢ncia euclidiana ou a similaridade de Jaccard podem complementar ou substituir a similaridade do cosseno, dependendo da tarefa especÃ­fica.*

> A distÃ¢ncia euclidiana Ã© dada por $||a-b||$, que quantifica a distÃ¢ncia "em linha reta" entre dois vetores. A similaridade de Jaccard, por sua vez, Ã© Ãºtil para medir a similaridade entre conjuntos e pode ser aplicada aos *embeddings* contextuais transformados em conjuntos de caracterÃ­sticas.
>
> No entanto, a similaridade do cosseno, por ser uma mÃ©trica que compara o Ã¢ngulo entre vetores, Ã© mais apropriada para avaliar a similaridade semÃ¢ntica pois captura informaÃ§Ãµes de direÃ§Ã£o e nÃ£o de magnitude, e Ã© menos sensÃ­vel a mudanÃ§as na escala dos vetores.

**CorolÃ¡rio 2.1** *A escolha da mÃ©trica de similaridade deve considerar as caracterÃ­sticas especÃ­ficas dos embeddings e a tarefa em questÃ£o. Em algumas situaÃ§Ãµes, mÃ©tricas que enfatizam a magnitude, como a distÃ¢ncia euclidiana, podem ser mais adequadas, enquanto em outras, mÃ©tricas que enfatizam a direÃ§Ã£o, como o cosseno, sÃ£o preferÃ­veis. A combinaÃ§Ã£o de diferentes mÃ©tricas pode tambÃ©m oferecer uma avaliaÃ§Ã£o mais robusta da similaridade semÃ¢ntica.*

**Teorema 4.** *A normalizaÃ§Ã£o z-score transforma os vetores de forma que a mÃ©dia dos vetores normalizados seja sempre o vetor nulo, e que o desvio padrÃ£o das dimensÃµes de cada vetor normalizado seja unitÃ¡rio.*

*Prova:*
I. Seja $x_i$ a $i$-Ã©sima componente de um vetor $x$,  $\mu_i$ a mÃ©dia da $i$-Ã©sima dimensÃ£o dos vetores do corpus e $\sigma_i$ o desvio padrÃ£o da $i$-Ã©sima dimensÃ£o dos vetores do corpus.
II. ApÃ³s a normalizaÃ§Ã£o z-score, a $i$-Ã©sima componente do vetor transformado $z$ Ã© dada por:
$z_i = \frac{x_i - \mu_i}{\sigma_i}$
III. A mÃ©dia do vetor transformado $z$ Ã© dada por:
$E[z_i] = E[\frac{x_i - \mu_i}{\sigma_i}] = \frac{1}{\sigma_i}(E[x_i] - \mu_i) = \frac{1}{\sigma_i}(\mu_i - \mu_i) = 0$
IV. Portanto, a mÃ©dia de todas as dimensÃµes dos vetores normalizados Ã© zero, o que significa que a mÃ©dia dos vetores normalizados Ã© o vetor nulo.
V. O desvio padrÃ£o da $i$-Ã©sima componente do vetor normalizado $z$ Ã© dado por:
$DP[z_i] = DP[\frac{x_i - \mu_i}{\sigma_i}] = \frac{1}{\sigma_i}DP[x_i - \mu_i] = \frac{1}{\sigma_i}DP[x_i] = \frac{\sigma_i}{\sigma_i} = 1$
VI. Portanto, o desvio padrÃ£o das dimensÃµes de cada vetor normalizado Ã© unitÃ¡rio. $\blacksquare$

### ConclusÃ£o
A mediÃ§Ã£o da similaridade semÃ¢ntica com **contextual embeddings** e a funÃ§Ã£o cosseno Ã© uma tÃ©cnica poderosa em PLN. No entanto, o problema da **anisotropia** impÃµe desafios que requerem tÃ©cnicas de normalizaÃ§Ã£o, como a z-score, para atenuar a tendÃªncia de vetores apontarem para a mesma direÃ§Ã£o e prejudicarem a avaliaÃ§Ã£o da similaridade semÃ¢ntica [^1]. Embora a normalizaÃ§Ã£o z-score seja eficaz, ela nÃ£o elimina completamente os problemas, especialmente para palavras muito frequentes, o que destaca a importÃ¢ncia de combinar o cosseno com outras medidas de similaridade e a escolha da normalizaÃ§Ã£o mais adequada, dependendo da tarefa em mÃ£os. O estudo contÃ­nuo sobre representaÃ§Ãµes contextuais e mÃ©tricas de similaridade Ã© essencial para aprimorar a precisÃ£o e a eficÃ¡cia de modelos de linguagem em diversas aplicaÃ§Ãµes de PLN. [^1]

### ReferÃªncias
[^1]: CapÃ­tulo 11 do livro-texto fornecido.
<!-- END -->
