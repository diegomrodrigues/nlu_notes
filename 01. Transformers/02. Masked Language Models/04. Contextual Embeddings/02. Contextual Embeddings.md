## Contextual Embeddings e Aplica√ß√µes em Similiaridade Sem√¢ntica e Word Sense Disambiguation

### Introdu√ß√£o
Como discutido anteriormente, os **contextual embeddings** representam um avan√ßo significativo na forma como modelos de linguagem compreendem e representam palavras e textos [^1]. Em contraste com os *embeddings est√°ticos*, que fornecem uma √∫nica representa√ß√£o vetorial para cada palavra, os embeddings contextuais capturam as nuances e varia√ß√µes de significado de uma palavra dependendo do contexto em que aparece [^1]. Este cap√≠tulo aprofunda o uso de embeddings contextuais em duas tarefas importantes: medir a similaridade sem√¢ntica entre palavras em diferentes contextos e realizar a *word sense disambiguation* (WSD), ou seja, identificar o sentido correto de uma palavra em uma dada frase [^1].

### Conceitos Fundamentais
Como vimos, os embeddings contextuais s√£o gerados por modelos de linguagem como o BERT, onde cada token da entrada √© mapeado para um vetor de alta dimensionalidade que √© sens√≠vel ao contexto da senten√ßa [^1]. Cada vetor representa n√£o apenas a palavra, mas tamb√©m a sua rela√ß√£o com todas as outras palavras na frase. Em outras palavras, cada vetor √© uma fun√ß√£o do contexto em que a palavra aparece [^1].

**Similaridade Sem√¢ntica com Embeddings Contextuais**: A similaridade sem√¢ntica entre palavras ou frases pode ser medida atrav√©s da compara√ß√£o de seus embeddings contextuais. Uma medida comum para isso √© o **cosseno** entre os vetores. Intuitivamente, quanto mais pr√≥ximos (em termos de √¢ngulo) estiverem dois vetores, mais similares semanticamente ser√£o as palavras ou frases que eles representam [^1]. No entanto, como mencionado anteriormente, a **anisotropia** dos embeddings contextuais exige t√©cnicas de normaliza√ß√£o antes de usar a similaridade do cosseno para quantificar a proximidade sem√¢ntica [^1].

**Word Sense Disambiguation (WSD)**: Uma das principais aplica√ß√µes de embeddings contextuais √© na tarefa de WSD [^1]. WSD √© o problema de determinar qual o sentido (ou significado) correto de uma palavra amb√≠gua em um determinado contexto. Como vimos [^1], os embeddings contextuais conseguem capturar a *polissemia* das palavras (a exist√™ncia de m√∫ltiplos significados) atrav√©s de vetores distintos para cada sentido [^1].

**A Rela√ß√£o entre Similiaridade Sem√¢ntica e WSD**: A WSD se beneficia da capacidade dos embeddings contextuais de representarem diferentes sentidos de palavras amb√≠guas, o que √© essencial para tarefas onde precisamos de uma compreens√£o profunda do texto, como tradu√ß√£o autom√°tica ou sumariza√ß√£o de textos. A similiaridade sem√¢ntica, por sua vez, desempenha um papel importante em WSD ao identificar inst√¢ncias de palavras que compartilham significados semelhantes, ajudando a discriminar entre diferentes sentidos de uma mesma palavra em um contexto espec√≠fico.

### Desenvolvimento
**Medindo Similaridade Sem√¢ntica Contextual**:
Para medir a similaridade sem√¢ntica entre duas palavras no contexto, ou entre duas frases, podemos usar as seguintes etapas:
1. **Gera√ß√£o de Embeddings:** Usamos um modelo de linguagem pr√©-treinado para gerar os embeddings contextuais para cada palavra ou frase de interesse [^1].
2. **Normaliza√ß√£o:** Aplicamos t√©cnicas de normaliza√ß√£o, como o z-score, para corrigir a anisotropia dos embeddings [^1].
3. **C√°lculo da Similaridade:** Calculamos o cosseno entre os embeddings normalizados [^1]. O resultado ser√° um valor entre -1 e 1, onde valores mais altos indicam maior similaridade sem√¢ntica.

Como discutido anteriormente, a normaliza√ß√£o z-score √© dada por:
$$z = \frac{x-\mu}{\sigma}$$
[^1]
onde $x$ √© o vetor do *embedding* original, $\mu$ √© a m√©dia dos vetores no corpus, e $\sigma$ √© o desvio padr√£o dos vetores no corpus [^1]. O cosseno entre dois vetores $a$ e $b$ √© dado por:
$$ \text{cosine}(a,b) = \frac{a \cdot b}{||a|| \cdot ||b||} $$
[^1]
onde $\cdot$ representa o produto escalar e $|| \cdot ||$ representa a norma do vetor.
> üí° **Exemplo Num√©rico:** Vamos considerar duas frases: "O *gato* est√° dormindo no tapete" e "O *felino* ca√ßou um rato". Ap√≥s processar essas frases com um modelo BERT, hipoteticamente obtemos os seguintes embeddings para "gato" e "felino":
>
> $v_{gato} = [1.2, -0.5, 0.8, 0.3, -0.1]$
> $v_{felino} = [1.1, -0.4, 0.9, 0.4, 0.0]$
>
> Suponha que ap√≥s analisar todo o corpus, a m√©dia $\mu$ e desvio padr√£o $\sigma$ dos embeddings sejam:
>
> $\mu = [0.0, 0.0, 0.0, 0.0, 0.0]$
> $\sigma = [1.0, 1.0, 1.0, 1.0, 1.0]$
>
> Aplicando a normaliza√ß√£o z-score:
>
> $z_{gato} = \frac{v_{gato} - \mu}{\sigma} = [1.2, -0.5, 0.8, 0.3, -0.1]$
> $z_{felino} = \frac{v_{felino} - \mu}{\sigma} = [1.1, -0.4, 0.9, 0.4, 0.0]$
>
> Calculamos o produto escalar:
>
> $z_{gato} \cdot z_{felino} = (1.2 \times 1.1) + (-0.5 \times -0.4) + (0.8 \times 0.9) + (0.3 \times 0.4) + (-0.1 \times 0.0) = 1.32 + 0.2 + 0.72 + 0.12 + 0 = 2.36$
>
> Calculamos as normas:
>
> $||z_{gato}|| = \sqrt{1.2^2 + (-0.5)^2 + 0.8^2 + 0.3^2 + (-0.1)^2} = \sqrt{1.44 + 0.25 + 0.64 + 0.09 + 0.01} = \sqrt{2.43} \approx 1.56$
> $||z_{felino}|| = \sqrt{1.1^2 + (-0.4)^2 + 0.9^2 + 0.4^2 + 0.0^2} = \sqrt{1.21 + 0.16 + 0.81 + 0.16 + 0} = \sqrt{2.34} \approx 1.53$
>
> Finalmente, o cosseno da similaridade:
>
> $\text{cosine}(z_{gato}, z_{felino}) = \frac{2.36}{1.56 \times 1.53} \approx \frac{2.36}{2.3868} \approx 0.988$
>
> Um valor pr√≥ximo de 1 indica que as palavras "gato" e "felino", nos contextos dados, s√£o semanticamente muito similares. Se considerarmos duas outras frases "O *banco* est√° lotado" e "O *banco* √© seguro", onde as palavras tem o mesmo sentido, podemos obter o mesmo resultado. Por outro lado, a similaridade seria bem menor se compararmos os *embeddings* de "O *rato* roeu a roupa" e "O *mouse* do computador quebrou" que, apesar de "rato" e "mouse" serem sin√¥nimos em alguns contextos, t√™m significados distintos nas frases dadas.
>
> Note que o c√°lculo do cosseno √© feito usando os vetores j√° normalizados. Se calcularmos usando os vetores n√£o normalizados, podemos obter resultados diferentes. A normaliza√ß√£o √© importante para que a magnitude dos vetores n√£o afete a medida da similaridade, focando no √¢ngulo entre eles que representa a rela√ß√£o sem√¢ntica.

**Word Sense Disambiguation (WSD) com Embeddings Contextuais**:
A WSD √© uma tarefa de classifica√ß√£o, na qual o modelo deve atribuir o sentido correto para uma palavra em um determinado contexto. Embeddings contextuais podem ser utilizados nessa tarefa atrav√©s dos seguintes passos:
1. **Gera√ß√£o de Embeddings de Sentido**: Para cada sentido de uma palavra amb√≠gua no vocabul√°rio, obtemos exemplos de frases onde esse sentido aparece (por exemplo, atrav√©s de WordNet [^1]). Geramos o *embedding* contextual de cada palavra amb√≠gua nestas frases e calculamos a m√©dia desses *embeddings* para obter um *embedding* de sentido que representa o significado espec√≠fico da palavra [^1].
2. **Gera√ß√£o de Embeddings de Contexto**: Para cada ocorr√™ncia da palavra amb√≠gua em um novo texto, geramos o *embedding* contextual usando o mesmo modelo.
3. **Disambigua√ß√£o**: Calculamos a similaridade (e.g., cosseno) entre o *embedding* de contexto e cada um dos *embeddings* de sentido. O sentido que obtiver a maior similaridade √© considerado o sentido correto da palavra no contexto [^1].

> üí° **Exemplo Num√©rico:** Vamos considerar o exemplo cl√°ssico da palavra "banco". Para o sentido de "institui√ß√£o financeira", temos exemplos de frases como "fui ao *banco* sacar dinheiro" e "o *banco* liberou o empr√©stimo". Suponha que, ap√≥s calcular os *embeddings* contextuais dessas frases e fazer a m√©dia, obtivemos o seguinte *embedding* de sentido:
>
> $v_{banco\_instituicao} = [0.7, -0.2, 0.5, 0.1, 0.3]$
>
> Para o sentido de "assento", temos frases como "o *banco* da pra√ßa estava sujo" e "sentei no *banco* para descansar". Da mesma forma, ap√≥s calcular os *embeddings* e fazer a m√©dia, obtivemos:
>
> $v_{banco\_assento} = [0.2, 0.6, -0.1, 0.4, -0.2]$
>
> Agora, para a nova frase "vou ao *banco* resolver um problema", calculamos o embedding contextual da palavra "banco", $v_{banco\_contexto}$, usando o mesmo modelo BERT. Suponha que obtemos:
>
> $v_{banco\_contexto} = [0.6, -0.1, 0.4, 0.2, 0.2]$
>
> Calculamos o cosseno da similaridade entre $v_{banco\_contexto}$ e cada *embedding* de sentido (usando o mesmo processo do exemplo anterior):
>
> $\text{cosine}(v_{banco\_contexto}, v_{banco\_instituicao}) \approx 0.95$
> $\text{cosine}(v_{banco\_contexto}, v_{banco\_assento}) \approx 0.30$
>
> Como a similaridade com $v_{banco\_instituicao}$ √© maior, inferimos que o sentido correto da palavra "banco" na frase "vou ao *banco* resolver um problema" √© "institui√ß√£o financeira". Este processo mostra como os *embeddings* contextuais ajudam a desambiguar palavras. Se a frase fosse "sentei no *banco* da pra√ßa", a similaridade com $v_{banco\_assento}$ seria maior, indicando o sentido correto "assento".

**Lema 2.**  *A similaridade de cosseno entre dois contextual embeddings normalizados √© invariante √† escala dos vetores*.

*Prova:*
I. Sejam $a$ e $b$ dois vetores representando embeddings contextuais, e $\lambda$ um escalar.

II. Sejam $a'$ e $b'$ vetores transformados por uma escala $\lambda$, i.e.,  $a' = \lambda a$ e $b' = \lambda b$.

III. A similaridade de cosseno entre $a$ e $b$ √© dada por:
  $$ \text{cosine}(a,b) = \frac{a \cdot b}{||a|| \cdot ||b||} $$

IV. A similaridade de cosseno entre $a'$ e $b'$ √© dada por:
  $$ \text{cosine}(a',b') = \frac{a' \cdot b'}{||a'|| \cdot ||b'||} = \frac{(\lambda a) \cdot (\lambda b)}{||\lambda a|| \cdot ||\lambda b||} = \frac{\lambda^2 (a \cdot b)}{|\lambda| ||a|| \cdot |\lambda| ||b||} = \frac{\lambda^2 (a \cdot b)}{\lambda^2 ||a|| \cdot ||b||} = \frac{a \cdot b}{||a|| \cdot ||b||}$$

V. Como $\text{cosine}(a,b) = \text{cosine}(a',b')$, a similaridade de cosseno √© invariante √† escala. $\blacksquare$

**Lema 2.1.** *A normaliza√ß√£o z-score preserva a dire√ß√£o dos vetores, alterando apenas sua magnitude.*

*Prova:*
I. Seja $x$ um vetor *embedding* original.
II. A normaliza√ß√£o z-score produz o vetor $z = \frac{x-\mu}{\sigma}$.
III. O vetor $x - \mu$ representa uma transla√ß√£o do vetor $x$. A transla√ß√£o n√£o altera a dire√ß√£o do vetor, apenas sua posi√ß√£o no espa√ßo.
IV. A divis√£o por $\sigma$ escala o vetor resultante. A escala n√£o altera a dire√ß√£o do vetor, apenas sua magnitude.
V. Portanto, a normaliza√ß√£o z-score preserva a dire√ß√£o do vetor original. $\blacksquare$

**Teorema 1.** *A similaridade de cosseno entre dois vetores normalizados pelo z-score √© equivalente a calcular a similaridade de cosseno entre os vetores originais centrados pela m√©dia do corpus e depois normalizados pela norma.*

*Prova:*
I. Sejam $x$ e $y$ dois vetores originais.
II. Os vetores normalizados por z-score s√£o $z_x = \frac{x-\mu}{\sigma_x}$ e $z_y = \frac{y-\mu}{\sigma_y}$, onde $\mu$ √© a m√©dia e $\sigma_x$ e $\sigma_y$ s√£o os desvios padr√µes dos vetores no corpus.
III. A similaridade de cosseno entre os vetores normalizados √©:
    $$ \text{cosine}(z_x, z_y) = \frac{z_x \cdot z_y}{||z_x|| \cdot ||z_y||} =  \frac{\frac{x-\mu}{\sigma_x} \cdot \frac{y-\mu}{\sigma_y}}{||\frac{x-\mu}{\sigma_x}|| \cdot ||\frac{y-\mu}{\sigma_y}||} = \frac{\frac{(x-\mu) \cdot (y-\mu)}{\sigma_x \sigma_y}}{\frac{||x-\mu||}{\sigma_x} \frac{||y-\mu||}{\sigma_y}} = \frac{(x-\mu) \cdot (y-\mu)}{||x-\mu|| \cdot ||y-\mu||}$$

IV. O vetor centrado pela m√©dia do corpus √© $x' = x - \mu$ e $y' = y - \mu$
V. A similaridade de cosseno entre os vetores centrados e normalizados pela norma √©:
    $$ \text{cosine}\left(\frac{x'}{||x'||}, \frac{y'}{||y'||}\right) = \frac{\frac{x'}{||x'||} \cdot \frac{y'}{||y'||}}{||\frac{x'}{||x'||}|| \cdot ||\frac{y'}{||y'||}||} =  \frac{\frac{x' \cdot y'}{||x'|| \cdot ||y'||}}{1 \cdot 1} = \frac{x' \cdot y'}{||x'|| \cdot ||y'||} = \frac{(x-\mu) \cdot (y-\mu)}{||x-\mu|| \cdot ||y-\mu||}$$

VI. Portanto, a similaridade de cosseno entre vetores normalizados por z-score √© equivalente a calcular a similaridade de cosseno entre os vetores centrados pela m√©dia e normalizados pela norma. $\blacksquare$

**Observa√ß√£o 2.** Em pr√°tica, outras m√©tricas de similaridade al√©m do cosseno podem ser utilizadas, como a dist√¢ncia euclidiana ou a similaridade de Jaccard.

**Observa√ß√£o 3.** O processo de WSD utilizando contextual embeddings pode ser melhorado com abordagens que incluem modelos de aprendizado de m√°quina, como redes neurais, para classificar o sentido correto de uma palavra, mas em geral, os *embeddings* contextuais s√£o usados para extrair caracter√≠sticas do contexto para os modelos de classifica√ß√£o.

### Conclus√£o
Contextual embeddings s√£o uma ferramenta poderosa para medir a similaridade sem√¢ntica entre palavras e frases, al√©m de serem cruciais para tarefas de *word sense disambiguation* (WSD). Atrav√©s de sua capacidade de capturar a polissemia das palavras e as nuances contextuais, os *embeddings* contextuais permitem que modelos de linguagem compreendam textos de forma mais precisa e eficiente. As tarefas de similaridade sem√¢ntica e WSD s√£o fundamentais para muitas aplica√ß√µes de PLN, como tradu√ß√£o autom√°tica, sumariza√ß√£o de texto, an√°lise de sentimentos e muitos outros. O uso de *embeddings* contextuais, portanto, continua sendo um tema de pesquisa ativa e um componente essencial em sistemas avan√ßados de PLN. [^1]

### Refer√™ncias
[^1]: Cap√≠tulo 11 do livro-texto fornecido.
<!-- END -->
