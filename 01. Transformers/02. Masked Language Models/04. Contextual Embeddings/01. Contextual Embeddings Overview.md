## Contextual Embeddings: Representa√ß√µes Din√¢micas da Linguagem

### Introdu√ß√£o
Este cap√≠tulo explora o conceito de **contextual embeddings**, um avan√ßo fundamental na representa√ß√£o de palavras e tokens em modelos de linguagem, especialmente no contexto de *masked language models* como o BERT [^1]. Diferentemente dos m√©todos tradicionais, como word2vec e GloVe, que atribuem um vetor est√°tico a cada palavra no vocabul√°rio [^1], os contextual embeddings s√£o din√¢micos, ajustando-se ao contexto espec√≠fico em que um token aparece. Essa abordagem permite que modelos de linguagem capturem nuances e ambiguidades de significado, representando cada ocorr√™ncia de uma palavra com um vetor √∫nico e contextualizado. Vamos aprofundar a compreens√£o desses embeddings, suas caracter√≠sticas e como eles s√£o gerados, al√©m de explorar suas aplica√ß√µes, como em tarefas de similaridade sem√¢ntica e *word sense disambiguation*.

### Conceitos Fundamentais
Como vimos anteriormente [^1], modelos *bidirectional transformer encoders*, como o BERT, empregam um mecanismo de autoaten√ß√£o que permite que cada token considere todos os outros tokens na sequ√™ncia de entrada. Essa capacidade √© crucial para a gera√ß√£o de embeddings contextuais. Cada camada do transformer produz uma representa√ß√£o vetorial para cada token, e essas representa√ß√µes se tornam cada vez mais sens√≠veis ao contexto √† medida que a informa√ß√£o passa pelas diversas camadas da rede neural [^1].

**Gera√ß√£o de Contextual Embeddings**: Ao contr√°rio de modelos como o Word2Vec que geram um √∫nico vetor est√°tico para cada palavra [^1], modelos como BERT geram representa√ß√µes vetoriais para cada palavra a partir do contexto em que aparecem. Mais precisamente, dada uma sequ√™ncia de tokens de entrada $x_1, \ldots, x_n$, cada token $x_i$ √© representado por um vetor $z_i$, que √© a sa√≠da da √∫ltima camada do modelo para o token $x_i$ [^1]. A representa√ß√£o de cada token depende, portanto, da intera√ß√£o com todos os outros tokens da sequ√™ncia. A representa√ß√£o $z_i$ serve, ent√£o, como um *contextual embedding* de $x_i$. Em geral, para obter uma representa√ß√£o mais robusta para cada token, em vez de usar apenas a sa√≠da da √∫ltima camada, tamb√©m se calcula a m√©dia das sa√≠das de algumas das √∫ltimas camadas do modelo [^1].

**Diferen√ßa para Embeddings Est√°ticos**: √â crucial distinguir os contextual embeddings dos embeddings est√°ticos. Enquanto os embeddings est√°ticos, como os gerados pelo word2vec, representam o significado de um *tipo* de palavra, os contextual embeddings representam o significado de uma *inst√¢ncia* de palavra em um determinado contexto [^1]. Por exemplo, a palavra "banco" teria apenas um vetor em word2vec, mas em um modelo de *masked language modeling* como o BERT, "banco" teria diferentes vetores dependendo do contexto (banco como institui√ß√£o financeira, ou banco como assento).
> üí° **Exemplo Num√©rico:** Imagine que temos duas frases: "O banco da pra√ßa estava sujo" e "Fui ao banco pagar contas". Num modelo de embeddings est√°ticos (como Word2Vec), a palavra "banco" seria representada pelo mesmo vetor num√©rico em ambas as frases. Digamos que esse vetor seja `[0.2, -0.5, 0.8, ..., 0.1]`. J√° num modelo de contextual embeddings (como BERT), a palavra "banco" teria vetores diferentes para cada ocorr√™ncia. Poder√≠amos ter, por exemplo, `[0.3, -0.4, 0.9, ..., 0.2]` para a primeira frase (banco como assento) e `[0.1, -0.6, 0.7, ..., 0.0]` para a segunda (banco como institui√ß√£o financeira). Esses vetores, embora similares, capturam a diferen√ßa de significado no contexto.

**Representa√ß√£o da Sem√¢ntica**: Os contextual embeddings capturam n√£o apenas as rela√ß√µes sem√¢nticas entre palavras, mas tamb√©m suas nuances contextuais. Por exemplo, em frases como "O *banco* da pra√ßa estava cheio" e "Fui ao *banco* depositar dinheiro", a palavra "banco" seria representada por vetores diferentes, refletindo os diferentes sentidos da palavra em cada contexto.

**Ambiguidade e *Word Sense Disambiguation***: A capacidade de contextual embeddings em representar m√∫ltiplos significados para uma mesma palavra √© fundamental para tarefas de *word sense disambiguation* (WSD) [^1]. Como vimos anteriormente, palavras s√£o frequentemente amb√≠guas, com m√∫ltiplos sentidos poss√≠veis (poli-semia) [^1]. Os contextual embeddings capturam esses diferentes sentidos e podem ser usados em tarefas onde o sentido correto deve ser determinado.
> üí° **Exemplo Num√©rico:** Na tarefa de WSD, o modelo BERT poderia receber a frase "Ele foi pescar na *manga* do rio" e, com base no contexto (pescar, rio), gerar um embedding para "manga" que se assemelhe mais ao sentido de "parte do rio" do que ao sentido de "fruta". Similarmente, se a frase fosse "Ele comeu uma *manga* deliciosa", o embedding para "manga" seria pr√≥ximo ao vetor que representa a fruta.

**Cosine Similarity**: Uma forma comum de medir a similaridade entre dois *contextual embeddings* √© utilizando a fun√ß√£o cosseno [^1]. A intui√ß√£o √© que, se dois vetores representam palavras usadas em um contexto similar, a fun√ß√£o cosseno entre eles ter√° um valor alto. No entanto, como os vetores obtidos diretamente de modelos *transformer*, como BERT, tendem a ser muito similares entre si, algumas transforma√ß√µes nesses vetores podem ser necess√°rias para que a similaridade entre dois embeddings reflita realmente a proximidade sem√¢ntica das palavras que eles representam [^1].
> üí° **Exemplo Num√©rico:**  Suponha que temos os vetores $v_1 = [0.8, 0.6]$ e $v_2 = [0.9, 0.4]$, ambos representando palavras em contextos semelhantes, e o vetor $v_3=[0.1, 0.9]$ representando uma palavra em um contexto diferente. A similaridade de cosseno entre $v_1$ e $v_2$ √©:
>
> $$\text{cos}(v_1, v_2) = \frac{v_1 \cdot v_2}{||v_1|| \cdot ||v_2||} = \frac{(0.8)(0.9) + (0.6)(0.4)}{\sqrt{0.8^2 + 0.6^2}\sqrt{0.9^2 + 0.4^2}} = \frac{0.72 + 0.24}{\sqrt{1}\sqrt{0.97}} \approx \frac{0.96}{0.985} \approx 0.975$$.
>
> J√° a similaridade de cosseno entre $v_1$ e $v_3$ √©:
>
> $$\text{cos}(v_1, v_3) = \frac{v_1 \cdot v_3}{||v_1|| \cdot ||v_3||} = \frac{(0.8)(0.1) + (0.6)(0.9)}{\sqrt{0.8^2 + 0.6^2}\sqrt{0.1^2 + 0.9^2}} = \frac{0.08 + 0.54}{\sqrt{1}\sqrt{0.82}} \approx \frac{0.62}{0.905} \approx 0.685$$.
>
> Como esperado, $v_1$ e $v_2$, que representam palavras em contextos similares, t√™m uma maior similaridade de cosseno do que $v_1$ e $v_3$.

**Anisotropia e Normaliza√ß√£o**: Uma caracter√≠stica observada nos contextual embeddings √© a **anisotropia** [^1], a tend√™ncia de todos os vetores apontarem aproximadamente para a mesma dire√ß√£o no espa√ßo vetorial. Isso significa que, mesmo palavras com significados distintos, podem ter representa√ß√µes com alta similaridade de cosseno. Para mitigar esse problema, t√©cnicas de normaliza√ß√£o, como o z-score, s√£o aplicadas para centralizar e uniformizar as distribui√ß√µes dos vetores [^1].

$$
\mu = \frac{1}{|C|} \sum_{x \in C} x
$$
[^1]

$$
\sigma = \sqrt{\frac{1}{|C|} \sum_{x \in C} (x - \mu)^2 }
$$
[^1]

$$
z = \frac{x-\mu}{\sigma}
$$
[^1]

onde $C$ √© o conjunto de embeddings no corpus, $\mu$ √© a m√©dia dos embeddings, e $\sigma$ √© o desvio padr√£o.
> üí° **Exemplo Num√©rico:** Suponha que temos um conjunto de embeddings (ap√≥s o processo de obten√ß√£o via BERT) de duas dimens√µes, $C = \{v_1, v_2, v_3\}$, onde $v_1=[1, 2]$, $v_2=[1.5, 2.5]$, e $v_3=[0.8, 1.8]$. Primeiro calculamos a m√©dia ($\mu$) de todos os vetores:
>
> $\mu = \frac{1}{3} (v_1 + v_2 + v_3) = \frac{1}{3} ([1, 2] + [1.5, 2.5] + [0.8, 1.8]) = \frac{1}{3} [3.3, 6.3] = [1.1, 2.1]$.
>
> Agora calculamos o desvio padr√£o ($\sigma$) para cada dimens√£o:
>
> $\sigma_1 = \sqrt{\frac{1}{3}((1-1.1)^2 + (1.5-1.1)^2 + (0.8-1.1)^2)} = \sqrt{\frac{1}{3}(0.01 + 0.16 + 0.09)} = \sqrt{\frac{0.26}{3}} \approx 0.294$.
>
> $\sigma_2 = \sqrt{\frac{1}{3}((2-2.1)^2 + (2.5-2.1)^2 + (1.8-2.1)^2)} = \sqrt{\frac{1}{3}(0.01 + 0.16 + 0.09)} = \sqrt{\frac{0.26}{3}} \approx 0.294$.
>
> Finalmente, aplicamos a normaliza√ß√£o z-score em cada vetor:
>
> $z_1 = \frac{v_1 - \mu}{\sigma} = \frac{[1, 2] - [1.1, 2.1]}{[0.294, 0.294]} = [\frac{-0.1}{0.294}, \frac{-0.1}{0.294}] \approx [-0.34, -0.34]$.
>
> $z_2 = \frac{v_2 - \mu}{\sigma} = \frac{[1.5, 2.5] - [1.1, 2.1]}{[0.294, 0.294]} = [\frac{0.4}{0.294}, \frac{0.4}{0.294}] \approx [1.36, 1.36]$.
>
> $z_3 = \frac{v_3 - \mu}{\sigma} = \frac{[0.8, 1.8] - [1.1, 2.1]}{[0.294, 0.294]} = [\frac{-0.3}{0.294}, \frac{-0.3}{0.294}] \approx [-1.02, -1.02]$.
>
> Note que ap√≥s a normaliza√ß√£o os vetores est√£o mais dispersos em rela√ß√£o √† origem.

**Lema 1.** *A normaliza√ß√£o z-score preserva a dire√ß√£o dos vetores no espa√ßo vetorial.*

*Prova:*
I.  Seja $x$ um vetor original, $\mu$ a m√©dia dos vetores no conjunto $C$, e $\sigma$ o desvio padr√£o dos vetores em $C$. O vetor normalizado $z$ √© dado por:
    $$z = \frac{x - \mu}{\sigma} = \frac{1}{\sigma}x - \frac{\mu}{\sigma}$$

II. A dire√ß√£o de um vetor $x$ √© dada por $\frac{x}{||x||}$, onde $||x||$ √© a norma de $x$.

III. A dire√ß√£o do vetor transformado $z$ √© dada por:
    $$\frac{z}{||z||} = \frac{\frac{1}{\sigma}x - \frac{\mu}{\sigma}}{||\frac{1}{\sigma}x - \frac{\mu}{\sigma}||} $$

IV. Podemos analisar o efeito da transforma√ß√£o $x' = x - \mu$. A subtra√ß√£o de um vetor constante $\mu$ desloca o vetor $x$, mas n√£o muda sua dire√ß√£o, pois a dire√ß√£o √© relativa √† origem do espa√ßo vetorial. Portanto, a dire√ß√£o de $x$ e $x'$ s√£o as mesmas, i.e., $\frac{x}{||x||} = \frac{x'}{||x'||}$.

V. Podemos analisar o efeito da transforma√ß√£o $x'' = \frac{x'}{\sigma} = \frac{x-\mu}{\sigma}$. A divis√£o por $\sigma$ apenas escala o vetor $x'$, o que tamb√©m n√£o altera sua dire√ß√£o, pois a dire√ß√£o √© relativa √† origem. Portanto, a dire√ß√£o de $x'$ e $x''$ s√£o as mesmas, i.e., $\frac{x'}{||x'||} = \frac{x''}{||x''||}$.

VI. Dado que a dire√ß√£o de $x$ e $x'$ s√£o iguais e a dire√ß√£o de $x'$ e $x''$ s√£o iguais, a dire√ß√£o de $x$ e $x''$ s√£o iguais. Portanto, a normaliza√ß√£o z-score preserva a dire√ß√£o do vetor original $x$. $\blacksquare$

**Teorema 1.** *A normaliza√ß√£o z-score dos embeddings aumenta a dispers√£o dos vetores, diminuindo a similaridade de cosseno entre vetores semanticamente distintos e aumentando a similaridade de cosseno entre vetores semanticamente pr√≥ximos*.

*Prova:*
I. A anisotropia observada em contextual embeddings faz com que vetores semanticamente distintos tenham alta similaridade de cosseno devido √† sua proximidade direcional no espa√ßo vetorial. Isso significa que os √¢ngulos entre vetores semanticamente distintos s√£o pequenos.

II. O processo de normaliza√ß√£o z-score, atrav√©s da centraliza√ß√£o (subtra√ß√£o da m√©dia $\mu$) e da uniformiza√ß√£o (divis√£o pelo desvio padr√£o $\sigma$), tem como objetivo aumentar a dispers√£o dos vetores, fazendo com que vetores semanticamente distintos fiquem mais afastados no espa√ßo vetorial.

III. A centraliza√ß√£o dos vetores, pela subtra√ß√£o da m√©dia ($\mu$), desloca todos os vetores de forma que a m√©dia do conjunto de vetores esteja na origem do espa√ßo vetorial. Isso n√£o altera os √¢ngulos relativos entre os vetores, mas aumenta a dist√¢ncia entre os vetores e a origem.

IV. A divis√£o pelo desvio padr√£o ($\sigma$) uniformiza a escala dos vetores. Esta opera√ß√£o aumenta a dist√¢ncia entre vetores semanticamente distintos sem alterar a dire√ß√£o dos vetores, de acordo com o Lema 1.

V. Ao aumentar a dispers√£o dos vetores, a normaliza√ß√£o faz com que o √¢ngulo entre vetores semanticamente similares se mantenha pequeno e o √¢ngulo entre vetores semanticamente distintos aumente.

VI. Dado que a similaridade de cosseno √© dada pelo cosseno do √¢ngulo entre dois vetores, a normaliza√ß√£o z-score faz com que a similaridade de cosseno entre vetores semanticamente distintos diminua e a similaridade entre vetores semanticamente pr√≥ximos aumente. $\blacksquare$

**Observa√ß√£o 1.** √â importante notar que a normaliza√ß√£o z-score n√£o √© a √∫nica t√©cnica de normaliza√ß√£o aplic√°vel aos contextual embeddings, mas √© uma das mais utilizadas e eficazes. Outras t√©cnicas como normaliza√ß√£o L2 e t√©cnicas de *whitening* podem tamb√©m ser utilizadas para atenuar a anisotropia dos embeddings.

### Conclus√£o
Contextual embeddings s√£o uma representa√ß√£o mais rica e precisa de palavras e tokens em modelos de linguagem, superando as limita√ß√µes dos embeddings est√°ticos. Atrav√©s do uso de modelos *bidirectional transformer encoders*, esses embeddings conseguem capturar nuances de significado, ambiguidades e rela√ß√µes sem√¢nticas que s√£o cruciais para muitas tarefas de Processamento de Linguagem Natural (PLN). Os contextual embeddings se tornaram um componente essencial em modelos avan√ßados de PLN, abrindo caminho para avan√ßos significativos em diversas aplica√ß√µes, como *word sense disambiguation*, similaridade sem√¢ntica, an√°lise de sentimentos e muito mais. [^1].

### Refer√™ncias
[^1]: Cap√≠tulo 11 do livro-texto fornecido.
<!-- END -->
