## Fine-Tuning para Sequence Labeling: Named Entity Recognition

### Introdu√ß√£o
Em continuidade ao t√≥pico de **fine-tuning** apresentado no contexto anterior [^12], este cap√≠tulo se aprofunda em uma aplica√ß√£o espec√≠fica: **sequence labeling**, com foco no **Named Entity Recognition (NER)** [^15]. Como j√° discutido, o fine-tuning possibilita adaptar modelos de linguagem pr√©-treinados para tarefas espec√≠ficas, adicionando camadas especializadas (ou "heads") e utilizando dados rotulados para ajustar os par√¢metros [^13]. Em sequence labeling, o objetivo √© atribuir um r√≥tulo a cada token em uma sequ√™ncia de texto [^15], permitindo, por exemplo, identificar e classificar entidades nomeadas em um texto [^15]. O NER, portanto, √© um exemplo de sequence labeling que envolve a identifica√ß√£o de *spans* de texto que correspondem a nomes pr√≥prios e a classifica√ß√£o desses *spans* em categorias pr√©-definidas [^15]. Este cap√≠tulo explorar√° os detalhes do NER e como o fine-tuning de modelos de linguagem pr√©-treinados pode ser aplicado a esta tarefa [^15].

### Conceitos Fundamentais
O **Named Entity Recognition (NER)** √© uma tarefa fundamental em Natural Language Processing (NLP) que visa identificar e categorizar *named entities* em um texto [^15]. Uma *named entity* √©, em termos gerais, qualquer coisa que possa ser referida por um nome pr√≥prio, como pessoas, locais ou organiza√ß√µes [^15]. A tarefa de NER envolve a identifica√ß√£o de *spans* de texto que correspondem a nomes pr√≥prios e a classifica√ß√£o desses *spans* em categorias pr√©-definidas [^15].

Os tipos de *named entities* mais comuns incluem:
- **PER** (Pessoa): Nomes de pessoas.
- **LOC** (Localiza√ß√£o): Nomes de locais geogr√°ficos.
- **ORG** (Organiza√ß√£o): Nomes de empresas, institui√ß√µes, etc.
- **GPE** (Entidade Geopol√≠tica): Nomes de pa√≠ses, estados, etc. [^15]

Al√©m dessas categorias, o termo *named entity* √© frequentemente estendido para incluir express√µes temporais (datas e horas) e num√©ricas (pre√ßos) [^15].
√â importante ressaltar que o NER √© um problema desafiador devido √† ambiguidade de segmenta√ß√£o e tipo [^16]. Um mesmo *span* de texto pode n√£o ser uma *named entity* ou pode se referir a diferentes tipos, como o exemplo de "Washington" [^16].

Para lidar com essa complexidade, o NER √© frequentemente abordado como uma tarefa de **sequence labeling**, onde cada *token* na sequ√™ncia de entrada √© rotulado [^15]. Uma t√©cnica comum para sequence labeling em NER √© o **BIO tagging** [^16]. No BIO tagging, cada *token* √© rotulado com um dos seguintes tags:

- **B-Type**: Indica que o *token* inicia um *span* de uma entidade do tipo "Type".
- **I-Type**: Indica que o *token* est√° dentro de um *span* de uma entidade do tipo "Type".
- **O**: Indica que o *token* n√£o faz parte de nenhuma *named entity*. [^16]

O BIO tagging permite que o modelo capture a fronteira e o tipo de *named entities* [^16]. Existem variantes como o **IO tagging** e o **BIOES tagging**, sendo o BIOES o mais completo pois adiciona um tag E para o final de um *span* e o tag S para um *span* com apenas um *token* [^16].

> üí° **Exemplo Num√©rico:**
> Considere a frase: "Barack Obama visitou a Fran√ßa em 2015."
> Aplicando o BIO tagging para NER, obter√≠amos:
>
> | Token    | Tag     |
> |----------|---------|
> | Barack   | B-PER   |
> | Obama    | I-PER   |
> | visitou  | O       |
> | a        | O       |
> | Fran√ßa   | B-LOC   |
> | em       | O       |
> | 2015     | O       |
> | .        | O       |
>
> Neste exemplo, "Barack Obama" √© marcado como uma entidade do tipo `PER` (pessoa), e "Fran√ßa" como uma entidade do tipo `LOC` (localiza√ß√£o). Os demais tokens s√£o marcados como `O`, indicando que n√£o fazem parte de nenhuma entidade nomeada.
>
> Agora, vamos considerar o BIOES tagging. A frase "A Apple lan√ßou o iPhone" ficaria:
>
> | Token    | Tag     |
> |----------|---------|
> | A       | O      |
> | Apple   | S-ORG   |
> | lan√ßou   | O       |
> | o       | O      |
> | iPhone  | S-PROD    |
>
> Neste caso, Apple e iPhone, que s√£o entidades de um √∫nico token, s√£o marcadas com o tag S, representando um *span* √∫nico.

**Lema 1**
A escolha da estrat√©gia de tagging (BIO, IO, BIOES) pode impactar o desempenho do modelo NER, especialmente em casos de entidades aninhadas ou adjacentes. O BIOES tagging, por exemplo, oferece maior capacidade de expressar limites de entidades, o que pode levar a um desempenho superior em conjuntos de dados mais complexos.

**Prova:**
A prova √© emp√≠rica, baseada na observa√ß√£o do desempenho de modelos NER em diferentes configura√ß√µes de tagging. O BIOES, ao fornecer tags para o in√≠cio, continua√ß√£o, fim e entidades √∫nicas de um token, permite uma representa√ß√£o mais fina das fronteiras de entidades. Em contraste, o BIO tagging representa apenas o in√≠cio e a continua√ß√£o. IO tagging carece de informa√ß√µes sobre o in√≠cio da entidade, o que pode confundir modelos e diminuir a precis√£o. Portanto, em casos de entidades aninhadas ou adjacentes, o BIOES pode ter um desempenho superior.
‚ñ†

**Sequence Labeling com Modelos de Linguagem Pr√©-treinados**

Em sequence labeling para NER usando modelos de linguagem pr√©-treinados, cada *token* na sequ√™ncia de entrada √© processado pelo modelo, resultando em um vetor de sa√≠da correspondente [^17]. Este vetor de sa√≠da √© ent√£o passado para um classificador, que produz uma distribui√ß√£o de probabilidade sobre os poss√≠veis tags para aquele *token* [^17].
O classificador pode ser uma simples camada *feedforward*, onde o vetor de sa√≠da do modelo √© multiplicado por uma matriz de pesos $W_k$ de tamanho $[d \times k]$, onde $d$ √© a dimens√£o do vetor de sa√≠da do modelo e $k$ √© o n√∫mero de tags poss√≠veis [^17]. A sa√≠da do classificador √© ent√£o passada por uma fun√ß√£o *softmax* para gerar uma distribui√ß√£o de probabilidade sobre os tags [^17]:
$$
    y_i = \text{softmax}(h_i W_k)
$$
Onde $y_i$ √© o vetor de probabilidades sobre os tags, $h_i$ √© o vetor de sa√≠da do modelo para o *token* i, e $W_k$ √© a matriz de pesos do classificador [^17].
Alternativamente, a distribui√ß√£o sobre os r√≥tulos gerada pelo *softmax* pode ser passada para uma camada de **Conditional Random Field (CRF)** que pode modelar transi√ß√µes entre os r√≥tulos [^17]. Uma abordagem *greedy* pode ser usada para obter os r√≥tulos, utilizando o argmax de cada distribui√ß√£o [^17]:
$$
    t_i = \text{argmax}(y_i)
$$
onde $t_i$ √© o r√≥tulo predito para o token i [^17].

> üí° **Exemplo Num√©rico:**
>
> Suponha que a dimens√£o do vetor de sa√≠da do modelo ($h_i$) seja $d=768$ e temos $k=5$ tags poss√≠veis (B-PER, I-PER, B-LOC, I-LOC, O). A matriz de pesos $W_k$ ter√° dimens√£o $[768 \times 5]$.
>
> Vamos considerar o *token* "Obama". Ap√≥s passar pelo modelo, obtemos o vetor $h_i$ (um vetor de 768 dimens√µes):
> ```python
> import numpy as np
>
> h_i = np.random.rand(768)
> W_k = np.random.rand(768, 5)
>
> # Calcula a sa√≠da do classificador
> logits = np.dot(h_i, W_k)
>
> # Aplica softmax para obter a distribui√ß√£o de probabilidade
> def softmax(x):
>     e_x = np.exp(x - np.max(x))
>     return e_x / e_x.sum()
>
> y_i = softmax(logits)
> print("Probabilidades (y_i) para cada tag:", y_i)
>
> # Encontra o r√≥tulo predito usando argmax
> t_i = np.argmax(y_i)
> print("√çndice do tag predito (t_i):", t_i)
>
> tags = ["B-PER", "I-PER", "B-LOC", "I-LOC", "O"]
> print("Tag predito:", tags[t_i])
> ```
> Suponha que o resultado de `y_i` seja `[0.1, 0.7, 0.05, 0.05, 0.1]`.  O tag predito ser√° `I-PER` (√≠ndice 1).
>
>
> Se usarmos um CRF, o CRF leva em considera√ß√£o a probabilidade de transi√ß√£o entre as tags. Por exemplo, a sequ√™ncia `B-PER I-LOC` √© pouco prov√°vel de ocorrer. O CRF ajusta as probabilidades dos r√≥tulos, considerando a sequ√™ncia anterior, para garantir a coer√™ncia da sequ√™ncia de tags. Por exemplo, se um token √© precedido por `B-PER`, o CRF dar√° maior probabilidade para o token seguinte ser `I-PER` ou `O`, reduzindo a probabilidade de ser `B-LOC`.

**Teorema 1**
A utiliza√ß√£o de um CRF ap√≥s o softmax pode levar a um desempenho superior no NER quando comparada a uma abordagem puramente greedy.

**Prova:**
A prova baseia-se no fato de que o CRF, ao modelar as depend√™ncias entre as tags, captura as rela√ß√µes sequenciais que a abordagem greedy ignora. Por exemplo, a sequ√™ncia de tags "I-PER B-ORG" √© inv√°lida em uma sequ√™ncia BIO. Um modelo greedy pode prever tais sequ√™ncias ao analisar cada tag isoladamente. O CRF, por sua vez, ao modelar a probabilidade de transi√ß√µes entre tags (como B-PER seguido de I-PER e n√£o B-ORG), imp√µe restri√ß√µes na sa√≠da, garantindo a validade das sequ√™ncias de tags preditas. Portanto, um CRF pode levar a um desempenho superior em casos onde a consist√™ncia da sequ√™ncia √© importante.
‚ñ†

**Tokeniza√ß√£o e Alinhamento de R√≥tulos**
Um desafio surge quando se usa modelos baseados em *subwords* como o WordPiece [^17]. As *named entities* s√£o geralmente rotuladas no n√≠vel da palavra, enquanto os modelos de linguagem operam no n√≠vel do *subword*, o que causa um desalinhamento [^17].
Para lidar com esse desalinhamento, durante o treinamento, um r√≥tulo de n√≠vel de palavra √© atribu√≠do a todos os *subwords* derivados dessa palavra. Durante a decodifica√ß√£o, a abordagem mais simples √© usar o r√≥tulo associado ao primeiro *subword* da palavra [^18]. M√©todos mais complexos combinam as distribui√ß√µes de r√≥tulos de todos os *subwords* para gerar uma previs√£o de r√≥tulo de n√≠vel de palavra otimizada [^18].

> üí° **Exemplo Num√©rico:**
>
> Considere a palavra "Villanueva". Um tokenizador WordPiece pode dividir em "Villa", "##nue", "##va".
>
> Se a palavra "Villanueva" estiver rotulada como `I-PER`, no treinamento, os *subwords* "Villa", "##nue", "##va" ser√£o rotulados como `I-PER`.
>
> Durante a decodifica√ß√£o, usando a abordagem mais simples, o r√≥tulo `I-PER` associado ao primeiro *subword* "Villa" seria atribu√≠do √† palavra inteira "Villanueva".
>
> Uma abordagem mais complexa pode calcular a m√©dia das probabilidades dos r√≥tulos para cada *subword*, ou usar a probabilidade do r√≥tulo mais frequente.
>
> Por exemplo, vamos supor que o modelo gere as seguintes probabilidades para os *subwords*:
>
> - "Villa": `[0.1, 0.8, 0.05, 0.02, 0.03]` (correspondendo √†s probabilidades de `B-PER`, `I-PER`, `B-LOC`, `I-LOC`, `O`)
> - "##nue": `[0.05, 0.9, 0.02, 0.01, 0.02]`
> - "##va": `[0.03, 0.85, 0.04, 0.03, 0.05]`
>
> A m√©dia das probabilidades seria: `[0.06, 0.88, 0.04, 0.02, 0.03]`, ainda resultando em `I-PER` como o r√≥tulo mais prov√°vel.  A escolha do r√≥tulo mais frequente entre os *subwords* tamb√©m seria `I-PER` neste caso.

**Proposi√ß√£o 1**
A agrega√ß√£o das distribui√ß√µes de probabilidade dos r√≥tulos de *subwords* pode ser realizada por diferentes m√©todos, como a m√©dia das probabilidades, ou utilizando a probabilidade do r√≥tulo mais frequente ou com maior score. Cada m√©todo pode ter vantagens e desvantagens dependendo da distribui√ß√£o de *subwords* e da tarefa.

**Prova:**
A prova √© baseada na avalia√ß√£o emp√≠rica dos diferentes m√©todos de agrega√ß√£o.
I.  A m√©dia de probabilidades pode suavizar o ru√≠do na predi√ß√£o.
II. A escolha do r√≥tulo mais frequente pode aumentar a confian√ßa na predi√ß√£o, caso os *subwords* da palavra estejam consistentemente relacionados a um r√≥tulo.
III. Utilizar a probabilidade m√°xima pode levar a erros se a distribui√ß√£o dos *subwords* estiver ruidosa.
IV. A escolha do m√©todo ideal √© emp√≠rica e depende da distribui√ß√£o dos *subwords*, da tarefa e do modelo de linguagem.
V. M√©todos que levam em conta as particularidades da tokeniza√ß√£o e das distribui√ß√µes geradas pelo modelo podem levar a melhores resultados.
‚ñ†

**Avalia√ß√£o de NER**
Os modelos de NER s√£o avaliados utilizando m√©tricas de **recall**, **precision** e **F1-measure** [^18]. O recall √© a propor√ß√£o de respostas corretamente rotuladas em rela√ß√£o ao total que deveria ter sido rotulado. A precis√£o √© a propor√ß√£o de respostas corretamente rotuladas em rela√ß√£o ao total rotulado e o F1-measure √© a m√©dia harm√¥nica dos dois [^18].

A avalia√ß√£o do NER se concentra em entidades, n√£o em palavras individuais, considerando cada entidade como uma unidade de resposta [^18]. Por exemplo, se o modelo rotular "Jane" como pessoa mas n√£o "Jane Villanueva", contar√° um falso negativo para I-PER e um falso positivo para O [^18]. Isso gera um desafio de avalia√ß√£o pois a unidade de treinamento e avalia√ß√£o n√£o s√£o iguais.

> üí° **Exemplo Num√©rico:**
>
> Vamos analisar um exemplo de avalia√ß√£o. Suponha que tenhamos a seguinte frase com entidades reais:
>
> **Frase:** "Elon Musk, CEO da Tesla, visitou Berlim."
>
> **Entidades Reais:**
>   - "Elon Musk" (PER)
>   - "Tesla" (ORG)
>   - "Berlim" (LOC)
>
> Suponha que um modelo NER produza as seguintes entidades:
>
> **Entidades Preditas:**
>   - "Elon" (PER)
>   - "Tesla" (ORG)
>   - "Berlim" (LOC)
>
>
> **C√°lculo:**
>
>   -   **Verdadeiros Positivos (TP):**  "Tesla" e "Berlim" foram preditos corretamente. 2 TPs.
>   -   **Falsos Positivos (FP):**  "Elon" foi predito como PER, mas a entidade completa √© "Elon Musk". 1 FP
>   -   **Falsos Negativos (FN):** "Elon Musk" foi parcialmente predito, mas a entidade completa ( "Elon Musk") n√£o. 1 FN
>
> **M√©tricas:**
>
>   -   **Precis√£o:** $\frac{TP}{TP+FP} = \frac{2}{2+1} = 0.67$
>   -   **Recall:** $\frac{TP}{TP+FN} = \frac{2}{2+1} = 0.67$
>   -   **F1-measure:** $2 \times \frac{\text{Precis√£o} \times \text{Recall}}{\text{Precis√£o} + \text{Recall}} = 2 \times \frac{0.67 \times 0.67}{0.67+0.67} = 0.67$
>
> Agora, suponha outro modelo que previu:
>
> **Entidades Preditas:**
>   - "Elon Musk" (PER)
>   - "Tesla" (ORG)
>   - "Berlim" (LOC)
>
> Para este modelo, temos:
>
>  -   **Verdadeiros Positivos (TP):**  "Elon Musk", "Tesla" e "Berlim" foram preditos corretamente. 3 TPs.
>   -   **Falsos Positivos (FP):** 0 FPs
>   -   **Falsos Negativos (FN):** 0 FNs
>
> **M√©tricas:**
>
>   -   **Precis√£o:** $\frac{TP}{TP+FP} = \frac{3}{3+0} = 1.0$
>   -   **Recall:** $\frac{TP}{TP+FN} = \frac{3}{3+0} = 1.0$
>   -   **F1-measure:** $2 \times \frac{\text{Precis√£o} \times \text{Recall}}{\text{Precis√£o} + \text{Recall}} = 2 \times \frac{1.0 \times 1.0}{1.0+1.0} = 1.0$
>
> Este exemplo ilustra como as m√©tricas de avalia√ß√£o s√£o calculadas em NER, mostrando que o segundo modelo tem melhor desempenho.

**Corol√°rio 1**
Uma an√°lise detalhada dos erros cometidos pelo modelo NER pode revelar padr√µes ou dificuldades espec√≠ficas do modelo. Erros podem ser categorizados como falsos positivos, falsos negativos, erros de tipo, erros de limite, ou combina√ß√µes desses, fornecendo insights sobre como o modelo pode ser melhorado.

**Prova:**
A an√°lise de erros √© um componente essencial do desenvolvimento de modelos.
I. Categorizar os erros por tipo (falsos positivos, falsos negativos, erros de classifica√ß√£o de tipo, erros de limite) permite que se identifiquem padr√µes no comportamento do modelo e identifique tipos de erros recorrentes.
II. Por exemplo, um modelo pode ter dificuldade em reconhecer entidades com limites amb√≠guos, ou distinguir entre tipos semelhantes de entidades (por exemplo, LOC vs. GPE).
III. Essa an√°lise detalhada permite que se tomem decis√µes mais informadas sobre como melhorar o treinamento do modelo, como aumentar a quantidade de dados espec√≠ficos, ajustar as t√©cnicas de tokeniza√ß√£o ou explorar arquiteturas de modelos mais complexas.
‚ñ†

### Conclus√£o
Este cap√≠tulo detalhou o processo de fine-tuning para a tarefa de **Named Entity Recognition (NER)**, uma aplica√ß√£o essencial de sequence labeling. Foi apresentado que modelos de linguagem pr√©-treinados, combinados com t√©cnicas de sequence labeling como o BIO tagging, e camadas adicionais de classifica√ß√£o, podem ser efetivamente adaptados para a identifica√ß√£o e categoriza√ß√£o de *named entities*. Foi discutido o desafio de desalinhamento entre *subwords* e a necessidade de ajustar o treinamento e avalia√ß√£o. As m√©tricas de avalia√ß√£o como recall, precis√£o e F1-measure foram definidas para quantificar a qualidade dos modelos. O conhecimento sobre NER e sequence labeling usando modelos de linguagem pr√©-treinados abre caminho para construir aplica√ß√µes mais inteligentes e informadas em NLP.

### Refer√™ncias
[^12]:  No contexto anterior, foi discutido o conceito de *finetuning*, onde modelos pr√©-treinados s√£o adaptados para tarefas espec√≠ficas atrav√©s da adi√ß√£o de camadas especializadas e do ajuste dos par√¢metros usando dados rotulados.
[^13]:  O processo de *finetuning* envolve a adi√ß√£o de camadas espec√≠ficas da aplica√ß√£o (um *head*) sobre o modelo pr√©-treinado e o uso de dados rotulados para treinar esses par√¢metros adicionais, enquanto os par√¢metros do modelo pr√©-treinado s√£o geralmente ajustados de maneira m√≠nima ou congelados.
[^15]: A tarefa de reconhecimento de entidade nomeada (NER) √© um exemplo de sequence labeling que envolve a identifica√ß√£o de *spans* de texto que correspondem a nomes pr√≥prios (pessoas, locais, organiza√ß√µes) e a classifica√ß√£o desses *spans* em categorias pr√©-definidas.
[^16]:  O BIO tagging √© um m√©todo utilizado em tarefas de sequence labeling, como NER, para capturar as fronteiras e os tipos de *named entities*, usando tags como B-*Type* (in√≠cio de *span*), I-*Type* (dentro de um *span*) e O (fora de qualquer *span*).
[^17]: Em sequence labeling, a sa√≠da do modelo de linguagem para cada *token* √© passada para um classificador que produz uma distribui√ß√£o de probabilidade sobre os poss√≠veis r√≥tulos para esse *token*.
[^18]: A avalia√ß√£o de modelos de NER √© feita usando as m√©tricas de *recall*, *precision* e F1-measure.
<!-- END -->
