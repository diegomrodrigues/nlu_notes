## DeepSeekMath-Base 7B: Arquitetura e Treinamento

### Introdução
Este capítulo se aprofundará na arquitetura e no regime de treinamento do **DeepSeekMath-Base 7B**, um modelo base com fortes habilidades de raciocínio, especialmente em matemática, conforme introduzido no artigo "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [^1]. Detalharemos a inicialização, o corpus de treinamento e as configurações utilizadas para criar este modelo fundamental.

### Conceitos Fundamentais

O DeepSeekMath-Base 7B representa um avanço significativo no campo de modelos de linguagem para raciocínio matemático. De acordo com o texto [^8], o modelo é **inicializado** com o **DeepSeek-Coder-Base-v1.5 7B** (Guo et al., 2024), demonstrando que o *code training* prévio contribui para a capacidade de resolução de problemas matemáticos.

O **corpus de treinamento** do DeepSeekMath-Base 7B é uma mistura diversificada, com a seguinte distribuição [^8]:
*   56% do **DeepSeekMath Corpus**
*   4% do **AlgebraicStack**
*   10% de artigos do **arXiv**
*   20% de código do **Github**
*   10% de dados de linguagem natural do **Common Crawl**, tanto em inglês quanto em chinês.

A escolha de incluir uma variedade de fontes de dados é crucial para a capacidade do modelo de lidar com diversos problemas matemáticos e para sua proficiência em diferentes línguas. De acordo com o artigo, o *DeepSeekMath Corpus* é construído a partir de uma coleta sistemática de páginas web relacionadas a matemática, começando com um *seed corpus* de alta qualidade e expandindo iterativamente usando um classificador *fastText* [^4].

As **configurações de treinamento** do DeepSeekMath-Base 7B seguem as diretrizes especificadas na Seção 2.2.1 do artigo [^8]. O texto especifica que [^8]:
*   O modelo foi treinado por **500B tokens**
*   A **taxa de aprendizado** máxima é de **4.2e-4**
*   O **tamanho do lote** é de **10M tokens**.

A arquitetura do modelo é referenciada no artigo, porém sem muitos detalhes técnicos [^6].

A inclusão de tokens de código para treinamento contínuo é uma estratégia deliberada para preservar as capacidades de codificação do modelo. Conforme observado no texto [^10], o DeepSeekMath-Base 7B mantém efetivamente o desempenho do DeepSeek-Coder-Base-v1.5 nos benchmarks de codificação, demonstrando que essa estratégia não compromete suas habilidades matemáticas recém-adquiridas.

A eficácia dessa abordagem é apoiada pelas avaliações detalhadas apresentadas na Tabela 2 [^8], que demonstram a liderança do DeepSeekMath-Base 7B no desempenho em relação a outros modelos base de código aberto em benchmarks como GSM8K, MATH, OCW e SAT. Notavelmente, ele supera os modelos base existentes em mais de 10% no conjunto de dados MATH de nível de competição.

### Conclusão

O DeepSeekMath-Base 7B exemplifica a importância de uma inicialização cuidadosa, um conjunto de dados de treinamento bem selecionado e configurações de treinamento apropriadas para criar modelos de linguagem competentes em raciocínio matemático. A decisão de inicializar com um modelo de codificação, juntamente com a combinação estratégica de dados matemáticos, código e linguagem natural, resultou em um modelo base que supera seus pares de código aberto e se aproxima do desempenho de modelos fechados maiores, como o Minerva 540B. Este trabalho estabelece uma base sólida para o desenvolvimento de modelos de raciocínio matemático mais avançados e destaca o potencial de aproveitamento dos dados disponíveis publicamente para construir modelos de alto desempenho.

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^4]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^6]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^8]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^10]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->