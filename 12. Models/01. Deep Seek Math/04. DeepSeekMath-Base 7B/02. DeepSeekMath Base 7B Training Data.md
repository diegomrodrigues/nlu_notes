## DeepSeekMath-Base 7B: Habilidades de Raciocínio Avançadas
### Introdução
Em continuidade ao capítulo anterior, que introduziu as capacidades do modelo DeepSeekMath-Base 7B [^1], este capítulo aprofunda-se nos detalhes da sua construção e treinamento, com foco na mistura de dados utilizada e nos ajustes finos realizados na arquitetura do modelo. Como vimos anteriormente, o DeepSeekMath-Base 7B demonstra habilidades notáveis em raciocínio matemático, compreensão de linguagem natural e capacidades de programação. Este modelo se destaca por sua proficiência em gerar soluções matemáticas autocontidas, utilizar ferramentas para resolução de problemas e conduzir a prova formal de teoremas [^8].
### Conceitos Fundamentais
O DeepSeekMath-Base 7B é construído sobre a base do DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), representando uma evolução significativa no campo dos LLMs [^8]. A inicialização com o DeepSeek-Coder-Base-v1.5 7B demonstra a importância de um modelo base pré-treinado em código para o desenvolvimento de habilidades de raciocínio matemático.

O treinamento do DeepSeekMath-Base 7B envolveu 500B tokens, distribuídos da seguinte forma [^8]:
*   56% do DeepSeekMath Corpus
*   4% de AlgebraicStack
*   10% de arXiv
*   20% de código Github
*   10% de dados de linguagem natural do Common Crawl em inglês e chinês

Esta mistura diversificada de dados é crucial para o sucesso do modelo, permitindo que ele aprenda a raciocinar sobre matemática a partir de diferentes perspectivas e em diferentes idiomas. A inclusão de código Github e dados de linguagem natural também permite que o modelo mantenha suas habilidades de programação e compreensão de linguagem.

A arquitetura do modelo segue as configurações de treinamento especificadas na Seção 2.2.1 [^8]. A taxa de aprendizado máxima é ajustada para 4.2e-4, e o tamanho do lote é definido como 10M tokens [^8]. Esses ajustes finos são essenciais para otimizar o desempenho do modelo em tarefas matemáticas.

A inclusão de tokens de código para treinamento contínuo permite que o DeepSeekMath-Base 7B mantenha efetivamente o desempenho do DeepSeek-Coder-Base-v1.5 nos dois *benchmarks* de codificação [^10]. Isso demonstra a importância do treinamento contínuo para evitar o esquecimento catastrófico e garantir que o modelo mantenha suas habilidades em todas as áreas.
### Conclusão
O DeepSeekMath-Base 7B representa um avanço significativo no desenvolvimento de LLMs capazes de raciocínio matemático avançado. A sua construção cuidadosa, com base em DeepSeek-Coder-Base-v1.5 7B e treinado com uma mistura diversificada de dados, demonstra a importância de um modelo base pré-treinado e de dados de alta qualidade para o sucesso do modelo [^8]. A combinação de dados matemáticos, código e linguagem natural permite que o modelo aprenda a raciocinar sobre matemática a partir de diferentes perspectivas e em diferentes idiomas. Em adição a isto, a inclusão de *tokens* de código para treinamento contínuo permite que o DeepSeekMath-Base 7B mantenha efetivamente o desempenho do DeepSeek-Coder-Base-v1.5 nos dois *benchmarks* de codificação [^10].
### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^8]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024.
[^10]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024.
<!-- END -->