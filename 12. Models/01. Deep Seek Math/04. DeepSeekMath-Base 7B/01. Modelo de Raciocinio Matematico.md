## DeepSeekMath-Base 7B: Habilidades de Raciocínio Avançadas
### Introdução
O estudo de Large Language Models (LLMs) para raciocínio matemático tem ganhado destaque devido à sua complexidade inerente e natureza estruturada [^1]. Neste capítulo, exploramos o modelo DeepSeekMath-Base 7B, com foco em suas habilidades de raciocínio matemático, compreensão de linguagem natural e capacidades de programação. Este modelo se destaca por sua proficiência em gerar soluções matemáticas autocontidas, utilizar ferramentas para resolução de problemas e conduzir a prova formal de teoremas.
### Conceitos Fundamentais
O DeepSeekMath-Base 7B representa um avanço significativo no campo dos LLMs, demonstrando habilidades notáveis sem a necessidade de ferramentas externas. Este modelo é inicializado com DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024) e treinado em 500B tokens [^8]. A distribuição dos dados de treinamento é composta por 56% do DeepSeekMath Corpus, 4% de AlgebraicStack, 10% de arXiv, 20% de código Github e os restantes 10% de dados de linguagem natural do Common Crawl em inglês e chinês [^8].

O modelo é avaliado em diversos aspectos, incluindo [^8]:
*   **Resolução de Problemas Matemáticos com Raciocínio Passo a Passo:** Avaliação do desempenho na resolução de problemas matemáticos usando *few-shot chain-of-thought prompting* (Wei et al., 2022) em oito *benchmarks* em inglês e chinês, abrangendo raciocínio quantitativo e problemas de múltipla escolha.
*   **Resolução de Problemas Matemáticos com Uso de Ferramentas:** Avaliação do raciocínio matemático com auxílio de programas em GSM8K e MATH, utilizando *few-shot program-of-thought prompting* (Chen et al., 2022; Gao et al., 2023), onde o modelo escreve um programa em Python para resolver problemas complexos, utilizando bibliotecas como `math` e `sympy`.
*   **Matemática Formal:** Avaliação da tarefa de prova informal-para-formal (Jiang et al., 2022) no miniF2F (Zheng et al., 2021) com Isabelle (Wenzel et al., 2008) como assistente de prova.
*   **Compreensão de Linguagem Natural, Raciocínio e Código:** Avaliação no MMLU (Hendrycks et al., 2020), BBH (Suzgun et al., 2022), HumanEval (Chen et al., 2021) e MBPP (Austin et al., 2021).

Os resultados mostram que o DeepSeekMath-Base 7B supera os modelos de base de código aberto em todos os oito *benchmarks*, incluindo Mistral 7B (Jiang et al., 2023) e Llemma 34B (Azerbayev et al., 2023). Ele supera os modelos de base *open-source* existentes em mais de 10% absoluto no conjunto de dados MATH em nível de competição e supera o Minerva 540B (Lewkowycz et al., 2022a), que é um modelo de base *closed-source* 77 vezes maior e construído sobre PaLM (Lewkowycz et al., 2022b) [^8].

Ao avaliar a capacidade de resolução de problemas matemáticos com o uso de ferramentas, o DeepSeekMath-Base 7B supera o estado da arte anterior, Llemma 34B [^9]. Na tarefa de prova informal-para-formal, o DeepSeekMath-Base 7B também demonstra forte desempenho na autoformalização de provas [^9].

Além disso, o modelo demonstra melhorias significativas no MMLU e BBH em relação ao seu precursor, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024), indicando o impacto positivo do treinamento em matemática na compreensão e raciocínio da linguagem [^10]. A inclusão de tokens de código para treinamento contínuo permite que o DeepSeekMath-Base 7B mantenha efetivamente o desempenho do DeepSeek-Coder-Base-v1.5 nos dois *benchmarks* de codificação [^10].

Em resumo, o DeepSeekMath-Base 7B demonstra fortes habilidades de raciocínio, especialmente em matemática, sem depender de ferramentas externas [^8]. Também demonstra bom desempenho em compreensão de linguagem natural, raciocínio e habilidades de programação [^8].
### Conclusão
O DeepSeekMath-Base 7B representa um marco importante no desenvolvimento de LLMs capazes de raciocínio matemático avançado. Sua capacidade de gerar soluções autocontidas, utilizar ferramentas e realizar provas formais, juntamente com seu desempenho sólido em outras tarefas de NLU e programação, demonstra o potencial de modelos especializados treinados em *corpora* de alta qualidade para resolver problemas complexos. No entanto, os resultados mostram que os documentos do arXiv parecem ineficazes na melhoria do raciocínio matemático [^17]. Os modelos treinados num *corpus* somente arXiv não mostram melhorias notáveis ou mesmo deterioração em vários *benchmarks* matemáticos de diferentes complexidades.
### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^8]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024.
[^9]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024.
[^10]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024.
[^17]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models, 2024.
<!-- END -->