## Group Relative Policy Optimization (GRPO): Otimização e Regularização

### Introdução
Em continuidade ao estudo de Large Language Models (LLMs) e Natural Language Understanding, e particularmente a modelos voltados para o raciocínio matemático, exploraremos em detalhes o Group Relative Policy Optimization (GRPO). GRPO é uma variação do Proximal Policy Optimization (PPO) [^26] projetada para aprimorar o raciocínio matemático e otimizar o uso de memória simultaneamente. Conforme mencionado na abstract [^1], o GRPO é fundamental para o desempenho do DeepSeekMath 7B.

### Conceitos Fundamentais

O GRPO surge como uma alternativa eficiente ao PPO, especialmente no contexto de LLMs [^2, ^8]. A principal diferença reside na forma como a *baseline* é estimada. Enquanto o PPO tradicional utiliza um *critic model* para essa finalidade, o GRPO abandona essa abordagem e estima a *baseline* a partir das *group scores* [^3, ^1]. Essa simplificação reduz significativamente os recursos de treinamento necessários [^3, ^1].

**O Objetivo Substituto do GRPO**
O objetivo do GRPO é otimizar os LLMs, maximizando o seguinte objetivo substituto [^11]:

$$
I_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \left[ \frac{1}{G} \sum_{i=1}^G \sum_{t=1}^M \min\left(\frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})} A_{i,t}, \text{clip}\left(\frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\epsilon, 1+\epsilon\right) A_{i,t}\right) - \beta D_{KL}[\pi_\theta || \pi_{ref}] \right]
$$

onde:

*   $q$ representa a questão (*question*) [^11].
*   $P(Q)$ denota a distribuição das questões [^11].
*   $\{o_i\}_{i=1}^G$ representa um grupo de $G$ saídas (*outputs*) amostradas da política antiga $\pi_{\theta_{old}}$ dado a questão $q$ [^11].
*   $\pi_\theta(o_{i,t}|q, o_{i,<t})$ é a probabilidade da saída $o_{i,t}$ no passo $t$, dado a questão $q$ e as saídas anteriores $o_{i,<t}$ usando a política atual $\pi_\theta$ [^11].
*   $\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})$ é a probabilidade da saída $o_{i,t}$ no passo $t$, dado a questão $q$ e as saídas anteriores $o_{i,<t}$ usando a política antiga $\pi_{\theta_{old}}$ [^11].
*   $A_{i,t}$ é a vantagem (*advantage*) no passo $t$ para a saída $o_i$ [^11].
*   $\text{clip}(x, 1-\epsilon, 1+\epsilon)$ é a função de *clipping* que limita a razão de probabilidade entre $1-\epsilon$ e $1+\epsilon$, onde $\epsilon$ é um hiperparâmetro [^11].
*   $D_{KL}[\pi_\theta || \pi_{ref}]$ representa a divergência de Kullback-Leibler (KL) entre a política atual $\pi_\theta$ e uma política de referência $\pi_{ref}$ [^11].
*   $\beta$ é um coeficiente que controla a força da penalidade KL [^11].

**Componentes e Interpretação**

O objetivo substituto do GRPO é composto por dois termos principais:

1.  **Função de Clipe:** O primeiro termo dentro do somatório envolve a função de clipe (*clipping function*). Esta função limita o quão longe a razão entre a política atual e a política antiga pode se desviar de 1. O objetivo dessa função é evitar atualizações muito grandes na política, que podem levar à instabilidade no treinamento [^11, ^26]. A vantagem $A_{i,t}$ atua como um multiplicador, incentivando ações que levam a recompensas mais altas e desencorajando ações que levam a recompensas mais baixas [^11].

2.  **Penalidade KL:** O segundo termo é uma penalidade de divergência KL (*KL divergence penalty*). A divergência KL mede a diferença entre duas distribuições de probabilidade. Neste caso, mede a diferença entre a política atual $\pi_\theta$ e uma política de referência $\pi_{ref}$ [^11]. A política de referência é geralmente a política antes da atualização. Ao adicionar uma penalidade KL, o GRPO incentiva a política atual a permanecer próxima da política de referência. Isso ajuda a estabilizar o treinamento e evitar o *overfitting* [^11].

**Vantagens e Group Scores**

No GRPO, a vantagem $A_{i,t}$ é calculada com base nas recompensas relativas das saídas dentro de cada grupo [^11]. Isso se alinha bem com a natureza comparativa dos *reward models*, que são frequentemente treinados em conjuntos de dados de comparações entre saídas da mesma questão [^11]. Em vez de adicionar uma penalidade KL na recompensa, o GRPO regulariza diretamente adicionando a divergência KL entre a política treinada e a política de referência à perda, evitando complicar o cálculo de $A_{i,t}$ [^11].

### Conclusão

O GRPO apresenta uma alternativa eficiente e eficaz ao PPO tradicional, especialmente para o ajuste fino de LLMs para tarefas complexas como o raciocínio matemático. Ao evitar o uso de um modelo crítico e estimar a *baseline* a partir de *group scores*, o GRPO reduz os custos computacionais e de memória, tornando-o uma opção prática para o treinamento de modelos de linguagem em larga escala. A maximização do objetivo substituto, que inclui uma função de clipe e uma penalidade KL, garante um treinamento estável e evita o *overfitting*. A utilização de recompensas relativas dentro de cada grupo alinha-se bem com a natureza comparativa dos *reward models*, melhorando ainda mais o desempenho do GRPO. Os resultados obtidos com o DeepSeekMath [^1] demonstram o potencial do GRPO para aprimorar o raciocínio matemático e outras capacidades complexas dos LLMs.

### Referências
[^1]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. 2024.
[^2]: Anil et al. Gemini: A family of highly capable multimodal models. 2023.
[^3]: DeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. 2024.
[^26]: Schulman et al. Proximal policy optimization algorithms. 2017.
<!-- END -->