## Group Relative Policy Optimization (GRPO)

### Introdução

Este capítulo aprofunda o conceito de **Group Relative Policy Optimization (GRPO)**, um algoritmo de *reinforcement learning (RL)* inovador projetado para aprimorar as capacidades de raciocínio matemático de *Large Language Models (LLMs)* após a fase de *Supervised Fine-Tuning (SFT)* [^1, ^11]. O GRPO, conforme apresentado em DeepSeekMath [^1], oferece uma alternativa eficiente ao *Proximal Policy Optimization (PPO)*, reduzindo significativamente a demanda por recursos computacionais. Este capítulo explora os mecanismos subjacentes do GRPO, destacando sua capacidade de otimizar o raciocínio matemático, evitando a complexidade de um modelo *critic* tradicional [^1, ^13].

### Conceitos Fundamentais

#### A Necessidade de Reinforcement Learning Após Supervised Fine-Tuning
Após o *Supervised Fine-Tuning (SFT)*, os *Large Language Models (LLMs)* demonstram um desempenho notável em uma variedade de tarefas. No entanto, otimizar esses modelos para raciocínio matemático complexo requer refinamentos adicionais. O *Reinforcement Learning (RL)* surge como uma abordagem poderosa para aprimorar ainda mais esses modelos, guiando-os para estratégias de resolução de problemas mais eficazes [^1, ^11].

#### Proximal Policy Optimization (PPO) e suas limitações
O *Proximal Policy Optimization (PPO)* é um algoritmo *actor-critic RL* amplamente utilizado no *fine-tuning RL* de *LLMs* [^11]. O PPO maximiza um objetivo substituto que equilibra a exploração e a explotação, atualizando iterativamente a política enquanto a mantém próxima da política antiga [^1, ^11]. No entanto, o PPO tradicional incorpora um modelo *critic* para estimar a função de valor, introduzindo complexidade computacional significativa [^1]. O *value function* empregado no PPO é tipicamente outro modelo de tamanho comparável ao modelo de política, trazendo uma carga substancial de memória e computacional [^13].

#### Group Relative Policy Optimization (GRPO): Uma Abordagem Eficiente
O *Group Relative Policy Optimization (GRPO)* é uma variante do *PPO* que aborda as limitações do PPO tradicional eliminando a necessidade de um modelo *critic* [^1]. Em vez de depender de um modelo *critic*, o GRPO estima a linha de base a partir de *group scores*, reduzindo substancialmente os recursos de treinamento em comparação com o PPO [^1].

**Mecanismo do GRPO**: Para cada questão $q$, o GRPO amostra um grupo de outputs $\{o_1, o_2, \dots, o_G\}$ da política antiga $\pi_{\theta_{old}}$ e então otimiza o modelo de política maximizando o seguinte objetivo [^13]:

$$
I_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \left[ \frac{1}{G} \sum_{i=1}^G \sum_{t=1}^M \min \left( \frac{\pi_{\theta}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})} A_{i,t}, clip\left( \frac{\pi_{\theta}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\epsilon, 1+\epsilon \right) A_{i,t} \right) - \beta D_{KL}[\pi_{\theta}||\pi_{ref}] \right]
$$

Onde $\epsilon$ e $\beta$ são hiperparâmetros, e $A_{i,t}$ é a vantagem calculada baseada em recompensas relativas dos outputs dentro de cada grupo.

**Vantagens do GRPO**:

1.  **Eficiência Computacional**: Ao eliminar o modelo *critic*, o GRPO reduz a demanda por memória e computação, tornando-o mais adequado para treinamento em larga escala de LLMs [^13].

2.  **Alinhamento com Modelos de Recompensa**: A forma relativa do GRPO de calcular as vantagens alinha-se bem com a natureza comparativa dos modelos de recompensa, que são normalmente treinados em conjuntos de dados de comparações entre outputs na mesma questão [^13].

3.  **Estabilização do Treinamento**: Em vez de adicionar uma penalidade KL na recompensa, o GRPO regulariza adicionando diretamente a divergência KL entre a política treinada e a política de referência na perda, evitando complicar o cálculo de $A_{i,t}$ [^13].

4.  **Melhora no Desempenho**: Os resultados experimentais demonstram que o GRPO melhora significativamente o desempenho dos modelos *instruction-tuned*, atingindo resultados de última geração em benchmarks de raciocínio matemático [^1].

### Conclusão

O **Group Relative Policy Optimization (GRPO)** representa um avanço significativo no treinamento de LLMs para raciocínio matemático [^1]. Ao evitar a necessidade de um modelo *critic* e estimar a linha de base a partir de *group scores*, o GRPO alcança eficiência computacional e desempenho aprimorado [^1]. O alinhamento do GRPO com modelos de recompensa e sua abordagem de regularização KL contribuem para sua eficácia em aprimorar as capacidades de raciocínio matemático dos LLMs [^13]. O GRPO é uma técnica promissora para *reinforcement learning* de *LLMs*, oferecendo um equilíbrio entre desempenho e eficiência computacional, tornando-o uma ferramenta valiosa para aprimorar o raciocínio matemático em *Large Language Models (LLMs)* [^1].

### Referências
[^1]: Zhihong Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^11]: Luo, H. et al. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.
[^13]: Schulman, J. et al. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
<!-- END -->