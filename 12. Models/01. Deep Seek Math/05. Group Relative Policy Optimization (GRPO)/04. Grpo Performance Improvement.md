## Melhoria de Desempenho com GRPO: Dentro e Fora do Domínio

### Introdução

Este capítulo, em continuidade à nossa exploração do Group Relative Policy Optimization (GRPO) no contexto do DeepSeekMath, examina o impacto significativo do GRPO no desempenho do modelo, tanto dentro quanto fora do domínio do ajuste fino por instrução. O GRPO, como vimos, é uma técnica de aprendizado por reforço (RL) que aprimora o raciocínio matemático e simultaneamente otimiza o uso da memória, representando um avanço importante na aplicação de RL a LLMs [^11]. Exploraremos como o GRPO, mesmo utilizando um subconjunto dos dados de ajuste fino por instrução, consegue um aumento substancial no desempenho, e como esse aumento se manifesta não apenas em tarefas de raciocínio matemático, mas também em tarefas fora do domínio.

### Conceitos Fundamentais

O DeepSeekMath-Instruct, resultado do ajuste fino supervisionado (SFT) [^1], serve como ponto de partida para a aplicação do GRPO. Uma das características notáveis do GRPO é sua capacidade de obter melhorias significativas mesmo quando treinado apenas com um subconjunto dos dados de ajuste fino por instrução. Isso implica uma eficiência considerável em termos de recursos computacionais e de dados, tornando o GRPO uma opção atraente para o ajuste fino de LLMs em larga escala. Especificamente, o GRPO no DeepSeekMath é treinado nos dados do formato de cadeia de pensamento (chain-of-thought) relacionados a GSM8K e MATH dos dados SFT, o que consiste em cerca de 144 mil questões.

**Desempenho Dentro do Domínio**

O foco principal do treinamento com GRPO no DeepSeekMath é o aprimoramento das capacidades de raciocínio matemático. As tarefas de raciocínio matemático, como aquelas encontradas nos datasets GSM8K e MATH, são consideradas tarefas *dentro do domínio* para o GRPO, já que os dados utilizados no treinamento do GRPO são derivados dos dados de SFT que se concentram nessas áreas [^1, ^11].

**Desempenho Fora do Domínio**

Um achado notável é que o treinamento com GRPO não apenas melhora o desempenho dentro do domínio, mas também resulta em ganhos em tarefas de raciocínio matemático que estão *fora do domínio* dos dados de treinamento do GRPO. Esses benchmarks podem incluir desafios em áreas como raciocínio de senso comum, compreensão de linguagem natural ou tarefas de resolução de problemas que não são diretamente abordadas nos dados de treinamento do GRPO. Exemplos notórios incluem o CMATH (Chinês) [^1] e o MMLU (Massive Multitask Language Understanding) [^1].

**Mecanismo de Generalização**

O mecanismo subjacente à generalização obtida pelo GRPO reside na sua capacidade de refinar a política do modelo de uma forma que promove a *robustez* e a *transferência de conhecimento*. O processo de otimização do GRPO, ao focar nas relações relativas entre diferentes saídas (através das *group scores*), capacita o modelo a aprender estratégias de raciocínio mais generalizáveis [^11]. Em outras palavras, o modelo não está simplesmente memorizando soluções específicas para problemas de treinamento, mas sim aprendendo a abordar uma ampla gama de desafios de raciocínio de uma perspectiva mais fundamental.

**Exemplo de melhorias com GRPO e SFT para um melhor entendimento (com dados do contexto)**

As tabelas 5 (com e sem tool use) demonstram claramente as melhorias no desempenho de SFT para RL [^1].

| Modelo                 | Size | GSM8K   | MATH    | MGSM-zh | CMATH   |
| ---------------------- | ---- | ------- | ------- | ------- | ------- |
| DeepSeekMath-Instruct | 7B   | 82.9%   | 46.8%   | 73.2%   | 84.6%   |
| DeepSeekMath-RL       | 7B   | **88.2%**   | **51.7%**   | **79.6%**   | **88.8%**   |

As melhorias são claras tanto com e sem tool use.

### Conclusão

A capacidade do GRPO de obter melhorias substanciais no desempenho, mesmo com um subconjunto de dados de ajuste fino e de generalizar para tarefas fora do domínio, ressalta a eficácia desta técnica como uma ferramenta valiosa para aprimorar as capacidades de raciocínio matemático de LLMs [^11]. Ao aprender estratégias de raciocínio generalizáveis, o GRPO permite que os modelos abordem uma ampla gama de desafios de raciocínio com maior confiança e precisão. Os resultados obtidos com o DeepSeekMath [^1] servem como um testemunho do potencial do GRPO para impulsionar o progresso no campo de Natural Language Understanding e Inteligência Artificial.

### Referências
[^1]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. 2024.
[^11]: Schulman et al. Proximal policy optimization algorithms. 2017.
<!-- END -->