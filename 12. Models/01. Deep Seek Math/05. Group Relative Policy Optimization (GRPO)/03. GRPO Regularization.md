## Group Relative Policy Optimization (GRPO): Regularização via Divergência KL

### Introdução

Em continuidade à análise detalhada do Group Relative Policy Optimization (GRPO) e sua aplicação em LLMs para raciocínio matemático [^3, ^1], este capítulo se aprofundará na técnica específica de regularização utilizada no GRPO, que se distingue pela adição direta da divergência KL entre a política treinada e a política de referência à função de perda. Esta abordagem inovadora visa simplificar o cálculo do termo de vantagem ($A_{i,t}$), elemento crucial na otimização da política.

### Conceitos Fundamentais

Como discutido anteriormente, o GRPO é uma variação do PPO que otimiza LLMs maximizando o seguinte objetivo substituto [^11]:

$$
I_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \left[ \frac{1}{G} \sum_{i=1}^G \sum_{t=1}^M \min\left(\frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})} A_{i,t}, \text{clip}\left(\frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\epsilon, 1+\epsilon\right) A_{i,t}\right) - \beta D_{KL}[\pi_\theta || \pi_{ref}] \right]
$$

A principal diferença entre o GRPO e o PPO reside na estratégia de regularização [^11]. Enquanto o PPO tradicionalmente incorpora uma penalidade KL *dentro* do cálculo da recompensa, o GRPO opta por uma abordagem mais direta, adicionando a divergência KL diretamente à função de perda [^11].

**Regularização Direta vs. Indireta**

A regularização indireta, comumente empregada no PPO, envolve a modificação da recompensa $r_t$ no passo *t* através da introdução de uma penalidade KL [^11, ^26]:

$$
r_t = r_q(q, o_{<t}) - \beta \log \frac{\pi_\theta(o_t|q, o_{<t})}{\pi_{ref}(o_t|q, o_{<t})}
$$

Esta formulação afeta o cálculo da vantagem $A_t$, uma vez que a vantagem é definida em termos das recompensas futuras e do valor estimado do estado [^11, ^26]. A regularização indireta, portanto, impacta a vantagem de forma complexa.

Em contraste, o GRPO adota uma regularização *direta*, adicionando a divergência KL como um termo separado na função de perda, conforme demonstrado na equação do objetivo substituto. Isto significa que o termo de vantagem $A_{i,t}$ é computado *independentemente* da divergência KL.

**Benefícios da Regularização Direta**

A regularização direta oferece diversas vantagens potenciais:

1.  **Simplicidade:** Ao separar a regularização da computação da vantagem, o GRPO simplifica o processo de treinamento. A vantagem $A_{i,t}$ pode ser calculada utilizando métodos padrão, sem a necessidade de compensar a penalidade KL [^11].

2.  **Interpretabilidade:** A regularização direta torna mais fácil a interpretação do impacto da penalidade KL no treinamento. Ao observar a magnitude do termo $D_{KL}[\pi_\theta || \pi_{ref}]$ na função de perda, é possível quantificar diretamente o quão longe a política treinada está se desviando da política de referência [^11].

3.  **Flexibilidade:** A regularização direta oferece maior flexibilidade no ajuste da força da penalidade KL. O coeficiente $\beta$ pode ser otimizado independentemente de outros hiperparâmetros [^11].

**Implementação Prática**

A implementação da regularização direta no GRPO é relativamente simples. Durante cada iteração de treinamento, a divergência KL entre a política atual e a política de referência é calculada e adicionada à função de perda, ponderada pelo coeficiente $\beta$ [^11]. A política é então atualizada utilizando um algoritmo de otimização padrão, como Adam [^6].

### Conclusão

A escolha da regularização via adição direta da divergência KL no GRPO representa uma decisão de design fundamental que contribui para a sua eficiência e estabilidade [^11]. Ao evitar a complexidade da incorporação da penalidade KL no cálculo da recompensa e da vantagem, o GRPO simplifica o processo de treinamento e facilita a interpretação e o ajuste da regularização. Esta abordagem inovadora, combinada com a estimativa da *baseline* a partir das *group scores*, permite ao GRPO alcançar um desempenho superior no ajuste fino de LLMs para tarefas de raciocínio matemático, como demonstrado pelo DeepSeekMath [^1].

### Referências
[^1]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. 2024.
[^3]: DeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. 2024.
[^6]: Loshchilov and Hutter. Decoupled weight decay regularization. 2017.
[^11]: Shao et al. "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models." 2024.
[^26]: Schulman et al. Proximal policy optimization algorithms. 2017.

<!-- END -->