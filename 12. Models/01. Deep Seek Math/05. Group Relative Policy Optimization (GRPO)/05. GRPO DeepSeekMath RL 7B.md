## GRPO no Treinamento do DeepSeekMath-RL 7B: Precisão Aprimorada em Raciocínio Matemático

### Introdução

Este capítulo centra-se na aplicação do Group Relative Policy Optimization (GRPO) no treinamento do DeepSeekMath-RL 7B, um modelo de linguagem projetado para raciocínio matemático avançado [^1]. Exploraremos como o GRPO contribui para o desempenho excepcional do modelo, atingindo precisões notáveis de 88.2% em GSM8K e 51.7% em MATH [^1], utilizando raciocínio em cadeia de pensamento (*chain-of-thought*).

### Conceitos Fundamentais

Como já discutido, o GRPO [^11] surge como uma alternativa eficaz ao Proximal Policy Optimization (PPO) para ajuste fino de LLMs, minimizando a necessidade de recursos computacionais intensivos. Sua principal característica é a estimativa da *baseline* a partir de *group scores*, que se alinha com a natureza comparativa dos *reward models* [^1].

No DeepSeekMath-RL 7B, o GRPO é empregado para refinar o modelo instruído (DeepSeekMath-Instruct 7B), usando os dados de treinamento do RL, que são perguntas no formato de *cadeia de pensamento* relacionadas a GSM8K e MATH do conjunto de dados SFT [^1]. O modelo é treinado com uma taxa de aprendizado de 1e-6 e um coeficiente KL de 0.04. Para cada pergunta, 64 saídas são amostradas, e o comprimento máximo é definido como 1024. O modelo de política passa por uma única atualização após cada etapa de exploração [^1].

**Desempenho Superior com GRPO**

Os resultados mostram que o DeepSeekMath-RL 7B alcança precisões de 88.2% e 51.7% em GSM8K e MATH, respectivamente [^1]. Esse desempenho supera todos os modelos de código aberto na faixa de 7B a 70B, bem como a maioria dos modelos proprietários [^1]. Notavelmente, o DeepSeekMath-RL 7B é treinado apenas com dados de ajuste de instrução no formato de cadeia de pensamento de GSM8K e MATH, começando com DeepSeekMath-Instruct 7B [^1]. Apesar do escopo limitado de seus dados de treinamento, ele supera o DeepSeekMath-Instruct 7B em todas as métricas de avaliação, demonstrando a eficácia do aprendizado por reforço com GRPO [^1].

**Raciocínio em Cadeia de Pensamento (*Chain-of-Thought Reasoning*)**

O raciocínio em cadeia de pensamento (CoT) é uma técnica que envolve a geração de etapas intermediárias de raciocínio, levando à solução final [^1, ^11]. Ao decompor problemas complexos em etapas menores e mais gerenciáveis, o CoT permite que os LLMs explorem o processo de raciocínio com mais profundidade e gerem soluções mais precisas [^1]. A utilização de CoT no treinamento do DeepSeekMath-RL 7B contribui significativamente para sua capacidade de raciocínio matemático avançado [^1].

**Iterative RL com GRPO**

À medida que o processo de treinamento de aprendizado por reforço avança, o modelo de recompensa antigo pode não ser suficiente para supervisionar o modelo de política atual [^1]. Portanto, também é explorado o RL iterativo com GRPO [^1]. Em GRPO iterativo, novos conjuntos de treinamento para o modelo de recompensa são gerados com base nos resultados de amostragem do modelo de política e treinam continuamente o modelo de recompensa antigo usando um mecanismo de repetição que incorpora 10% de dados históricos. Em seguida, definimos o modelo de referência como o modelo de política e continuamente treinamos o modelo de política com o novo modelo de recompensa [^1].

**O Algoritmo 1 (do paper) sumariza a abordagem**:

1.  **Entrada**: modelo de política inicial  $\pi_{\theta_{init}}$; modelos de recompensa $r_{\phi}$; prompts de tarefa $D$; hiperparâmetros $\epsilon, \beta, \mu$
2.  **Saída**: modelo de política $\pi_\theta$

O algoritmo então itera pelas seguintes etapas:

1.  inicialize o modelo de política $\pi_\theta \leftarrow \pi_{\theta_{init}}$

2.  **para** iteração = 1, ..., I **faça**:

    a. Amostre um lote  $D_b$  de  $D$

    b. Atualize o antigo modelo de política $\pi_{\theta_{old}} \leftarrow \pi_\theta$

    c. Amostre saídas G  $\{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(\cdot | q)$  para cada pergunta  $q \in D_b$

    d. Compute recompensas  $\{r_i\}_1^G$  para cada saída amostrada  $o_i$  executando  $r_\phi$

    e. Compute  $A_{i,t}$  para o t-ésimo token de  $o_i$  através da estimativa de vantagem relativa do grupo

    f. **para** iteração GRPO = 1, ...,  $\mu$  **faça**:

    i. Atualize o modelo de política  $\pi_\theta$  maximizando o objetivo GRPO

    g. Atualize $r_\phi$ através de treinamento contínuo usando um mecanismo de repetição

Em outras palavras, o processo de Iterative RL visa aumentar a robustez do modelo ao introduzir um ciclo de feedback no qual o modelo de política é usado para gerar dados para treinar o modelo de recompensa, que por sua vez é usado para treinar o modelo de política [^11].

### Conclusão

A aplicação do GRPO no treinamento do DeepSeekMath-RL 7B resulta em melhorias notáveis em precisão e desempenho [^1]. As altas precisões alcançadas em GSM8K e MATH demonstram a capacidade do GRPO de refinar as capacidades de raciocínio matemático do modelo, aproveitando o raciocínio em cadeia de pensamento e as *group scores* [^11]. O GRPO é uma ferramenta valiosa para aprimorar as habilidades de raciocínio matemático de LLMs, proporcionando avanços significativos no campo da inteligência artificial [^1].

### Referências
[^1]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. 2024.
[^11]: Schulman et al. Proximal policy optimization algorithms. 2017.
<!-- END -->