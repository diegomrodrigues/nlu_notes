## Experimentos de Online vs. Offline Training no GRPO: Análise e Implicações

### Introdução

Este capítulo visa aprofundar a compreensão dos elementos essenciais que impulsionam o sucesso do aprendizado por reforço (RL) na otimização de Large Language Models (LLMs) para raciocínio matemático, com um foco específico nos experimentos de *online vs. offline training* conduzidos no contexto do Group Relative Policy Optimization (GRPO). O objetivo é elucidar por que o RL melhora o desempenho dos modelos ajustados por instrução e identificar potenciais direções para alcançar um RL ainda mais eficaz, conforme sumariado no artigo "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" [^1]. A diferenciação entre *online* e *offline training* reside na fonte dos dados utilizados para o treinamento da política.

### Conceitos Fundamentais

No contexto do GRPO, como vimos, busca-se maximizar o seguinte objetivo substituto [^11]:

$$
I_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \left[ \frac{1}{G} \sum_{i=1}^G \sum_{t=1}^M \min\left(\frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})} A_{i,t}, \text{clip}\left(\frac{\pi_\theta(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\epsilon, 1+\epsilon\right) A_{i,t}\right) - \beta D_{KL}[\pi_\theta || \pi_{ref}] \right]
$$

A principal diferença entre os regimes de *online* e *offline training* reside na origem dos dados amostrados para o conjunto de treinamento, representados pelo termo $q \sim P(Q)$ na formulação acima.

**Definição de Online vs. Offline Sampling (Data Source)**

*   **Online Sampling:** Refere-se a um processo onde os dados de treinamento são gerados *em tempo real* pela política que está sendo treinada [^11]. Em cada iteração, o modelo explora o ambiente e utiliza suas próprias ações (com base na política atual) para coletar dados de treinamento. Essa abordagem permite que o modelo se adapte dinamicamente ao seu próprio comportamento, explorando regiões do espaço de estados que são relevantes para sua política atual [^1].

*   **Offline Sampling:** Por outro lado, envolve o uso de um conjunto de dados *pré-existente* e fixo para o treinamento. Os dados não são influenciados pelo comportamento do modelo que está sendo treinado. O modelo aprende a partir de experiências passadas, mas não tem a capacidade de explorar ativamente o ambiente com base em seu próprio desempenho [^1].

A tabela abaixo sumariza as principais diferenças entre Online e Offline RL no contexto do GRPO:

| Característica        | Online RL                                   | Offline RL                                      |
| --------------------- | -------------------------------------------- | ----------------------------------------------- |
| Fonte dos Dados        | Exploração em tempo real pela política atual | Conjunto de dados fixo e pré-existente           |
| Adaptação              | Adapta-se dinamicamente ao comportamento     | Não se adapta ao comportamento                     |
| Exploração             | Explora ativamente o ambiente               | Não explora ativamente o ambiente                  |
| Potencial de Descoberta | Maior potencial para descobrir soluções novas | Limitado ao conhecimento contido no conjunto fixo |

#### Implementação de Online e Offline RL no DeepSeekMath

No contexto do DeepSeekMath, os experimentos de *online vs. offline training* [^1] visam entender melhor como a escolha da fonte de dados afeta o desempenho do modelo. O regime de offline RL usa outputs amostrados do SFT.

#### Resultados Experimentais

Ao analisar os resultados experimentais apresentados no artigo "DeepSeekMath", observa-se que o *online training* frequentemente supera o *offline training* em termos de desempenho [^1]. A Figura 5 [^1] demonstra que o Online RFT é comparável ao RFT (Offline) no estágio inicial de treinamento, mas ganha uma vantagem absoluta nos estágios posteriores [^1].

**Interpretação dos Resultados**

A superioridade do *online training* pode ser atribuída à sua capacidade de gerar dados que são mais relevantes para a política atual do modelo [^1]. No início do treinamento, o modelo tem pouco conhecimento e a diferença entre o *actor* e o modelo SFT é pequena. No entanto, à medida que o modelo aprende, os dados amostrados pelo *actor* revelam padrões que o modelo de SFT desconhece.

**Lemma 1:** Em estágios avançados de treinamento, o Online RL é mais eficiente pois explora as fronteiras do conhecimento do modelo, gerando dados mais informativos que o Offline RL.

*Prova (Argumentativa):* O Online RL permite que o modelo descubra soluções que não estão presentes nos dados iniciais de treinamento, levando a um aprimoramento contínuo [^1]. $\blacksquare$

### Potenciais Direções para RL Mais Eficaz

Com base nos resultados experimentais e na análise do GRPO, o artigo "DeepSeekMath" aponta para diversas direções para o desenvolvimento de algoritmos de RL mais eficazes para LLMs [^1]:

1.  **Data Source (Fonte de Dados):** Explorar prompts de perguntas fora da distribuição e estratégias avançadas de amostragem. A utilização de algoritmos de busca em árvore (tree search) pode auxiliar na descoberta de soluções mais complexas [^1].

2.  **Algorithm (Algoritmo):** Desenvolver algoritmos robustos a sinais de recompensa ruidosos, como os métodos de *weak-to-strong generalization* [^1].

3.  **Reward Function (Função de Recompensa):** Aprimorar a capacidade de generalização dos modelos de recompensa, refletir a incerteza do modelo de recompensa, construir modelos de recompensa de processo de alta qualidade [^1].

### Conclusão

Os experimentos de *online vs. offline training* no contexto do GRPO fornecem *insights* valiosos sobre a importância da fonte de dados no aprendizado por reforço de LLMs [^1]. A capacidade do *online training* de se adaptar dinamicamente ao comportamento do modelo e explorar regiões desconhecidas do espaço de estados resulta em um desempenho superior em comparação com o *offline training*. A combinação de estratégias de *online training* com o algoritmo GRPO e o *chain-of-thought* pode aumentar a robustez e generalização do modelo para raciocínio matemático. Além disso, a discussão sobre a análise do GRPO fornece *insights* valiosos sobre a eficácia do treinamento de código, a importância dos dados de treinamento e o potencial de várias técnicas de RL. Ao refinar a fonte de dados, o algoritmo de RL e a função de recompensa, é possível alcançar um RL ainda mais eficaz e impulsionar o progresso no campo de Natural Language Understanding e Inteligência Artificial.

### Referências

[^1]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. 2024.
[^11]: Schulman et al. Proximal policy optimization algorithms. 2017.
<!-- END -->