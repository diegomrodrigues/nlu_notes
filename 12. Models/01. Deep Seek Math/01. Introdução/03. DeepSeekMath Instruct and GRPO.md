## Ajuste Fino Supervisionado e Aprendizado por Reforço no DeepSeekMath

### Introdução
Em continuidade aos capítulos anteriores, este capítulo detalha o processo de **instruction tuning matemático** aplicado ao *DeepSeekMath-Base*, bem como a aplicação do **Group Relative Policy Optimization (GRPO)** para aprimorar ainda mais as capacidades do modelo [^1]. O *instruction tuning* e o *aprendizado por reforço* são técnicas cruciais para refinar LLMs e otimizar seu desempenho em tarefas específicas [^1]. Aqui, exploramos como essas técnicas foram aplicadas ao *DeepSeekMath* para obter resultados notáveis em *benchmarks* de raciocínio matemático.

### Conceitos Fundamentais

**Instruction Tuning Matemático:** Após a fase de pré-treinamento, o *DeepSeekMath-Base* é submetido a um processo de *instruction tuning* matemático [^1]. Este processo envolve o ajuste fino do modelo em um conjunto de dados cuidadosamente selecionado de problemas matemáticos, juntamente com suas soluções correspondentes [^1]. O objetivo é ensinar ao modelo como seguir instruções e gerar soluções precisas para problemas matemáticos [^1].

Os dados utilizados para o *instruction tuning* incluem [^1]:
- *Chain-of-thought (CoT)*: Problemas resolvidos com explicações passo a passo, demonstrando o processo de raciocínio.
- *Program-of-thought (PoT)*: Problemas resolvidos através da geração de código *Python* que pode ser executado para obter a solução.
- *Raciocínio integrado a ferramentas*: Problemas resolvidos utilizando ferramentas externas, como calculadoras ou sistemas de álgebra computacional.

A aplicação do *instruction tuning* matemático com *chain-of-thought*, *program-of-thought*, e dados de raciocínio integrado a ferramentas resulta no modelo *DeepSeekMath-Instruct 7B*, que supera todos os seus equivalentes de 7B [^1].

**Group Relative Policy Optimization (GRPO):** Para aprimorar ainda mais as capacidades do *DeepSeekMath-Instruct*, é aplicado o *Group Relative Policy Optimization (GRPO)* [^1]. O *GRPO* é uma variante do algoritmo de aprendizado por reforço *Proximal Policy Optimization (PPO)* (Schulman et al., 2017), projetada especificamente para melhorar o raciocínio matemático [^1].

A principal inovação do *GRPO* é a eliminação da necessidade de um *critic model*, que é uma parte essencial do *PPO* padrão [^1]. Em vez disso, o *GRPO* estima a *baseline* a partir das *group scores*, o que reduz significativamente os recursos computacionais necessários para o treinamento [^1].
```
Proximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is
widely used in the RL fine-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizes
LLMs by maximizing the following surrogate objective:
1
(min{
IPPO(0) = E[q ~ P(Q),ο ~ πθold (O|q)]
IM
t=
πθ (Otq, O<t)
πθold (Otq, O<t)
)
-At, clip
πθ (Otq, O<t)
)
-At,1 - 8,1+ε Ατ
πθold (Otq, O<t)'
(1)
where πθ and πθold are the current and old policy models, and q, o are questions and outputs
sampled from the question dataset and the old policy πθold, respectively. ɛ is a clipping-related
hyper-parameter introduced in PPO for stabilizing training. At is the advantage, which is
computed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based
```
O *GRPO* obtém uma melhoria substancial em relação ao *DeepSeekMath-Instruct*, incluindo tarefas matemáticas *in-domain* (GSM8K: 82.9% → 88.2%, MATH: 46.8% → 51.7%) e *out-of-domain* (e.g., CMATH: 84.6% → 88.8%) durante a fase de aprendizado por reforço [^1].

**Dados de Treinamento:** O uso de um subconjunto de dados de *instruction tuning* em inglês com *GRPO* leva a uma melhoria substancial sobre o *DeepSeekMath-Instruct*, incluindo tarefas matemáticas *in-domain* e *out-of-domain* [^1]. Isso sugere que o *GRPO* é capaz de generalizar para além dos dados de treinamento específicos e melhorar o desempenho em uma variedade de tarefas matemáticas [^1].

**Vantagens do GRPO:** O *GRPO* oferece várias vantagens em relação ao *PPO* padrão [^1]:
- **Menor consumo de memória:** A eliminação do *critic model* reduz significativamente o consumo de memória, tornando o treinamento mais eficiente.
- **Melhor generalização:** O *GRPO* demonstra melhor capacidade de generalização para tarefas matemáticas *out-of-domain*, indicando que ele é capaz de aprender representações mais robustas e adaptáveis.
- **Desempenho superior:** O *GRPO* obtém melhorias significativas em relação ao *DeepSeekMath-Instruct*, demonstrando sua eficácia em aprimorar o raciocínio matemático.

**Unificação de Métodos de RL:** O artigo fornece um paradigma unificado para entender diferentes métodos de RL, como *Rejection Sampling Fine-Tuning (RFT)*, *Direct Preference Optimization (DPO)*, *PPO* e *GRPO* [^1]. Todos esses métodos são conceptualizados como técnicas de RL diretas ou simplificadas [^1].

### Conclusão
O *instruction tuning* matemático e a aplicação do *GRPO* são componentes essenciais para o sucesso do *DeepSeekMath* [^1]. Essas técnicas permitem que o modelo aprenda a seguir instruções, gerar soluções precisas e generalizar para uma variedade de tarefas matemáticas [^1]. O *GRPO*, em particular, representa um avanço significativo no aprendizado por reforço para LLMs, oferecendo melhor desempenho com menor consumo de memória [^1].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->