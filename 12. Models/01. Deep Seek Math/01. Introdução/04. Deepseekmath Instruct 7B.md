## Desempenho e Comparativos do DeepSeekMath-Instruct 7B e GRPO

### Introdução

Este capítulo se dedica a analisar o desempenho do *DeepSeekMath-Instruct 7B* em comparação com outros modelos, bem como os benefícios específicos do uso do *Group Relative Policy Optimization (GRPO)* [^1]. Avaliar o desempenho relativo dos modelos e as vantagens de técnicas específicas como o *GRPO* é crucial para entender o progresso no campo do raciocínio matemático com *LLMs*.

### Conceitos Fundamentais

**Desempenho do DeepSeekMath-Instruct 7B:** O *DeepSeekMath-Instruct 7B* demonstra um desempenho notável de raciocínio passo a passo [^1]. Notavelmente, no conjunto de dados *MATH* em nível de competição, o modelo supera todos os modelos de código aberto e a maioria dos modelos proprietários (por pelo menos 9% absoluto), mesmo aqueles substancialmente maiores [^1]. Isso inclui modelos que foram aprimorados por meio de aprendizado por reforço focado em matemática [^1]. Embora o *DeepSeekMath-Instruct 7B* rivalize com os modelos proprietários chineses *GLM-4* e *Baichuan-3* no *MATH*, ainda fica abaixo do *GPT-4* e do *Gemini Ultra* [^1].

**GRPO e Melhorias de Desempenho:** A aplicação do *GRPO*, usando um subconjunto de dados de *instruction tuning* em inglês, leva a uma melhoria substancial sobre o *DeepSeekMath-Instruct*, incluindo tarefas matemáticas *in-domain* e *out-of-domain* [^1]. Isso significa que o *GRPO* não apenas ajuda o modelo a melhorar em tarefas que ele já domina (in-domain), mas também permite que ele generalize melhor para novos tipos de problemas matemáticos (out-of-domain) [^1].

A tabela abaixo resume o impacto do GRPO nos benchmarks GSM8K, MATH e CMATH [^1]:
- GSM8K: 82.9% → 88.2%
- MATH: 46.8% → 51.7%
- CMATH: 84.6% → 88.8%

**Unificação dos Métodos de RL:** O artigo apresenta um paradigma unificado para entender diferentes métodos, como *RFT*, *DPO*, *PPO* e *GRPO* [^1]. Ele encontra que todos esses métodos são conceitualizados como técnicas de *RL* diretas ou simplificadas [^1]. O artigo também conduz experimentos extensivos, e.g., treinamento online versus offline, supervisão de resultado versus processo, *single-turn* versus *iterative RL* e assim por diante, para investigar profundamente os elementos essenciais deste paradigma [^1].

**DeepSeekMath-RL 7B:** O modelo final, *DeepSeekMath-RL 7B*, atinge precisões de 88.2% e 51.7% em *GSM8K* e *MATH*, respectivamente [^1], utilizando raciocínio *chain-of-thought*. Este desempenho supera todos os modelos de código aberto na faixa de 7B a 70B, bem como a maioria dos modelos de código fechado [^1]. Notavelmente, o *DeepSeekMath-RL 7B* é treinado apenas em dados de *instruction tuning* no formato *chain-of-thought* de *GSM8K* e *MATH*, começando com *DeepSeekMath-Instruct 7B* [^1]. Apesar do escopo restrito de seus dados de treinamento, ele supera o *DeepSeekMath-Instruct 7B* em todas as métricas de avaliação, demonstrando a eficácia do aprendizado por reforço [^1].

**Resultados do Tool Use:** Quando os modelos são autorizados a integrar raciocínio em linguagem natural e uso de ferramentas baseadas em programa para resolução de problemas, o *DeepSeekMath-Instruct 7B* se aproxima de uma precisão de 60% no *MATH*, superando todos os modelos de código aberto existentes [^1]. Nos outros *benchmarks*, nosso modelo é competitivo com o *DeepSeek-LLM-Chat 67B*, o *state-of-the-art* anterior que é 10 vezes maior [^1].

### Conclusão

O *DeepSeekMath-Instruct 7B* representa um avanço significativo em *LLMs* para raciocínio matemático [^1]. Ele supera muitos modelos, tanto de código aberto quanto proprietários, em *benchmarks* desafiadores [^1]. O *GRPO* é uma técnica valiosa para melhorar ainda mais o desempenho, permitindo melhor generalização e eficiência computacional [^1]. As melhorias de desempenho *in-domain* e *out-of-domain* resultantes do *GRPO* são particularmente notáveis [^1]. Em essência, o *DeepSeekMath-Instruct 7B* é um modelo robusto e eficaz para tarefas de raciocínio matemático.

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->