## DeepSeekMath: Detalhes e Impacto

### Introdução
Em continuidade ao capítulo introdutório sobre o impacto dos LLMs no raciocínio matemático, este capítulo se aprofunda em um estudo de caso específico: o DeepSeekMath [^1]. Este modelo representa um avanço notável nas capacidades de raciocínio matemático de LLMs de código aberto, aproximando-se do desempenho de modelos proprietários como o GPT-4 [^1]. Exploraremos a arquitetura, os dados de treinamento e as técnicas de otimização que impulsionam o desempenho superior do DeepSeekMath, destacando sua relevância para a comunidade acadêmica e o futuro do raciocínio automatizado.

### Conceitos Fundamentais
O DeepSeekMath é um modelo de linguagem projetado especificamente para o domínio da matemática [^1]. Sua importância reside em sua capacidade de superar significativamente as limitações dos modelos de código aberto existentes e se aproximar do desempenho de modelos proprietários de última geração em *benchmarks acadêmicos* [^1].

**DeepSeekMath Corpus:** A criação de um *corpus* de treinamento de alta qualidade é um aspecto crucial do sucesso do DeepSeekMath [^1]. O *DeepSeekMath Corpus* é composto por 120 bilhões de *tokens* extraídos da *Common Crawl* [^1], um vasto arquivo da web. Este *corpus* é meticulosamente selecionado e refinado através de um *pipeline* de seleção de dados [^1], e a utilização de classificadores como o *fastText* [^1] (Joulin et al., 2016), conforme mencionado no capítulo anterior. A alta qualidade do *corpus* permite que o modelo base *DeepSeekMath-Base 7B* alcance 64.2% no *GSM8K* (Cobbe et al., 2021) e 36.2% no conjunto de dados *MATH* (Hendrycks et al., 2021), superando o *Minerva 540B* (Lewkowycz et al., 2022a) [^1].

**Arquitetura e Treinamento:** O *DeepSeekMath-Base* é inicializado com o *DeepSeek-Coder-Base-v1.5 7B* (Guo et al., 2024) e treinado com 500B tokens [^1]. A distribuição dos dados é: 56% do *DeepSeekMath Corpus*, 4% do *AlgebraicStack*, 10% do *arXiv*, 20% do código *Github*, e os 10% restantes são dados em linguagem natural do *Common Crawl* em inglês e chinês [^1]. O modelo é treinado com as configurações especificadas na Seção 2.2.1 do artigo [^1], com uma taxa de aprendizagem máxima de 4.2e-4 e um tamanho de lote de 10M tokens [^1].

**Multilingualidade:** O DeepSeekMath Corpus é *multilingue*, e isso leva a melhorias em *benchmarks* matemáticos chineses [^1]. Isso demonstra que a inclusão de dados em diversos idiomas pode aprimorar a capacidade do modelo de raciocinar matematicamente em diferentes contextos culturais e linguísticos [^1].

**Ajuste Fino e Aprendizado por Reforço:** O modelo *DeepSeekMath-Instruct 7B*, resultante do ajuste fino supervisionado (Supervised Fine-Tuning - SFT) e do aprendizado por reforço (Reinforcement Learning - RL), supera todos os concorrentes de 7B e é comparável aos modelos ajustados por instrução de código aberto de 70B [^1]. O artigo introduz o *Group Relative Policy Optimization (GRPO)*, uma variação do *Proximal Policy Optimization (PPO)* (Schulman et al., 2017), que melhora as capacidades de raciocínio matemático e otimiza o uso da memória [^1].

**Benchmarks e Avaliação:** O DeepSeekMath foi avaliado em vários *benchmarks* matemáticos, incluindo *GSM8K*, *MATH*, *OCW*, *SAT*, *MMLU-STEM*, *CMATH*, *Gaokao-MathCloze* e *Gaokao-MathQA* [^1]. Os resultados demonstram que o modelo supera os modelos de código aberto existentes e se aproxima do desempenho de modelos proprietários [^1]. O artigo ainda faz referência a testes sobre a capacidade de resolver problemas usando *tool use*, com *Python* [^1].

**Performance Detalhada:** O modelo base *DeepSeekMath-Base 7B* alcança *64.2%* no *GSM8K* e *36.2%* no conjunto de dados *MATH*, superando o *Minerva 540B* [^1]. O modelo *DeepSeekMath-Instruct 7B* demonstra forte desempenho de raciocínio passo a passo, superando todos os modelos de código aberto e a maioria dos modelos proprietários no conjunto de dados *MATH* [^1].

### Conclusão
O DeepSeekMath representa um avanço significativo no campo do raciocínio matemático automatizado com LLMs [^1]. Ao demonstrar capacidades superiores aos modelos de código aberto existentes e se aproximar do desempenho de modelos proprietários, o DeepSeekMath abre novas possibilidades para a resolução de problemas e a descoberta científica [^1]. A combinação de um *corpus* de treinamento de alta qualidade, uma arquitetura de modelo otimizada e técnicas de ajuste fino avançadas é fundamental para o sucesso do DeepSeekMath [^1]. Os avanços futuros nesta área prometem avanços ainda maiores no desenvolvimento de LLMs capazes de raciocinar matematicamente com precisão e eficiência [^1].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->