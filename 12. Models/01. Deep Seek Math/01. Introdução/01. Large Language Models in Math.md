## LLMs e o Raciocínio Matemático: Uma Introdução

### Introdução
O advento dos **Large Language Models (LLMs)** representa um marco significativo na interseção entre inteligência artificial e raciocínio matemático [^1]. Esses modelos demonstraram capacidades notáveis, transformando a maneira como abordamos e resolvemos problemas complexos em diversas áreas da matemática [^1]. Este capítulo explorará como os LLMs revolucionaram a abordagem do raciocínio matemático na inteligência artificial, impulsionando avanços significativos tanto no benchmark de raciocínio quantitativo quanto no benchmark de raciocínio geométrico [^1]. Apesar dos avanços, modelos de ponta como o GPT-4 (OpenAI, 2023) e Gemini-Ultra (Anil et al., 2023) não estão publicamente disponíveis, e os modelos de código aberto acessíveis atualmente ficam consideravelmente atrás em desempenho [^1].

### Conceitos Fundamentais

Os LLMs provaram ser instrumentais em ajudar humanos a resolver problemas matemáticos complexos (Tao, 2023) [^1]. Contudo, a trajetória dos LLMs no domínio do raciocínio matemático é marcada por desafios inerentes à natureza complexa e estruturada da matemática. Os LLMs devem demonstrar proficiência na compreensão de conceitos abstratos, na manipulação de símbolos e na aplicação de regras lógicas para derivar soluções precisas.

Modelos como o DeepSeekMath [^1], exploram a vasta quantidade de dados disponíveis na web para aprimorar suas capacidades. A criação de corpus de treinamento de alta qualidade, como o DeepSeekMath Corpus com 120B tokens, é crucial para o sucesso desses modelos [^1]. A engenharia meticulosa de pipelines de seleção de dados e a utilização de classificadores como o fastText (Joulin et al., 2016) são essenciais para extrair informações relevantes de fontes como o Common Crawl [^1].

A capacidade de um LLM de raciocinar matematicamente não depende apenas do número de parâmetros [^1]. Modelos menores, pré-treinados com dados de alta qualidade, podem alcançar um desempenho surpreendentemente bom, como demonstrado pelo modelo DeepSeekMath-Base 7B [^1], que atinge 64.2% no GSM8K (Cobbe et al., 2021) e 36.2% no competition-level MATH dataset (Hendrycks et al., 2021), superando o Minerva 540B (Lewkowycz et al., 2022a) [^1]. Além disso, o treinamento em dados multilingues pode levar a melhorias em benchmarks matemáticos em diferentes idiomas [^1].

O ajuste fino (fine-tuning) supervisionado e o aprendizado por reforço (Reinforcement Learning - RL) são técnicas importantes para aprimorar ainda mais as capacidades dos LLMs em raciocínio matemático [^1]. O uso de dados de *chain-of-thought*, *program-of-thought* e raciocínio integrado a ferramentas pode melhorar significativamente o desempenho [^1]. Além disso, algoritmos de RL como o *Group Relative Policy Optimization (GRPO)* [^1], podem otimizar o uso da memória e melhorar a capacidade de raciocínio.

### Conclusão

Os LLMs estão se tornando cada vez mais capazes de lidar com tarefas complexas de raciocínio matemático. Ao alavancar grandes quantidades de dados, otimizar arquiteturas de modelos e empregar técnicas de ajuste fino avançadas, esses modelos estão abrindo novas possibilidades para a resolução de problemas e a descoberta científica [^1]. O desenvolvimento contínuo de LLMs no domínio do raciocínio matemático promete avanços ainda maiores no futuro, com implicações significativas para diversos campos, incluindo ciência, tecnologia, engenharia e matemática [^1].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->