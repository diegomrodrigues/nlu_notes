## Supervised Fine-Tuning (SFT) e DeepSeekMath-Instruct 7B: Avaliação em Benchmarks de Raciocínio Quantitativo

### Introdução
Após a etapa de pre-training, os modelos de linguagem são frequentemente submetidos a um processo de *fine-tuning* supervisionado (SFT) para alinhar seu comportamento com tarefas específicas e melhorar seu desempenho em *benchmarks* relevantes [^10]. Este capítulo detalha a avaliação do modelo *fine-tuned* DeepSeekMath-Instruct 7B em *benchmarks* de raciocínio quantitativo, tanto com quanto sem o uso de ferramentas externas. A avaliação compara o modelo com outros líderes da época, incluindo modelos *closed-source* como GPT-4 e Gemini, e modelos *open-source* como DeepSeek-LLM-Chat e Qwen.

### Conceitos Fundamentais
O processo de *fine-tuning* supervisionado (SFT) envolve o ajuste de um modelo pré-treinado usando um conjunto de dados rotulado, específico para a tarefa em questão [^10]. No caso de DeepSeekMath-Instruct 7B, o modelo base DeepSeekMath-Base é *fine-tuned* com um conjunto de dados de instrução matemática que abrange problemas em inglês e chinês, provenientes de diferentes campos matemáticos e com níveis de complexidade variados [^10]. Esses problemas são emparelhados com soluções no formato de *chain-of-thought* (CoT), *program-of-thought* (PoT) e formato de raciocínio integrado a ferramentas [^10].

O conjunto de dados SFT consiste em 776K exemplos de treinamento e inclui:
*   **Conjuntos de dados matemáticos em inglês**: Anotações de problemas GSM8K e MATH com soluções integradas a ferramentas, juntamente com um subconjunto de MathInstruct e o conjunto de treinamento de Lila-OOD, onde os problemas são resolvidos com CoT ou PoT [^10]. A coleção em inglês abrange diversos campos da matemática, como álgebra, probabilidade, teoria dos números, cálculo e geometria [^10].
*   **Conjuntos de dados matemáticos em chinês**: Problemas matemáticos chineses K-12 que abrangem 76 subtópicos, como equações lineares, com soluções anotadas em CoT e formato de raciocínio integrado a ferramentas [^10].

O modelo DeepSeekMath-Instruct 7B é treinado por 500 passos com um tamanho de *batch* de 256 e uma taxa de aprendizado constante de 5e-5 [^10].

#### Avaliação em Benchmarks de Raciocínio Quantitativo
A avaliação do DeepSeekMath-Instruct 7B é realizada em quatro *benchmarks* de raciocínio quantitativo em inglês e chinês, tanto sem quanto com o uso de ferramentas [^10]. Os modelos são comparados com os seguintes modelos líderes da época:

*   **Modelos *Closed-Source***: Incluem a família GPT, com GPT-4 e GPT-4 Code Interpreter, Gemini Ultra e Pro, Inflection-2 e Grok-1 [^10]. Também são incluídos modelos lançados recentemente por empresas chinesas, como Baichuan-3 e o mais recente GLM-4 [^10].
*   **Modelos *Open-Source***: Incluem modelos gerais como DeepSeek-LLM-Chat 67B, Qwen 72B, SeaLLM-v2 7B, e ChatGLM3 6B [^10]. Também são incluídos modelos com aprimoramentos em matemática, como InternLM2-Math 20B, Math-Shepherd-Mistral 7B, a série WizardMath, MetaMath 70B, TORA 34B e MAmmoTH 70B [^11].

A Tabela 5 [^12] apresenta o desempenho dos modelos *open-source* e *closed-source* com raciocínio *chain-of-thought* e raciocínio integrado a ferramentas em *benchmarks* em inglês e chinês. Sob a avaliação onde o uso de ferramentas não é permitido, DeepSeekMath-Instruct 7B demonstra forte desempenho de raciocínio passo a passo [^11]. Notavelmente, no *benchmark* MATH, o modelo supera todos os modelos *open-source* e a maioria dos modelos proprietários (por exemplo, Inflection-2 e Gemini Pro) em pelo menos 9% absoluto [^11]. Isso é verdade mesmo para modelos que são substancialmente maiores (por exemplo, Qwen 72B) ou foram especificamente aprimorados por meio de aprendizado por reforço focado em matemática (por exemplo, WizardMath-v1.1 7B) [^11]. Embora DeepSeekMath-Instruct 7B rivalize com os modelos proprietários chineses GLM-4 e Baichuan-3 no MATH, ele ainda tem um desempenho inferior ao GPT-4 e Gemini Ultra [^11].

Sob a avaliação onde os modelos são autorizados a integrar raciocínio de linguagem natural e uso de ferramentas baseadas em programa para resolução de problemas, DeepSeekMath-Instruct 7B se aproxima de uma precisão de 60% no MATH, superando todos os modelos *open-source* existentes [^11]. Nos outros *benchmarks*, nosso modelo é competitivo com DeepSeek-LLM-Chat 67B, o estado da arte anterior que é 10 vezes maior [^11].

### Conclusão
A avaliação abrangente do DeepSeekMath-Instruct 7B demonstra sua capacidade de alcançar desempenho competitivo em tarefas de raciocínio matemático, tanto sem quanto com o uso de ferramentas [^11, 12]. Os resultados destacam a importância do *fine-tuning* supervisionado com dados de alta qualidade e formatos de solução adequados, como CoT e PoT [^10]. A análise comparativa com modelos *closed-source* e *open-source* fornece *insights* valiosos sobre os pontos fortes e fracos do DeepSeekMath-Instruct 7B, orientando o desenvolvimento futuro de modelos de linguagem para raciocínio matemático [^11].

### Referências
[^10]: Seção 3.2 do artigo: Training and Evaluating DeepSeekMath-Instruct 7B
[^11]: Seção 3.2 do artigo: Training and Evaluating DeepSeekMath-Instruct 7B
[^12]: Tabela 5 do artigo: Performance of Open- and Closed-Source models with both Chain-of-Thought and Tool-Integrated Reasoning on English and Chinese Benchmarks.
<!-- END -->