## Supervised Fine-Tuning (SFT) e DeepSeekMath-Instruct 7B
### Introdução

Dando continuidade ao desenvolvimento de modelos capazes de raciocínio matemático avançado, esta seção detalha o processo de ajuste fino supervisionado aplicado ao modelo DeepSeekMath-Base 7B, resultando no DeepSeekMath-Instruct 7B. Este processo é crucial para alinhar o modelo base com instruções específicas e melhorar seu desempenho em tarefas matemáticas complexas.

### Conceitos Fundamentais

O ajuste fino instrucional matemático é uma etapa essencial para especializar um modelo de linguagem pré-treinado em tarefas específicas [^1]. No contexto de DeepSeekMath-Instruct 7B, o processo visa refinar o conhecimento adquirido durante a fase de pré-treinamento, adaptando-o para responder de forma eficaz a instruções relacionadas a problemas matemáticos.

O ajuste fino supervisionado (SFT) envolve o uso de um conjunto de dados rotulado para treinar um modelo pré-treinado. O objetivo é minimizar a diferença entre as previsões do modelo e os rótulos fornecidos no conjunto de dados de treinamento. Este processo ajuda o modelo a aprender a associar entradas (instruções) com saídas desejadas (soluções matemáticas).

#### Detalhes do Ajuste Fino Instrucional

O modelo DeepSeekMath-Instruct 7B passa por um ajuste fino instrucional matemático baseado no DeepSeekMath-Base [^1]. Este processo envolve a utilização de dados especificamente projetados para aprimorar as capacidades de raciocínio matemático do modelo. Os detalhes cruciais deste ajuste fino são os seguintes:

*   **Concatenação de exemplos de treinamento:** Os exemplos de treinamento são concatenados aleatoriamente até atingir um comprimento de contexto máximo de 4K tokens [^1]. Isso permite que o modelo processe informações mais longas e complexas, melhorando sua capacidade de lidar com problemas matemáticos que exigem várias etapas de raciocínio.
*   **Número de passos de treinamento:** O modelo é treinado por 500 passos [^1]. Este número de passos é cuidadosamente escolhido para equilibrar o tempo de treinamento e o desempenho do modelo.
*   **Tamanho do lote:** Um tamanho de lote de 256 é utilizado [^1]. O tamanho do lote afeta a estabilidade e a velocidade do treinamento.
*   **Taxa de aprendizado:** Uma taxa de aprendizado constante de 5e-5 é aplicada [^1]. A taxa de aprendizado controla o tamanho das atualizações nos pesos do modelo durante o treinamento.

Estes parâmetros de treinamento são cruciais para o desempenho do DeepSeekMath-Instruct 7B. O comprimento máximo do contexto de 4K tokens permite que o modelo lide com problemas complexos que exigem múltiplas etapas de raciocínio [^1]. O número de passos de treinamento, o tamanho do lote e a taxa de aprendizado são cuidadosamente ajustados para equilibrar o tempo de treinamento e o desempenho do modelo [^1].

#### Dados de treinamento SFT

O conjunto de dados usado para o ajuste fino instrucional é construído para cobrir problemas em inglês e chinês de diferentes áreas matemáticas e níveis de complexidade [^1]. Os problemas são emparelhados com soluções em formato de cadeia de pensamento (CoT), programa de pensamento (PoT) e formato de raciocínio integrado a ferramentas [^1]. O número total de exemplos de treinamento é de 776K [^1].

### Conclusão

O ajuste fino instrucional matemático do DeepSeekMath-Base 7B resulta no modelo DeepSeekMath-Instruct 7B. Através do uso de uma concatenação aleatória de exemplos de treinamento até um comprimento de contexto máximo de 4K tokens, um número cuidadoso de passos de treinamento, um tamanho de lote apropriado e uma taxa de aprendizado constante, o DeepSeekMath-Instruct 7B aprimora suas habilidades em raciocínio matemático e se destaca em benchmarks complexos.

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->