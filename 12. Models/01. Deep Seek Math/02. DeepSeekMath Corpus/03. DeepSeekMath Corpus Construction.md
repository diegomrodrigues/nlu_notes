## DeepSeekMath Corpus: Deduplicação e Otimização do Common Crawl

### Introdução
Este capítulo aborda as técnicas de **deduplicação** aplicadas na construção do **DeepSeekMath Corpus**, um recurso crucial para o desenvolvimento de *Large Language Models* (LLMs) com capacidades de raciocínio matemático aprimoradas. Em continuidade aos capítulos anteriores que detalharam a coleta, refinamento e características multilíngues do corpus [^1, 4, 5, 6], este capítulo explora especificamente o processo de **deduplicação baseada em URL** e **técnicas de quase-deduplicação** empregadas para reduzir o tamanho do *Common Crawl* (CC) original. Essas técnicas são essenciais para otimizar o corpus, removendo dados redundantes e focando em conteúdo único e relevante para o treinamento matemático.

### Conceitos Fundamentais

O *Common Crawl* (CC) [^4] é uma vasta fonte de dados da web, mas sua natureza abrangente significa que contém muita redundância e conteúdo de baixa qualidade que pode prejudicar o processo de treinamento do modelo. Para mitigar esses problemas, a construção do DeepSeekMath Corpus incorpora etapas de deduplicação que se concentram em remover conteúdo duplicado e quase duplicado.

1.  **Deduplicação Baseada em URL**: Inicialmente, o processo de construção do DeepSeekMath Corpus emprega técnicas de **deduplicação baseada em URL** para reduzir o tamanho do *Common Crawl* original [^5]. A deduplicação baseada em URL envolve a identificação e remoção de páginas da web com URLs idênticos. Este é um passo essencial para eliminar duplicatas exatas que podem inflar artificialmente o tamanho do *corpus* sem adicionar informações novas.
2.  **Técnicas de Quase-Deduplicação**: Além da deduplicação exata, são aplicadas **técnicas de quase-deduplicação** [^5]. Essas técnicas visam identificar e remover páginas da web que são muito semelhantes, mas não idênticas. Isso pode incluir páginas com pequenas variações no conteúdo (por exemplo, marcadores de data ou texto de direitos autorais) ou espelhamento de conteúdo entre diferentes sites.

    A importância dessas técnicas reside na capacidade de evitar a redundância e a repetição de informações, permitindo que o modelo aprenda de forma mais eficiente e generalizada. Ao reduzir o ruído no *corpus*, a quase-deduplicação ajuda a melhorar a qualidade dos dados de treinamento, levando a um melhor desempenho do modelo em tarefas de raciocínio matemático.
3.  **Redução Resultante no Tamanho do Corpus**: Após a aplicação dessas técnicas de deduplicação, o tamanho do *Common Crawl* original é reduzido para 40B de páginas da web HTML [^5]. Essa redução é significativa, pois foca o *corpus* em conteúdo mais exclusivo e relevante. A consequente diminuição no tamanho do *corpus* também beneficia a eficiência computacional do processo de treinamento, reduzindo os requisitos de armazenamento e os tempos de treinamento.
4.  **Importância da Qualidade em Relação à Quantidade**: Embora a escala seja um fator importante no pré-treinamento de LLMs [^6], a qualidade dos dados é igualmente crítica. As técnicas de deduplicação contribuem diretamente para melhorar a qualidade geral dos dados, garantindo que o modelo seja treinado em um conjunto de dados mais limpo e representativo [^5]. Isso, por sua vez, leva a modelos mais robustos e generalizáveis.

Em resumo, a deduplicação baseada em URL e as técnicas de quase-deduplicação representam uma etapa fundamental no processo de construção do DeepSeekMath Corpus [^5]. Ao remover conteúdo redundante e de baixa qualidade, essas técnicas ajudam a otimizar o *corpus* para tarefas de pré-treinamento, levando a modelos mais eficientes e precisos para raciocínio matemático.

### Conclusão

Em continuidade à discussão anterior sobre a construção do DeepSeekMath Corpus, este capítulo destacou o papel crítico da deduplicação baseada em URL e das técnicas de quase-deduplicação na otimização do *Common Crawl*. O resultado da aplicação dessas técnicas, reduzindo o *corpus* para 40B de páginas web HTML, demonstra um compromisso em priorizar a qualidade dos dados. Nos capítulos seguintes, aprofundaremos como o corpus otimizado é utilizado para o treinamento de modelos.

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^4]: Seção 2 do artigo: Math Pre-Training
[^5]: Seção 2.1 do artigo: Data Collection and Decontamination
[^6]: Seção 2.2 do artigo: Validating the Quality of the DeepSeekMath Corpus
<!-- END -->