## GRPO e Melhorias no Desempenho em Raciocínio Matemático
### Introdução
Em continuidade à discussão sobre a Otimização de Política Relativa do Grupo (GRPO) e sua eficiência computacional, este capítulo detalha como a aplicação do GRPO a um subconjunto de dados de ajuste fino de instruções em inglês leva a melhorias substanciais no desempenho do modelo, tanto em tarefas matemáticas dentro do domínio quanto fora do domínio [^1]. Essa análise revela a eficácia do GRPO em aprimorar as capacidades de raciocínio matemático dos modelos de linguagem, mesmo com recursos de treinamento limitados [^1, ^2, ^11].

### Conceitos Fundamentais

**DeepSeekMath-Instruct e Dados de Ajuste Fino**
Conforme mencionado anteriormente, DeepSeekMath-Instruct é um modelo de linguagem ajustado finamente (fine-tuned) em um conjunto de dados de instruções matemáticas [^1]. Este conjunto de dados é usado para treinar o modelo a seguir instruções e resolver problemas matemáticos de vários tipos [^1]. O ajuste fino supervisionado (SFT) é uma etapa crucial para alinhar o modelo de linguagem com o comportamento desejado, melhorando sua capacidade de raciocínio e resolução de problemas [^1, ^11].

**Melhorias de Desempenho com GRPO**
O artigo original [^1] demonstra que, ao aplicar o GRPO usando apenas um subconjunto de dados de ajuste fino de instruções em inglês, é possível obter melhorias notáveis em relação ao já robusto DeepSeekMath-Instruct [^1]. Especificamente, as melhorias foram observadas tanto em tarefas matemáticas *dentro do domínio* (GSM8K e MATH) quanto em tarefas *fora do domínio* (CMATH) [^1].

*   **Dentro do Domínio:** As tarefas dentro do domínio são aquelas que se assemelham aos dados utilizados durante o ajuste fino supervisionado [^1]. No caso do DeepSeekMath, GSM8K (grade school math 8K) e MATH são benchmarks projetados para avaliar as habilidades de raciocínio matemático [^1, ^5]. Os resultados mostram um aumento significativo na precisão nessas tarefas:
    *   GSM8K: 82,9% → 88,2% [^1]
    *   MATH: 46,8% → 51,7% [^1]
*   **Fora do Domínio:** As tarefas fora do domínio são aquelas que diferem dos dados utilizados durante o ajuste fino supervisionado [^1]. CMATH é um benchmark que avalia as habilidades de raciocínio matemático em chinês, demonstrando a capacidade do modelo de generalizar para diferentes idiomas e tipos de problemas [^1, ^6]. O GRPO também resultou em uma melhoria notável no CMATH:
    *   CMATH: 84,6% → 88,8% [^1]

Esses resultados indicam que o GRPO não apenas melhora o desempenho do modelo em tarefas para as quais foi explicitamente treinado, mas também aprimora sua capacidade de generalização para tarefas relacionadas, mas diferentes [^1].

**Interpretação dos Resultados**
A melhoria observada com o GRPO pode ser atribuída à sua capacidade de otimizar a política do modelo de linguagem de forma mais eficiente, mesmo com dados limitados [^1, ^2, ^11]. Ao contrário do PPO, que requer o treinamento de uma função de valor separada, o GRPO estima a linha de base diretamente a partir das recompensas relativas das saídas do grupo, reduzindo a complexidade e o custo computacional [^1, ^2, ^11].

Além disso, a regularização KL direta utilizada no GRPO ajuda a evitar a otimização excessiva e a estabilizar o processo de aprendizado, permitindo que o modelo aprenda a gerar saídas mais precisas e coerentes [^1, ^2, ^11].

**Aplicações e Implicações**
A capacidade do GRPO de melhorar o desempenho de modelos de linguagem com recursos limitados tem implicações significativas para aplicações práticas [^1, ^2, ^11]. Permite o desenvolvimento de modelos mais eficientes e acessíveis, que podem ser implementados em ambientes com restrições computacionais ou de dados [^1, ^2, ^11].

Além disso, a melhoria na generalização para tarefas fora do domínio sugere que o GRPO pode ser uma ferramenta valiosa para desenvolver modelos de linguagem mais robustos e adaptáveis, capazes de lidar com uma ampla gama de problemas e cenários [^1].

### Conclusão
O GRPO demonstra ser uma técnica eficaz para aprimorar o raciocínio matemático em modelos de linguagem, mesmo quando aplicado a subconjuntos de dados de ajuste fino [^1, ^2, ^11]. As melhorias observadas tanto em tarefas dentro quanto fora do domínio destacam a capacidade do GRPO de otimizar a política do modelo de forma eficiente e de promover a generalização [^1, ^2, ^11]. Essa abordagem oferece uma alternativa promissora ao PPO padrão, especialmente em cenários com recursos computacionais limitados [^1, ^2, ^11].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^2]: J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[^5]: D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[^6]: T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.
[^11]: L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.
<!-- END -->