## GRPO: Estimativa da Linha de Base e Redução de Recursos de Treinamento
### Introdução
Este capítulo expande o conceito de Otimização de Política Relativa do Grupo (GRPO) introduzido anteriormente, detalhando como o GRPO dispensa o modelo crítico, estimando a linha de base a partir de pontuações de grupo, o que leva a uma redução significativa nos recursos de treinamento [^1, ^2, ^11]. Este aprimoramento aborda uma das principais limitações do Proximal Policy Optimization (PPO), que requer o treinamento de uma função de valor com complexidade comparável ao modelo de política [^1, ^2, ^11].

### Conceitos Fundamentais

**Limitações do PPO e Necessidade do Modelo Crítico**
Como vimos anteriormente, o PPO utiliza uma função de valor $V_ψ$ para estimar a vantagem $A_t$ [^1, ^2, ^11, ^13]. Essa função de valor atua como uma linha de base para reduzir a variância no processo de aprendizado por reforço, permitindo uma estimativa mais estável e eficiente dos gradientes de política [^1, ^2, ^11, ^13]. No entanto, a função de valor é tipicamente implementada como outro modelo neural com um número de parâmetros comparável ao modelo de política [^1, ^2, ^11]. Isso implica um custo computacional e de memória significativo, especialmente ao trabalhar com LLMs [^1, ^2, ^11].

Além disso, no contexto de LLMs, a recompensa é frequentemente atribuída apenas ao último token gerado [^1, ^2, ^11]. Isso torna o treinamento da função de valor um desafio, pois o modelo precisa aprender a associar recompensas esparsas com ações tomadas ao longo de toda a sequência gerada [^1, ^2, ^11].

**GRPO: Estimativa da Linha de Base a partir de Pontuações de Grupo**
Para mitigar as limitações do PPO, o GRPO adota uma abordagem diferente para estimar a linha de base [^1, ^2, ^11]. Em vez de treinar uma função de valor separada, o GRPO estima a linha de base calculando a recompensa média de múltiplas saídas geradas em resposta à mesma pergunta [^1, ^2, ^11].

Formalmente, para uma pergunta $q$, o GRPO amostra um grupo de $G$ saídas $\{o_1, o_2, ..., o_G\}$ da política antiga $π_{θold}$. A linha de base $b$ para cada saída $o_i$ no grupo é então calculada como a recompensa média das saídas no grupo:

$$
b = \frac{1}{G} \sum_{i=1}^{G} r(o_i),
$$

onde $r(o_i)$ é a recompensa associada à saída $o_i$ [^1, ^2, ^11]. A vantagem $A_{i,t}$ é então calculada usando essa linha de base, permitindo que o modelo de política seja otimizado sem a necessidade de uma função de valor separada [^1, ^2, ^11].

**Vantagens da Abordagem do GRPO**
A principal vantagem do GRPO é a redução significativa nos recursos computacionais e de memória necessários para o treinamento [^1, ^2, ^11]. Ao eliminar a necessidade de treinar e manter uma função de valor separada, o GRPO torna-se mais eficiente e escalável, especialmente para LLMs de grande porte [^1, ^2, ^11].

Além disso, a utilização da recompensa média do grupo como linha de base alinha-se naturalmente com a natureza comparativa dos modelos de recompensa [^1, ^2, ^11]. Os modelos de recompensa são frequentemente treinados para discriminar entre diferentes saídas em resposta à mesma entrada, tornando a recompensa relativa uma medida mais informativa para o aprendizado por reforço [^1, ^2, ^11].

**Regularização via Divergência KL**
Em PPO, para mitigar a otimização excessiva do modelo de recompensa, é comum adicionar uma penalidade KL por token de um modelo de referência na recompensa em cada token:
$$
r_t = r_q(q, o_{<t}) - \beta \log \frac{\pi_\theta (o_t|q, o_{<t})}{\pi_{ref} (o_t|q, o_{<t})},
$$
onde $\beta$ é um coeficiente que controla a força da regularização [^1, ^2, ^11].

No entanto, GRPO regulariza diretamente adicionando a divergência KL entre a política treinada e a política de referência à perda, evitando complicar o cálculo de $A_{i,t}$ [^1, ^2, ^11].

### Conclusão
O GRPO oferece uma solução eficaz para reduzir os custos computacionais e de memória associados ao treinamento de modelos de linguagem usando aprendizado por reforço [^1, ^2, ^11]. Ao estimar a linha de base diretamente a partir de pontuações de grupo e utilizar uma regularização KL direta, o GRPO simplifica o processo de treinamento e melhora a eficiência do aprendizado por reforço em LLMs [^1, ^2, ^11].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^2]: J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[^11]: L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.
[^13]: J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.
<!-- END -->