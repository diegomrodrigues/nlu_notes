## Otimização de Política Relativa do Grupo (GRPO)

### Introdução
Em continuidade ao tópico de modelos de linguagem e suas aplicações em raciocínio matemático, este capítulo se aprofundará em uma técnica específica de otimização: a Otimização de Política Relativa do Grupo (GRPO). O GRPO é uma variante do algoritmo de aprendizado por reforço (RL) da Otimização de Política Proximal (PPO) [^1, ^2, ^11]. O objetivo é apresentar um estudo detalhado do GRPO, focando em sua formulação matemática, suas vantagens em relação ao PPO padrão e suas aplicações em modelos de linguagem para raciocínio matemático [^1, ^2, ^11].

### Conceitos Fundamentais

**Proximal Policy Optimization (PPO)** [^2, ^11]
O Proximal Policy Optimization (PPO) é um algoritmo de aprendizado por reforço *actor-critic* amplamente utilizado na fase de ajuste fino (fine-tuning) RL de LLMs [^1, ^2, ^11]. Ele otimiza os LLMs maximizando o seguinte objetivo substituto:
$$
IPPO(0) = E[q \sim P(Q), ο \sim πθold (O|q)] \sum_{t=1}^{M} min \left(\frac{πθ (0t|q, 0<t)}{πθold (0t|q, 0<t)}At, clip \left(\frac{πθ (0t|q, 0<t)}{πθold (0t|q, 0<t)},1 - ε,1 +ε \right) At \right),
$$
onde:
*   $πθ$ e $πθold$ são os modelos de política atual e antigo, respectivamente.
*   $q, o$ são perguntas e saídas amostradas do conjunto de dados de perguntas e da política antiga $πθold$, respectivamente.
*   $ε$ é um hiperparâmetro relacionado ao clipping introduzido no PPO para estabilizar o treinamento.
*   $At$ é a vantagem, que é calculada aplicando a Estimativa de Vantagem Generalizada (GAE) [^13], com base nas recompensas $\{r>t\}$ e uma função de valor aprendida $Vψ$ [^1, ^2, ^11].

No PPO, uma função de valor precisa ser treinada junto com o modelo de política, e para mitigar a otimização excessiva do modelo de recompensa, a abordagem padrão é adicionar uma penalidade KL por token de um modelo de referência na recompensa em cada token [^1, ^2, ^11]:
$$
rt = rq(q, 0<t) – β log \frac{πθ (0t|q, 0<t)}{πref (0t|q, 0<t)},
$$

Como a função de valor empregada no PPO é tipicamente outro modelo de tamanho comparável ao modelo de política, ela traz um substancial fardo de memória e computacional [^1, ^2, ^11]. Além disso, durante o treinamento de RL, a função de valor é tratada como uma linha de base no cálculo da vantagem para redução de variância. No contexto LLM, usualmente apenas o último token é atribuído a uma pontuação de recompensa pelo modelo de recompensa, o que pode complicar o treinamento de uma função de valor que seja precisa em cada token.

**Group Relative Policy Optimization (GRPO)** [^1, ^2, ^11]

O Group Relative Policy Optimization (GRPO) foi projetado para superar as limitações do PPO padrão [^1, ^2, ^11].  Conforme demonstrado na Figura 4 do artigo original [^1], o GRPO dispensa a necessidade de uma aproximação adicional da função de valor como no PPO, e ao invés disso usa a recompensa média de múltiplas saídas amostradas, produzidas em resposta à mesma pergunta, como a linha de base [^1, ^2, ^11].

Especificamente, para cada pergunta $q$, o GRPO amostra um grupo de saídas $\{0_1, 0_2, ..., 0_G\}$ da política antiga $πθold$ e então otimiza o modelo de política maximizando o seguinte objetivo [^1, ^2, ^11]:

$$
IGRPO(0) = E[q \sim P(Q), \{0_i\}_{i=1}^G \sim πθold (O|q)]  \frac{1}{G} \sum_{i=1}^G \sum_{t=1}^{M} min \left(\frac{πθ (0_{i,t}|q, 0_{i,<t})}{πθold (0_{i,t}|q, 0_{i,<t})}A_{i,t}, clip \left( \frac{πθ (0_{i,t}|q, 0_{i,<t})}{πθold (0_{i,t}|q, 0_{i,<t})}, 1 - ε, 1 + ε \right) A_{i,t} \right)
$$

onde $ε$ e $β$ são hiperparâmetros, e $A_{i,t}$ é a vantagem calculada baseada em recompensas relativas das saídas dentro de cada grupo apenas [^1, ^2, ^11].

A forma relativa do grupo que o GRPO aproveita para calcular as vantagens se alinha bem com a natureza comparativa dos modelos de recompensa, já que modelos de recompensa são tipicamente treinados em conjuntos de dados de comparações entre saídas na mesma questão [^1, ^2, ^11]. Também, observe que, ao invés de adicionar penalidade KL na recompensa, o GRPO regulariza diretamente adicionando a divergência KL entre a política treinada e a política de referência à perda, evitando complicar o cálculo de $A_{i,t}$ [^1, ^2, ^11].

### Conclusão
O GRPO oferece uma alternativa eficiente ao PPO para otimizar modelos de linguagem, especialmente em tarefas que exigem raciocínio matemático [^1, ^2, ^11]. Ao evitar a necessidade de treinar uma função de valor separada e usar recompensas relativas do grupo, o GRPO reduz o consumo de memória e simplifica o processo de treinamento [^1, ^2, ^11]. Os resultados experimentais demonstram que o GRPO pode melhorar significativamente as capacidades de raciocínio matemático dos LLMs [^1, ^2, ^11].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^2]: J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[^11]: L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.
[^13]: J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.
<!-- END -->