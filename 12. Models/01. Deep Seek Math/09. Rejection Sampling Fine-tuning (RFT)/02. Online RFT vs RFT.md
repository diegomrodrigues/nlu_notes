## DeepSeekMath: Detalhes e Análise da Abordagem

### Introdução

Este capítulo se aprofunda na variante **Online Rejection Sampling Fine-tuning (Online RFT)** do Rejection Sampling Fine-tuning (RFT), explorando suas nuances e diferenças em relação ao RFT tradicional. Construindo sobre os conceitos de RFT apresentados anteriormente, analisaremos como o Online RFT amostra saídas diretamente do modelo de política em tempo real, $\pi_{\theta}$, em vez do modelo Supervised Fine-Tuned (SFT), $\pi_{sft}$ [^11]. Esta abordagem dinâmica e em tempo real apresenta desafios e oportunidades únicas para aprimorar modelos de linguagem, especialmente em tarefas de raciocínio complexas.

### Conceitos Fundamentais

Conforme vimos no capítulo anterior, o RFT envolve duas fases principais: amostragem de múltiplas saídas de um LLM SFT e, em seguida, treinamento seletivo com apenas as saídas que contêm a resposta correta [^8]. O Online RFT, por outro lado, modifica a primeira fase, amostrando saídas diretamente do modelo de política atual ($\pi_{\theta}$) durante o processo de treinamento.

A diferença fundamental reside na fonte das amostras utilizadas para o ajuste fino. No RFT, as amostras são derivadas de um modelo SFT preexistente, enquanto no Online RFT, as amostras evoluem dinamicamente com o modelo de política em tempo real. Essa abordagem tem implicações significativas na exploração do espaço de soluções e na adaptação do modelo às distribuições de dados em constante mudança.

Formalmente, o gradiente do Online RFT é definido como [^11]:

$$
\nabla_{\theta}I_{OnlineRFT}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta}(O|q)} \left[ \frac{1}{|o|} \sum_{t=1}^{|o|} I(o) \nabla_{\theta} \log \pi_{\theta}(o_t|q, o_{<t}) \right]
$$

Onde:

*   $I_{OnlineRFT}(\theta)$ representa o objetivo do RFT online.
*   $q \sim P_{sft}(Q)$ significa que a questão ($q$) é amostrada da distribuição do dataset SFT ($P_{sft}(Q)$).
*   $o \sim \pi_{\theta}(O|q)$ indica que as saídas ($o$) são amostradas da política do modelo *em tempo real* ($\pi_{\theta}$) dado a questão $q$.
*   $I(o)$ é uma função indicadora que retorna 1 se a saída $o$ contém a resposta correta e 0 caso contrário.
*   $\pi_{\theta}(o_t|q, o_{<t})$ é a probabilidade do token $o_t$ dado a questão $q$ e os tokens precedentes $o_{<t}$.

**Comparativo RFT vs. Online RFT:**

| Característica      | RFT                                                                         | Online RFT                                                                          |
| :----------------- | :-------------------------------------------------------------------------- | :------------------------------------------------------------------------------------ |
| Fonte das Amostras | Modelo SFT preexistente ($\pi_{sft}$)                                          | Modelo de política em tempo real ($\pi_{\theta}$)                                          |
| Dinamismo          | Estático; as amostras são geradas com base em um modelo fixo.                | Dinâmico; as amostras evoluem com o treinamento do modelo.                          |
| Exploração         | Limitada à capacidade do modelo SFT original.                                | Potencialmente maior; permite explorar soluções além do alcance do modelo SFT inicial. |
| Adaptação          | Menor capacidade de adaptação às mudanças na distribuição de dados.          | Maior capacidade de adaptação, pois o modelo aprende com suas próprias gerações.     |

**Vantagens e Desvantagens do Online RFT:**

*   **Vantagens:**
    *   Maior capacidade de explorar o espaço de soluções, potencialmente descobrindo soluções mais eficazes.
    *   Melhor adaptação às mudanças na distribuição de dados, pois o modelo aprende com suas próprias gerações em tempo real.
    *   Potencial para superar as limitações do modelo SFT original.

*   **Desvantagens:**
    *   Maior risco de instabilidade no treinamento, pois o modelo pode se tornar excessivamente confiante em suas próprias gerações, mesmo que imperfeitas.
    *   Necessidade de um controle cuidadoso dos hiperparâmetros e das estratégias de amostragem para evitar o colapso do modelo.
    *   Potencialmente mais intensivo em computação, pois a amostragem em tempo real requer uma avaliação constante do modelo.

### Conclusão

O Online RFT representa uma evolução do RFT tradicional, oferecendo um caminho promissor para o aprimoramento de LLMs em tarefas complexas. Ao amostrar saídas diretamente do modelo de política em tempo real, o Online RFT permite uma exploração mais dinâmica do espaço de soluções e uma melhor adaptação às mudanças na distribuição de dados. No entanto, essa abordagem também apresenta desafios únicos, como a necessidade de mitigar a instabilidade do treinamento e garantir uma exploração eficaz do espaço de soluções. Estratégias de implementação cuidadosas e ajustes de hiperparâmetros são cruciais para aproveitar ao máximo o potencial do Online RFT e desenvolver modelos de linguagem mais robustos e precisos.

### Referências
[^8]: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^11]: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->