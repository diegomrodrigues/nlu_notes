## Rejection Sampling Fine-tuning (RFT)

### Introdução

Este capítulo explora o **Rejection Sampling Fine-tuning (RFT)**, uma técnica avançada para aprimorar Large Language Models (LLMs), especialmente em tarefas que exigem alta precisão e correção, como raciocínio matemático. RFT é uma abordagem que se baseia no ajuste fino supervisionado (Supervised Fine-Tuning - SFT), adicionando uma etapa de amostragem seletiva para refinar ainda mais o modelo. Esta técnica se mostra eficaz para melhorar a qualidade das respostas geradas pelos LLMs, focando em saídas que atendem a critérios específicos de correção.

### Conceitos Fundamentais

O processo de Rejection Sampling Fine-tuning (RFT) pode ser dividido em duas fases principais:

1.  **Amostragem de Múltiplas Saídas:** Para cada questão, o RFT amostra várias saídas ($o$) de LLMs previamente ajustados de forma supervisionada (Supervised Fine-Tuned - SFT). Essas saídas representam diferentes respostas ou soluções possíveis para a mesma questão.
2.  **Treinamento Seletivo com Saídas Corretas:** O LLM é então treinado *exclusivamente* nas saídas amostradas que contêm a resposta correta. Este passo crucial garante que o modelo se concentre em associações que levam a resultados precisos e confiáveis.

Formalmente, o objetivo do RFT é maximizar a seguinte função objetivo [^8]:

$$
I_{RFT}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{sft}(O|q)} \left[ \frac{1}{|o|} \sum_{t=1}^{|o|} I(o) \log \pi_{\theta}(o_t|q, o_{<t}) \right]
$$

Onde:

*   $I_{RFT}(\theta)$ representa o objetivo do RFT.
*   $q \sim P_{sft}(Q)$ significa que a questão ($q$) é amostrada da distribuição do dataset SFT ($P_{sft}(Q)$).
*   $o \sim \pi_{sft}(O|q)$ indica que as saídas ($o$) são amostradas da política do modelo SFT ($\pi_{sft}$) dado a questão $q$.
*   $I(o)$ é uma função indicadora que retorna 1 se a saída $o$ contém a resposta correta e 0 caso contrário.
*   $\pi_{\theta}(o_t|q, o_{<t})$ é a probabilidade do token $o_t$ dado a questão $q$ e os tokens precedentes $o_{<t}$.

O gradiente desta função objetivo é dado por [^9]:

$$
\nabla_{\theta}I_{RFT}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{sft}(O|q)} \left[ \frac{1}{|o|} \sum_{t=1}^{|o|} I(o) \nabla_{\theta} \log \pi_{\theta}(o_t|q, o_{<t}) \right]
$$

No Rejection Sampling Fine-Tuning [^8], inicialmente, várias saídas são amostradas de LLMs ajustados de forma supervisionada para cada questão. Em seguida, o LLM é treinado nas saídas amostradas que contêm a resposta correta.

> O dataset consiste em questões do SFT e saídas amostradas do modelo SFT. A função de recompensa é baseada em regras, indicando se a resposta está correta ou não. O coeficiente de gradiente é definido como:

$$
GC_{RFT}(q, o, t) = I(o) =
\begin{cases}
1 & \text{se a resposta de } o \text{ está correta} \\
0 & \text{se a resposta de } o \text{ está incorreta}
\end{cases}
$$

#### Data Source

O dataset é construído a partir de questões do dataset de ajuste fino supervisionado (SFT), juntamente com as saídas amostradas do modelo SFT. Essencialmente, o RFT refina o modelo usando suas próprias gerações, mas apenas aquelas consideradas corretas.

#### Reward Function

A função de recompensa é baseada em regras ($Rule$). Ela atribui uma recompensa de 1 se a saída ($o$) contém a resposta correta e 0 caso contrário. Esta função simples, mas eficaz, direciona o modelo a priorizar a correção.

#### Online Rejection Sampling Fine-tuning (Online RFT)

A única diferença entre o RFT e o Online RFT é que as saídas do Online RFT são amostradas do modelo de política em tempo real $\pi_{\theta}$, em vez do modelo SFT $\pi_{sft}$ [^11]. Portanto, o gradiente do RFT online é:

$$
\nabla_{\theta}I_{OnlineRFT}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta}(O|q)} \left[ \frac{1}{|o|} \sum_{t=1}^{|o|} I(o) \nabla_{\theta} \log \pi_{\theta}(o_t|q, o_{<t}) \right]
$$

### Conclusão

O Rejection Sampling Fine-tuning (RFT) oferece uma estratégia poderosa para aprimorar LLMs, especialmente em domínios onde a precisão é crítica. Ao focar o treinamento em saídas que atendem a critérios rigorosos de correção, o RFT pode melhorar significativamente a confiabilidade e a precisão das respostas geradas pelo modelo. Essa abordagem se mostra promissora para uma variedade de aplicações, incluindo resolução de problemas matemáticos, raciocínio lógico e tarefas que exigem alta fidelidade.

### Referências
[^8]: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^9]: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^11]: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->