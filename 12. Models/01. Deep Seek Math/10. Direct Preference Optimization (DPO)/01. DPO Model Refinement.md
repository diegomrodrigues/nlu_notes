## Direct Preference Optimization (DPO)

### Introdução

Este capítulo dedica-se ao exame aprofundado do Direct Preference Optimization (DPO), uma técnica avançada para o ajuste fino de Large Language Models (LLMs). O DPO representa uma abordagem inovadora que se distancia dos métodos tradicionais de reinforcement learning, oferecendo uma alternativa mais estável e eficiente para alinhar modelos de linguagem com as preferências humanas. O foco central deste capítulo é a exploração de como o DPO refina ainda mais um modelo Supervised Fine-Tuning (SFT) ajustando-o em saídas aumentadas amostradas do modelo SFT, utilizando uma perda DPO em pares. Ao longo deste estudo, detalharemos os fundamentos teóricos do DPO, suas vantagens e desvantagens, e forneceremos uma análise comparativa com outras técnicas de ajuste fino.

### Conceitos Fundamentais

O Direct Preference Optimization (DPO) surge como uma alternativa promissora aos métodos de Reinforcement Learning from Human Feedback (RLHF). RLHF, apesar de eficaz, introduz complexidades significativas no treinamento de LLMs, como a necessidade de treinar um modelo de recompensa separado e lidar com instabilidades no processo de reinforcement learning. O DPO, por outro lado, elimina a necessidade de um modelo de recompensa explícito, otimizando diretamente o modelo de linguagem com base em preferências humanas expressas em pares de comparações.

A essência do DPO reside na sua capacidade de transformar preferências em uma função de perda que pode ser diretamente otimizada. No contexto deste documento, o DPO refina o modelo SFT ajustando-o em saídas aumentadas amostradas do próprio modelo SFT [^11, ^19]. Especificamente, o modelo é treinado para discriminar entre pares de respostas geradas, onde uma resposta é preferida em relação à outra. A função objetivo do DPO é projetada para maximizar a probabilidade de que a resposta preferida seja classificada como tal, enquanto minimiza a probabilidade de que a resposta não preferida seja erroneamente classificada como preferida [^29].

A função objetivo do DPO é expressa como [^29]:
$$
\mathcal{L}_{\text{DPO}}(\theta) = \mathbb{E}_{(q, o^+, o^-) \sim \pi_{\text{sft}}(O|q)} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(o^+|q)}{\pi_{\text{ref}}(o^+|q)} - \beta \log \frac{\pi_{\theta}(o^-|q)}{\pi_{\text{ref}}(o^-|q)} \right) \right]
$$

Onde:
*   $\theta$ representa os parâmetros do modelo a serem otimizados.
*   $q$ é a *query* ou prompt de entrada.
*   $o^+$ é a saída preferida para a *query* $q$.
*   $o^-$ é a saída não preferida para a *query* $q$.
*   $\pi_{\theta}(o|q)$ é a probabilidade do modelo gerar a saída $o$ dada a *query* $q$.
*   $\pi_{\text{ref}}(o|q)$ é o modelo de referência, tipicamente o modelo SFT original, usado para estabilizar o treinamento e evitar o colapso do modelo.
*   $\sigma$ é a função sigmoide, que transforma a diferença logarítmica das probabilidades em uma probabilidade entre 0 e 1.
*   $\beta$ é um hiperparâmetro que controla a força da regularização e a importância relativa das preferências.

A intuição por trás desta função de perda é clara: o modelo é incentivado a aumentar a probabilidade das saídas preferidas e diminuir a probabilidade das saídas não preferidas, em relação ao modelo de referência [^29]. O hiperparâmetro $\beta$ desempenha um papel crucial no equilíbrio entre aderir às preferências humanas e manter a diversidade e a generalização do modelo.

O gradiente da função objetivo do DPO é dado por [^29]:
$$
\nabla_{\theta}\mathcal{L}_{\text{DPO}}(\theta) = \mathbb{E}_{(q, o^+, o^-) \sim \pi_{\text{sft}}(O|q)} \left[ \frac{1}{|o^+|} \sum_{t=1}^{|o^+|} \text{GCDPO}(q, o, t) \nabla_{\theta} \log \pi_{\theta}(o^+_t|q, o^+_{<t}) - \frac{1}{|o^-|} \sum_{t=1}^{|o^-|} \text{GCDPO}(q, o, t) \nabla_{\theta} \log \pi_{\theta}(o^-_t|q, o^-_{<t}) \right]
$$

Onde GCDPO(q, o, t) é definido como [^29]:

$$
\text{GCDPO}(q, o, t) = \sigma \left( \beta \log \frac{\pi_{\theta}(o^+|q)}{\pi_{\text{ref}}(o^+|q)} - \beta \log \frac{\pi_{\theta}(o^-|q)}{\pi_{\text{ref}}(o^-|q)} \right)
$$

Esse gradiente guia o ajuste fino do modelo, atualizando os parâmetros $\theta$ de forma a alinhar o modelo com as preferências expressas nos dados de treinamento.

A escolha do modelo de referência $\pi_{\text{ref}}$ é um aspecto crítico do DPO. Normalmente, o modelo SFT original é utilizado como referência, atuando como um ponto de ancoragem que impede o modelo de se desviar excessivamente das suas capacidades iniciais e de sofrer *colapso*. O uso do modelo SFT também ajuda a preservar propriedades importantes como a fluência e a coerência do texto gerado [^29].

### Vantagens e Desvantagens

O DPO apresenta várias vantagens em relação aos métodos tradicionais de RLHF [^11, ^19]:

*   **Estabilidade:** Ao otimizar diretamente com base em preferências, o DPO evita as instabilidades comuns em RL, tornando o treinamento mais robusto e previsível.
*   **Eficiência:** A eliminação do modelo de recompensa simplifica o processo de treinamento, reduzindo a complexidade computacional e o tempo necessário para o ajuste fino.
*   **Simplicidade:** O DPO é conceitualmente mais simples do que o RLHF, facilitando a implementação e a experimentação.

No entanto, o DPO também apresenta algumas desvantagens:

*   **Dependência da Qualidade dos Dados:** O desempenho do DPO é altamente dependente da qualidade e representatividade dos dados de preferências. Dados ruidosos ou tendenciosos podem levar a um alinhamento inadequado do modelo.
*   **Escolha do Modelo de Referência:** A escolha do modelo de referência pode impactar significativamente o desempenho do DPO. Um modelo de referência inadequado pode limitar a capacidade do modelo de explorar novas soluções e alinhar-se às preferências.
*   **Necessidade de Dados em Pares:** O DPO requer dados de preferências em pares (ou seja, comparações entre duas saídas), o que pode ser mais difícil e caro de obter do que dados de avaliação única.

### DPO vs. RFT

Ainda no âmbito do ajuste fino de modelos de linguagem, Rejection Sampling Fine-tuning (RFT) surge como uma técnica alternativa para o alinhamento de modelos. RFT realiza um ajuste fino adicional no modelo SFT, amostrando outputs filtrados com base em *queries* SFT. RFT, por sua vez, filtra as saídas baseado na correção das respostas. Online Rejection Sampling Fine-tuning (Online RFT), diferente de RFT, inicia o modelo de política usando o modelo SFT e o refina ajustando finamente as saídas aumentadas amostradas do modelo de política em *real-time* [^11].

### Conclusão

O Direct Preference Optimization (DPO) representa uma abordagem inovadora e promissora para o ajuste fino de LLMs, oferecendo uma alternativa mais estável, eficiente e simples aos métodos tradicionais de reinforcement learning [^11, ^19, ^29]. Ao otimizar diretamente com base em preferências humanas expressas em pares de comparações, o DPO evita as complexidades e instabilidades associadas ao treinamento de modelos de recompensa separados. Embora apresente algumas desvantagens, como a dependência da qualidade dos dados de preferências e a necessidade de dados em pares, o DPO demonstra um grande potencial para alinhar modelos de linguagem com as preferências humanas de forma mais eficaz e robusta.

A capacidade do DPO de refinar ainda mais um modelo SFT ajustando-o em saídas aumentadas amostradas do próprio modelo SFT, utilizando uma função de perda bem definida e um modelo de referência para estabilização, torna-o uma ferramenta valiosa para o desenvolvimento de LLMs mais alinhados, úteis e seguros [^11, ^19, ^29].

### Referências
[^11]: Shao, Zhihong, et al. "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models." *arXiv preprint arXiv:2402.03300* (2024).
[^19]: Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35:27730–27744, 2022.
[^29]: Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. 2023.
<!-- END -->