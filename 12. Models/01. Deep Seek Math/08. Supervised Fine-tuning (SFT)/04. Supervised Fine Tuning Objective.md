## Supervised Fine-tuning (SFT)

### Introdução
Em continuidade aos capítulos anteriores que detalham o Supervised Fine-tuning (SFT) e sua aplicação no desenvolvimento de modelos de linguagem matemática [^1, 10], este capítulo se concentra no objetivo específico de maximizar uma função envolvendo a probabilidade logarítmica de saídas condicionadas às entradas [^28]. Discutiremos como o conjunto de dados empregado para SFT pode ser considerado como seleção humana e como esse processo contribui para a eficácia do modelo [^19]. Expandindo sobre os formatos de solução CoT, PoT e raciocínio integrado a ferramentas [^10], este capítulo explora a justificativa teórica para o objetivo de maximização em SFT e suas implicações práticas para o desenvolvimento de modelos de raciocínio matemático [^8].

### Conceitos Fundamentais

Como estabelecido, Supervised Fine-tuning (SFT) é uma técnica crítica para otimizar modelos de linguagem, especialmente aqueles destinados a tarefas complexas como raciocínio matemático [^1, 8, 10]. O objetivo central do SFT é ajustar os parâmetros $\theta$ do modelo para que ele gere saídas que se alinhem estreitamente com as soluções fornecidas no conjunto de dados de treinamento [^28]. Este objetivo é formalizado pela maximização da função objetiva apresentada anteriormente [^28]:

$$
\mathcal{J}_{SFT}(\theta) = \mathbb{E}_{(q, o) \sim P_{SFT}(Q,O)} \left[ \frac{1}{|o|} \sum_{t=1}^{|o|} \log \pi_{\theta}(o_t | q, o_{<t}) \right] \eqno{(6)}
$$

Esta equação representa a probabilidade logarítmica média de gerar a solução correta ($o$) dado o problema ($q$) e o contexto anterior ($o_{<t}$), sobre a distribuição do conjunto de dados SFT $P_{SFT}(Q,O)$ [^19, 28]. A maximização desta função objetiva orienta o modelo para aprender a mapear entradas para saídas da forma mais precisa possível, dentro das restrições de sua arquitetura e capacidades [^11, 13, 28].

**Interpretação do Conjunto de Dados SFT como Seleção Humana**

Uma perspectiva crucial para entender a eficácia do SFT é reconhecer que o conjunto de dados $P_{SFT}(Q,O)$ não é aleatório, mas sim um produto da *seleção humana* [^19]. Em outras palavras, os problemas e soluções incluídos no conjunto de dados SFT são escolhidos e anotados por especialistas humanos ou anotadores treinados [^10]. Essa seleção humana introduz um forte viés para a qualidade, correção e relevância [^10].

*   *Qualidade*: As soluções incluídas no conjunto de dados são tipicamente bem escritas, organizadas e fáceis de entender [^10].
*   *Correção*: As soluções são verificadas para serem precisas e corretas, minimizando o risco de treinar o modelo com informações incorretas [^10].
*   *Relevância*: Os problemas e soluções são selecionados para representar uma gama diversificada de conceitos e técnicas matemáticas, garantindo que o modelo desenvolva um entendimento abrangente do domínio [^10].

Ao maximizar a probabilidade logarítmica sobre este conjunto de dados selecionado por humanos, o modelo está essencialmente aprendendo a *imitar o comportamento de especialistas humanos* [^1, 10]. Ele está aprendendo a gerar soluções que não apenas são matematicamente corretas, mas também refletem o estilo, o raciocínio e a organização de solucionadores de problemas humanos competentes [^10].

**Implicações para o Treinamento**

O reconhecimento da seleção humana no conjunto de dados SFT tem várias implicações importantes para o processo de treinamento [^10, 19]:

1.  *Importância da Qualidade dos Dados*: A qualidade do conjunto de dados SFT é de extrema importância [^10]. Dados de alta qualidade levam a um treinamento mais eficaz e a um melhor desempenho do modelo.
2.  *Necessidade de Diversidade*: O conjunto de dados SFT deve ser diversificado para cobrir uma ampla gama de conceitos e técnicas matemáticas [^10]. Isto garante que o modelo generalize bem para problemas novos e desconhecidos.
3.  *Papel do Projeto de Dados*: Os formatos das soluções (CoT, PoT, raciocínio integrado a ferramentas) devem ser cuidadosamente projetados para facilitar o aprendizado e o raciocínio eficazes [^10].
4.  *Viés e Alinhamento*: Os modelos SFT podem herdar vieses presentes no conjunto de dados de treinamento [10]. Técnicas de alinhamento podem ser necessárias para mitigar esses vieses e garantir que o modelo se comporte de forma ética e responsável.

**O Gradiente do Objetivo SFT**

Como apresentado anteriormente, o gradiente do objetivo SFT é [^28]:

$$
\nabla_{\theta} \mathcal{J}_{SFT} = \mathbb{E}_{(q, o) \sim P_{SFT}(Q,O)} \left[ \frac{1}{|o|} \sum_{t=1}^{|o|} \nabla_{\theta} \log \pi_{\theta}(o_t | q, o_{<t}) \right] \eqno{(7)}
$$

Este gradiente direciona o processo de otimização, guiando os parâmetros do modelo em direção a valores que maximizam a probabilidade das soluções corretas [^11, 13, 28]. O gradiente é calculado em relação aos parâmetros do modelo ($\theta$) e depende da distribuição do conjunto de dados SFT ($P_{SFT}(Q,O)$) [^19, 28].

### Conclusão

No Supervised Fine-tuning (SFT), o objetivo de maximizar a probabilidade logarítmica de saídas condicionadas às entradas é fundamental para construir modelos de linguagem capazes de raciocínio matemático [^1, 10, 28]. A interpretação do conjunto de dados SFT como um produto de seleção humana destaca a importância da qualidade, correção e relevância dos dados de treinamento [^19]. Ao aprender a imitar o comportamento de especialistas humanos, o modelo pode desenvolver um profundo entendimento de conceitos matemáticos e aprender a resolver problemas de forma eficaz [^10]. Compreender o objetivo da maximização e suas implicações para o treinamento é essencial para desenvolver modelos de linguagem matemática de alto desempenho [^1, 10, 11, 13].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.

[^8]: Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming – the rise of code intelligence, 2024.

[^10]: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

[^11]: Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.

[^13]: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*, 2017.

[^19]: Methods Table 10 | The data source and gradient coefficient of different methods. *P*sft denotes the data distribution of supervised fine-tuning datasets. πθsft and rθ denote the supervised fine-tuned model and the real-time policy model during the online training process, respectively.

[^28]: Appendix A.1.1 Supervised Fine-tuning
<!-- END -->