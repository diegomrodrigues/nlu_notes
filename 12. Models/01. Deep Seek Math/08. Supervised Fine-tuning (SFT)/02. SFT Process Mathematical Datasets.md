## Supervised Fine-tuning (SFT)

### Introdução
Em continuidade ao capítulo anterior sobre Supervised Fine-tuning (SFT) e seu papel fundamental no desenvolvimento de modelos de linguagem matemática de alto desempenho [^1, 10], este capítulo se aprofunda no processo específico de SFT aplicado ao DeepSeekMath [^1], com foco particular nos conjuntos de dados matemáticos em inglês e chinês [^10] utilizados durante o fine-tuning. Discutiremos como esses conjuntos de dados são construídos [^10], anotados [^10], e utilizados para otimizar o modelo para raciocínio matemático complexo [^10]. Expandindo o conceito apresentado de SFT, este capítulo visa fornecer uma compreensão mais granular de como os modelos podem ser adaptados a domínios específicos, como matemática, por meio de conjuntos de dados supervisionados [^8, 10].

### Conceitos Fundamentais

Como vimos anteriormente, o Supervised Fine-tuning (SFT) é uma técnica essencial para otimizar modelos de linguagem, em especial aqueles destinados a tarefas complexas como raciocínio matemático [^1, 8, 10]. A equação geral para o objetivo do SFT e seu gradiente foram apresentados anteriormente [^28]. No contexto do DeepSeekMath [^1], o processo de SFT envolve o uso de conjuntos de dados meticulosamente selecionados e anotados [^10] para adaptar um modelo pré-treinado para resolver problemas matemáticos complexos [^1, 10]. O foco deste capítulo são os datasets matemáticos em inglês e chinês utilizados no SFT [^10].

**Conjuntos de Dados Matemáticos em Inglês**

A base do SFT para o DeepSeekMath inclui a curadoria e anotação de vários conjuntos de dados matemáticos em inglês [^10]:

1.  *Anotação de Problemas GSM8K e MATH*: Os conhecidos conjuntos de dados GSM8K (Cobbe et al., 2021) e MATH (Hendrycks et al., 2021) são enriquecidos com soluções integradas a ferramentas [^10]. A integração de ferramentas permite que o modelo aprenda a utilizar recursos externos, como calculadoras ou sistemas de álgebra computacional [^10].

2.  *Adoção de um Subconjunto do MathInstruct*: O conjunto de dados MathInstruct (Yue et al., 2023) fornece um amplo conjunto de problemas matemáticos com soluções detalhadas [^10]. A adoção de um subconjunto permite concentrar o treinamento em áreas específicas de interesse ou desafios [^10].

3.  *Inclusão do Conjunto de Treinamento Lila-OOD*: O Lila-OOD (Mishra et al., 2022) oferece uma coleção diversificada de problemas resolvidos usando Chain-of-Thought (CoT) ou Program-of-Thought (PoT) [^10]. CoT e PoT são técnicas que incentivam o modelo a gerar passos de raciocínio intermediários, imitando o processo de resolução de problemas humanos [^10].

A coleção inglesa resultante cobre um amplo espectro de domínios matemáticos [^10]:

*   Álgebra
*   Probabilidade
*   Teoria dos Números
*   Cálculo
*   Geometria

**Conjuntos de Dados Matemáticos em Chinês**

O DeepSeekMath também incorpora conjuntos de dados matemáticos em chinês para garantir o desempenho multilíngue [^10, 7]. Estes conjuntos de dados são construídos coletando problemas matemáticos chineses K-12 [^10], que abrangem 76 sub-tópicos, tais como:

*   Equações Lineares

Esses problemas são anotados com soluções utilizando tanto Chain-of-Thought (CoT) quanto formatos de raciocínio integrados a ferramentas [^10]. Isso permite que o modelo aprenda a resolver problemas em chinês, mantendo ao mesmo tempo a capacidade de utilizar ferramentas externas [^10].

**Formato das Soluções**

Conforme mencionado anteriormente, as soluções são fornecidas em três formatos principais [^10]:

1.  *Chain-of-Thought (CoT)*: O modelo gera uma série de etapas de raciocínio intermediárias que levam à solução final [^10].
2.  *Program-of-Thought (PoT)*: O modelo escreve um programa para resolver o problema, executando-o para obter a resposta [^10].
3.  *Raciocínio Integrado a Ferramentas*: O modelo utiliza ferramentas externas, como calculadoras ou sistemas de álgebra computacional, como parte do processo de resolução [^10].

O número total de exemplos de treinamento usados no SFT para DeepSeekMath é de aproximadamente 776K [^10].

**Configuração do Treinamento**
No DeepSeekMath-Instruct 7B [^10], os exemplos de treinamento são concatenados de forma aleatória até que um comprimento máximo de contexto de 4K tokens seja alcançado [^10]. O modelo é treinado por 500 passos, utilizando um tamanho de lote de 256 e uma taxa de aprendizado constante de 5e-5 [^10].

**Avaliação**
Os modelos resultantes são rigorosamente avaliados em benchmarks de raciocínio quantitativo em inglês e chinês, tanto com quanto sem o uso de ferramentas [^10]. Essa avaliação visa quantificar as capacidades matemáticas do modelo e compará-las com as de outros modelos líderes, de código aberto e fechado [^10].

### Conclusão

O SFT é um componente crítico no desenvolvimento de modelos de linguagem matemática de alto desempenho como o DeepSeekMath [^1, 10]. Ao usar conjuntos de dados de alta qualidade, diversificados, e cuidadosamente anotados, abrangendo problemas em inglês e chinês [^10], os modelos podem aprender a resolver problemas matemáticos complexos com maior precisão e eficiência [^1, 10]. A combinação de Chain-of-Thought, Program-of-Thought, e raciocínio integrado a ferramentas permite que o modelo desenvolva um entendimento profundo de conceitos matemáticos e aplique várias técnicas de resolução de problemas [^10].

Este processo de fine-tuning, exemplificado pelo DeepSeekMath [^1], sublinha a importância da curadoria de dados e do design de tarefas no desenvolvimento de modelos de linguagem competentes em domínios específicos [^10].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.

[^7]: Figure 3 | Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora.

[^8]: Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming – the rise of code intelligence, 2024.

[^10]: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

[^28]: Appendix A.1.1 Supervised Fine-tuning
<!-- END -->