## Supervised Fine-tuning (SFT)

### Introdução
Este capítulo, em continuidade aos anteriores sobre Supervised Fine-tuning (SFT), explora os formatos específicos das soluções usadas durante o SFT para modelos de raciocínio matemático, como o DeepSeekMath [^1, 10]. Especificamente, o foco está nos formatos de Chain-of-Thought (CoT), Program-of-Thought (PoT) e raciocínio integrado a ferramentas [^10], e como eles contribuem para as habilidades de resolução de problemas do modelo. Aprofundando os conceitos de conjuntos de dados e processo de treinamento, este capítulo explora a importância de projetar dados de treinamento que imitem processos de raciocínio humano e permitam o uso de ferramentas externas [^8, 10].

### Conceitos Fundamentais

Como estabelecido, Supervised Fine-tuning (SFT) é crucial para o desenvolvimento de modelos de linguagem capazes de raciocínio matemático [^1, 8, 10]. Além de curar um conjunto de dados diversificado, o formato das soluções dentro desse conjunto de dados desempenha um papel significativo no desempenho do modelo [^10]. DeepSeekMath emprega três formatos de solução principais [^10]: Chain-of-Thought (CoT), Program-of-Thought (PoT) e raciocínio integrado a ferramentas [^10].

**1. Chain-of-Thought (CoT)**

Chain-of-Thought (CoT) prompting é uma técnica que incentiva o modelo a gerar uma série de etapas de raciocínio intermediárias que levam à solução final [^10]. Essas etapas de raciocínio servem como uma explicação passo a passo do processo de resolução de problemas, permitindo que o modelo decomponha problemas complexos em subproblemas menores e mais gerenciáveis [^10].

O processo CoT pode ser descrito formalmente como a geração de uma sequência de tokens $o = (o_1, o_2, ..., o_{|o|})$ onde cada token $o_t$ representa um passo no processo de raciocínio [^11, 13]. O objetivo é maximizar a probabilidade de gerar a sequência correta de passos de raciocínio dados o problema $q$, ou seja, maximizar $\pi_{\theta}(o | q)$ [^28].

Ao treinar com dados CoT, o modelo aprende a:

*   Decompor problemas complexos [^10]
*   Raciocinar passo a passo [^10]
*   Gerar explicações coerentes [^10]

**2. Program-of-Thought (PoT)**

Program-of-Thought (PoT) prompting envolve o modelo escrevendo um programa (por exemplo, em Python) para resolver o problema e, em seguida, executando esse programa para obter a resposta [^10]. PoT permite que o modelo alavanque o poder da computação para resolver problemas matemáticos complexos, especialmente aqueles que envolvem cálculos complexos ou manipulação de dados [^10].

No PoT, a saída $o$ representa um programa que, quando executado, produz a solução para o problema $q$ [^10]. O modelo aprende a gerar programas sintaticamente corretos e semanticamente significativos que podem resolver uma ampla gama de problemas matemáticos [^10].

Treinar com dados PoT permite que o modelo:

*   Aproveitar o poder da computação [^10]
*   Resolver problemas complexos com maior precisão [^10]
*   Realizar manipulação simbólica e numérical [^10]

Chen et al. (2022) e Gao et al. (2023) exploram em detalhe a aplicação do PoT.

**3. Raciocínio Integrado a Ferramentas**

O raciocínio integrado a ferramentas permite que o modelo utilize ferramentas externas, como calculadoras, sistemas de álgebra computacional ou buscadores da web, como parte do processo de resolução de problemas [^10]. Este formato de solução permite que o modelo alavanque o conhecimento e as capacidades de ferramentas externas para resolver problemas que estão além de suas próprias capacidades [^10].

No raciocínio integrado a ferramentas, a saída $o$ representa uma sequência de ações que incluem tanto os passos de raciocínio quanto as chamadas de ferramentas, ou seja, $o = (a_1, a_2, ..., a_{|o|})$ onde cada ação $a_i$ pode ser um passo de raciocínio ou uma chamada de ferramenta [^10]. O modelo aprende a determinar quando é apropriado usar uma ferramenta, qual ferramenta usar e como interpretar a saída da ferramenta para progredir em direção à solução [^10].

Ao treinar com dados de raciocínio integrado a ferramentas, o modelo aprende a:

*   Determinar quando usar ferramentas externas [^10]
*   Selecionar as ferramentas apropriadas para a tarefa [^10]
*   Interpretar a saída da ferramenta [^10]
*   Integrar o conhecimento externo ao raciocínio [^10]

Gou et al. (2023) fornecem informações adicionais sobre raciocínio integrado a ferramentas.

**A Importância de Múltiplos Formatos**

A utilização de múltiplos formatos de solução (CoT, PoT e raciocínio integrado a ferramentas) no conjunto de dados SFT para DeepSeekMath permite que o modelo aprenda uma abordagem multifacetada para a resolução de problemas [^10]. Ao ser exposto a diferentes formatos, o modelo aprende a:

*   Adaptar sua estratégia de resolução de problemas ao problema específico [^10]
*   Alavancar seus próprios pontos fortes [^10]
*   Compensar suas próprias fraquezas [^10]
*   Desenvolver um entendimento mais profundo de conceitos matemáticos [^10]

### Conclusão

Os formatos de solução utilizados durante o Supervised Fine-tuning (SFT) desempenham um papel crítico na modelagem das capacidades de raciocínio matemático de modelos de linguagem como o DeepSeekMath [^1, 10]. Ao incorporar Chain-of-Thought (CoT), Program-of-Thought (PoT) e raciocínio integrado a ferramentas, o conjunto de dados SFT permite que o modelo aprenda a abordar problemas matemáticos de uma variedade de perspectivas, aproveitando tanto suas próprias capacidades de raciocínio quanto o conhecimento e o poder de ferramentas externas [^10]. Esta abordagem multifacetada leva a um entendimento mais profundo de conceitos matemáticos e aprimora as habilidades de resolução de problemas [^1, 10].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.

[^8]: Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming – the rise of code intelligence, 2024.

[^10]: DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models

[^11]: Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.

[^13]: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*, 2017.

[^28]: Appendix A.1.1 Supervised Fine-tuning
<!-- END -->