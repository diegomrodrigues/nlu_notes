## DeepSeekMath: Superando os Limites do Raciocínio Matemático em Modelos de Linguagem Abertos
### O Modelo Base DeepSeekMath-Base 7B: Avaliação e Desempenho

#### Introdução

Em continuidade aos capítulos anteriores sobre a construção do DeepSeekMath Corpus [^2, 4, 5] e o processo de *supervised fine-tuning* (SFT) para instrução matemática [^10, 11], este capítulo foca no desempenho e nas características do modelo base DeepSeekMath-Base 7B. Este modelo, pre-treinado com o DeepSeekMath Corpus, demonstra notáveis habilidades de raciocínio, servindo como um componente fundamental para a obtenção de resultados de ponta em *benchmarks* de matemática [^1, 2]. Analisaremos aqui as capacidades do modelo base e o seu desempenho em diversas tarefas, com especial atenção para a sua superioridade em relação a outros modelos *open-source*.

#### Arquitetura e Treinamento do DeepSeekMath-Base 7B

O modelo DeepSeekMath-Base 7B é inicializado com o DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024) [^2, 8]. A escolha de um modelo pré-treinado em código é motivada pela observação de que este tipo de inicialização oferece melhores resultados em comparação com *large language models* (LLMs) de propósito geral [^2]. O modelo é treinado por 500B *tokens*, com a seguinte distribuição de dados [^8]:

*   56% do DeepSeekMath Corpus [^8]
*   4% de AlgebraicStack [^8]
*   10% de arXiv [^8]
*   20% de código Github [^8]
*   10% de dados em linguagem natural do Common Crawl, em inglês e chinês [^8]

As configurações de treinamento seguem as especificações da Seção 2.2.1 [^8], com algumas adaptações: a taxa de aprendizado máxima é definida em 4.2e-4, e o tamanho do *batch* é de 10M *tokens* [^8].

#### Avaliação do Desempenho em Benchmarks Matemáticos

O desempenho do DeepSeekMath-Base 7B é avaliado em diversos *benchmarks* para quantificar as suas capacidades de raciocínio matemático [^8]. O modelo é testado em oito *benchmarks* em inglês e chinês, utilizando *few-shot chain-of-thought prompting* (Wei et al., 2022) [^8]. Esses *benchmarks* abrangem raciocínio quantitativo (por exemplo, GSM8K, MATH, CMATH) e problemas de múltipla escolha (por exemplo, MMLU-STEM, Gaokao-MathQA), cobrindo diversas áreas da matemática, desde o nível elementar até o universitário [^8].

Os resultados da avaliação (Tabela 2 em [^8]) demonstram que o DeepSeekMath-Base 7B supera todos os modelos base *open-source*, incluindo o Mistral 7B (Jiang et al., 2023) e o Llemma 34B (Azerbayev et al., 2023) [^8]. Notavelmente, no *benchmark* MATH, o DeepSeekMath-Base 7B supera os modelos base *open-source* em mais de 10% absoluto [^8].

É crucial destacar que o modelo alcança 64,2% no GSM8K (Cobbe et al., 2021) e 36,2% no *dataset* MATH em nível de competição (Hendrycks et al., 2021) [^8]. Esse desempenho supera o Minerva 540B (Lewkowycz et al., 2022a), um modelo *closed-source* 77 vezes maior, que é construído sobre o PaLM (Lewkowycz et al., 2022b) e treinado com dados matemáticos adicionais [^8].

#### Raciocínio com Ferramentas e Prova Formal

Além da resolução de problemas matemáticos com raciocínio passo a passo, o DeepSeekMath-Base 7B é avaliado em sua capacidade de utilizar ferramentas e realizar provas formais [^9].

*   **Raciocínio com Ferramentas:** Avalia-se o raciocínio matemático assistido por programa no GSM8K e MATH utilizando *few-shot program-of-thought prompting* (Chen et al., 2022; Gao et al., 2023) [^9]. Os modelos são instruídos a resolver cada problema escrevendo um programa Python que utiliza bibliotecas como `math` e `sympy` para cálculos complexos. O resultado da execução do programa é considerado a resposta. O DeepSeekMath-Base 7B supera o estado da arte anterior, Llemma 34B, nesta tarefa [^9].
*   **Prova Formal:** Avalia-se o DeepSeekMath-Base 7B na tarefa de prova informal para formal (Jiang et al., 2022) [^9]. O objetivo é gerar uma prova formal baseada em uma declaração informal, um equivalente formal da declaração e uma prova informal. A avaliação é realizada no miniF2F (Zheng et al., 2021), um *benchmark* para matemática formal de nível de Olimpíada. O DeepSeekMath-Base 7B demonstra forte desempenho na automatização da prova [^9].

#### Outras Capacidades: Linguagem Natural e Código

Para construir um perfil abrangente do modelo base, avalia-se o DeepSeekMath-Base 7B em suas capacidades de compreensão de linguagem natural, raciocínio e habilidades de codificação [^9]. Utiliza-se os seguintes *benchmarks* [^9]:

*   **MMLU** (Massive Multitask Language Understanding): Avalia a compreensão geral da linguagem em 57 tarefas de múltipla escolha [^9] (Hendrycks et al., 2020).
*   **BBH** (BIG-Bench Hard): Consiste em 23 tarefas desafiadoras que requerem raciocínio em múltiplas etapas [^9] (Suzgun et al., 2022).
*   **HumanEval** e **MBPP**: Amplamente utilizados para avaliar modelos de linguagem de código [^9] (Chen et al., 2021; Austin et al., 2021).

Os resultados demonstram que o pré-treinamento em matemática beneficia o modelo tanto na compreensão da linguagem quanto no desempenho do raciocínio [^9].

#### Conclusão

O DeepSeekMath-Base 7B representa um avanço significativo no desenvolvimento de modelos de linguagem com capacidades robustas de raciocínio matemático [^8]. Ao demonstrar um desempenho superior em diversos *benchmarks* e tarefas, o modelo estabelece um novo patamar para os LLMs *open-source* na área de matemática [^9]. A combinação de uma arquitetura bem projetada, treinamento em um *corpus* de alta qualidade e avaliações rigorosas contribui para o sucesso do DeepSeekMath-Base 7B [^1, 2]. Este modelo serve como uma base sólida para o desenvolvimento de modelos ainda mais avançados, como o DeepSeekMath-Instruct 7B [^11] e o DeepSeekMath-RL [^11].

#### Referências
[^1]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^2]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^4]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^5]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^8]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^9]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^10]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^11]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->