## DeepSeekMath: Superando os Limites do Raciocínio Matemático em Modelos de Linguagem Abertos
### Afinação Supervisionada Fina: Instrução Matemática

#### Introdução
Em continuidade ao processo de construção do DeepSeekMath Corpus e pré-treinamento do modelo base, DeepSeekMath-Base, este capítulo detalha o processo de afinação supervisionada fina (SFT) [^1, 2]. A afinação supervisionada de modelos de linguagem com *datasets* cuidadosamente construídos é uma etapa crucial para melhorar o desempenho em tarefas específicas [^10]. No caso do DeepSeekMath, a afinação instrucional matemática visa aprimorar as habilidades de raciocínio matemático do modelo, utilizando uma variedade de técnicas e *datasets* [^2].

#### Metodologia de Afinação Supervisionada Fina

Após o pré-treinamento no DeepSeekMath Corpus, o modelo DeepSeekMath-Base passa por um processo de afinação supervisionada fina com foco em instrução matemática [^2]. Este processo utiliza uma combinação de técnicas, incluindo *chain-of-thought* (CoT) [^2, 10], *program-of-thought* (PoT) [^2, 10], e raciocínio integrado a ferramentas, com o objetivo de refinar as capacidades de raciocínio matemático do modelo [^2].

**Dataset de Instrução Matemática:**
O *dataset* de afinação instrucional matemática é construído abrangendo problemas em inglês e chinês de diferentes áreas matemáticas e de vários níveis de complexidade [^10]. Os problemas são emparelhados com soluções nos formatos CoT, PoT e raciocínio integrado a ferramentas [^10]. O número total de exemplos de treinamento é de 776K [^10].

**Componentes do Dataset:**
*   **Datasets matemáticos em inglês:** Anota-se problemas GSM8K e MATH com soluções integradas a ferramentas e adota-se um subconjunto de MathInstruct (Yue et al., 2023), juntamente com o conjunto de treinamento de Lila-OOD (Mishra et al., 2022), onde os problemas são resolvidos com CoT ou PoT [^10]. A coleção em inglês abrange diversos campos da matemática, por exemplo, álgebra, probabilidade, teoria dos números, cálculo e geometria [^10].
*   **Datasets matemáticos em chinês:** Coleta-se problemas matemáticos chineses K-12 abrangendo 76 sub-tópicos, como equações lineares, com soluções anotadas em CoT e formato de raciocínio integrado a ferramentas [^10].

**Procedimento de Treinamento:**

Os exemplos de treinamento são concatenados aleatoriamente até atingir um comprimento máximo de contexto de 4K *tokens* [^10]. Treina-se o modelo por 500 *steps* com um tamanho de *batch* de 256 e uma taxa de aprendizado constante de 5e-5 [^10].

#### Avaliação do DeepSeekMath-Instruct 7B

O desempenho matemático dos modelos é avaliado com e sem o uso de ferramentas, em 4 *benchmarks* de raciocínio quantitativo em inglês e chinês [^10]. O modelo é comparado com os principais modelos da época [^10]:

*   **Modelos *Closed-source***: Inclui a família GPT, sendo GPT-4 e GPT-4 Code Interpreter os mais capazes [^10] (OpenAI, 2023), Gemini Ultra e Pro (Anil et al., 2023), Inflection-2 (Inflection AI, 2023), Grok-1, bem como modelos lançados recentemente por empresas chinesas, incluindo Baichuan-3, o mais recente GLM-4 da família GLM (Du et al., 2022) [^10].
    > Estes modelos são para fins gerais, a maioria dos quais passou por uma série de procedimentos de alinhamento.

*   **Modelos *Open-source***: Inclui modelos gerais como DeepSeek-LLM-Chat 67B (DeepSeek-AI, 2024), Qwen 72B (Bai et al., 2023), SeaLLM-v2 7B (Nguyen et al., 2023), e ChatGLM3 6B (ChatGLM3 Team, 2023), bem como modelos com melhorias em matemática, incluindo InternLM2-Math 20B que se baseia no InternLM2 e passou por treinamento matemático seguido de ajuste de instrução, Math-Shepherd-Mistral 7B que aplica treinamento PPO (Schulman et al., 2017) ao Mistral 7B (Jiang et al., 2023) com um modelo de recompensa supervisionado por processo, a série WizardMath (Luo et al., 2023) que melhora o raciocínio matemático no Mistral 7B e Llama-2 70B (Touvron et al., 2023) usando evolve-instruct e treinamento PPO com problemas de treinamento provenientes principalmente de GSM8K e MATH, MetaMath 70B (Yu et al., 2023) que é Llama-2 70B ajustado em uma versão aumentada de GSM8K e MATH, TORA 34B Gou et al. (2023) que é CodeLlama 34B ajustado para fazer raciocínio matemático integrado a ferramentas, MAmmoTH 70B (Yue et al., 2023) que é Llama-2 70B instruído no MathInstruct [^10].

#### Resultados da Avaliação

Os resultados da avaliação demonstram que o DeepSeekMath-Instruct 7B apresenta um forte desempenho de raciocínio passo a passo [^11]. No *dataset* MATH, o modelo supera todos os modelos *open-source* e a maioria dos modelos proprietários (por exemplo, Inflection-2 e Gemini Pro) em pelo menos 9% absoluto [^11]. Sob a configuração de avaliação onde os modelos podem integrar raciocínio de linguagem natural e uso de ferramentas baseadas em programa para resolução de problemas, DeepSeekMath-Instruct 7B se aproxima de uma precisão de 60% no MATH, superando todos os modelos *open-source* existentes [^11]. Nos outros *benchmarks*, o modelo é competitivo com o DeepSeek-LLM-Chat 67B, o estado da arte anterior que é 10 vezes maior [^11].

#### Conclusão

A afinação supervisionada fina com instrução matemática é um passo crucial para equipar os LLMs com capacidades avançadas de raciocínio [^2, 10]. A metodologia adotada no DeepSeekMath-Instruct 7B, que inclui uma cuidadosa curadoria de *datasets*, técnicas de afinação diversas e avaliações rigorosas, resulta em um modelo que supera significativamente seus concorrentes em tarefas de raciocínio matemático [^11]. Este trabalho destaca a importância de *datasets* de alta qualidade e metodologias de treinamento eficazes para o avanço da inteligência artificial na área de matemática [^2].

#### Referências
[^2]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^10]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^11]: Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
<!-- END -->