## Arquitetura do Modelo DeepSeekMath: Inicialização com DeepSeek-Coder-Base-v1.5

### Introdução
Este capítulo explora a decisão de inicializar o DeepSeekMath-Base com o DeepSeek-Coder-Base-v1.5 [^1], investigando as razões pelas quais começar com um modelo de treinamento de código é considerado uma escolha superior em comparação com um *LLM* geral. Esta análise se apoia nos conceitos de otimização, tamanho do lote, comprimento do contexto e *GRPO* discutidos nos capítulos anteriores, fornecendo uma compreensão abrangente da arquitetura do DeepSeekMath e das escolhas de *design* que contribuem para seu desempenho em tarefas de raciocínio matemático.

### Conceitos Fundamentais

A inicialização do DeepSeekMath-Base com DeepSeek-Coder-Base-v1.5 [^1] é uma decisão arquitetural chave que tem implicações significativas para as capacidades do modelo. Essa escolha sugere que um modelo pré-treinado em código oferece vantagens distintas sobre um *LLM* geral quando se trata de desenvolver habilidades de raciocínio matemático [^1].

**Vantagens do Treinamento com Código para Raciocínio Matemático:**
A literatura e as experiências práticas sugerem que o treinamento com código pode melhorar significativamente as capacidades de raciocínio dos modelos de linguagem, especialmente no domínio matemático [^3]. Existem várias razões para isso:

1.  ***Estrutura e Formalidade***: A linguagem de programação, por sua própria natureza, é estruturada e formal. O treinamento com código força o modelo a aprender a manipular símbolos e estruturas lógicas de forma precisa e consistente. Essa capacidade de processar informações formais e estruturadas é diretamente transferível para o raciocínio matemático, que também requer manipulação precisa de símbolos e estruturas lógicas [^3].
2.  ***Pensamento Algorítmico***: O treinamento com código expõe o modelo ao pensamento algorítmico, que envolve decompor problemas complexos em etapas menores e gerenciáveis. Essa habilidade de decompor problemas e aplicar soluções passo a passo é fundamental para o raciocínio matemático, que muitas vezes requer a resolução de problemas complexos por meio de uma sequência lógica de etapas [^3].
3.  ***Raciocínio Lógico***: O código depende fortemente do raciocínio lógico, onde as declarações devem seguir uma lógica precisa para que o programa seja executado corretamente. O treinamento com código força o modelo a aprender a identificar e aplicar padrões lógicos, o que é crucial para o raciocínio matemático, que também depende fortemente da aplicação de princípios lógicos [^3].

**DeepSeek-Coder-Base-v1.5 como Ponto de Partida:**
O DeepSeek-Coder-Base-v1.5 [^7] é um modelo de linguagem especificamente treinado em código. Ao inicializar o DeepSeekMath-Base com este modelo, a equipe de *design* aproveita as vantagens inerentes do treinamento com código, permitindo que o modelo comece com uma base sólida em raciocínio lógico, pensamento algorítmico e manipulação de informações estruturadas.

**Comparação com LLMs Gerais:**
Em contraste com os modelos de código, os *LLMs* gerais são tipicamente treinados em uma gama mais ampla de dados textuais, incluindo texto narrativo, conversacional e informativo. Embora esses modelos possam ter um amplo conhecimento do mundo e habilidades de compreensão da linguagem, eles podem carecer da precisão e estrutura necessárias para o raciocínio matemático [^1].

A escolha de inicializar o DeepSeekMath-Base com um modelo de código sugere que os benefícios do raciocínio estruturado e do pensamento algorítmico superam as vantagens de um amplo conhecimento do mundo no contexto do raciocínio matemático.

**Math Training Melhora as Habilidades em MMLU e BBH**
Além de raciocínio matemático, foi observado que o treinamento matemático também melhora a capacidade do modelo em benchmarks como *Massive Multitask Language Understanding (MMLU)* [^2] e *BIG-Bench Hard (BBH)*. Isso indica que não só aumenta as capacidades matemáticas do modelo, mas também amplifica as capacidades gerais de raciocínio.

**Aproveitando os Recursos de Dados:**

A eficácia da inicialização com um modelo de treinamento de código também depende da disponibilidade e qualidade dos dados usados nas etapas subsequentes do treinamento. Conforme discutido no capítulo anterior, o DeepSeekMath Corpus, um conjunto de dados de alta qualidade abrangendo conteúdo matemático multilingue, desempenha um papel vital no aprimoramento das capacidades de raciocínio matemático do modelo.

### Conclusão

A decisão de inicializar o DeepSeekMath-Base com o DeepSeek-Coder-Base-v1.5 reflete uma compreensão profunda das *trade-offs* envolvidas no *design* de modelos de linguagem para raciocínio matemático [^1]. Ao começar com um modelo treinado em código, a equipe de *design* garante que o DeepSeekMath-Base tenha uma base sólida em raciocínio lógico, pensamento algorítmico e manipulação de informações estruturadas, capacidades cruciais para o sucesso em tarefas de raciocínio matemático. Essa abordagem, combinada com uma seleção cuidadosa de dados de treinamento e técnicas de otimização eficientes, como *GRPO*, contribui para o desempenho notável do DeepSeekMath em *benchmarks* de raciocínio matemático. A arquitetura do modelo DeepSeekMath se beneficia de uma estratégia de otimização cuidadosamente projetada, que inclui o otimizador AdamW com configurações específicas e uma programação de taxa de aprendizado *multi-etapa*. Esses componentes trabalham em conjunto para garantir um treinamento eficiente e estável, resultando em um modelo com capacidades notáveis de raciocínio matemático [^1]. Estudos demonstraram que o *code training* prévio ao treinamento matemático melhora a capacidade dos modelos de resolver problemas matemáticos, tanto com quanto sem o uso de ferramentas [^3]. Além disso, o uso do *DeepSeekMath Corpus* demonstrou ser de alta qualidade e cobre conteúdo matemático multilingue [^4].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^2]: D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.
[^3]: W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022.
[^4]: K. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality mathematical web text. CoRR, abs/2310.06786, 2023.
[^5]: D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[^6]: I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
[^7]: D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming — the rise of code intelligence, 2024.
<!-- END -->