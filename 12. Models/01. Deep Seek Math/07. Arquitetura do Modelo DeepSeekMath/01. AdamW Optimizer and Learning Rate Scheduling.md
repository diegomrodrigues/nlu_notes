## Arquitetura do Modelo DeepSeekMath: Otimização e Treinamento Detalhado

### Introdução
Este capítulo explora a arquitetura do modelo DeepSeekMath, focando nos detalhes do processo de treinamento, incluindo o otimizador **AdamW** e a programação de taxa de aprendizado *multi-etapa*. O objetivo é fornecer uma compreensão profunda das técnicas de otimização utilizadas para alcançar o desempenho notável demonstrado pelo modelo [^1].

### Conceitos Fundamentais

O modelo DeepSeekMath, como demonstrado em [^1], [^2], [^3], representa um avanço significativo no raciocínio matemático em *Large Language Models (LLMs)*. Para atingir esse nível de capacidade, a arquitetura do modelo passa por um rigoroso processo de treinamento, que inclui a utilização de técnicas de otimização bem definidas [^6].

**Otimizador AdamW**:
O otimizador **AdamW** [^6] é uma extensão do algoritmo Adam que aborda as limitações do *weight decay* em cenários de otimização adaptativa. AdamW separa o *weight decay* da atualização do gradiente, resultando em melhor generalização e desempenho superior, especialmente em modelos profundos como os LLMs. No contexto do DeepSeekMath, o otimizador AdamW é configurado com os seguintes parâmetros:

*   **β1 (Beta 1)**: Este parâmetro controla a taxa de decaimento exponencial das médias móveis dos gradientes. No treinamento do DeepSeekMath, β1 é definido como 0.9 [^6].
*   **β2 (Beta 2)**: Este parâmetro controla a taxa de decaimento exponencial das médias móveis dos gradientes quadrados. No treinamento do DeepSeekMath, β2 é definido como 0.95 [^6].
*   **Weight Decay**: O *weight decay* é uma técnica de regularização que penaliza pesos grandes, evitando o *overfitting*. No treinamento do DeepSeekMath, o *weight decay* é definido como 0.1 [^6].

**Programação de Taxa de Aprendizado Multi-Etapa**:
A *taxa de aprendizado* é um hiperparâmetro crucial que controla o tamanho do passo durante a otimização. Uma taxa de aprendizado inadequada pode levar a convergência lenta ou divergência. A programação de taxa de aprendizado *multi-etapa* é uma técnica que ajusta dinamicamente a taxa de aprendizado durante o treinamento, permitindo uma convergência mais rápida e precisa. No treinamento do DeepSeekMath [^6], a programação da taxa de aprendizado segue as seguintes etapas:

1.  **Warmup**: A taxa de aprendizado aumenta linearmente a partir de zero até um valor máximo ao longo de 2.000 passos (*warmup steps*). Esta fase inicial ajuda a estabilizar o treinamento, evitando grandes atualizações no início do processo [^6].
2.  **Decaimento**: Após os *warmup steps*, a taxa de aprendizado é reduzida para 31.6% do seu valor máximo após 80% do processo de treinamento. Isso ajuda a refinar a busca e a convergir para um mínimo local mais preciso [^6].
3.  **Decaimento Final**: Após 90% do processo de treinamento, a taxa de aprendizado é ainda mais reduzida para 10% do seu valor máximo. Essa etapa final garante uma convergência fina e evita oscilações em torno do mínimo [^6].

A taxa de aprendizado máxima é definida como 5.3e-4 para o treinamento inicial de modelos com 1.3B de parâmetros [^6] e como 4.2e-4 para o treinamento do DeepSeekMath-Base 7B [^7]. Um tamanho de *batch* de 4M *tokens* com um comprimento de contexto de 4K é usado [^6]. No treinamento de DeepSeekMath-Base 7B, um tamanho de *batch* de 10M *tokens* é usado [^7].

**Aplicações e Resultados**:
O uso do otimizador AdamW com a programação de taxa de aprendizado *multi-etapa* contribui significativamente para o desempenho do DeepSeekMath. A combinação dessas técnicas permite:

*   Treinamento estável e eficiente, evitando divergência e convergindo rapidamente [^6].
*   Melhor generalização, reduzindo o *overfitting* e melhorando o desempenho em dados não vistos [^6].
*   Ajuste fino da taxa de aprendizado, adaptando-se às diferentes fases do treinamento e garantindo a convergência para um mínimo ideal [^6].

### Conclusão
A arquitetura do modelo DeepSeekMath se beneficia de uma estratégia de otimização cuidadosamente projetada, que inclui o otimizador AdamW com configurações específicas e uma programação de taxa de aprendizado *multi-etapa*. Esses componentes trabalham em conjunto para garantir um treinamento eficiente e estável, resultando em um modelo com capacidades notáveis de raciocínio matemático [^1]. Estudos demonstraram que o *code training* prévio ao treinamento matemático melhora a capacidade dos modelos de resolver problemas matemáticos, tanto com quanto sem o uso de ferramentas [^3]. Além disso, o uso do *DeepSeekMath Corpus* demonstrou ser de alta qualidade e cobre conteúdo matemático multilingue [^4].

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^2]: D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.
[^3]: W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022.
[^4]: K. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality mathematical web text. CoRR, abs/2310.06786, 2023.
[^5]: D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[^6]: I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
[^7]: D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming — the rise of code intelligence, 2024.
<!-- END -->