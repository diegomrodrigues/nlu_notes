## Arquitetura do Modelo DeepSeekMath: Raciocínio Matemático e Otimização da Política Relativa do Grupo

### Introdução
Este capítulo explora os dois fatores principais que contribuem para a capacidade de raciocínio matemático do DeepSeekMath: o aproveitamento de dados da web publicamente disponíveis através de um *pipeline* de seleção de dados meticulosamente projetado e a introdução da *Otimização da Política Relativa do Grupo (GRPO)* [^1]. Este capítulo expande a discussão anterior sobre a arquitetura do modelo, tamanho do lote e comprimento do contexto, fornecendo um *insight* mais profundo sobre os mecanismos que permitem o desempenho superior do DeepSeekMath em tarefas de raciocínio matemático.

### Conceitos Fundamentais

O sucesso do DeepSeekMath em benchmarks de raciocínio matemático é atribuído a dois pilares principais: a qualidade dos dados de treinamento e a eficiência do algoritmo de otimização utilizado.

**Aproveitamento de Dados da Web através de um *Pipeline* de Seleção Meticulosamente Projetado**
O primeiro fator crucial é a utilização de um *pipeline* de seleção de dados meticulosamente projetado para aproveitar o potencial significativo dos dados da *web* publicamente disponíveis [^1]. Esta abordagem envolve várias etapas para garantir a qualidade e relevância dos dados de treinamento:

1.  ***Data Collection and Decontamination***: O processo de construção do *DeepSeekMath Corpus* inicia-se com a coleta e descontaminação de dados da *Common Crawl*. A metodologia emprega um *pipeline* iterativo que sistematicamente coleta um *corpus* matemático de larga escala, começando com um *corpus* semente (*seed corpus*) e expandindo-o por meio de técnicas de classificação e anotação humana [^1], [^4].
2.  ***Iterative Pipeline***: A coleta de dados é realizada através de um *pipeline* iterativo. Na primeira iteração, um modelo *fastText* é treinado com um *corpus* semente, como o *OpenWebMath* [^4], e usado para identificar páginas *web* relacionadas à matemática na *Common Crawl*. Nas iterações subsequentes, as páginas *web* identificadas são manualmente anotadas para refinar o modelo *fastText*, permitindo a descoberta de mais dados matemáticos [^1].
3.  ***Data Filtering and Cleaning***: Os dados coletados são filtrados para remover conteúdo de baixa qualidade e evitar a contaminação por *benchmarks*. Isso inclui a remoção de páginas *web* que contêm perguntas ou respostas de *benchmarks* matemáticos conhecidos [^1].
4.  ***Quality Validation***: A qualidade do *DeepSeekMath Corpus* é validada por meio de experimentos de *pre-training*. Os resultados mostram que o *corpus* é de alta qualidade, cobre conteúdo matemático multilingue e é o maior em tamanho em comparação com outros *corpora* matemáticos disponíveis publicamente [^1].

**Otimização da Política Relativa do Grupo (GRPO)**
O segundo fator-chave é a introdução do algoritmo *Group Relative Policy Optimization (GRPO)*, uma variação do *Proximal Policy Optimization (PPO)* [^1]. O *GRPO* foi projetado para melhorar as capacidades de raciocínio matemático, otimizando simultaneamente o uso da memória.

1.  ***PPO Limitations***: *Proximal Policy Optimization (PPO)* é um algoritmo de aprendizado por reforço *actor-critic* amplamente utilizado no *fine-tuning* de *LLMs*. No entanto, o *PPO* requer o treinamento de uma função de valor (*value function*) em conjunto com o modelo de política, o que adiciona uma sobrecarga computacional significativa e aumenta o uso da memória [^1].
2.  ***GRPO Approach***: O *GRPO* supera as limitações do *PPO* ao renunciar ao modelo *critic* e, em vez disso, estimar a *baseline* a partir de pontuações de grupo (*group scores*). Esta abordagem reduz significativamente os recursos de treinamento necessários em comparação com o *Proximal Policy Optimization (PPO)*. O *GRPO* calcula as vantagens com base em recompensas relativas das saídas dentro de cada grupo, que se alinha bem com a natureza comparativa dos modelos de recompensa [^1].
3.  ***GRPO Formulation***: O *GRPO* otimiza o modelo de política maximizando o seguinte objetivo substituto (*surrogate objective*):

    $$
    \mathcal{L}_{GRPO}(\theta) = \mathbb{E}_{q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}} (O|q)} \left[ \frac{1}{G} \sum_{i=1}^G \sum_{t=1}^M \min \left( \frac{\pi_{\theta}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}} (o_{i,t}|q, o_{i,<t})} A_{i,t}, \text{clip} \left( \frac{\pi_{\theta}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}} (o_{i,t}|q, o_{i,<t})}, 1 - \epsilon, 1 + \epsilon \right) A_{i,t} \right) - \beta D_{KL} [\pi_{\theta}||\pi_{ref}] \right]
    $$

    onde $\epsilon$ e $\beta$ são hiperparâmetros, e $A_{i,t}$ é a vantagem calculada com base nas recompensas relativas das saídas dentro de cada grupo [^1].
4.  ***Advantages of GRPO***: Ao eliminar a necessidade de uma função de valor separada, o *GRPO* reduz significativamente o consumo de memória e a complexidade computacional. Além disso, a abordagem relativa do grupo se alinha bem com a natureza comparativa dos modelos de recompensa, que são tipicamente treinados em *datasets* de comparações entre saídas na mesma questão.

### Conclusão

A capacidade de raciocínio matemático do DeepSeekMath é o resultado de uma combinação estratégica de seleção de dados de alta qualidade e um algoritmo de otimização eficiente [^1]. Ao aproveitar um *pipeline* de seleção de dados meticulosamente projetado, o DeepSeekMath é capaz de treinar em um *corpus* vasto e relevante de dados matemáticos. Além disso, a introdução do algoritmo *GRPO* permite um *fine-tuning* eficiente do modelo, melhorando as capacidades de raciocínio matemático e reduzindo o consumo de memória. As escolhas do tamanho do lote e do comprimento do contexto são cuidadosamente equilibradas para manter a eficiência computacional e a capacidade de aprendizado [^6]. Em conjunto com a arquitetura do modelo DeepSeekMath, esses fatores contribuem para seu desempenho notável em tarefas de raciocínio matemático.

### Referências
[^1]: Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^2]: D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.
[^3]: W. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022.
[^4]: K. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality mathematical web text. CoRR, abs/2310.06786, 2023.
[^5]: D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[^6]: I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.
[^7]: D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming — the rise of code intelligence, 2024.
<!-- END -->