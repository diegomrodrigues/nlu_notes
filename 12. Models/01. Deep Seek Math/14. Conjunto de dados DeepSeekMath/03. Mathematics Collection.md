## DeepSeekMath: A Coleção de Dados em Inglês para Ajuste Fino Supervisionado

### Introdução
Construindo sobre a discussão do *dataset* de Ajuste Fino Supervisionado (SFT) utilizado para instruir o modelo DeepSeekMath [^1], este capítulo detalha a composição da coleção de dados em inglês, especificamente focando nos campos da matemática que são abrangidos. Como mencionado anteriormente, a diversidade e a abrangência do *dataset* SFT são cruciais para aprimorar as capacidades de raciocínio matemático dos *Large Language Models* (LLMs) [^6]. Dando continuidade ao tópico anterior, que detalhou os formatos CoT e *tool-integrated reasoning* [^1], agora focaremos no conteúdo matemático específico presente no conjunto de dados em inglês.

### Composição da Coleção de Dados em Inglês
A coleção de dados em inglês para o ajuste fino do DeepSeekMath [^1] foi cuidadosamente selecionada para abranger uma ampla gama de tópicos e habilidades matemáticas. O conjunto de dados é composto por [^1]:

1.  Anotações dos problemas GSM8K e MATH com soluções integradas a ferramentas.
2.  Um subconjunto de MathInstruct.
3.  O conjunto de treinamento Lila-OOD (os problemas são resolvidos com CoT ou PoT).

Essa coleção cobre diversos campos da matemática [^1], que serão detalhados a seguir.

### Campos da Matemática Abrangidos
A coleção em inglês abrange diversos campos da matemática, garantindo uma ampla exposição aos LLMs e promovendo uma compreensão abrangente das nuances matemáticas. Os principais campos cobertos são [^1]:

1.  **Álgebra**:
    *   Inclui problemas envolvendo equações lineares, quadráticas e polinomiais, bem como sistemas de equações.
    *   Abrange tópicos como fatoração, simplificação de expressões algébricas e resolução de desigualdades.
    *   Problemas de álgebra ajudam os modelos a desenvolverem habilidades de manipulação simbólica e raciocínio abstrato.

2.  **Probabilidade**:
    *   Envolve problemas relacionados ao cálculo de probabilidades de eventos, probabilidades condicionais e distribuições de probabilidade.
    *   Inclui tópicos como combinações, permutações e esperança matemática.
    *   Problemas de probabilidade auxiliam os modelos a entenderem conceitos estatísticos e a aplicarem princípios de probabilidade para resolver problemas do mundo real.

3.  **Teoria dos Números**:
    *   Abrange problemas relacionados a números inteiros, divisibilidade, números primos e congruências.
    *   Inclui tópicos como o Teorema de Fermat, o Teorema de Euler e a Criptografia RSA.
    *   Problemas de teoria dos números desafiam os modelos a aplicarem princípios matemáticos rigorosos e a desenvolverem habilidades de raciocínio dedutivo.

4.  **Cálculo**:
    *   Envolve problemas relacionados a limites, derivadas, integrais e equações diferenciais.
    *   Inclui tópicos como otimização, taxas de variação e área sob curvas.
    *   Problemas de cálculo requerem que os modelos entendam os princípios do cálculo e os apliquem para resolver problemas complexos.

5.  **Geometria**:
    *   Abrange problemas relacionados a formas geométricas, áreas, volumes e relações geométricas.
    *   Inclui tópicos como geometria euclidiana, geometria analítica e trigonometria.
    *   Problemas de geometria auxiliam os modelos a visualizarem conceitos matemáticos e a aplicarem princípios geométricos para resolver problemas espaciais.

### Integração com os Formatos CoT e *Tool-Integrated Reasoning*
A diversidade de campos da matemática na coleção de dados em inglês é ainda mais enriquecida pela integração com os formatos CoT e *tool-integrated reasoning*, conforme discutido anteriormente [^1, ^7, ^8]. A combinação desses formatos com problemas de álgebra, probabilidade, teoria dos números, cálculo e geometria permite que os modelos aprendam a [^1]:

*   Decompor problemas complexos em etapas de raciocínio lógicas (CoT).
*   Utilizar ferramentas externas (calculadoras, *solvers*) para auxiliar na resolução de problemas (raciocínio integrado a ferramentas).
*   Aplicar princípios matemáticos para resolver problemas em diversos domínios.
*   Generalizar o conhecimento para resolver novos problemas não vistos durante o treinamento.

### Resultados Experimentais e Validação
A eficácia da coleção de dados em inglês, juntamente com o *dataset* chinês, é demonstrada pelos resultados experimentais obtidos com o modelo DeepSeekMath [^1]. Como mencionado anteriormente, o modelo superou todos os modelos *open-source* e a maioria dos modelos proprietários no *benchmark* MATH [^1]. Esses resultados validam a importância de um *dataset* SFT diversificado e abrangente para aprimorar as capacidades de raciocínio matemático dos LLMs [^1, ^6].

### Conclusão
A coleção de dados em inglês para o ajuste fino do DeepSeekMath [^1] abrange uma ampla gama de campos da matemática, incluindo álgebra, probabilidade, teoria dos números, cálculo e geometria. A integração desses campos com os formatos CoT e *tool-integrated reasoning* permite que os modelos aprendam a resolver problemas matemáticos de forma eficaz e abrangente. Os resultados experimentais demonstram a eficácia do *dataset* SFT no aprimoramento das capacidades de raciocínio matemático do DeepSeekMath [^1], tornando-o um recurso valioso para pesquisa e desenvolvimento futuros [^1, ^6].

### Referências
[^1]: Zhihong Shao et al. "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models." ArXiv abs/2402.03300 (2024)
[^6]: Wang, Z., Xia, R., & Liu, P. (2023). Generative AI for math: Part I - mathpile: A billion-token-scale pretraining corpus for math. *CoRR, abs/2312.17120*.
[^7]: Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. *NeurIPS*.
[^8]: Chen, W., Ma, X., Wang, X., & Cohen, W. W. (2022). Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. *CoRR, abs/2211.12588*.
## Título Conciso
### Escalonamento da Pré-Formação Matemática e Otimização da Política Relativa ao Grupo
### Introdução
Este capítulo explora o processo de escalonamento da pré-formação matemática e a introdução da otimização da política relativa ao grupo (GRPO) dentro do contexto de modelos de linguagem abertos, especificamente o DeepSeekMath [^1]. A necessidade de modelos de linguagem matemáticos robustos e acessíveis é crucial devido às limitações dos modelos fechados, como GPT-4 e Gemini-Ultra [^2].

### Conceitos Fundamentais
#### Pré-Formação Matemática em Escala
A pré-formação em escala é vital para o desenvolvimento de modelos de linguagem capazes de lidar com raciocínio matemático complexo. DeepSeekMath emprega um corpus de pré-treinamento de alta qualidade e em larga escala, o DeepSeekMath Corpus, contendo 120 bilhões de tokens matemáticos [^2]. A construção deste corpus envolve uma *pipeline* iterativa para coletar dados matemáticos do Common Crawl, começando com um *corpus seed* inicial (OpenWebMath) e expandindo-o sistematicamente [^2, 4].

**Pipeline de Coleta de Dados:**
1.  **Treinamento de um modelo FastText:** Utilização do *corpus seed* inicial (OpenWebMath) para treinar um classificador FastText [^2, 4].
2.  **Recall de páginas web relacionadas à matemática:** Emprego do modelo FastText para buscar páginas web relevantes do Common Crawl [^5].
3.  **Anotação e Refinamento:** Anotação manual e refinamento dos dados coletados para garantir alta qualidade [^5].
4.  **Iteração:** Repetição dos passos anteriores para expandir o *corpus* e melhorar a precisão do modelo FastText [^5].

A qualidade do DeepSeekMath Corpus é validada através de experimentos de pré-treinamento, comparando-o com outros *corpora* matemáticos como MathPile e OpenWebMath [^6]. Os resultados demonstram que o DeepSeekMath Corpus oferece alta qualidade, cobertura multilíngue e é o maior em tamanho [^6]. O modelo base DeepSeekMath-Base 7B alcança um desempenho comparável ao Minerva 540B, um modelo *closed-source* [^6].

**Lema 1:** *Um modelo menor pré-treinado em dados de alta qualidade pode atingir um desempenho competitivo em relação a modelos maiores treinados em dados de qualidade inferior.* [^6]

**Corolário 1:** *A qualidade dos dados de treinamento é um fator crítico no desenvolvimento de modelos de linguagem matemática eficazes.*

#### Otimização da Política Relativa ao Grupo (GRPO)
Para melhorar ainda mais as capacidades de raciocínio matemático, o artigo introduz o Group Relative Policy Optimization (GRPO), uma variação do Proximal Policy Optimization (PPO) [^2]. GRPO é projetado para otimizar modelos de linguagem, maximizando um objetivo substituto:

$$I_{GRPO}(\theta) = E[q \sim P(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)] \frac{1}{G} \sum_{i=1}^G \left[ \frac{1}{M} \sum_{t=1}^M \min\left( \frac{\pi_{\theta} (o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})} A_{i,t}, clip\left( \frac{\pi_{\theta} (o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\epsilon, 1+\epsilon \right) A_{i,t} \right) - \beta D_{KL} [\pi_{\theta}||\pi_{ref}] \right]$$ [^13]

O GRPO difere do PPO tradicional, pois dispensa a necessidade de um modelo crítico, estimando, em vez disso, a *baseline* a partir de pontuações de grupo [^3]. Isso reduz significativamente os recursos de treinamento. No GRPO, para cada questão $q$, um grupo de saídas $\{o_1, o_2, \dots, o_G\}$ é amostrado da antiga política $\pi_{\theta_{old}}$ [^13]. A política é então otimizada maximizando o objetivo acima. A vantagem $A_{i,t}$ é calculada com base nas recompensas relativas das saídas dentro de cada grupo.

**Lema 2:** *GRPO pode melhorar o desempenho de modelos ajustados por instrução usando apenas os dados de ajuste de instrução.* [^3]

**Corolário 2:** *A utilização de recompensas relativas dentro de um grupo alinha-se bem com a natureza comparativa dos modelos de recompensa.*

GRPO também regulariza adicionando diretamente a divergência KL entre a política treinada e a política de referência, evitando complicar o cálculo de $A_{i,t}$ [^13].

### Conclusão
Este capítulo detalhou as estratégias empregadas para o desenvolvimento de modelos de linguagem matemática robustos no DeepSeekMath. Escalando a pré-formação matemática com o DeepSeekMath Corpus e otimizando o processo de aprendizado por reforço com GRPO, o trabalho apresenta avanços significativos no desempenho de modelos de linguagem abertos em *benchmarks* matemáticos complexos. Em continuidade ao trabalho anterior, DeepSeekMath-Instruct 7B atinge uma acurácia de 46.8% no *benchmark* MATH [^2]. Além disso, DeepSeekMath-RL atinge uma acurácia de 51.7% no MATH, superando todos os modelos *open-source* de 7B a 70B [^2].

### Referências
[^1]: Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., ... & Guo, D. (2024). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.
[^2]: Anil, R., Borgeaud, S., Wu, Y., Alayrac, J., Yu, J., Soricut, R., ... & Johnson, M. (2023). Gemini: A family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*.
[^3]: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*.
[^4]: Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., & Mikolov, T. (2016). Fasttext. zip: Compressing text classification models. *arXiv preprint arXiv:1612.03651*.
[^5]: Paster, K., Santos, M. D., Azerbayev, Z., & Ba, J. (2023). Openwebmath: An open dataset of high-quality mathematical web text. *CoRR, abs/2310.06786*.
[^6]: Wang, Z., Xia, R., & Liu, P. (2023). Generative AI for math: Part I - mathpile: A billion-token-scale pretraining corpus for math. *CoRR, abs/2312.17120*.
[^7]: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2020). Measuring massive multitask language understanding. *arXiv preprint arXiv:2009.03300*.
[^8]: Chen, W., Ma, X., Wang, X., & Cohen, W. W. (2022). Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. *CoRR, abs/2211.12588*.
[^13]: Schulman, J., Levine, S., Moritz, P., Jordan, M., & Abbeel, P. (2015). High-dimensional continuous control using generalized advantage estimation. *arXiv preprint arXiv:1506.02438*.
<!-- END -->