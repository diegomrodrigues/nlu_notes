## DeepSeekMath: Conjunto de Dados de Ajuste Fino Supervisionado (SFT) para Instruções Matemáticas

### Introdução
Expandindo a discussão sobre o conjunto de dados DeepSeekMath [^1] e, em particular, sobre a criação de um conjunto de dados de problemas matemáticos chineses K-12 [^1], este capítulo aprofunda-se no *dataset* de ajuste fino supervisionado (SFT) usado para instruir o modelo DeepSeekMath [^1]. A criação de um conjunto de dados SFT de alta qualidade é crucial para ajustar os *Large Language Models* (LLMs), aprimorando sua capacidade de entender e resolver problemas matemáticos de forma eficaz. Em continuidade ao tópico anterior, que detalhou o processo de criação do *corpus* para pre-treinamento [^5], agora exploraremos o conjunto de dados utilizado especificamente para o ajuste fino das instruções.

### O Conjunto de Dados de Ajuste Fino Supervisionado (SFT)
O *dataset* de ajuste fino de instruções matemáticas aborda problemas em inglês e chinês de diferentes campos da matemática e níveis de complexidade variados [^1]. Cada problema é emparelhado com soluções em diversos formatos para enriquecer o processo de aprendizagem do modelo [^1, ^7]. Os formatos incluem [^1, ^8]:

1.  **Chain-of-Thought (CoT)**: Decomposição do problema em etapas lógicas intermediárias, permitindo que o modelo siga o processo de raciocínio passo a passo [^1, ^7].
2.  **Program-of-Thought (PoT)**: Utilização de programas de computador para resolver o problema, integrando código ao processo de raciocínio [^1, ^8].
3.  **Raciocínio Integrado a Ferramentas**: Combinação de raciocínio textual com o uso de ferramentas externas, como calculadoras ou *solvers* simbólicos [^1, ^8].
    *   Conforme mencionado anteriormente, o *dataset* chinês K-12 também emprega esses dois formatos de anotação [^1].

A diversidade nos formatos de solução permite que o modelo DeepSeekMath aprenda diferentes estratégias de resolução de problemas e melhore sua capacidade de lidar com uma ampla gama de desafios matemáticos [^1].

### Composição do Conjunto de Dados SFT
O *dataset* SFT é composto por um total de 776 mil exemplos de treinamento [^1]. Este conjunto diversificado é crucial para garantir que o modelo aprenda a generalizar bem e aplicar seu conhecimento a novos problemas [^1, ^6]. A composição específica do *dataset* inclui:

1.  ***Datasets* Matemáticos em Inglês**:
    *   Anotações de problemas GSM8K e MATH com soluções integradas a ferramentas.
    *   Subconjunto de MathInstruct [^1]
    *   Conjunto de treinamento Lila-OOD (Problems are solved with CoT or PoT) [^1]
    *   A coleção em inglês abrange diversos campos da matemática, como álgebra, probabilidade, teoria dos números, cálculo e geometria [^1].

2.  ***Datasets* Matemáticos em Chinês**:
    *   Problemas matemáticos chineses K-12 que abrangem 76 subtópicos, como equações lineares, com soluções anotadas em formatos CoT e de raciocínio integrado a ferramentas [^1]. Como vimos anteriormente, esse *dataset* é crucial para melhorar o desempenho em *benchmarks* chineses [^6].

### Processo de Treinamento e Avaliação
O processo de treinamento envolve o ajuste fino do modelo DeepSeekMath-Base em *datasets* matemáticos em inglês e chinês, concatenando exemplos de treinamento aleatoriamente até atingir um comprimento máximo de contexto de 4K *tokens* [^1]. O modelo é treinado por 500 passos com um tamanho de *batch* de 256 e uma taxa de aprendizado constante de 5e-5 [^1]. A avaliação do desempenho matemático dos modelos é realizada com e sem o uso de ferramentas, em quatro *benchmarks* de raciocínio quantitativo em inglês e chinês [^1].

### Comparação com Modelos Líderes
Os modelos são comparados com os principais modelos da época, incluindo modelos *closed-source* como GPT-4 e Gemini, e modelos *open-source* como DeepSeek-LLM-Chat, Qwen e SeaLLM [^1]. Os resultados demonstram que o DeepSeekMath-Instruct 7B supera todos os modelos *open-source* e a maioria dos modelos proprietários no *benchmark* MATH, demonstrando a eficácia do *dataset* SFT e do processo de treinamento [^1].

### Conclusão

A criação de um conjunto de dados de ajuste fino supervisionado abrangente e diversificado é essencial para aprimorar as capacidades de raciocínio matemático dos LLMs. O *dataset* SFT utilizado para treinar o DeepSeekMath [^1] abrange problemas em inglês e chinês, e soluções em diversos formatos, permitindo que o modelo aprenda diferentes estratégias de resolução de problemas. Os resultados experimentais demonstram a eficácia do *dataset* SFT no aprimoramento das capacidades de raciocínio matemático do DeepSeekMath [^1], tornando-o um recurso valioso para pesquisa e desenvolvimento futuros [^1, ^6].

### Referências

[^1]: Zhihong Shao et al. "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models." ArXiv abs/2402.03300 (2024)
[^2]: Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., & Tang, J. (2022). GLM: General language model pretraining with autoregressive blank infilling. *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, 320–335.
[^3]: Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y. K., Luo, F., Xiong, Y., & Liang, W. (2024). Deepseek-coder: When the large language model meets programming – the rise of code intelligence.
[^4]: Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2020). Measuring massive multitask language understanding. *arXiv preprint arXiv:2009.03300*.
[^5]: Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., & Mikolov, T. (2016). Fasttext. zip: Compressing text classification models. *arXiv preprint arXiv:1612.03651*.
[^6]: Wang, Z., Xia, R., & Liu, P. (2023). Generative AI for math: Part I - mathpile: A billion-token-scale pretraining corpus for math. *CoRR, abs/2312.17120*.
[^7]: Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. *NeurIPS*.
[^8]: Chen, W., Ma, X., Wang, X., & Cohen, W. W. (2022). Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. *CoRR, abs/2211.12588*.
[^9]: Paster, K. D., Santos, M. D., Azerbayev, Z., & Ba, J. (2023). Openwebmath: An open dataset of high-quality mathematical web text. *CoRR, abs/2310.06786*.
<!-- END -->